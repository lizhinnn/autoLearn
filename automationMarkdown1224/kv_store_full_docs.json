{
  "doc-245f069dcd67844b9a6574b547e8e642": {
    "content": "#  \n\n# Modern Control Engineering  \n\nFifth Edition  \n\nKatsuhiko Ogata  \n\n# Prentice Hall  \n\nVP/Editorial Director, Engineering/Computer Science: Marcia J. Horton   \nAssistant/Supervisor: Dolores Mars   \nSenior Editor:Andrew Gilfillan   \nAssociate Editor:Alice Dworkin   \nEditorial Assistant:William Opaluch   \nDirector of Marketing: Margaret Waples   \nSenior Marketing Manager:Tim Galligan   \nMarketing Assistant: Mack Patterson   \nSenior Managing Editor: Scott Disanno   \nArt Editor: Greg Dulles   \nSenior Operations Supervisor:Alan Fischer   \nOperations Specialist: Lisa McDowell   \nArt Director: Kenny Beck   \nCover Designer: Carole Anson   \nMedia Editor: Daniel Sandin  \n\nCredits and acknowledgments borrowed from other sources and reproduced, with permission, in this textbook appear on appropriate page within text.  \n\nMATLAB is a registered trademark of The Mathworks, Inc., 3 Apple Hill Drive, Natick MA 01760-2098.  \n\nCopyright $\\copyright$ 2010, 2002, 1997, 1990, 1970 Pearson Education, Inc., publishing as Prentice Hall, One Lake Street, Upper Saddle River, New Jersey 07458.All rights reserved. Manufactured in the United States of America.This publication is protected by Copyright, and permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise.To obtain permission(s) to use material from this work, please submit a written request to Pearson Education, Inc., Permissions Department, One Lake Street, Upper Saddle River, New Jersey 07458.  \n\nMany of the designations by manufacturers and seller to distinguish their products are claimed as trademarks.Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.  \n\nLibrary of Congress Cataloging-in-Publication Data on File  \n\nISBN 10: 0-13-615673-8   \nISBN 13: 978-0-13-615673-4  \n\nContents  \n\nPreface  \n\n# Chapter 1 Introduction to Control Systems  \n\n1–1 Introduction 1   \n1–2 Examples of Control Systems 4   \n1–3 Closed-Loop Control Versus Open-Loop Control 7   \n1–4 Design and Compensation of Control Systems 9   \n1–5 Outline of the Book 10  \n\n# Chapter 2 Mathematical Modeling of Control Systems  \n\n13  \n\n2–1 Introduction 13   \n2–2 Transfer Function and Impulse-Response Function 15   \n2–3 Automatic Control Systems 17   \n2–4 Modeling in State Space 29   \n2–5 State-Space Representation of Scalar Differential   \nEquation Systems 35   \n2–6 Transformation of Mathematical Models with MATLAB 39  \n\n2–7 Linearization of Nonlinear Mathematical Models 43 Example Problems and Solutions 46 Problems 60  \n\n# Chapter 3 Mathematical Modeling of Mechanical Systems and Electrical Systems  \n\n3–1 Introduction 63   \n3–2 Mathematical Modeling of Mechanical Systems 63   \n3–3 Mathematical Modeling of Electrical Systems 72 Example Problems and Solutions 86 Problems 97  \n\n# Chapter 4 Mathematical Modeling of Fluid Systems and Thermal Systems  \n\n4–1 Introduction 100   \n4–2 Liquid-Level Systems 101   \n4–3 Pneumatic Systems 106   \n4–4 Hydraulic Systems 123   \n4–5 Thermal Systems 136 Example Problems and Solutions 140 Problems 152  \n\n# Chapter 5 Transient and Steady-State Response Analyses  \n\n5–1 Introduction 159   \n5–2 First-Order Systems 161   \n5–3 Second-Order Systems 164   \n5–4 Higher-Order Systems 179   \n5–5 Transient-Response Analysis with MATLAB 183   \n5–6 Routh’s Stability Criterion 212   \n5–7 Effects of Integral and Derivative Control Actions on System Performance 218   \n5–8 Steady-State Errors in Unity-Feedback Control Systems 225 Example Problems and Solutions 231 Problems 263  \n\n6–1 Introduction 269   \n6–2 Root-Locus Plots 270   \n6–3 Plotting Root Loci with MATLAB 290   \n6–4 Root-Locus Plots of Positive Feedback Systems 303   \n6–5 Root-Locus Approach to Control-Systems Design 308   \n6–6 Lead Compensation 311   \n6–7 Lag Compensation 321   \n6–8 Lag–Lead Compensation 330   \n6–9 Parallel Compensation 342   \nExample Problems and Solutions 347   \nProblems 394  \n\nChapter 7 Control Systems Analysis and Design by the Frequency-Response Method  \n\n7–1 Introduction 398   \n7–2 Bode Diagrams 403   \n7–3 Polar Plots 427   \n7–4 Log-Magnitude-versus-Phase Plots 443   \n7–5 Nyquist Stability Criterion 445   \n7–6 Stability Analysis 454   \n7–7 Relative Stability Analysis 462   \n7–8 Closed-Loop Frequency Response of Unity-Feedback   \nSystems 477   \n7–9 Experimental Determination of Transfer Functions 486   \n7–10 Control Systems Design by Frequency-Response Approach 491   \n7–11 Lead Compensation 493   \n7–12 Lag Compensation 502   \n7–13 Lag–Lead Compensation 511   \nExample Problems and Solutions 521   \nProblems 561  \n\n# Chapter 8 PID Controllers and Modified PID Controllers  \n\n8–1 Introduction 567   \n8–2 Ziegler–Nichols Rules for Tuning PID Controllers 568  \n\n8–3 Design of PID Controllers with Frequency-Response Approach 577   \n8–4 Design of PID Controllers with Computational Optimization Approach 583   \n8–5 Modifications of PID Control Schemes 590   \n8–6 Two-Degrees-of-Freedom Control 592   \n8–7 Zero-Placement Approach to Improve Response Characteristics 595 Example Problems and Solutions 614 Problems 641  \n\n# Chapter 9 Control Systems Analysis in State Space  \n\n9–1 Introduction 648   \n9–2 State-Space Representations of Transfer-Function Systems 649   \n9–3 Transformation of System Models with MATLAB 656   \n9–4 Solving the Time-Invariant State Equation 660   \n9–5 Some Useful Results in Vector-Matrix Analysis 668   \n9–6 Controllability 675   \n9–7 Observability 682 Example Problems and Solutions 688 Problems 720  \n\n# Chapter 10 Control Systems Design in State Space  \n\n10–1 Introduction 722   \n10–2 Pole Placement 723   \n10–3 Solving Pole-Placement Problems with MATLAB 735   \n10–4 Design of Servo Systems 739   \n10–5 State Observers 751   \n10–6 Design of Regulator Systems with Observers 778   \n10–7 Design of Control Systems with Observers 786   \n10–8 Quadratic Optimal Regulator Systems 793   \n10–9 Robust Control Systems 806   \nExample Problems and Solutions 817   \nProblems 855   \nAppendix A Laplace Transform Tables 859   \nAppendix B Partial-Fraction Expansion 867   \nAppendix C Vector-Matrix Algebra 874   \nReferences 882   \nIndex 886  \n\n# This page intentionally left blank  \n\nThis book introduces important concepts in the analysis and design of control systems. Readers will find it to be a clear and understandable textbook for control system courses at colleges and universities. It is written for senior electrical, mechanical, aerospace, or chemical engineering students. The reader is expected to have fulfilled the following prerequisites: introductory courses on differential equations, Laplace transforms, vectormatrix analysis, circuit analysis, mechanics, and introductory thermodynamics.  \n\nThe main revisions made in this edition are as follows:  \n\n•The use of MATLAB for obtaining responses of control systems to various inputs has been increased.   \n•The usefulness of the computational optimization approach with MATLAB has been demonstrated.   \n•New example problems have been added throughout the book.   \n•Materials in the previous edition that are of secondary importance have been deleted in order to provide space for more important subjects. Signal flow graphs were dropped from the book. A chapter on Laplace transform was deleted. Instead, Laplace transform tables, and partial-fraction expansion with MATLAB are presented in Appendix A and Appendix B, respectively.   \n•A short summary of vector-matrix analysis is presented in Appendix C; this will help the reader to find the inverses of n x n matrices that may be involved in the analysis and design of control systems.  \n\nThis edition of Modern Control Engineering is organized into ten chapters.The outline of this book is as follows: Chapter 1 presents an introduction to control systems. Chapter 2 deals with mathematical modeling of control systems.A linearization technique for nonlinear mathematical models is presented in this chapter. Chapter 3 derives mathematical models of mechanical systems and electrical systems. Chapter 4 discusses mathematical  \n\nmodeling of fluid systems (such as liquid-level systems, pneumatic systems, and hydraulic   \nsystems) and thermal systems. Chapter 5 treats transient response and steady-state analyses of control systems.   \nMATLAB is used extensively for obtaining transient response curves. Routh’s stability   \ncriterion is presented for stability analysis of control systems. Hurwitz stability criterion   \nis also presented. Chapter 6 discusses the root-locus analysis and design of control systems, including   \npositive feedback systems and conditionally stable systems Plotting root loci with MAT  \nLAB is discussed in detail. Design of lead, lag, and lag-lead compensators with the root  \nlocus method is included. Chapter 7 treats the frequency-response analysis and design of control systems.The   \nNyquist stability criterion is presented in an easily understandable manner.The Bode di  \nagram approach to the design of lead, lag, and lag-lead compensators is discussed. Chapter 8 deals with basic and modified PID controllers. Computational approaches   \nfor obtaining optimal parameter values for PID controllers are discussed in detail, par  \nticularly with respect to satisfying requirements for step-response characteristics. Chapter 9 treats basic analyses of control systems in state space. Concepts of con  \ntrollability and observability are discussed in detail. Chapter 10 deals with control systems design in state space.The discussions include   \npole placement, state observers, and quadratic optimal control. An introductory dis  \ncussion of robust control systems is presented at the end of Chapter 10. The book has been arranged toward facilitating the student’s gradual understanding   \nof control theory. Highly mathematical arguments are carefully avoided in the presen  \ntation of the materials. Statement proofs are provided whenever they contribute to the   \nunderstanding of the subject matter presented. Special effort has been made to provide example problems at strategic points so that   \nthe reader will have a clear understanding of the subject matter discussed. In addition,   \na number of solved problems (A-problems) are provided at the end of each chapter,   \nexcept Chapter 1.The reader is encouraged to study all such solved problems carefully;   \nthis will allow the reader to obtain a deeper understanding of the topics discussed. In   \naddition, many problems (without solutions) are provided at the end of each chapter,   \nexcept Chapter 1. The unsolved problems (B-problems) may be used as homework or   \nquiz problems. If this book is used as a text for a semester course (with 56 or so lecture hours),a good   \nportion of the material may be covered by skipping certain subjects. Because of the   \nabundance of example problems and solved problems (A-problems) that might answer   \nmany possible questions that the reader might have, this book can also serve as a self  \nstudy book for practicing engineers who wish to study basic control theories. I would like to thank the following reviewers for this edition of the book:Mark Camp  \nbell, Cornell University; Henry Sodano, Arizona State University; and Atul G. Kelkar,   \nIowa State University.Finally,I wish to offer my deep appreciation to Ms.Alice Dworkin,   \nAssociate Editor, Mr. Scott Disanno, Senior Managing Editor, and all the people in  \nvolved in this publishing project, for the speedy yet superb production of this book.  \n\nKatsuhiko Ogata  \n\n# Introduction to Control Systems  \n\n# 1–1 INTRODUCTION  \n\nControl theories commonly used today are classical control theory (also called conventional control theory), modern control theory, and robust control theory.This book presents comprehensive treatments of the analysis and design of control systems based on the classical control theory and modern control theory.A brief introduction of robust control theory is included in Chapter 10.  \n\nAutomatic control is essential in any field of engineering and science. Automatic control is an important and integral part of space-vehicle systems, robotic systems, modern manufacturing systems, and any industrial operations involving control of temperature, pressure, humidity, flow, etc. It is desirable that most engineers and scientists are familiar with theory and practice of automatic control.  \n\nThis book is intended to be a text book on control systems at the senior level at a college or university.All necessary background materials are included in the book. Mathematical background materials related to Laplace transforms and vector-matrix analysis are presented separately in appendixes.  \n\nBrief Review of Historical Developments of Control Theories and Practices. The first significant work in automatic control was James Watt’s centrifugal governor for the speed control of a steam engine in the eighteenth century. Other significant works in the early stages of development of control theory were due to Minorsky, Hazen, and Nyquist, among many others. In 1922, Minorsky worked on automatic controllers for steering ships and showed how stability could be determined from the differential equations describing the system. In 1932, Nyquist developed a relatively simple procedure for determining the stability of closed-loop systems on the basis of open-loop response to steady-state sinusoidal inputs. In 1934, Hazen, who introduced the term servomechanisms for position control systems, discussed the design of relay servomechanisms capable of closely following a changing input.  \n\nDuring the decade of the 1940s, frequency-response methods (especially the Bode diagram methods due to Bode) made it possible for engineers to design linear closedloop control systems that satisfied performance requirements. Many industrial control systems in 1940s and 1950s used PID controllers to control pressure, temperature, etc. In the early 1940s Ziegler and Nichols suggested rules for tuning PID controllers, called Ziegler–Nichols tuning rules. From the end of the 1940s to the 1950s, the root-locus method due to Evans was fully developed.  \n\nThe frequency-response and root-locus methods, which are the core of classical control theory, lead to systems that are stable and satisfy a set of more or less arbitrary performance requirements. Such systems are, in general, acceptable but not optimal in any meaningful sense. Since the late 1950s, the emphasis in control design problems has been shifted from the design of one of many systems that work to the design of one optimal system in some meaningful sense.  \n\nAs modern plants with many inputs and outputs become more and more complex, the description of a modern control system requires a large number of equations. Classical control theory, which deals only with single-input, single-output systems, becomes powerless for multiple-input, multiple-output systems. Since about 1960, because the availability of digital computers made possible time-domain analysis of complex systems, modern control theory, based on time-domain analysis and synthesis using state variables, has been developed to cope with the increased complexity of modern plants and the stringent requirements on accuracy, weight, and cost in military, space, and industrial applications.  \n\nDuring the years from 1960 to 1980, optimal control of both deterministic and stochastic systems, as well as adaptive and learning control of complex systems, were fully investigated. From 1980s to 1990s, developments in modern control theory were centered around robust control and associated topics.  \n\nModern control theory is based on time-domain analysis of differential equation systems. Modern control theory made the design of control systems simpler because the theory is based on a model of an actual control system. However, the system’s stability is sensitive to the error between the actual system and its model. This means that when the designed controller based on a model is applied to the actual system, the system may not be stable. To avoid this situation, we design the control system by first setting up the range of possible errors and then designing the controller in such a way that, if the error of the system stays within the assumed range, the designed control system will stay stable. The design method based on this principle is called robust control theory.This theory incorporates both the frequencyresponse approach and the time-domain approach.The theory is mathematically very complex.  \n\nBecause this theory requires mathematical background at the graduate level, inclusion of robust control theory in this book is limited to introductory aspects only. The reader interested in details of robust control theory should take a graduate-level control course at an established college or university.  \n\nDefinitions. Before we can discuss control systems, some basic terminologies must be defined.  \n\nControlled Variable and Control Signal or Manipulated Variable. The controlled variable is the quantity or condition that is measured and controlled.The control signal or manipulated variable is the quantity or condition that is varied by the controller so as to affect the value of the controlled variable. Normally, the controlled variable is the output of the system. Control means measuring the value of the controlled variable of the system and applying the control signal to the system to correct or limit deviation of the measured value from a desired value.  \n\nIn studying control engineering, we need to define additional terms that are necessary to describe control systems.  \n\nPlants. A plant may be a piece of equipment, perhaps just a set of machine parts functioning together, the purpose of which is to perform a particular operation. In this book, we shall call any physical object to be controlled (such as a mechanical device, a heating furnace, a chemical reactor, or a spacecraft) a plant.  \n\nProcesses. The Merriam–Webster Dictionary defines a process to be a natural, progressively continuing operation or development marked by a series of gradual changes that succeed one another in a relatively fixed way and lead toward a particular result or end; or an artificial or voluntary, progressively continuing operation that consists of a series of controlled actions or movements systematically directed toward a particular result or end. In this book we shall call any operation to be controlled a process. Examples are chemical, economic, and biological processes.  \n\nSystems. A system is a combination of components that act together and perform a certain objective. A system need not be physical. The concept of the system can be applied to abstract, dynamic phenomena such as those encountered in economics. The word system should,therefore,be interpreted to imply physical,biological,economic,and the like, systems.  \n\nDisturbances. A disturbance is a signal that tends to adversely affect the value of the output of a system. If a disturbance is generated within the system, it is called internal, while an external disturbance is generated outside the system and is an input.  \n\nFeedback Control. Feedback control refers to an operation that, in the presence of disturbances, tends to reduce the difference between the output of a system and some reference input and does so on the basis of this difference. Here only unpredictable disturbances are so specified, since predictable or known disturbances can always be compensated for within the system.  \n\nIn this section we shall present a few examples of control systems.  \n\nSpeed Control System. The basic principle of a Watt’s speed governor for an engine is illustrated in the schematic diagram of Figure 1–1. The amount of fuel admitted to the engine is adjusted according to the difference between the desired and the actual engine speeds.  \n\nThe sequence of actions may be stated as follows: The speed governor is adjusted such that, at the desired speed, no pressured oil will flow into either side of the power cylinder. If the actual speed drops below the desired value due to disturbance, then the decrease in the centrifugal force of the speed governor causes the control valve to move downward, supplying more fuel, and the speed of the engine increases until the desired value is reached. On the other hand, if the speed of the engine increases above the desired value, then the increase in the centrifugal force of the governor causes the control valve to move upward. This decreases the supply of fuel, and the speed of the engine decreases until the desired value is reached.  \n\nIn this speed control system, the plant (controlled system) is the engine and the controlled variable is the speed of the engine. The difference between the desired speed and the actual speed is the error signal.The control signal (the amount of fuel) to be applied to the plant (engine) is the actuating signal. The external input to disturb the controlled variable is the disturbance. An unexpected change in the load is a disturbance.  \n\nTemperature Control System. Figure 1–2 shows a schematic diagram of temperature control of an electric furnace.The temperature in the electric furnace is measured by a thermometer, which is an analog device.The analog temperature is converted to a digital temperature by an $\\mathbf{A}/\\mathbf{D}$ converter. The digital temperature is fed to a controller through an interface.This digital temperature is compared with the programmed input temperature, and if there is any discrepancy (error), the controller sends out a signal to the heater, through an interface, amplifier, and relay, to bring the furnace temperature to a desired value.  \n\n![](images/d3e8e9884c4077b01010646aa5b82afd4aba96de3efd16b8b7933711a04ba8d2.jpg)  \nFigure 1–1 Speed control system.  \n\n![](images/37592f4f43c68fb72f70478e40fabc0e1093f8106a53cd7216f3ab40aa9c4c19.jpg)  \nFigure 1–2 Temperature control system.  \n\nBusiness Systems. A business system may consist of many groups. Each task assigned to a group will represent a dynamic element of the system. Feedback methods of reporting the accomplishments of each group must be established in such a system for proper operation.The cross-coupling between functional groups must be made a minimum in order to reduce undesirable delay times in the system. The smaller this crosscoupling, the smoother the flow of work signals and materials will be.  \n\nA business system is a closed-loop system.A good design will reduce the managerial control required.Note that disturbances in this system are the lack of personnel or materials, interruption of communication, human errors, and the like.  \n\nThe establishment of a well-founded estimating system based on statistics is mandatory to proper management.It is a well-known fact that the performance of such a system can be improved by the use of lead time, or anticipation.  \n\nTo apply control theory to improve the performance of such a system, we must represent the dynamic characteristic of the component groups of the system by a relatively simple set of equations.  \n\nAlthough it is certainly a difficult problem to derive mathematical representations of the component groups, the application of optimization techniques to business systems significantly improves the performance of the business system.  \n\nConsider, as an example, an engineering organizational system that is composed of major groups such as management, research and development, preliminary design, experiments, product design and drafting, fabrication and assembling, and tesing. These groups are interconnected to make up the whole operation.  \n\nSuch a system may be analyzed by reducing it to the most elementary set of components necessary that can provide the analytical detail required and by representing the dynamic characteristics of each component by a set of simple equations. (The dynamic performance of such a system may be determined from the relation between progressive accomplishment and time.)  \n\n![](images/2e3e26403c852131842b246a4ea1d294c464872ac327ab533761e652115d81c8.jpg)  \nFigure 1–3 Block diagram of an engineering organizational system.  \n\nA functional block diagram may be drawn by using blocks to represent the functional activities and interconnecting signal lines to represent the information or product output of the system operation. Figure 1–3 is a possible block diagram for this system.  \n\nRobust Control System. The first step in the design of a control system is to obtain a mathematical model of the plant or control object. In reality, any model of a plant we want to control will include an error in the modeling process.That is, the actual plant differs from the model to be used in the design of the control system.  \n\nTo ensure the controller designed based on a model will work satisfactorily when this controller is used with the actual plant, one reasonable approach is to assume from the start that there is an uncertainty or error between the actual plant and its mathematical model and include such uncertainty or error in the design process of the control system. The control system designed based on this approach is called a robust control system.  \n\nSuppose that the actual plant we want to control is $\\widetilde G(s)$ and the mathematical model of the actual plant is $G(s)$ , that is,  \n\n$\\begin{array}{l}{{\\widetilde{G}(s)\\,=\\,\\mathrm{actual~plant~model~that~has~uncertainty}\\;\\Delta(s)}}\\\\ {{\\,}}\\\\ {{G(s)\\,=\\,\\mathrm{nominal~plant~model~to~be~used~for~designing~the~cont}}}\\end{array}$ $\\widetilde G(s)$ and $G(s)$ may be related by a multiplicative factor such as  \n\n$$\n\\widetilde{G}(s)\\,=\\,G(s)[1+\\Delta(s)]\n$$  \n\nor an additive factor  \n\n$$\n\\widetilde{G}(s)\\,=\\,G(s)\\,+\\,\\Delta(s)\n$$  \n\nor in other forms.  \n\nSince the exact description of the uncertainty or error $\\Delta(s)$ is unknown, we use an estimate of $\\Delta(s)$ and use this estimate, $W(s)$ , in the design of the controller. $W(s)$ is a scalar transfer function such that  \n\n$$\n\\|\\Delta(s)\\|_{\\infty}<\\|W(s)\\|_{\\infty}=\\operatorname*{max}_{0\\leq\\omega\\leq\\infty}|W(j\\omega)|\n$$  \n\nwhere $\\|W(s)\\|_{\\infty}$ is the maximum value of $|W(j\\omega)|$ for $0\\leq\\omega\\leq\\infty$ and is called the H infinity norm of $W(s)$ .  \n\nUsing the small gain theorem, the design procedure here boils down to the determination of the controller $K(s)$ such that the inequality  \n\n$$\n\\left\\|\\frac{W(s)}{1+K(s)G(s)}\\right\\|_{\\infty}<1\n$$  \n\nis satisfied, where $G(s)$ is the transfer function of the model used in the design process, $K(s)$ is the transfer function of the controller, and $W(s)$ is the chosen transfer function to approximate $\\Delta(s)$ . In most practical cases, we must satisfy more than one such inequality that involves $G(s),K(s)$ , and $W(s)$ ’s. For example, to guarantee robust stability and robust performance we may require two inequalities, such as  \n\n$$\n\\begin{array}{r l}&{\\left\\|\\frac{W_{m}(s)K(s)G(s)}{1\\,+\\,\\,K(s)G(s)}\\right\\|_{\\infty}<1\\quad\\mathrm{~for~robust~stability}}\\\\ &{\\left\\|\\frac{W_{s}(s)}{1\\,+\\,\\,K(s)G(s)}\\right\\|_{\\infty}<1\\quad\\mathrm{~for~robust~performance}}\\end{array}\n$$  \n\nbe satisfied. (These inequalities are derived in Section 10–9.) There are many different such inequalities that need to be satisfied in many different robust control systems. (Robust stability means that the controller $K(s)$ guarantees internal stability of all systems that belong to a group of systems that include the system with the actual plant. Robust performance means the specified performance is satisfied in all systems that belong to the group.) In this book all the plants of control systems we discuss are assumed to be known precisely, except the plants we discuss in Section 10–9 where an introductory aspect of robust control theory is presented.  \n\n# 1–3 CLOSED-LOOP CONTROL VERSUS OPEN-LOOP CONTROL  \n\nFeedback Control Systems. A system that maintains a prescribed relationship between the output and the reference input by comparing them and using the difference as a means of control is called a feedback control system. An example would be a roomtemperature control system. By measuring the actual room temperature and comparing it with the reference temperature (desired temperature), the thermostat turns the heating or cooling equipment on or off in such a way as to ensure that the room temperature remains at a comfortable level regardless of outside conditions.  \n\nFeedback control systems are not limited to engineering but can be found in various nonengineering fields as well.The human body, for instance, is a highly advanced feedback control system. Both body temperature and blood pressure are kept constant by means of physiological feedback. In fact, feedback performs a vital function: It makes the human body relatively insensitive to external disturbances, thus enabling it to function properly in a changing environment.  \n\nClosed-Loop Control Systems. Feedback control systems are often referred to as closed-loop control systems. In practice, the terms feedback control and closed-loop control are used interchangeably. In a closed-loop control system the actuating error signal, which is the difference between the input signal and the feedback signal (which may be the output signal itself or a function of the output signal and its derivatives and/or integrals), is fed to the controller so as to reduce the error and bring the output of the system to a desired value.The term closed-loop control always implies the use of feedback control action in order to reduce system error.  \n\nOpen-Loop Control Systems. Those systems in which the output has no effect on the control action are called open-loop control systems. In other words, in an openloop control system the output is neither measured nor fed back for comparison with the input. One practical example is a washing machine. Soaking, washing, and rinsing in the washer operate on a time basis. The machine does not measure the output signal, that is, the cleanliness of the clothes.  \n\nIn any open-loop control system the output is not compared with the reference input. Thus, to each reference input there corresponds a fixed operating condition; as a result, the accuracy of the system depends on calibration. In the presence of disturbances, an open-loop control system will not perform the desired task. Open-loop control can be used, in practice, only if the relationship between the input and output is known and if there are neither internal nor external disturbances. Clearly, such systems are not feedback control systems. Note that any control system that operates on a time basis is open loop. For instance, traffic control by means of signals operated on a time basis is another example of open-loop control.  \n\nClosed-Loop versus Open-Loop Control Systems. An advantage of the closedloop control system is the fact that the use of feedback makes the system response relatively insensitive to external disturbances and internal variations in system parameters. It is thus possible to use relatively inaccurate and inexpensive components to obtain the accurate control of a given plant, whereas doing so is impossible in the open-loop case.  \n\nFrom the point of view of stability, the open-loop control system is easier to build because system stability is not a major problem. On the other hand, stability is a major problem in the closed-loop control system, which may tend to overcorrect errors and thereby can cause oscillations of constant or changing amplitude.  \n\nIt should be emphasized that for systems in which the inputs are known ahead of time and in which there are no disturbances it is advisable to use open-loop control. Closed-loop control systems have advantages only when unpredictable disturbances and/or unpredictable variations in system components are present. Note that the output power rating partially determines the cost, weight, and size of a control system. The number of components used in a closed-loop control system is more than that for a corresponding open-loop control system. Thus, the closed-loop control system is generally higher in cost and power.To decrease the required power of a system, openloop control may be used where applicable. A proper combination of open-loop and closed-loop controls is usually less expensive and will give satisfactory overall system performance.  \n\nMost analyses and designs of control systems presented in this book are concerned with closed-loop control systems. Under certain circumstances (such as where no disturbances exist or the output is hard to measure) open-loop control systems may be desired.Therefore, it is worthwhile to summarize the advantages and disadvantages of using open-loop control systems.  \n\nThe major advantages of open-loop control systems are as follows:   \n1. Simple construction and ease of maintenance.   \n2. Less expensive than a corresponding closed-loop system.   \n3. There is no stability problem.   \n4. Convenient when output is hard to measure or measuring the output precisely is economically not feasible.(For example,in the washer system,it would be quite expensive to provide a device to measure the quality of the washer’s output, cleanliness of the clothes.)   \nThe major disadvantages of open-loop control systems are as follows:   \n1. Disturbances and changes in calibration cause errors, and the output may be different from what is desired.   \n2. To maintain the required quality in the output, recalibration is necessary from time to time.  \n\n# 1–4 DESIGN AND COMPENSATION OF CONTROL SYSTEMS  \n\nThis book discusses basic aspects of the design and compensation of control systems. Compensation is the modification of the system dynamics to satisfy the given specifications.The approaches to control system design and compensation used in this book are the root-locus approach, frequency-response approach, and the state-space approach. Such control systems design and compensation will be presented in Chapters 6, 7, 9 and 10. The PID-based compensational approach to control systems design is given in Chapter 8.  \n\nIn the actual design of a control system, whether to use an electronic, pneumatic, or hydraulic compensator is a matter that must be decided partially based on the nature of the controlled plant. For example, if the controlled plant involves flammable fluid, then we have to choose pneumatic components (both a compensator and an actuator) to avoid the possibility of sparks. If, however, no fire hazard exists, then electronic compensators are most commonly used.(In fact,we often transform nonelectrical signals into electrical signals because of the simplicity of transmission, increased accuracy, increased reliability, ease of compensation, and the like.)  \n\nPerformance Specifications. Control systems are designed to perform specific tasks. The requirements imposed on the control system are usually spelled out as performance specifications.The specifications may be given in terms of transient response requirements (such as the maximum overshoot and settling time in step response) and of steady-state requirements (such as steady-state error in following ramp input) or may be given in frequency-response terms. The specifications of a control system must be given before the design process begins.  \n\nFor routine design problems, the performance specifications (which relate to accuracy, relative stability, and speed of response) may be given in terms of precise numerical values.In other cases they may be given partially in terms of precise numerical values and partially in terms of qualitative statements. In the latter case the specifications may have to be modified during the course of design, since the given specifications may never be satisfied (because of conflicting requirements) or may lead to a very expensive system.  \n\nGenerally, the performance specifications should not be more stringent than necessary to perform the given task. If the accuracy at steady-state operation is of prime importance in a given control system, then we should not require unnecessarily rigid performance specifications on the transient response, since such specifications will require expensive components. Remember that the most important part of control system design is to state the performance specifications precisely so that they will yield an optimal control system for the given purpose.  \n\nSystem Compensation. Setting the gain is the first step in adjusting the system for satisfactory performance. In many practical cases, however, the adjustment of the gain alone may not provide sufficient alteration of the system behavior to meet the given specifications. As is frequently the case, increasing the gain value will improve the steady-state behavior but will result in poor stability or even instability. It is then necessary to redesign the system (by modifying the structure or by incorporating additional devices or components) to alter the overall behavior so that the system will behave as desired. Such a redesign or addition of a suitable device is called compensation. A device inserted into the system for the purpose of satisfying the specifications is called a compensator. The compensator compensates for deficient performance of the original system.  \n\nDesign Procedures. In the process of designing a control system, we set up a mathematical model of the control system and adjust the parameters of a compensator. The most time-consuming part of the work is the checking of the system performance by analysis with each adjustment of the parameters.The designer should use MATLAB or other available computer package to avoid much of the numerical drudgery necessary for this checking.  \n\nOnce a satisfactory mathematical model has been obtained, the designer must construct a prototype and test the open-loop system. If absolute stability of the closed loop is assured, the designer closes the loop and tests the performance of the resulting closedloop system. Because of the neglected loading effects among the components, nonlinearities, distributed parameters, and so on, which were not taken into consideration in the original design work, the actual performance of the prototype system will probably differ from the theoretical predictions. Thus the first design may not satisfy all the requirements on performance. The designer must adjust system parameters and make changes in the prototype until the system meets the specificications. In doing this, he or she must analyze each trial, and the results of the analysis must be incorporated into the next trial.The designer must see that the final system meets the performance apecifications and, at the same time, is reliable and economical.  \n\n# 1–5 OUTLINE OF THE BOOK  \n\nThis text is organized into 10 chapters.The outline of each chapter may be summarized   \nas follows: Chapter 1 presents an introduction to this book.  \n\nChapter 2 deals with mathematical modeling of control systems that are described by linear differential equations. Specifically, transfer function expressions of differential equation systems are derived.Also, state-space expressions of differential equation systems are derived. MATLAB is used to transform mathematical models from transfer functions to state-space equations and vice versa.This book treats linear systems in detail. If the mathematical model of any system is nonlinear, it needs to be linearized before applying theories presented in this book. A technique to linearize nonlinear mathematical models is presented in this chapter.  \n\nChapter 3 derives mathematical models of various mechanical and electrical systems that appear frequently in control systems.  \n\nChapter 4 discusses various fluid systems and thermal systems, that appear in control systems.Fluid systems here include liquid-level systems,pneumatic systems,and hydraulic systems. Thermal systems such as temperature control systems are also discussed here. Control engineers must be familiar with all of these systems discussed in this chapter.  \n\nChapter 5 presents transient and steady-state response analyses of control systems defined in terms of transfer functions. MATLAB approach to obtain transient and steady-state response analyses is presented in detail. MATLAB approach to obtain three-dimensional plots is also presented. Stability analysis based on Routh’s stability criterion is included in this chapter and the Hurwitz stability criterion is briefly discussed.  \n\nChapter 6 treats the root-locus method of analysis and design of control systems. It is a graphical method for determining the locations of all closed-loop poles from the knowledge of the locations of the open-loop poles and zeros of a closed-loop system as a parameter (usually the gain) is varied from zero to infinity. This method was developed by W. R. Evans around 1950. These days MATLAB can produce root-locus plots easily and quickly.This chapter presents both a manual approach and a MATLAB approach to generate root-locus plots. Details of the design of control systems using lead compensators, lag compensators, are lag–lead compensators are presented in this chapter.  \n\nChapter 7 presents the frequency-response method of analysis and design of control systems. This is the oldest method of control systems analysis and design and was developed during 1940–1950 by Nyquist, Bode, Nichols, Hazen, among others.This chapter presents details of the frequency-response approach to control systems design using lead compensation technique, lag compensation technique, and lag–lead compensation technique. The frequency-response method was the most frequently used analysis and design method until the state-space method became popular. However, since H-infinity control for designing robust control systems has become popular, frequency response is gaining popularity again.  \n\nChapter 8 discusses PID controllers and modified ones such as multidegrees-offreedom PID controllers.The PID controller has three parameters; proportional gain, integral gain, and derivative gain. In industrial control systems more than half of the controllers used have been PID controllers.The performance of PID controllers depends on the relative magnitudes of those three parameters. Determination of the relative magnitudes of the three parameters is called tuning of PID controllers.  \n\nZiegler and Nichols proposed so-called “Ziegler–Nichols tuning rules” as early as 1942. Since then numerous tuning rules have been proposed.These days manufacturers of PID controllers have their own tuning rules. In this chapter we present a computer optimization approach using MATLAB to determine the three parameters to satisfy given transient response characteristics.The approach can be expanded to determine the three parameters to satisfy any specific given characteristics.  \n\nChapter 9 presents basic analysis of state-space equations. Concepts of controllability and observability, most important concepts in modern control theory, due to Kalman are discussed in full. In this chapter, solutions of state-space equations are derived in detail.  \n\nChapter 10 discusses state-space designs of control systems.This chapter first deals with pole placement problems and state observers.In control engineering,it is frequently desirable to set up a meaningful performance index and try to minimize it (or maximize it, as the case may be). If the performance index selected has a clear physical meaning, then this approach is quite useful to determine the optimal control variable.This chapter discusses the quadratic optimal regulator problem where we use a performance index which is an integral of a quadratic function of the state variables and the control variable.The integral is performed from $t\\,=\\,0$ to $t\\,=\\,\\infty,$ .This chapter concludes with a brief discussion of robust control systems.  \n\n# Mathematical Modeling of Control Systems  \n\nIn studying control systems the reader must be able to model dynamic systems in mathematical terms and analyze their dynamic characteristics.A mathematical model of a dynamic system is defined as a set of equations that represents the dynamics of the system accurately, or at least fairly well. Note that a mathematical model is not unique to a given system.A system may be represented in many different ways and, therefore, may have many mathematical models, depending on one’s perspective.  \n\nThe dynamics of many systems, whether they are mechanical, electrical, thermal, economic, biological, and so on, may be described in terms of differential equations. Such differential equations may be obtained by using physical laws governing a particular system—for example, Newton’s laws for mechanical systems and Kirchhoff’s laws for electrical systems. We must always keep in mind that deriving reasonable mathematical models is the most important part of the entire analysis of control systems.  \n\nThroughout this book we assume that the principle of causality applies to the systems considered.This means that the current output of the system (the output at time $t\\,=\\,0$ )depends on the past input (the input for $t<0$ ) but does not depend on the future input (the input for $t>0$ ).  \n\nMathematical Models. Mathematical models may assume many different forms. Depending on the particular system and the particular circumstances, one mathematical model may be better suited than other models. For example, in optimal control problems, it is advantageous to use state-space representations. On the other hand, for the transient-response or frequency-response analysis of single-input, single-output, linear, time-invariant systems, the transfer-function representation may be more convenient than any other. Once a mathematical model of a system is obtained, various analytical and computer tools can be used for analysis and synthesis purposes.  \n\nSimplicity Versus Accuracy. In obtaining a mathematical model, we must make a compromise between the simplicity of the model and the accuracy of the results of the analysis. In deriving a reasonably simplified mathematical model, we frequently find it necessary to ignore certain inherent physical properties of the system. In particular, if a linear lumped-parameter mathematical model (that is, one employing ordinary differential equations) is desired, it is always necessary to ignore certain nonlinearities and distributed parameters that may be present in the physical system. If the effects that these ignored properties have on the response are small,good agreement will be obtained between the results of the analysis of a mathematical model and the results of the experimental study of the physical system.  \n\nIn general,in solving a new problem,it is desirable to build a simplified model so that we can get a general feeling for the solution.A more complete mathematical model may then be built and used for a more accurate analysis.  \n\nWe must be well aware that a linear lumped-parameter model,which may be valid in low-frequency operations,may not be valid at sufficiently high frequencies,since the neglected property of distributed parameters may become an important factor in the dynamic behavior of the system. For example, the mass of a spring may be neglected in lowfrequency operations, but it becomes an important property of the system at high frequencies.(For the case where a mathematical model involves considerable errors,robust control theory may be applied. Robust control theory is presented in Chapter 10.)  \n\nLinear Systems. A system is called linear if the principle of superposition applies. The principle of superposition states that the response produced by the simultaneous application of two different forcing functions is the sum of the two individual responses. Hence, for the linear system, the response to several inputs can be calculated by treating one input at a time and adding the results. It is this principle that allows one to build up complicated solutions to the linear differential equation from simple solutions.  \n\nIn an experimental investigation of a dynamic system, if cause and effect are proportional, thus implying that the principle of superposition holds, then the system can be considered linear.  \n\nLinear Time-Invariant Systems and Linear Time-Varying Systems. A differential equation is linear if the coefficients are constants or functions only of the independent variable. Dynamic systems that are composed of linear time-invariant lumped-parameter components may be described by linear time-invariant differential equations—that is, constant-coefficient differential equations. Such systems are called linear time-invariant (or linear constant-coefficient ) systems. Systems that are represented by differential equations whose coefficients are functions of time are called linear time-varying systems. An example of a time-varying control system is a spacecraft control system. (The mass of a spacecraft changes due to fuel consumption.)  \n\nOutline of the Chapter. Section 2–1 has presented an introduction to the mathematical modeling of dynamic systems. Section 2–2 presents the transfer function and impulse-response function. Section 2–3 introduces automatic control systems and Section 2–4 discusses concepts of modeling in state space. Section 2–5 presents state-space representation of dynamic systems. Section 2–6 discusses transformation of mathematical models with MATLAB. Finally, Section 2–7 discusses linearization of nonlinear mathematical models.  \n\n# 2–2 TRANSFER FUNCTION AND IMPULSERESPONSE FUNCTION  \n\nIn control theory, functions called transfer functions are commonly used to characterize the input-output relationships of components or systems that can be described by linear, time-invariant, differential equations. We begin by defining the transfer function and follow with a derivation of the transfer function of a differential equation system. Then we discuss the impulse-response function.  \n\nTransfer Function. The transfer function of a linear, time-invariant, differential equation system is defined as the ratio of the Laplace transform of the output (response function) to the Laplace transform of the input (driving function) under the assumption that all initial conditions are zero.  \n\nConsider the linear time-invariant system defined by the following differential equation:  \n\n$$\n{\\begin{array}{r l l}{a_{0}^{(n)}~+~a_{1}^{(n-1)}\\quad\\cdot\\cdot+~a_{n-1}{\\dot{y}}~+~a_{n}y}\\\\ &{~}&{=b_{0}^{{\\mathrm{(}}m)}~+~b_{1}x^{\\mathrm{(}}+\\cdots+~b_{m-1}{\\dot{x}}~+~b_{m}x\\qquad(n\\geq m)}\\end{array}}\n$$  \n\nwhere $y$ is the output of the system and $x$ is the input.The transfer function of this system is the ratio of the Laplace transformed output to the Laplace transformed input when all initial conditions are zero, or  \n\n$$\n{\\begin{array}{r l}&{{\\mathrm{Transfer~function}}=G(s)={\\frac{{\\mathcal{L}}[{\\mathrm{output}}]}{{\\mathcal{L}}[{\\mathrm{input}}]}}\\bigg|_{z{\\mathrm{ero}}\\;{\\mathrm{initial}}\\;{\\mathrm{conditions}}}}\\\\ &{={\\frac{Y(s)}{X(s)}}={\\frac{b_{0}s^{m}\\;+\\;b_{1}s^{m-1}\\;+\\;\\cdots\\;+\\;b_{m-1}s\\;+\\;b_{m}}{a_{0}s^{n}\\;+\\;a_{1}s^{n-1}\\;+\\;\\cdots\\;+\\;a_{n-1}s\\;+\\;a_{n}}}}\\end{array}}\n$$  \n\nBy using the concept of transfer function, it is possible to represent system dynamics by algebraic equations in $s.$ If the highest power of $s$ in the denominator of the transfer function is equal to $n$ , the system is called an nth-order system .  \n\nComments on Transfer Function. The applicability of the concept of the transfer function is limited to linear, time-invariant, differential equation systems.The transfer function approach, however, is extensively used in the analysis and design of such systems. In what follows, we shall list important comments concerning the transfer function.(Note that a system referred to in the list is one described by a linear,time-invariant, differential equation.)  \n\n1. The transfer function of a system is a mathematical model in that it is an operational method of expressing the differential equation that relates the output variable to the input variable.   \n2. The transfer function is a property of a system itself, independent of the magnitude and nature of the input or driving function.   \n3. The transfer function includes the units necessary to relate the input to the output; however, it does not provide any information concerning the physical structure of the system. (The transfer functions of many physically different systems can be identical.)   \n4. If the transfer function of a system is known, the output or response can be studied for various forms of inputs with a view toward understanding the nature of the system.   \n5. If the transfer function of a system is unknown, it may be established experimentally by introducing known inputs and studying the output of the system. Once established, a transfer function gives a full description of the dynamic characteristics of the system, as distinct from its physical description.  \n\nConvolution Integral. For a linear, time-invariant system the transfer function $G(s)$ is  \n\n$$\nG(s)={\\frac{Y(s)}{X(s)}}\n$$  \n\nwhere $X(s)$ is the Laplace transform of the input to the system and $Y(s)$ is the Laplace transform of the output of the system, where we assume that all initial conditions involved are zero.It follows that the output $Y(s)$ can be written as the product of $G(s)$ and $X(s)$ , or  \n\n$$\nY(s)\\,=\\,G(s)X(s)\n$$  \n\nNote that multiplication in the complex domain is equivalent to convolution in the time domain (see Appendix A), so the inverse Laplace transform of Equation (2–1) is given by the following convolution integral:  \n\n$$\n\\begin{array}{l}{\\displaystyle y(t)\\,=\\,\\int_{0}^{t}\\!x(\\tau)g(t\\,-\\,\\tau)\\,d\\tau}\\\\ {\\,=\\,\\displaystyle\\int_{0}^{t}\\!g(\\tau)x(t\\,-\\,\\tau)\\,d\\tau}\\end{array}\n$$  \n\nwhere both $g(t)$ and $x(t)$ are 0 for $t<0$ .  \n\nImpulse-Response Function. Consider the output (response) of a linear timeinvariant system to a unit-impulse input when the initial conditions are zero. Since the Laplace transform of the unit-impulse function is unity, the Laplace transform of the output of the system is  \n\n$$\nY(s)=G(s)\n$$  \n\nThe inverse Laplace transform of the output given by Equation (2–2) gives the impulse response of the system.The inverse Laplace transform of $G(s)$ , or  \n\n$$\n\\mathcal{L}^{-1}\\big[G(s)\\big]\\,=\\,g(t)\n$$  \n\nis called the impulse-response function. This function $g(t)$ is also called the weighting function of the system.  \n\nThe impulse-response function $g(t)$ is thus the response of a linear time-invariant system to a unit-impulse input when the initial conditions are zero.The Laplace transform of this function gives the transfer function. Therefore, the transfer function and impulse-response function of a linear, time-invariant system contain the same information about the system dynamics. It is hence possible to obtain complete information about the dynamic characteristics of the system by exciting it with an impulse input and measuring the response. (In practice, a pulse input with a very short duration compared with the significant time constants of the system can be considered an impulse.)  \n\n# 2–3 AUTOMATIC CONTROL SYSTEMS  \n\nA control system may consist of a number of components. To show the functions performed by each component, in control engineering, we commonly use a diagram called the block diagram . This section first explains what a block diagram is. Next, it discusses introductory aspects of automatic control systems, including various control actions.Then,it presents a method for obtaining block diagrams for physical systems,and, finally, discusses techniques to simplify such diagrams.  \n\nBlock Diagrams. A block diagram of a system is a pictorial representation of the functions performed by each component and of the flow of signals. Such a diagram depicts the interrelationships that exist among the various components. Differing from a purely abstract mathematical representation, a block diagram has the advantage of indicating more realistically the signal flows of the actual system.  \n\nFigure 2–1   \nElement of a block diagram.  \n\nIn a block diagram all system variables are linked to each other through functional blocks.The functional block or simply block is a symbol for the mathematical operation on the input signal to the block that produces the output.The transfer functions of the components are usually entered in the corresponding blocks, which are connected by arrows to indicate the direction of the flow of signals. Note that the signal can pass only in the direction of the arrows.Thus a block diagram of a control system explicitly shows a unilateral property.  \n\nFigure 2–1 shows an element of the block diagram.The arrowhead pointing toward the block indicates the input, and the arrowhead leading away from the block represents the output. Such arrows are referred to as signals .  \n\n![](images/266e6ceff8f4b00fa784b5da5cd82b4f2a924c001ee0ac7b57c4855fe937246a.jpg)  \n\nNote that the dimension of the output signal from the block is the dimension of the input signal multiplied by the dimension of the transfer function in the block.  \n\nThe advantages of the block diagram representation of a system are that it is easy to form the overall block diagram for the entire system by merely connecting the blocks of the components according to the signal flow and that it is possible to evaluate the contribution of each component to the overall performance of the system.  \n\nIn general, the functional operation of the system can be visualized more readily by examining the block diagram than by examining the physical system itself. A block diagram contains information concerning dynamic behavior, but it does not include any information on the physical construction of the system. Consequently, many dissimilar and unrelated systems can be represented by the same block diagram.  \n\nIt should be noted that in a block diagram the main source of energy is not explicitly shown and that the block diagram of a given system is not unique.A number of different block diagrams can be drawn for a system, depending on the point of view of the analysis.  \n\n![](images/094d1909fd5a6faaf820bca83e97c51335cb379010abbf21c5bf9946c843bba6.jpg)  \n\nFigure 2–2 Summing point.  \n\nSumming Point. Referring to Figure 2–2, a circle with a cross is the symbol that indicates a summing operation. The plus or minus sign at each arrowhead indicates whether that signal is to be added or subtracted. It is important that the quantities being added or subtracted have the same dimensions and the same units.  \n\nBranch Point. A branch point is a point from which the signal from a block goes concurrently to other blocks or summing points.  \n\nBlock Diagram of a Closed-Loop System. Figure 2–3 shows an example of a block diagram of a closed-loop system. The output $C(s)$ is fed back to the summing point, where it is compared with the reference input $R(s)$ . The closed-loop nature of the system is clearly indicated by the figure. The output of the block, $C(s)$ in this case, is obtained by multiplying the transfer function $G(s)$ by the input to the block, $E(s)$ .Any linear control system may be represented by a block diagram consisting of blocks, summing points, and branch points.  \n\nWhen the output is fed back to the summing point for comparison with the input, it is necessary to convert the form of the output signal to that of the input signal. For example, in a temperature control system, the output signal is usually the controlled temperature.The output signal, which has the dimension of temperature, must be converted to a force or position or voltage before it can be compared with the input signal. This conversion is accomplished by the feedback element whose transfer function is $H(s)$ ,as shown in Figure 2–4.The role of the feedback element is to modify the output before it is compared with the input.(In most cases the feedback element is a sensor that measures  \n\n![](images/a3d75dfd883ae1f1c23f215f1fde7b69e77e03bbaf328e105f84da8bbbb9f131.jpg)  \nFigure 2–3 Block diagram of a closed-loop system.  \n\n![](images/49ce132b9024c641d66e09e3d7066b1440048209251add232ff640a669e8cd33.jpg)  \n\nFigure 2–4 Closed-loop system.  \n\nthe output of the plant.The output of the sensor is compared with the system input, and the actuating error signal is generated.) In the present example, the feedback signal that is fed back to the summing point for comparison with the input is $B(s)=H(s)C(s)$ .  \n\nOpen-Loop Transfer Function and Feedforward Transfer Function. Referring to Figure 2–4, the ratio of the feedback signal $B(s)$ to the actuating error signal $E(s)$ is called the open-loop transfer function .That is,  \n\n$$\n{\\mathrm{Open-loop~transfer~function}}={\\frac{B(s)}{E(s)}}=G(s)H(s)\n$$  \n\nThe ratio of the output $C(s)$ to the actuating error signal $E(s)$ is called the feedforward transfer function , so that  \n\n$$\n{\\mathrm{Feedforward~transfer~function}}={\\frac{C(s)}{E(s)}}=G(s)\n$$  \n\nIf the feedback transfer function $H(s)$ is unity, then the open-loop transfer function and the feedforward transfer function are the same.  \n\nClosed-Loop Transfer Function. For the system shown in Figure 2–4, the output $C(s)$ and input $R(s)$ are related as follows: since  \n\n$$\n\\begin{array}{l}{{C(s)=G(s)E(s)}}\\\\ {{E(s)=R(s)\\,-\\,B(s)}}\\\\ {{\\qquad=\\,R(s)\\,-\\,H(s)C(s)}}\\end{array}\n$$  \n\neliminating $E(s)$ from these equations gives  \n\nor  \n\n$$\n\\begin{array}{c}{{C(s)=G(s)\\big[R(s)\\,-\\,H(s)C(s)\\big]}}\\\\ {{{}}}\\\\ {{\\displaystyle\\qquad\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)H(s)}}}\\end{array}\n$$  \n\nThe transfer function relating $C(s)$ to $R(s)$ is called the closed-loop transfer function . It relates the closed-loop system dynamics to the dynamics of the feedforward elements and feedback elements.  \n\nFrom Equation $(2{-}3),C(s)$ is given by  \n\n$$\nC(s)\\,=\\,\\frac{G(s)}{1\\,+\\,G(s)H(s)}\\,R(s)\n$$  \n\nThus the output of the closed-loop system clearly depends on both the closed-loop transfer function and the nature of the input.  \n\nObtaining Cascaded, Parallel, and Feedback (Closed-Loop) Transfer Functions with MATLAB. In control-systems analysis, we frequently need to calculate the cascaded transfer functions, parallel-connected transfer functions, and feedback-connected (closed-loop) transfer functions. MATLAB has convenient commands to obtain the cascaded, parallel, and feedback (closed-loop) transfer functions.  \n\nSuppose that there are two components $G_{1}(s)$ and $G_{2}(s)$ connected differently as shown in Figure 2–5 (a), (b), and (c), where  \n\n$$\nG_{1}(s)=\\frac{\\mathrm{num}1}{\\mathrm{den1}},~~~~~G_{2}(s)=\\frac{\\mathrm{num}2}{\\mathrm{den}2}\n$$  \n\nTo obtain the transfer functions of the cascaded system, parallel system, or feedback (closed-loop) system, the following commands may be used:  \n\n$$\n{\\begin{array}{r l}&{[\\mathsf{n u m},\\,\\mathsf{d e n}]=\\mathsf{s e r i e s}(\\mathsf{n u m}1,\\mathsf{d e n}1,\\mathsf{n u m}2,\\mathsf{d e n}2)}\\\\ &{[\\mathsf{n u m},\\,\\mathsf{d e n}]=\\mathsf{p a r a l l e l}(\\mathsf{n u m}1,\\mathsf{d e n}1,\\mathsf{n u m}2,\\mathsf{d e n}2)}\\\\ &{[\\mathsf{n u m},\\,\\mathsf{d e n}]=\\mathsf{f e e d b a c k}(\\mathsf{n u m}1,\\mathsf{d e n}1,\\mathsf{n u m}2,\\mathsf{d e n}2)}\\end{array}}\n$$  \n\nAs an example, consider the case where  \n\n$$\nG_{1}(s)=\\frac{10}{s^{2}+2s+10}=\\frac{\\mathrm{num1}}{\\mathrm{den}1},\\qquad G_{2}(s)=\\frac{5}{s+5}=\\frac{\\mathrm{num2}}{\\mathrm{den}2}\n$$  \n\nMATLAB Program 2–1 gives $C(s)/R(s)\\,=\\,\\mathrm{num}/$ 'den for each arrangement of $G_{1}(s)$ and $G_{2}(s)$ .Note that the command  \n\ndisplays the num 'den that is,the transfer function $C(s)/R(s)]$ of the system considered.  \n\n![](images/146548a093ded64c8c256e5aad6b0de36cbc19533aaf050f009f77a1bcdb70ac.jpg)  \nFigure 2–5 (a) Cascaded system; (b) parallel system; (c) feedback (closedloop) system.   \nChapter 2 /Mathematical Modeling of Control Systems  \n\n![](images/a5bf4f1589a2fbf1430217066304baa1936898bc6e29591dbb9e9a598f692bad.jpg)  \n\n![](images/0f6a1257b19567aba1e5c7ec9038ce91d57609c2bc0014dd78574bbdf5749634.jpg)  \nFigure 2–6 Block diagram of an industrial control system, which consists of an automatic controller, an actuator, a plant, and a sensor (measuring element).  \n\nAutomatic Controllers. An automatic controller compares the actual value of the plant output with the reference input (desired value), determines the deviation, and produces a control signal that will reduce the deviation to zero or to a small value. The manner in which the automatic controller produces the control signal is called the control action . Figure 2–6 is a block diagram of an industrial control system, which consists of an automatic controller, an actuator, a plant, and a sensor (measuring element). The controller detects the actuating error signal, which is usually at a very low power level, and amplifies it to a sufficiently high level. The output of an automatic controller is fed to an actuator, such as an electric motor, a hydraulic motor, or a pneumatic motor or valve. (The actuator is a power device that produces the input to the plant according to the control signal so that the output signal will approach the reference input signal.)  \n\nThe sensor or measuring element is a device that converts the output variable into another suitable variable,such as a displacement,pressure,voltage,etc.,that can be used to compare the output to the reference input signal.This element is in the feedback path of the closed-loop system.The set point of the controller must be converted to a reference input with the same units as the feedback signal from the sensor or measuring element.  \n\nClassifications of Industrial Controllers. Most industrial controllers may be classified according to their control actions as:  \n\n1. Two-position or on–off controllers   \n2. Proportional controllers   \n3. Integral controllers   \n4. Proportional-plus-integral controllers   \n5. Proportional-plus-derivative controllers   \n6. Proportional-plus-integral-plus-derivative controllers  \n\nMost industrial controllers use electricity or pressurized fluid such as oil or air as power sources. Consequently, controllers may also be classified according to the kind of power employed in the operation, such as pneumatic controllers, hydraulic controllers, or electronic controllers. What kind of controller to use must be decided based on the nature of the plant and the operating conditions, including such considerations as safety, cost, availability, reliability, accuracy, weight, and size.  \n\nTwo-Position or On–Off Control Action. In a two-position control system, the actuating element has only two fixed positions, which are, in many cases, simply on and off.Two-position or on–off control is relatively simple and inexpensive and, for this reason, is very widely used in both industrial and domestic control systems.  \n\nLet the output signal from the controller be $u(t)$ and the actuating error signal be $e(t)$ .In two-position control, the signal $u(t)$ remains at either a maximum or minimum value, depending on whether the actuating error signal is positive or negative, so that  \n\n$$\n\\begin{array}{r l r}{u(t)=U_{1},}&{{}}&{\\mathrm{for}\\;e(t)>0}\\\\ {=U_{2},}&{{}}&{\\mathrm{for}\\;e(t)<0}\\end{array}\n$$  \n\nwhere $U_{1}$ and $U_{2}$ are constants. The minimum value $U_{2}$ is usually either zero or $-U_{1}$ .Two-position controllers are generally electrical devices, and an electric solenoid-operated valve is widely used in such controllers.Pneumatic proportional controllers with very high gains act as two-position controllers and are sometimes called pneumatic twoposition controllers.  \n\nFigures 2–7(a) and (b) show the block diagrams for two-position or on–off controllers. The range through which the actuating error signal must move before the switching occurs is called the differential gap . A differential gap is indicated in Figure 2–7(b). Such a differential gap causes the controller output $u(t)$ to maintain its present value until the actuating error signal has moved slightly beyond the zero value.In some cases,the differential gap is a result of unintentional friction and lost motion; however, quite often it is intentionally provided in order to prevent too-frequent operation of the on–off mechanism.  \n\n![](images/c22a334cb715a3d0cbbfb83488d4a739503a68a31a691eefdd3308b33d8c6aa1.jpg)  \nFigure 2–7 (a) Block diagram of an on–off controller; (b) block diagram of an on–off controller with differential gap.  \n\n![](images/b6cd494dbaf308bb830123e223e2b7179d29cb558cb870f2333d92ecf78e060c.jpg)  \n\nConsider the liquid-level control system shown in Figure 2–8(a),where the electromagnetic valve shown in Figure 2–8(b) is used for controlling the inflow rate.This valve is either open or closed.With this two-position control,the water inflow rate is either a positive constant or zero. As shown in Figure 2–9, the output signal continuously moves between the two limits required to cause the actuating element to move from one fixed position to the other. Notice that the output curve follows one of two exponential curves, one corresponding to the filling curve and the other to the emptying curve. Such output oscillation between two limits is a typical response characteristic of a system under two-position control.  \n\n![](images/f04aa1eaf6d24735a3d76fda6f77d8bd2cf3f4b13729c72d4524cc81bd5b60e6.jpg)  \nFigure 2–8 (a) Liquid-level control system; (b) electromagnetic valve.  \n\n# Figure 2–9  \n\nLevel $h(t)$ -versus $\\cdot t$ curve for the system shown in Figure 2–8(a).  \n\nFrom Figure 2–9, we notice that the amplitude of the output oscillation can be reduced by decreasing the differential gap. The decrease in the differential gap, however, increases the number of on–off switchings per minute and reduces the useful life of the component. The magnitude of the differential gap must be determined from such considerations as the accuracy required and the life of the component.  \n\nProportional Control Action. For a controller with proportional control action, the relationship between the output of the controller $u(t)$ and the actuating error signal $e(t)$ is  \n\n$$\nu(t)\\,=\\,K_{p}e(t)\n$$  \n\nor, in Laplace-transformed quantities,  \n\n$$\n{\\frac{U(s)}{E(s)}}=K_{p}\n$$  \n\nwhere $K_{p}$ is termed the proportional gain.  \n\nWhatever the actual mechanism may be and whatever the form of the operating power, the proportional controller is essentially an amplifier with an adjustable gain.  \n\nIntegral Control Action. In a controller with integral control action, the value of the controller output $u(t)$ is changed at a rate proportional to the actuating error signal $e(t)$ .That is,  \n\n$$\n\\frac{d u(t)}{d t}=K_{i}e(t)\n$$  \n\nor  \n\n$$\nu(t)\\,=\\,K_{i}\\int_{0}^{t}\\!e(t)\\,d t\n$$  \n\nwhere $K_{i}$ is an adjustable constant.The transfer function of the integral controller is  \n\n$$\n{\\frac{U(s)}{E(s)}}={\\frac{K_{i}}{s}}\n$$  \n\nProportional-Plus-Integral Control Action. The control action of a proportionalplus-integral controller is defined by  \n\n$$\nu(t)\\,=\\,K_{p}e(t)\\,+\\frac{K_{p}}{T_{i}}\\int_{0}^{t}\\!e(t)\\,d t\n$$  \n\nor the transfer function of the controller is  \n\n$$\n{\\frac{U(s)}{E(s)}}=\\,K_{p}\\bigg(1\\,+\\,{\\frac{1}{T_{i}s}}\\bigg)\n$$  \n\nwhere $T_{i}$ is called the integral time .  \n\nProportional-Plus-Derivative Control Action. The control action of a proportionalplus-derivative controller is defined by  \n\n$$\nu(t)\\,=\\,K_{p}e(t)\\,+\\,K_{p}T_{d}\\frac{d e(t)}{d t}\n$$  \n\nand the transfer function is  \n\n$$\n\\frac{U(s)}{E(s)}=K_{p}(1\\,+\\,T_{d}s)\n$$  \n\nwhere $T_{d}$ is called the derivative time .  \n\nProportional-Plus-Integral-Plus-Derivative Control Action. The combination of proportional control action, integral control action, and derivative control action is termed proportional-plus-integral-plus-derivative control action. It has the advantages of each of the three individual control actions. The equation of a controller with this combined action is given by  \n\n$$\nu(t)\\,=\\,K_{p}e(t)\\,+\\frac{K_{p}}{T_{i}}\\int_{0}^{t}e(t)\\,d t\\,+\\,K_{p}T_{d}\\frac{d e(t)}{d t}\n$$  \n\nor the transfer function is  \n\n$$\n\\frac{U(s)}{E(s)}=\\,K_{p}\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)\n$$  \n\nwhere $K_{p}$ is the proportional gain, $T_{i}$ is the integral time, and $T_{d}$ is the derivative time. The block diagram of a proportional-plus-integral-plus-derivative controller is shown in Figure 2–10.  \n\n![](images/f3e5d3d8c29b182c54e2cd5eac6ceed0a74cc3a4a1f8ea605f945dfd5122d878.jpg)  \nFigure 2–10 Block diagram of a proportional-plusintegral-plusderivative controller.  \n\n![](images/31f4488384f73758dabafb69be850180c80f421f8bc5b5ec090c4540ebda7cce.jpg)  \nFigure 2–11 Closed-loop system subjected to a disturbance.  \n\nClosed-Loop System Subjected to a Disturbance. Figure 2–11 shows a closedloop system subjected to a disturbance. When two inputs (the reference input and disturbance) are present in a linear time-invariant system, each input can be treated independently of the other; and the outputs corresponding to each input alone can be added to give the complete output.The way each input is introduced into the system is shown at the summing point by either a plus or minus sign.  \n\nConsider the system shown in Figure 2–11. In examining the effect of the disturbance $D(s)$ , we may assume that the reference input is zero; we may then calculate the response $C_{D}(s)$ to the disturbance only.This response can be found from  \n\n$$\n\\frac{C_{D}(s)}{D(s)}=\\frac{G_{2}(s)}{1\\nonumber+\\,G_{1}(s)G_{2}(s)H(s)}\n$$  \n\nOn the other hand, in considering the response to the reference input $R(s)$ ,we may assume that the disturbance is zero.Then the response $C_{R}(s)$ to the reference input $R(s)$ can be obtained from  \n\n$$\n\\frac{C_{R}(s)}{R(s)}=\\frac{G_{1}(s)G_{2}(s)}{1+G_{1}(s)G_{2}(s)H(s)}\n$$  \n\nThe response to the simultaneous application of the reference input and disturbance can be obtained by adding the two individual responses. In other words, the response $C(s)$ due to the simultaneous application of the reference input $R(s)$ and disturbance $D(s)$ is given by  \n\n$$\n\\begin{array}{l}{{C(s)\\,=\\,C_{R}(s)\\,+\\,C_{D}(s)}}\\\\ {{\\,}}\\\\ {{\\,=\\,\\displaystyle{\\frac{G_{2}(s)}{1\\,+\\,G_{1}(s)G_{2}(s)H(s)}}\\,\\bigl[G_{1}(s)R(s)\\,+\\,D(s)\\bigr]}}\\end{array}\n$$  \n\nConsider now the case where $|G_{1}(s)H(s)|\\,\\gg\\,1$ and $|G_{1}(s)G_{2}(s)H(s)|\\,\\gg\\,1.$ . In this case, the closed-loop transfer function $C_{D}(s)/D(s)$ becomes almost zero, and the effect of the disturbance is suppressed.This is an advantage of the closed-loop system.  \n\nOn the other hand, the closed-loop transfer function $C_{R}(s)/R(s)$ approaches $1/H(s)$ as the gain of $G_{1}(s)G_{2}(s)H(s)$ increases.This means that if $|G_{1}(s)G_{2}(s)H(s)|\\,\\gg\\,1$ , then the closed-loop transfer function $C_{R}(s)/R(s)$ becomes independent of $G_{1}(s)$ and $G_{2}(s)$ and inversely proportional to $H(s)$ , so that the variations of $G_{1}(s)$ and $G_{2}(s)$ do not affect the closed-loop transfer function $C_{R}(s)/R(s)$ . This is another advantage of the closed-loop system.It can easily be seen that any closed-loop system with unity feedback, $H(s)=1$ ,tends to equalize the input and output.  \n\nProcedures for Drawing a Block Diagram. To draw a block diagram for a system, first write the equations that describe the dynamic behavior of each component. Then take the Laplace transforms of these equations, assuming zero initial conditions, and represent each Laplace-transformed equation individually in block form. Finally, assemble the elements into a complete block diagram.  \n\n$$\n\\begin{array}{l}{i=\\frac{e_{i}\\,-\\,e_{o}}{R}}\\\\ {~~}\\\\ {e_{o}=\\frac{\\int\\!i\\,d t}{C}}\\end{array}\n$$  \n\nAs an example, consider the $R C$ circuit shown in Figure 2–12(a).The equations for this circuit are  \n\nThe Laplace transforms of Equations (2–4) and (2–5),with zero initial condition,become  \n\n![](images/da254e6debcad2d2019aeacc7aefb1556d42f2dc7dc849f427fdd7fb51f23201.jpg)  \nFigure 2–12 (a) $R C$ circuit; (b) block diagram representing Equation (2–6); (c) block diagram representing Equation (2–7); (d) block diagram of the $R C$ circuit.  \n\n$$\n\\begin{array}{c c c}{\\displaystyle{I(s)=\\frac{E_{i}(s)\\,-\\,E_{o}(s)}{R}}}\\\\ {\\displaystyle{E_{o}(s)=\\frac{I(s)}{C s}}}\\end{array}\n$$  \n\nEquation (2–6) represents a summing operation, and the corresponding diagram is shown in Figure 2–12(b).Equation (2–7) represents the block as shown in Figure 2–12(c). Assembling these two elements, we obtain the overall block diagram for the system as shown in Figure 2–12(d).  \n\nBlock Diagram Reduction. It is important to note that blocks can be connected in series only if the output of one block is not affected by the next following block. If there are any loading effects between the components, it is necessary to combine these components into a single block.  \n\nAny number of cascaded blocks representing nonloading components can be replaced by a single block, the transfer function of which is simply the product of the individual transfer functions.  \n\nA complicated block diagram involving many feedback loops can be simplified by a step-by-step rearrangement. Simplification of the block diagram by rearrangements considerably reduces the labor needed for subsequent mathematical analysis. It should be noted, however, that as the block diagram is simplified, the transfer functions in new blocks become more complex because new poles and new zeros are generated.  \n\n# EXAMPLE 2–1  \n\nConsider the system shown in Figure 2–13(a). Simplify this diagram.  \n\nBy moving the summing point of the negative feedback loop containing $H_{2}$ outside the positive feedback loop containing $H_{1}$ ,we obtain Figure 2–13(b).Eliminating the positive feedback loop, we have Figure 2–13(c).The elimination of the loop containing $H_{2}/G_{1}$ gives Figure 2–13(d).Finally, eliminating the feedback loop results in Figure 2–13(e).  \n\n![](images/87f0a860a2338b419f09bb164657c5da23c21ffe1510c7e2cac2dd37ef70bb80.jpg)  \nFigure 2–13 (a) Multiple-loop system; (b)–(e) successive reductions of the block diagram shown in (a).  \n\nNotice that the numerator of the closed-loop transfer function $C(s)/R(s)$ is the product of the transfer functions of the feedforward path.The denominator of $C(s)/R(s)$ is equal to  \n\n$$\n\\begin{array}{r l}{\\mathrm{~}}&{=1\\,+\\,\\left(-G_{1}G_{2}H_{1}\\,+\\,G_{2}G_{3}H_{2}\\,+\\,G_{1}G_{2}G_{3}\\right)}\\\\ {\\mathrm{~}}&{}\\\\ {\\mathrm{~}}&{=1\\,-\\,G_{1}G_{2}H_{1}\\,+\\,G_{2}G_{3}H_{2}\\,+\\,G_{1}G_{2}G_{3}}\\end{array}\n$$  \n\n(The positive feedback loop yields a negative term in the denominator.)  \n\n# 2–4 MODELING IN STATE SPACE  \n\nIn this section we shall present introductory material on state-space analysis of control systems.  \n\nModern Control Theory. The modern trend in engineering systems is toward greater complexity, due mainly to the requirements of complex tasks and good accuracy. Complex systems may have multiple inputs and multiple outputs and may be time varying. Because of the necessity of meeting increasingly stringent requirements on the performance of control systems, the increase in system complexity, and easy access to large scale computers, modern control theory, which is a new approach to the analysis and design of complex control systems, has been developed since around 1960.This new approach is based on the concept of state. The concept of state by itself is not new, since it has been in existence for a long time in the field of classical dynamics and other fields.  \n\nModern Control Theory Versus Conventional Control Theory. Modern control theory is contrasted with conventional control theory in that the former is applicable to multiple-input, multiple-output systems, which may be linear or nonlinear, time invariant or time varying, while the latter is applicable only to linear timeinvariant single-input, single-output systems. Also, modern control theory is essentially time-domain approach and frequency domain approach (in certain cases such as H-infinity control), while conventional control theory is a complex frequency-domain approach. Before we proceed further, we must define state, state variables, state vector, and state space.  \n\nState. The state of a dynamic system is the smallest set of variables (called state variables ) such that knowledge of these variables at $t\\,=\\,t_{0}$ ,together with knowledge of the input for $t\\,\\geq t_{0}$ ,completely determines the behavior of the system for any time $t\\geq t_{0}$ .  \n\nNote that the concept of state is by no means limited to physical systems. It is applicable to biological systems, economic systems, social systems, and others.  \n\nState Variables. The state variables of a dynamic system are the variables making up the smallest set of variables that determine the state of the dynamic system. If at least $n$ variables $x_{1}$ ,$x_{2},\\ldots,x_{n}$ are needed to completely describe the behavior of a dynamic system (so that once the input is given for $t\\geq t_{0}$ and the initial state at $t\\,=\\,t_{0}$ is specified, the future state of the system is completely determined), then such n variables are a set of state variables.  \n\nNote that state variables need not be physically measurable or observable quantities. Variables that do not represent physical quantities and those that are neither measurable nor observable can be chosen as state variables. Such freedom in choosing state variables is an advantage of the state-space methods. Practically, however, it is convenient to choose easily measurable quantities for the state variables, if this is possible at all, because optimal control laws will require the feedback of all state variables with suitable weighting.  \n\nState Vector. If $n$ state variables are needed to completely describe the behavior of a given system, then these $n$ state variables can be considered the $n$ components of a vector $\\mathbf{X}$ . Such a vector is called a state vector .A state vector is thus a vector that determines uniquely the system state ${\\bf x}(t)$ for any time $t\\geq t_{0}$ , once the state at $t\\,=\\,t_{0}$ is given and the input $u(t)$ for $t\\geq t_{0}$ is specified.  \n\nState Space. The $n$ -dimensional space whose coordinate axes consist of the $x_{1}$ axis, $x_{2}$ axis, $\\cdots,x_{n}$ axis, where $x_{1},x_{2},\\ldots,x_{n}$ are state variables, is called a state space .Any state can be represented by a point in the state space.  \n\nState-Space Equations. In state-space analysis we are concerned with three types of variables that are involved in the modeling of dynamic systems: input variables, output variables, and state variables. As we shall see in Section 2–5, the state-space representation for a given system is not unique, except that the number of state variables is the same for any of the different state-space representations of the same system.  \n\nThe dynamic system must involve elements that memorize the values of the input for $t\\geq t_{1}$ . Since integrators in a continuous-time control system serve as memory devices, the outputs of such integrators can be considered as the variables that define the internal state of the dynamic system.Thus the outputs of integrators serve as state variables. The number of state variables to completely define the dynamics of the system is equal to the number of integrators involved in the system.  \n\nAssume that a multiple-input,multiple-output system involves $n$ integrators.Assume also that there are $r$ inputs $u_{1}(t),u_{2}(t),\\ldots,u_{r}(t)$ and $_m$ outputs $y_{1}(t),y_{2}(t),\\ldots,y_{m}(t)$ .Define $n$ outputs of the integrators as state variables: $x_{1}(t),\\,x_{2}(t),\\ldots,\\,x_{n}(t)$ Then the system may be described by  \n\n$$\n{\\begin{array}{l}{{\\dot{x}}_{1}(t)\\,=\\,f_{1}\\!\\left(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{r};t\\right)}\\\\ {{\\dot{x}}_{2}(t)\\,=\\,f_{2}\\!\\left(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{r};t\\right)}\\\\ {\\quad\\qquad\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\qquad\\cdot}\\end{array}}\n$$  \n\n$$\n{\\dot{x}}_{n}(t)\\,=\\,f_{n}\\bigl(x_{1},\\,x_{2},\\,\\ldots,\\,x_{n};\\,u_{1},\\,u_{2},\\,\\ldots,\\,u_{r};\\,t\\bigr)\n$$  \n\nThe outputs $y_{1}(t),y_{2}(t),\\dots,y_{m}(t)$ of the system may be given by  \n\n$$\n{\\begin{array}{l}{y_{1}(t)\\,=\\,g_{1}\\!\\big(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{r};t\\big)}\\\\ {y_{2}(t)\\,=\\,g_{2}\\!\\big(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{r};t\\big)}\\\\ {\\qquad\\cdot}\\\\ {\\qquad\\cdot}\\\\ {\\qquad\\cdot}\\\\ {\\qquad\\cdot}\\end{array}}\n$$  \n\n$$\ny_{m}(t)\\,=\\,g_{m}\\bigl(x_{1},\\,x_{2},\\,.\\,.\\,.\\,,\\,x_{n};\\,u_{1},\\,u_{2},\\,.\\,.\\,.\\,,\\,u_{r};\\,t\\bigr)\n$$  \n\nIf we define  \n\n$$\n\\begin{array}{r}{\\mathbf{x}(t)=\\left[\\begin{array}{c}{x_{1}(t)}\\\\ {x_{2}(t)}\\\\ {.}\\\\ {.}\\\\ {.}\\\\ {x_{n}(t)}\\end{array}\\right],}\\\\ {\\mathbf{y}(t)=\\left[\\begin{array}{c}{y_{1}(t)}\\\\ {y_{2}(t)}\\\\ {.}\\\\ {.}\\\\ {.}\\\\ {y_{n}(t)}\\end{array}\\right],}\\\\ {\\mathbf{\\sigma}_{\\mathbf{y}}(t)=\\left[\\begin{array}{c}{x_{1}(t)}\\\\ {.}\\\\ {.}\\\\ {.}\\\\ {.}\\\\ {y_{n}(t)}\\end{array}\\right],}\\end{array}\n$$  \n\n$$\n\\mathbf{f}(\\mathbf{x},\\mathbf{u},t)=\\left[\\begin{array}{c}{f_{1}(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{l};t)}\\\\ {f_{2}(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{l};t)}\\\\ {\\vdots}\\\\ {\\cdot}\\\\ {f_{n}(x_{1},x_{2},\\dots,x_{n};u_{1},u_{2},\\dots,u_{l};t)}\\end{array}\\right],\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n$$  \n\n$$\n\\mathbf{u}(t)\\,=\\,{\\left[\\begin{array}{l}{u_{1}(t)}\\\\ {u_{2}(t)}\\\\ {\\quad}\\\\ {\\quad}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {u_{r}(t)}\\end{array}\\right]},\n$$  \n\nthen Equations (2–8) and (2–9) become  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}(t)\\,=\\,\\mathbf{f}(\\mathbf{x},\\mathbf{u},t)}\\\\ {\\ }\\\\ {\\mathbf{y}(t)\\,=\\,\\mathbf{g}(\\mathbf{x},\\mathbf{u},t)}\\end{array}\n$$  \n\nwhere Equation (2–10) is the state equation and Equation (2–11) is the output equation. If vector functions fand/or $\\mathbf{g}$ involve time $t$ explicitly, then the system is called a timevarying system.  \n\nIf Equations (2–10) and (2–11) are linearized about the operating state, then we have the following linearized state equation and output equation:  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}(t)=\\mathbf{A}(t)\\mathbf{x}(t)\\mathbf{\\delta}+\\mathbf{B}(t)\\mathbf{u}(t)}\\\\ {\\mathbf{y}(t)=\\mathbf{C}(t)\\mathbf{x}(t)\\mathbf{\\delta}+\\mathbf{D}(t)\\mathbf{u}(t)}\\end{array}\n$$  \n\nwhere $\\mathbf{A}(t)$ is called the state matrix, ${\\bf\\delta B}(t)$ the input matrix, $\\mathbf{C}(t)$ the output matrix, and $\\mathbf{D}(t)$ the direct transmission matrix. (Details of linearization of nonlinear systems about  \n\n![](images/a17093fc67c4a1d323fae71f5695f71938fd2886329efb291cff2ce6f6bfaf5d.jpg)  \nFigure 2–14 Block diagram of the linear, continuoustime control system represented in state space.  \n\n# EXAMPLE 2–2  \n\nthe operating state are discussed in Section 2–7.) A block diagram representation of Equations (2–12) and (2–13) is shown in Figure 2–14.  \n\nIf vector functions fand $\\mathbf{g}$ do not involve time $t$ explicitly then the system is called a time-invariant system. In this case, Equations (2–12) and (2–13) can be simplified to  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}(t)=\\mathbf{A}\\mathbf{x}(t)\\mathbf{\\Sigma}+\\mathbf{B}\\mathbf{u}(t)}\\\\ {\\dot{\\mathbf{y}}(t)=\\mathbf{C}\\mathbf{x}(t)+\\mathbf{D}\\mathbf{u}(t)}\\end{array}\n$$  \n\nEquation (2–14) is the state equation of the linear, time-invariant system and Equation (2–15) is the output equation for the same system. In this book we shall be concerned mostly with systems described by Equations (2–14) and (2–15).  \n\nIn what follows we shall present an example for deriving a state equation and output equation.  \n\nConsider the mechanical system shown in Figure 2–15. We assume that the system is linear. The external force $u(t)$ is the input to the system, and the displacement $y(t)$ of the mass is the output. The displacement $y(t)$ is measured from the equilibrium position in the absence of the external force.This system is a single-input, single-output system.  \n\nFrom the diagram, the system equation is  \n\n$$\nm{\\ddot{y}}\\,+\\,b{\\dot{y}}\\,+\\,k y\\,=\\,u\n$$  \n\nThis system is of second order.This means that the system involves two integrators. Let us define state variables $x_{1}(t)$ and $x_{2}(t)$ as  \n\n![](images/7445eaa2b078caca8976b1070404ef7eaacadb704c4384c13cfd9c913d42cd1f.jpg)  \n\n$$\n\\begin{array}{l}{{x_{1}(t)\\,=\\,y(t)}}\\\\ {{\\,}}\\\\ {{x_{2}(t)\\,=\\,{\\dot{y}}(t)}}\\end{array}\n$$  \n\nThen we obtain  \n\nFigure 2–15 Mechanical system.  \n\n$$\n\\begin{array}{l}{{\\dot{x}_{1}=\\,x_{2}}}\\\\ {{\\dot{x}_{2}=\\displaystyle\\frac{1}{m}\\left(-k y\\,-\\,b\\dot{y}\\right)+\\frac{1}{m}\\,u}}\\end{array}\n$$  \n\nor  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\dot{x}}_{1}\\,=\\,x_{2}}}\\\\ {{\\displaystyle{\\dot{x}}_{2}\\,=\\,-\\,{\\frac{k}{m}}\\,x_{1}\\,-\\,{\\frac{b}{m}}\\,x_{2}\\,+\\,{\\frac{1}{m}}\\,u}}\\end{array}\n$$  \n\nThe output equation is  \n\n$$\ny\\,=\\,x_{1}\n$$  \n\n![](images/cb9f721a1ca18c3d10aec217b214d03a8de15520e5f34c9b7d63347f5b8a6f33.jpg)  \nFigure 2–16 Block diagram of the mechanical system shown in Figure 2–15.  \n\nIn a vector-matrix form, Equations (2–17) and (2–18) can be written as  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {\\displaystyle-{\\frac{k}{m}}}&{\\displaystyle-{\\frac{b}{m}}}\\end{array}\\right]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+{\\left[\\!\\!{\\begin{array}{l}{0}\\\\ {\\displaystyle{\\frac{1}{m}}}\\end{array}}\\right]}u\n$$  \n\nThe output equation, Equation (2–19), can be written as  \n\n$$\ny\\,=\\,[1\\quad0]{\\sqcap}_{x_{2}}^{}\\]\n$$  \n\nEquation (2–20) is a state equation and Equation (2–21) is an output equation for the system. They are in the standard form:  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,D u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{\\,0}&{\\,1}\\\\ {\\,-{\\frac{k}{m}}}&{\\!-{\\frac{b}{m}}}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{\\,0}\\\\ {\\,1}\\\\ {\\,m}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{\\,0],\\qquad D=0\n$$  \n\nFigure 2–16 is a block diagram for the system. Notice that the outputs of the integrators are state variables.  \n\nCorrelation Between Transfer Functions and State-Space Equations. In what follows we shall show how to derive the transfer function of a single-input, single-output system from the state-space equations.  \n\nLet us consider the system whose transfer function is given by  \n\n$$\n{\\frac{Y(s)}{U(s)}}=G(s)\n$$  \n\nThis system may be represented in state space by the following equations:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{D}u}\\end{array}\n$$  \n\nwhere $\\mathbf{X}$ is the state vector, $u$ is the input, and $y$ is the output.The Laplace transforms of Equations (2–23) and (2–24) are given by  \n\n$$\n\\begin{array}{r}{s\\mathbf{X}(s)\\rightharpoonup\\mathbf{x}(0)=\\mathbf{A}\\mathbf{X}(s)\\,+\\,\\mathbf{B}U(s)}\\\\ {Y(s)\\,=\\,\\mathbf{C}\\mathbf{X}(s)\\,+\\,D U(s)}\\end{array}\n$$  \n\nSince the transfer function was previously defined as the ratio of the Laplace transform of the output to the Laplace transform of the input when the initial conditions were zero, we set ${\\bf x}(0)$ in Equation (2–25) to be zero.Then we have  \n\n$$\ns\\mathbf{X}(s)\\,-\\,\\mathbf{A}\\mathbf{X}(s)\\,=\\,\\mathbf{B}U(s)\n$$  \n\nor  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})\\mathbf{X}(s)\\,=\\,\\mathbf{B}U(s)\n$$  \n\nBy premultiplying $(s\\mathbf{I}-\\mathbf{A})^{-1}$ to both sides of this last equation, we obtain  \n\n$$\n\\mathbf{X}(s)\\,=\\,(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}U(s)\n$$  \n\nBy substituting Equation (2–27) into Equation (2–26), we get  \n\n$$\nY(s)\\,=\\,\\bigl[\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\,+\\,D\\bigr]U(s)\n$$  \n\nUpon comparing Equation (2–28) with Equation (2–22), we see that  \n\n$$\nG(s)\\,=\\,{\\bf C}(s{\\bf I}\\,-\\,{\\bf A})^{-1}{\\bf B}\\,+\\,D\n$$  \n\nThis is the transfer-function expression of the system in terms of $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ , and $D$ .  \n\nNote that the right-hand side of Equation (2–29) involves $(s\\mathbf{I}-\\mathbf{A})^{-1}$ .Hence $G(s)$ can be written as  \n\n$$\nG(s)={\\frac{Q(s)}{|s\\mathbf{I}-\\mathbf{A}|}}\n$$  \n\nwhere $Q(s)$ is a polynomial in $s$ . Notice that $|s\\mathbf{I}-\\mathbf{A}|$ is equal to the characteristic polynomial of $G(s)$ . In other words, the eigenvalues of $\\mathbf{A}$ are identical to the poles of $G(s)$ .  \n\n# EXAMPLE 2–3  \n\nConsider again the mechanical system shown in Figure 2–15. State-space equations for the system are given by Equations (2–20) and (2–21).We shall obtain the transfer function for the system from the state-space equations.  \n\nBy substituting $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ , and $D$ into Equation (2–29), we obtain  \n\n$$\n\\begin{array}{r l}&{G(s)=\\mathbf{C}(s\\mathbf{I}-\\mathbf{A})^{-1}\\mathbf{B}+D}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\left[1\\!\\!\\!\\!\\!\\begin{array}{c c}{\\!\\!\\!\\!\\!\\!\\!0}&{\\!\\!\\!\\!\\!\\!\\!0}\\\\ {\\!\\!\\!\\!\\!\\!0}&{\\!\\!\\!\\!\\!\\!\\!\\!0}\\end{array}\\right]\\!\\!\\!\\!\\left\\{\\!\\!\\begin{array}{c c}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$  \n\nNote that  \n\n$$\n\\left[{\\begin{array}{l l}{s}&{-1}\\\\ {{\\cfrac{k}{m}}}&{s+{\\cfrac{b}{m}}}\\end{array}}\\right]^{-1}={\\cfrac{1}{s^{2}+{\\cfrac{b}{m}}\\,s+{\\cfrac{k}{m}}\\,{\\left[\\begin{array}{l l}{-{\\cfrac{k}{m}}}&{s}\\\\ {-{\\cfrac{k}{m}}}&{s}\\end{array}\\right]}}}\n$$  \n\n(Refer to Appendix ${\\bf C}$ for the inverse of the $2\\times2$ matrix.) Thus, we have  \n\n$$\n\\begin{array}{r l}{G(s)\\displaystyle=\\left[1\\right.}&{\\!0\\displaystyle]\\,\\frac{1}{s^{2}+\\frac{b}{m}\\,s+\\frac{k}{m}}\\left[\\begin{array}{l l}{\\displaystyle s+\\frac{b}{m}}&{1}\\\\ {\\displaystyle-\\frac{k}{m}}&{s\\displaystyle\\right]\\left[\\frac{0}{m}\\right]}\\\\ {\\displaystyle=\\frac{1}{m s^{2}+b s+k}}\\end{array}\n$$  \n\nwhich is the transfer function of the system. The same transfer function can be obtained from Equation (2–16).  \n\nTransfer Matrix. Next, consider a multiple-input, multiple-output system.Assume that there are $r$ inputs $u_{1},u_{2},\\ldots,u_{r}$ ,and $_m$ outputs $y_{1},y_{2},\\dots,y_{m}$ .Define  \n\n$$\n\\mathbf{y}\\,=\\,\\left[\\begin{array}{c}{y_{1}}\\\\ {y_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {y_{m}}\\end{array}\\right]\\,,\\qquad\\mathbf{u}\\,=\\,\\left[\\begin{array}{c}{u_{1}}\\\\ {u_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {u_{r}}\\end{array}\\right]\\,.\n$$  \n\nThe transfer matrix $\\mathbf{G}(s)$ relates the output $\\mathbf{Y}(s)$ to the input $\\mathbf{U}(s)$ , or  \n\n$$\n\\mathbf{Y}(s)\\,=\\,\\mathbf{G}(s)\\mathbf{U}(s)\n$$  \n\nwhere $\\mathbf{G}(s)$ is given by  \n\n$$\n\\mathbf{G}(s)\\,=\\,\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\,+\\,\\mathbf{D}\n$$  \n\n[The derivation for this equation is the same as that for Equation (2–29).] Since the input vector $\\mathbf{u}$ is $r$ dimensional and the output vector $\\mathbf{y}$ is $_m$ dimensional,the transfer matrix $\\mathbf{G}(\\mathbf{s})$ is an $m\\times r$ matrix.  \n\n# 2–5 STATE-SPACE REPRESENTATION OF SCALAR DIFFERENTIAL EQUATION SYSTEMS  \n\nA dynamic system consisting of a finite number of lumped elements may be described by ordinary differential equations in which time is the independent variable. By use of vector-matrix notation, an nth-order differential equation may be expressed by a firstorder vector-matrix differential equation. If $n$ elements of the vector are a set of state variables, then the vector-matrix differential equation is a state equation. In this section we shall present methods for obtaining state-space representations of continuous-time systems.  \n\n# State-Space Representation of nth-Order Systems of Linear Differential Equations in which the Forcing Function Does Not Involve Derivative Terms. Consider the following nth-order system:  \n\n$$\n\\begin{array}{c}{{(n)}}\\\\ {{y\\;+}}\\end{array}\\!\\!{}a_{1}y\\;+\\;\\cdots\\,+\\;a_{n-1}\\dot{y}\\;+\\;a_{n}y\\,=\\,u\n$$  \n\nNoting that the knowledge of $y(0),\\dot{y}(0),\\ldots,\\stackrel{(n-1)}{y}(0)$ together with the input $u(t)$ for $t\\,\\geq\\,0$ ,determines completely the future behavior of the system, we may take $y(t),\\dot{y}(t),\\ldots,\\begin{array}{r}{{(n{-}1)}}\\\\ {y}\\end{array}(t)$ as a set of $n$ state variables. (Mathematically, such a choice of state variables is quite convenient.Practically,however,because higher-order derivative terms are inaccurate, due to the noise effects inherent in any practical situations, such a choice of the state variables may not be desirable.)  \n\nLet us define  \n\n$$\n\\begin{array}{c}{{x_{1}=y}}\\\\ {{x_{2}=\\dot{y}}}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\\\ {{x_{n}={\\pmod{2}}}}\\end{array}\n$$  \n\nThen Equation (2–30) can be written as  \n\n$$\n\\begin{array}{c}{\\dot{x}_{1}=\\,x_{2}}\\\\ {\\dot{x}_{2}=\\,x_{3}}\\\\ {\\dot{.}}\\end{array}\n$$  \n\n$$\n\\begin{array}{c}{{\\dot{x}}_{n-1}={x}_{n}}\\\\ {{\\dot{x}}_{n}=-a_{n}{x}_{1}-\\cdots-a_{1}{x}_{n}+u}\\end{array}\n$$  \n\nor  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{x}=\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right],\\quad\\quad\\mathbf{A}=\\left[\\begin{array}{c c c c c}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right],\\quad\\quad\\mathbf{B}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {1}\\end{array}\\right].\n$$  \n\nThe output can be given by  \n\n$$\ny=[1\\quad0\\quad\\cdots\\quad0]\\left[{\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}}\\right]\n$$  \n\nor  \n\n$$\ny=\\mathbf{C}\\mathbf{x}\n$$  \n\nwhere  \n\n$$\n\\mathbf{C}=[1\\;\\;\\;0\\;\\;\\;\\cdots\\;\\;\\;0]\n$$  \n\n[Note that $D$ in Equation (2–24) is zero.] The first-order differential equation, Equation (2–31), is the state equation, and the algebraic equation, Equation (2–32), is the output equation.  \n\nNote that the state-space representation for the transfer function system  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{1}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nis given also by Equations (2–31) and (2–32).  \n\nState-Space Representation of nth-Order Systems of Linear Differential Equations in which the Forcing Function Involves Derivative Terms. Consider the differential equation system that involves derivatives of the forcing function, such as  \n\n$$\n{\\overset{(n)}{y}}\\,+\\,a_{1}^{\\,\\,\\,(n-1)}\\,+\\,\\cdots\\,+\\,a_{n-1}{\\dot{y}}\\,+\\,a_{n}y\\,=\\,b_{0}^{\\,\\,(n)}\\,+\\,b_{1}^{\\,\\,(n-1)}\\,+\\,\\cdots\\,+\\,b_{n-1}{\\dot{u}}\\,+\\,b_{n}u\n$$  \n\nThe main problem in defining the state variables for this case lies in the derivative terms of the input $u$ . The state variables must be such that they will eliminate the derivatives of $u$ in the state equation.  \n\nOne way to obtain a state equation and output equation for this case is to define the following $n$ variables as a set of $n$ state variables:  \n\n$$\n\\begin{array}{r l}&{x_{1}=y\\,-\\,\\beta_{0}u}\\\\ &{x_{2}=\\dot{y}\\,-\\,\\beta_{0}\\dot{u}\\,-\\,\\beta_{1}u=\\,\\dot{x}_{1}-\\beta_{1}u}\\\\ &{x_{3}=\\dot{y}\\,-\\,\\beta_{0}\\dot{u}\\,-\\,\\beta_{1}\\dot{u}\\,-\\,\\beta_{2}u\\,=\\,\\dot{x}_{2}-\\,\\beta_{2}u}\\\\ &{\\phantom{x_{3}=}.}\\\\ &{\\phantom{x_{3}=}.}\\\\ &{\\phantom{x_{3}=}.}\\end{array}\n$$  \n\n$$\nx_{n}={\\begin{array}{c}{(n-1)}\\\\ {y}\\end{array}}-\\ \\beta_{0}u\\ -\\ \\beta_{1}u\\ -\\ \\cdots-\\ \\beta_{n-2}{\\dot{u}}\\ -\\ \\beta_{n-1}u\\,=\\,{\\dot{x}}_{n-1}\\,-\\ \\beta_{n-1}u\n$$  \n\nwhere $\\beta_{0},\\beta_{1},\\beta_{2},\\dots,\\beta_{n-1}$ are determined from  \n\n$$\n\\begin{array}{r l}&{\\beta_{0}=b_{0}}\\\\ &{\\beta_{1}=b_{1}-a_{1}\\beta_{0}}\\\\ &{\\beta_{2}=b_{2}-a_{1}\\beta_{1}-a_{2}\\beta_{0}}\\\\ &{\\beta_{3}=b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0}}\\\\ &{\\mathrm{~\\\\}.}\\\\ &{\\mathrm{~\\\\\\}.}\\\\ &{\\mathrm{~\\\\\\}}\\end{array}\n$$  \n\n$$\n\\beta_{n-1}=b_{n-1}-\\,a_{1}\\beta_{n-2}\\,-\\,\\cdots\\,-\\,a_{n-2}\\beta_{1}\\,-\\,a_{n-1}\\beta_{0}\n$$  \n\nWith this choice of state variables the existence and uniqueness of the solution of the state equation is guaranteed. (Note that this is not the only choice of a set of state variables.) With the present choice of state variables, we obtain  \n\n$$\n\\begin{array}{r}{\\dot{x}_{1}=x_{2}+\\beta_{1}u}\\\\ {\\dot{x}_{2}=x_{3}+\\beta_{2}u}\\\\ {\\dot{.}}\\end{array}\n$$  \n\n$$\n\\begin{array}{c}{{\\dot{x}_{n-1}=x_{n}+\\beta_{n-1}u}}\\\\ {{\\dot{x}_{n}=-a_{n}x_{1}-a_{n-1}x_{2}-\\cdots-a_{1}x_{n}+\\beta_{n}u}}\\end{array}\n$$  \n\nwhere $\\beta_{n}$ is given by  \n\n$$\n\\beta_{n}=b_{n}-a_{1}\\beta_{n-1}-\\cdots-a_{n-1}\\beta_{1}-a_{n-1}\\beta_{0}\n$$  \n\n[To derive Equation (2–36), see Problem A–2–6 .] In terms of vector-matrix equations, Equation (2–36) and the output equation can be written as  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n-1}}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&&{1}\\\\ {0}&{0}&{0}&{\\cdots}&{\\cdots}&{-a_{1}}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\beta_{1}}\\\\ {\\beta_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\beta_{n-1}}\\\\ {\\beta_{n}}\\end{array}\\right]}u\\,\n$$  \n\n$$\ny=[1\\quad0\\quad\\cdots\\quad0]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]+\\beta_{0}u\n$$  \n\nor  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\quad}\\\\ {y=\\mathbf{C}\\mathbf{x}+D u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{x}=\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right],\\quad\\quad\\mathbf{A}=\\left[\\begin{array}{c c c c c}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]\n$$  \n\n$$\n\\mathbf{B}={\\left[\\begin{array}{l}{\\beta_{1}}\\\\ {\\beta_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\beta_{n-1}}\\\\ {\\beta_{n}}\\end{array}\\right]},\\qquad\\mathbf{C}=[1\\quad0\\quad\\cdots\\quad0],\\qquad D=\\beta_{0}=b_{0}\n$$  \n\nIn this state-space representation, matrices $\\mathbf{A}$ and Care exactly the same as those for the system of Equation (2–30).The derivatives on the right-hand side of Equation (2–33) affect only the elements of the Bmatrix.  \n\nNote that the state-space representation for the transfer function  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nis given also by Equations (2–37) and (2–38).  \n\nThere are many ways to obtain state-space representations of systems. Methods for obtaining canonical representations of systems in state space (such as controllable canonical form, observable canonical form, diagonal canonical form, and Jordan canonical form) are presented in Chapter 9.  \n\nMATLAB can also be used to obtain state-space representations of systems from transfer-function representations, and vice versa.This subject is presented in Section 2–6.  \n\n# 2–6 TRANSFORMATION OF MATHEMATICAL MODELS WITH MATLAB  \n\nMATLAB is quite useful to transform the system model from transfer function to state space, and vice versa. We shall begin our discussion with transformation from transfer function to state space.  \n\nLet us write the closed-loop transfer function as  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{\\mathrm{numerator~polynomial~in~}}{\\mathrm{denominator~polynomial~in~}}}s={\\frac{\\mathrm{num}}{\\mathrm{den}}}\n$$  \n\nOnce we have this transfer-function expression, the MATLAB command  \n\n$$\n[\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D}]=\\mathsf{t f}2\\mathsf{s s}(\\mathsf{n u m},\\mathsf{d e n})\n$$  \n\nwill give a state-space representation. It is important to note that the state-space representation for any system is not unique. There are many (infinitely many) state-space representations for the same system.The MATLAB command gives one possible such state-space representation.  \n\nTransformation from Transfer Function to State Space Representation. Consider the transfer-function system  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{Y(s)}{U(s)}=\\frac{s}{(s\\,+\\,10)\\big(s^{2}\\,+\\,4s\\,+\\,16\\big)}}}\\\\ {\\displaystyle{=\\,\\frac{s}{s^{3}\\,+\\,14s^{2}\\,+\\,56s\\,+\\,160}}}\\end{array}\n$$  \n\nThere are many (infinitely many) possible state-space representations for this system. One possible state-space representation is  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\ \\ 0}&{\\ \\ 1}&{\\ \\ 0}\\\\ {\\ \\ 0}&{\\ \\ 0}&{\\ \\ 1}\\\\ {\\ \\ -160}&{\\ -56}&{-14}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\ \\ 0}\\\\ {\\ \\ 1}\\\\ {\\ \\ -14}\\end{array}\\right]}u\n$$  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+[0]u\n$$  \n\nAnother possible state-space representation (among infinitely many alternatives) is  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-14}&{-56}&{-160}\\\\ {1}&{0}&{0}\\\\ {0}&{1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}u\n$$  \n\n$$\ny=[0\\quad1\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\,+\\,[0]u\n$$  \n\nMATLAB transforms the transfer function given by Equation (2–39) into the state-space representation given by Equations (2–40) and (2–41). For the example system considered here, MATLAB Program 2–2 will produce matrices A ,B,C,and $D$ .  \n\n![](images/c0616a3e42092077df1de4948ba8497e49d9af0aa4f6376e5edc63eea99d9849.jpg)  \n\nTransformation from State Space Representation to Transfer Function. To obtain the transfer function from state-space equations, use the following command:  \n\n$$\n[\\mathsf{n u m},\\mathsf{d e n}]=\\mathsf{s s2t f}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{i}\\mathsf{u})\n$$  \n\niu must be specified for systems with more than one input. For example, if the system has three inputs $\\left(u1,u2,u3\\right)$ , then iu must be either 1, 2, or 3, where 1 implies u1, 2 implies $u2$ ,and 3 implies u3 .  \n\nIf the system has only one input, then either  \n\n$$\n[\\mathrm{num},\\mathrm{den}]=\\mathrm{ss}2\\mathrm{tf}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D})\n$$  \n\n$$\n[\\mathsf{n u m},\\mathsf{d e n}]=\\mathsf{s s}2\\mathsf{t f}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},1)\n$$  \n\nmay be used. For the case where the system has multiple inputs and multiple outputs, see Problem A–2–12 .  \n\nEXAMPLE 2–4 Obtain the transfer function of the system defined by the following state-space equations:  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\;0}&{\\;1}&{\\;0}\\\\ {\\;0}&{\\;0}&{\\;1}\\\\ {\\;-5}&{\\!-25}&{\\!-5}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\;0}\\\\ {\\;25}\\\\ {\\;-120}\\end{array}\\right]}u\n$$  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nMATLAB Program 2-3 will produce the transfer function for the given system.The transfer function obtained is given by  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{25s\\,+\\,5}{s^{3}\\,+\\,5s^{2}\\,+\\,25s\\,+\\,5}\n$$  \n\nMATLAB Program 2–3 A = [0   1   0;   0   0   1;   -5   -25   -5]; $\\mathsf{B}=[0;25;-120]\\,,$ ;${\\mathsf{C}}=[1\\quad0\\quad\\ 0]$ ;$\\mathsf{D}=[0]$ ;[num,den] $=$ ss2tf(A,B,C,D) num $=$   \n0   0.0000   25.0000  5.0000 den   \n1.0000   5.0000   25.0000  5.0000 $\\%$ \\*\\*\\*\\*\\* The same result can be obtained by entering the following command: \\*\\*\\*\\*\\* [$\\mathsf{n u m,d e n}]=\\mathsf{s s2t f}(\\mathsf{A,B,C,D},1)$ num $=$   \n0   0.0000   25.0000  5.0000 den $=$   \n1.0000   5.0000   25.0000  5.0000  \n\nNonlinear Systems. A system is nonlinear if the principle of superposition does not apply.Thus, for a nonlinear system the response to two inputs cannot be calculated by treating one input at a time and adding the results.  \n\nAlthough many physical relationships are often represented by linear equations, in most cases actual relationships are not quite linear. In fact, a careful study of physical systems reveals that even so-called “linear systems” are really linear only in limited operating ranges. In practice, many electromechanical systems, hydraulic systems, pneumatic systems, and so on, involve nonlinear relationships among the variables. For example, the output of a component may saturate for large input signals.There may be a dead space that affects small signals. (The dead space of a component is a small range of input variations to which the component is insensitive.) Square-law nonlinearity may occur in some components. For instance, dampers used in physical systems may be linear for low-velocity operations but may become nonlinear at high velocities, and the damping force may become proportional to the square of the operating velocity.  \n\nLinearization of Nonlinear Systems. In control engineering a normal operation of the system may be around an equilibrium point, and the signals may be considered small signals around the equilibrium. (It should be pointed out that there are many exceptions to such a case.) However, if the system operates around an equilibrium point and if the signals involved are small signals, then it is possible to approximate the nonlinear system by a linear system. Such a linear system is equivalent to the nonlinear system considered within a limited operating range. Such a linearized model (linear, time-invariant model) is very important in control engineering.  \n\nThe linearization procedure to be presented in the following is based on the expansion of nonlinear function into a Taylor series about the operating point and the retention of only the linear term. Because we neglect higher-order terms of the Taylor series expansion, these neglected terms must be small enough; that is, the variables deviate only slightly from the operating condition. (Otherwise, the result will be inaccurate.)  \n\nLinear Approximation of Nonlinear Mathematical Models. To obtain a linear mathematical model for a nonlinear system, we assume that the variables deviate only slightly from some operating condition. Consider a system whose input is $x(t)$ and output is $y(t)$ .The relationship between $y(t)$ and $x(t)$ is given by  \n\n$$\ny\\,=\\,f(x)\n$$  \n\nIf the normal operating condition corresponds to $\\overline{{x}},\\,\\overline{{y}}$ ,then Equation (2–42) may be expanded into a Taylor series about this point as follows:  \n\n$$\n\\begin{array}{l}{{y\\,=\\,f(x)}}\\\\ {{\\qquad=\\,f(\\bar{x})\\,+\\,\\displaystyle{\\frac{d f}{d x}}\\,(x\\,-\\,\\bar{x})\\,+\\,\\displaystyle{\\frac{1}{2!}}\\frac{d^{2}f}{d x^{2}}\\,(x\\,-\\,\\bar{x})^{2}\\,+\\,\\cdots}}\\end{array}\n$$  \n\nwhere the derivatives $d f/d x,d^{2}f/d x^{2},\\ldots$ are evaluated at $x\\,=\\,{\\bar{x}}$ .If the variation $x\\,-\\,\\bar{x}$ is small, we may neglect the higher-order terms in $x\\,-\\,{\\bar{x}}.$ .Then Equation (2–43) may be written as  \n\n$$\ny\\,=\\,{\\overline{{y}}}\\,+\\,K(x\\,-\\,{\\overline{{x}}})\n$$  \n\nwhere  \n\n$$\n{\\bar{y}}\\,=\\,f({\\bar{x}})\n$$  \n\n$$\nK={\\frac{d f}{d x}}\\bigg|_{x=\\bar{x}}\n$$  \n\nEquation (2–44) may be rewritten as  \n\n$$\ny\\,-\\,{\\bar{y}}\\,=\\,K(\\,x\\,-\\,{\\bar{x}}\\,)\n$$  \n\nwhich indicates that $y\\mathrm{~-~}\\bar{y}$ –is proportional to $x\\,-\\,{\\bar{x}}.$ –.Equation (2–45) gives a linear mathematical model for the nonlinear system given by Equation (2–42) near the operating point $x\\,=\\,{\\bar{x}},\\,y\\,=\\,{\\bar{y}}$ –.  \n\nNext, consider a nonlinear system whose output $y$ is a function of two inputs $x_{1}$ and $x_{2}$ ,so that  \n\n$$\ny\\,=\\,f{\\bigl(}\\,x_{1},\\,x_{2}{\\bigr)}\n$$  \n\nTo obtain a linear approximation to this nonlinear system,we may expand Equation (2–46) into a Taylor series about the normal operating point $\\overline{{x}}_{1},\\,\\overline{{x}}_{2}$ .Then Equation (2–46) becomes  \n\n$$\ny\\,=\\,f{\\bigl(}{\\bar{x}}_{1},{\\bar{x}}_{2}{\\bigr)}\\,+\\,\\left[{\\frac{\\partial f}{\\partial x_{1}}}{\\bigl(}x_{1}\\,-\\,{\\bar{x}}_{1}{\\bigr)}\\,+\\,{\\frac{\\partial f}{\\partial x_{2}}}{\\bigl(}x_{2}\\,-\\,{\\bar{x}}_{2}{\\bigr)}\\right]\n$$  \n\n$$\n\\begin{array}{l}{{\\displaystyle+\\,\\frac{1}{2!}\\left[\\frac{\\partial^{2}f}{\\partial x_{1}^{2}}\\,(x_{1}\\,-\\,\\bar{x}_{1})^{2}\\,+\\,2\\,\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{2}}\\,\\big(x_{1}\\,-\\,\\bar{x}_{1}\\big)(x_{2}\\,-\\,\\bar{x}_{2})\\right.}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{+\\,\\frac{\\partial^{2}f}{\\partial x_{2}^{2}}\\,\\big(x_{2}\\,-\\,\\bar{x}_{2})^{2}\\,\\Bigg]+\\cdots}}\\end{array}\n$$  \n\nwhere the partial derivatives are evaluated at $x_{1}={\\bar{x}}_{1}$ ,$x_{2}={\\bar{x}}_{2}$ .Near the normal operating point, the higher-order terms may be neglected.The linear mathematical model of this nonlinear system in the neighborhood of the normal operating condition is then given by  \n\n$$\ny\\,-\\,{\\bar{y}}\\,=\\,K_{1}(x_{1}\\,-\\,{\\bar{x}}_{1})\\,+\\,K_{2}(x_{2}\\,-\\,{\\bar{x}}_{2})\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{\\bar{y}\\,=\\,f\\big(\\bar{x}_{1},\\bar{x}_{2}\\big)}}\\\\ {{\\ }}\\\\ {{K_{1}=\\displaystyle\\frac{\\partial f}{\\partial x_{1}}\\,\\Bigg\\vert_{x_{1}=\\bar{x}_{1},\\,x_{2}=\\bar{x}_{2}}}}\\\\ {{\\ }}\\\\ {{K_{2}=\\displaystyle\\frac{\\partial f}{\\partial x_{2}}\\,\\Bigg\\vert_{x_{1}=\\bar{x}_{1},\\,x_{2}=\\bar{x}_{2}}}}\\end{array}\n$$  \n\nThe linearization technique presented here is valid in the vicinity of the operating condition.If the operating conditions vary widely,however,such linearized equations are not adequate, and nonlinear equations must be dealt with. It is important to remember that a particular mathematical model used in analysis and design may accurately represent the dynamics of an actual system for certain operating conditions, but may not be accurate for other operating conditions.  \n\nEXAMPLE 2–5 Linearize the nonlinear equation  \n\n$$\nz=x y\n$$  \n\nin the region $5\\leq x\\leq7,10\\leq y\\leq12.$ .Find the error if the linearized equation is used to calculate the value of $z$ when $x=5$ ,$y=10$ .  \n\nSince the region considered is given by $5\\leq x\\leq7,10\\leq y\\leq12$ ,choose $\\begin{array}{r}{\\bar{x}\\,=\\,6,\\,\\bar{y}\\,=\\,11.}\\end{array}$ Then $\\Bar{z}\\,=\\,\\Bar{x}\\,\\overline{{y}}\\,=\\,66.$ Let us obtain a linearized equation for the nonlinear equation near a point $\\overline{{x}}\\,=\\,6$ ,$\\bar{y}\\,=\\,11$ .  \n\nExpanding the nonlinear equation into a Taylor series about point $x\\,=\\,{\\bar{x}},y\\,=\\,{\\bar{y}}$ ––and neglecting the higher-order terms, we have  \n\n$$\nz\\mathrm{~-~}\\overline{{z}}\\mathrm{~=~}a\\big(x\\mathrm{~-~}\\overline{{x}}\\big)\\mathrm{~+~}b\\big(y\\mathrm{~-~}\\overline{{y}}\\big)\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{a=\\displaystyle\\frac{\\partial{\\left(x y\\right)}}{\\partial x}\\left|_{x=\\bar{x},\\,y=\\bar{y}}=\\bar{y}\\,=11}}\\\\ {{\\displaystyle}}\\\\ {{b=\\displaystyle\\frac{\\partial{\\left(x y\\right)}}{\\partial y}\\left|_{x=\\bar{x},\\,y=\\bar{y}}=\\bar{x}\\,=6}}\\end{array}\n$$  \n\nHence the linearized equation is  \n\n$$\nz\\mathrm{~-~}66\\mathrm{~=~}11\\big(x\\mathrm{~-~}6\\big)\\mathrm{~+~}6\\big(y\\mathrm{~-~}11\\big)\n$$  \n\nor  \n\n$$\nz\\,=\\,11x\\,+\\,6y\\,-\\,66\n$$  \n\nWhen $x=5,y=10$ ,the value of $z$ given by the linearized equation is  \n\n$$\nz=11x\\,+\\,6y\\,-\\,66=55\\,+\\,60\\,-\\,66=49\n$$  \n\nThe exact value of $z$ is $z=\\,x y=50.$ .The error is thus $50\\,-\\,49\\,=\\,1$ .In terms of percentage, the error is $2\\%$ .  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–2–1. Simplify the block diagram shown in Figure 2–17.  \n\nSolution. First, move the branch point of the path involving $H_{1}$ outside the loop involving $H_{2}$ , as shown in Figure 2–18(a). Then eliminating two loops results in Figure 2–18(b). Combining two blocks into one gives Figure 2–18(c).  \n\nA–2–2. Simplify the block diagram shown in Figure 2–19. Obtain the transfer function relating $C(s)$ and $R(s)$ .  \n\nFigure 2–17   \nBlock diagram of a system.  \n\n![](images/14faabd499393d4c4b00b0f48551ac43b9e982e78a587a484e87c440c5465f59.jpg)  \n\nFigure 2–18 Simplified block diagrams for the system shown in Figure 2–17.  \n\n![](images/c571aab7c8d7d55484933392606e4c90a0f8175b1c15600a126e953c848ad701.jpg)  \n\nFigure 2–19   \nBlock diagram of a system.  \n\n![](images/f6562977bd6305fb9c7a467987b15710d4362efe52f705fb189e21947e7bd453.jpg)  \nFigure 2–20 Reduction of the block diagram shown in Figure 2–19.   \n(c)  \n\nSolution. The block diagram of Figure 2–19 can be modified to that shown in Figure 2–20(a). Eliminating the minor feedforward path, we obtain Figure 2–20(b), which can be simplified to Figure 2–20(c).The transfer function $C(s)/R(s)$ is thus given by  \n\n$$\n\\frac{C(s)}{R(s)}=G_{1}G_{2}\\,+\\,G_{2}\\,+\\,1\n$$  \n\nThe same result can also be obtained by proceeding as follows: Since signal $X(s)$ is the sum of two signals $G_{1}R(s)$ and $R(s)$ ,we have  \n\n$$\nX(s)=G_{1}R(s)\\,+\\,R(s)\n$$  \n\nThe output signal $C(s)$ is the sum of $G_{2}X(s)$ and $R(s)$ .Hence  \n\n$$\nC(s)\\,=\\,G_{2}X(s)\\,+\\,R(s)\\,=\\,G_{2}\\bigl[G_{1}R(s)\\,+\\,R(s)\\bigr]\\,+\\,R(s)\n$$  \n\nAnd so we have the same result as before:  \n\n$$\n\\frac{C(s)}{R(s)}=G_{1}G_{2}\\,+\\,G_{2}\\,+\\,1\n$$  \n\nA–2–3. Simplify the block diagram shown in Figure 2–21. Then obtain the closed-loop transfer function $C(s)/R(s)$ .  \n\nFigure 2–21   \nBlock diagram of a system.  \n\n![](images/d78946bcb427abaec8a7ed1d7f8373e615465400aa87f8c648d480cd5d4b988e.jpg)  \n\n![](images/03c86b685061103cd165732b43ecac6a26a1f2bcf57b0c69282980f2b1f72512.jpg)  \nFigure 2–22 Successive reductions of the block diagram shown in Figure 2–21.  \n\nSolution. First move the branch point between $G_{3}$ and $G_{4}$ to the right-hand side of the loop containing $G_{3},G_{4}$ , and $H_{2}$ .Then move the summing point between $G_{1}$ and $G_{2}$ to the left-hand side of the first summing point. See Figure 2–22(a). By simplifying each loop, the block diagram can be modified as shown in Figure 2–22(b). Further simplification results in Figure 2–22(c), from which the closed-loop transfer function $C(s)/R(s)$ is obtained as  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G_{1}G_{2}G_{3}G_{4}}{1\\,+\\,G_{1}G_{2}H_{1}\\,+\\,G_{3}G_{4}H_{2}\\,-\\,G_{2}G_{3}H_{3}\\,+\\,G_{1}G_{2}G_{3}G_{4}H_{1}H_{2}}\n$$  \n\nA–2–4. Obtain transfer functions $C(s)/R(s)$ and $C(s)/D(s)$ of the system shown in Figure 2–23.  \n\nSolution. From Figure 2–23 we have  \n\n$$\n\\begin{array}{l}{{U(s)=G_{f}R(s)\\,+\\,G_{c}E(s)}}\\\\ {{C(s)=G_{p}\\bigl[D(s)\\,+\\,G_{1}U(s)\\bigr]}}\\\\ {{E(s)=R(s)-H C(s)}}\\end{array}\n$$  \n\n![](images/969c8a9fa1604aca72c6e74e0ce245924735916a151d7c08faa9797c19f2d019.jpg)  \nFigure 2–23 Control system with reference input and disturbance input.   \nChapter 2 /Mathematical Modeling of Control Systems  \n\nBy substituting Equation (2–47) into Equation (2–48), we get  \n\n$$\nC(s)\\,=\\,G_{p}D(s)\\,+\\,G_{1}G_{p}\\bigl[G_{f}R(s)\\,+\\,G_{c}E(s)\\bigr]\n$$  \n\nBy substituting Equation (2–49) into Equation (2–50), we obtain  \n\n$$\nC(s)\\,=\\,G_{p}D(s)\\,+\\,G_{1}G_{p}\\bigl\\{G_{f}R(s)\\,+\\,G_{c}\\bigl[R(s)\\,-\\,H C(s)\\bigr]\\bigr\\}\n$$  \n\nSolving this last equation for $C(s)$ ,we get  \n\n$$\nC(s)\\,+\\,G_{1}G_{p}G_{c}H C(s)\\,=\\,G_{p}D(s)\\,+\\,G_{1}G_{p}\\bigl(G_{f}\\,+\\,G_{c}\\bigr)R(s)\n$$  \n\nHence  \n\n$$\nC(s)\\,={\\frac{G_{p}D(s)\\,+\\,G_{1}G_{p}\\bigl(G_{f}\\,+\\,G_{c}\\bigr)R(s)}{1\\,+\\,G_{1}G_{p}G_{c}H}}\n$$  \n\nNote that Equation (2–51) gives the response $C(s)$ when both reference input $R(s)$ and disturbance input $D(s)$ are present.  \n\nTo find transfer function $C(s)/R(s)$ ,we let $D(s)\\,=\\,0$ in Equation (2–51).Then we obtain  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G_{1}G_{p}(G_{f}\\,+\\,G_{c})}{1\\,+\\,G_{1}G_{p}G_{c}H}\n$$  \n\nSimilarly, to obtain transfer function $C(s)/D(s)$ ,we let $R(s)\\,=\\,0$ in Equation (2–51). Then $C(s)/D(s)$ can be given by  \n\n$$\n\\frac{C(s)}{D(s)}=\\frac{G_{p}}{1\\,+\\,G_{1}G_{p}G_{c}H}\n$$  \n\nA–2–5. Figure 2–24 shows a system with two inputs and two outputs. Derive $C_{1}(s)/R_{1}(s)$ ,$C_{1}(s)/R_{2}(s)$ ,$C_{2}(s)/R_{1}(s)$ ,and $C_{2}(s)/R_{2}(s)$ .(In deriving outputs for $R_{1}(s)$ ,assume that $R_{2}(s)$ is zero, and vice versa.)  \n\n![](images/a2fc65a29123d4443f1884d92ed1352b0cdf39cddf9e99b705d2e4c0ff2ad5e6.jpg)  \nFigure 2–24 System with two inputs and two outputs.  \n\nSolution. From the figure, we obtain  \n\n$$\n\\begin{array}{c}{{C_{1}=G_{1}\\!\\left(R_{1}-G_{3}C_{2}\\right)}}\\\\ {{{}}}\\\\ {{C_{2}=G_{4}\\!\\left(R_{2}-G_{2}C_{1}\\right)}}\\end{array}\n$$  \n\nBy substituting Equation (2–53) into Equation (2–52), we obtain  \n\n$$\nC_{1}=G_{1}\\big[R_{1}\\,-\\,G_{3}G_{4}\\big(R_{2}\\,-\\,G_{2}C_{1}\\big)\\big]\n$$  \n\nBy substituting Equation (2–52) into Equation (2–53), we get  \n\n$$\nC_{2}=G_{4}[R_{2}-G_{2}G_{1}(R_{1}-G_{3}C_{2})]\n$$  \n\nSolving Equation (2–54) for $C_{1}$ ,we obtain  \n\n$$\nC_{1}={\\frac{G_{1}R_{1}-G_{1}G_{3}G_{4}R_{2}}{1-G_{1}G_{2}G_{3}G_{4}}}\n$$  \n\nSolving Equation (2–55) for $C_{2}$ gives  \n\n$$\nC_{2}=\\frac{-G_{1}G_{2}G_{4}R_{1}\\:+\\:G_{4}R_{2}}{1\\:-\\:G_{1}G_{2}G_{3}G_{4}}\n$$  \n\nEquations (2–56) and (2–57) can be combined in the form of the transfer matrix as follows:  \n\n$$\n\\left[C_{1}\\right]=\\left[\\begin{array}{l l}{\\displaystyle{\\frac{G_{1}}{1-G_{1}G_{2}G_{3}G_{4}}}}&{\\displaystyle{-\\frac{G_{1}G_{3}G_{4}}{1-G_{1}G_{2}G_{3}G_{4}}}}\\\\ {\\displaystyle{-\\frac{G_{1}G_{2}G_{4}}{1-G_{1}G_{2}G_{3}G_{4}}}}&{\\displaystyle{\\frac{G_{4}}{1-G_{1}G_{2}G_{3}G_{4}}}}\\end{array}\\right]\\left[\\!\\!\\sum_{R_{2}}\\right]\n$$  \n\nThen the transfer functions $C_{1}(s)/R_{1}(s),C_{1}(s)/R_{2}(s),C_{2}(s)/R_{1}(s)$ and $C_{2}(s)/R_{2}(s)$ can be obtained as follows:  \n\n$$\n\\begin{array}{l l}{{\\displaystyle{\\frac{C_{1}(s)}{R_{1}(s)}=\\frac{G_{1}}{1\\,-\\,G_{1}G_{2}G_{3}G_{4}},\\quad}}}&{{\\displaystyle{\\frac{C_{1}(s)}{R_{2}(s)}=-\\frac{G_{1}G_{3}G_{4}}{1\\,-\\,G_{1}G_{2}G_{3}G_{4}}}}}\\\\ {{\\displaystyle{\\frac{C_{2}(s)}{R_{1}(s)}=-\\frac{G_{1}G_{2}G_{4}}{1\\,-\\,G_{1}G_{2}G_{3}G_{4}},\\quad}}}&{{\\displaystyle{\\frac{C_{2}(s)}{R_{2}(s)}=\\frac{G_{4}}{1\\,-\\,G_{1}G_{2}G_{3}G_{4}}}}}\\end{array}\n$$  \n\nNote that Equations (2–56) and (2–57) give responses $C_{1}$ and $C_{2}$ ,respectively, when both inputs $R_{1}$ and $R_{2}$ are present.  \n\nNotice that when $R_{2}(s)\\,=\\,0$ ,the original block diagram can be simplified to those shown in Figures 2–25(a) and (b). Similarly, when $R_{1}(s)\\,=\\,0$ ,the original block diagram can be simplified to those shown in Figures 2–25(c) and (d). From these simplified block diagrams we can also obtain $C_{1}(s)/R_{1}(s)$ ,$C_{2}(s)/R_{1}(s)$ ,$C_{1}(s)/R_{2}(s)$ ,and $C_{2}(s)/R_{2}(s)$ ,as shown to the right of each corresponding block diagram.  \n\n![](images/3df9832b0b3b22b93dcff24f731b94b0c636e9322ba42e0ba91492ebc903045d.jpg)  \nFigure 2–25 Simplified block diagrams and corresponding closed-loop transfer functions.  \n\nA–2–6. Show that for the differential equation system  \n\n$$\n\\dddot{y}\\ +\\ a_{1}\\ddot{y}\\ +\\ a_{2}\\dot{y}\\ +\\ a_{3}y\\,=\\,b_{0}\\ddot{u}\\,+\\,b_{1}\\dot{u}\\,+\\,b_{2}\\dot{u}\\,+\\,b_{3}u\n$$  \n\nstate and output equations can be given, respectively, by  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\beta_{1}}\\\\ {\\beta_{2}}\\\\ {\\beta_{3}}\\end{array}\\right]}u\n$$  \n\nand  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\,+\\,\\beta_{0}u\n$$  \n\nwhere state variables are defined by  \n\n$$\n\\begin{array}{l}{{x_{1}=y\\,-\\,\\beta_{0}u}}\\\\ {{x_{2}=\\dot{y}\\,-\\,\\beta_{0}\\dot{u}\\,-\\,\\beta_{1}u\\,=\\,\\dot{x}_{1}\\,-\\,\\beta_{1}u}}\\\\ {{x_{3}=\\dot{y}\\,-\\,\\beta_{0}\\dot{u}\\,-\\,\\beta_{1}\\dot{u}\\,-\\,\\beta_{2}u\\,=\\,\\dot{x}_{2}\\,-\\,\\beta_{2}u}}\\end{array}\n$$  \n\nand  \n\n$$\n\\begin{array}{l}{\\beta_{0}=b_{0}}\\\\ {\\beta_{1}=b_{1}-a_{1}\\beta_{0}}\\\\ {\\beta_{2}=b_{2}-a_{1}\\beta_{1}-a_{2}\\beta_{0}}\\\\ {\\beta_{3}=b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0}}\\end{array}\n$$  \n\nSolution. From the definition of state variables $x_{2}$ and $x_{3}$ ,we have  \n\n$$\n\\begin{array}{l}{{\\dot{x}}_{1}=x_{2}+\\beta_{1}u}\\\\ {~~}\\\\ {{\\dot{x}}_{2}=x_{3}+\\beta_{2}u}\\end{array}\n$$  \n\nTo derive the equation for $\\dot{x}_{3}$ #,we first note from Equation (2–58) that  \n\n$$\n\\dddot{y}\\,=-a_{1}\\ddot{y}\\,-\\,a_{2}\\dot{y}\\,-\\,a_{3}y\\,+\\,b_{0}\\dddot{u}\\,+\\,b_{1}\\dot{u}\\,+\\,b_{2}\\dot{u}\\,+\\,b_{3}u\n$$  \n\nSince  \n\n$$\nx_{3}\\,=\\,\\\"_{y}\\,-\\,\\beta_{0}i i\\,-\\,\\beta_{1}\\dot{u}\\,-\\,\\beta_{2}u\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}{\\dot{x}_{3}=\\mathcal{Y}-\\beta_{0}\\tilde{u}-\\beta_{1}\\dot{u}-\\beta_{2}\\dot{u}}\\\\ {=(-a_{1}\\dot{y}-a_{2}\\dot{y}-a_{3}\\dot{y})+b_{0}\\tilde{u}+b_{1}\\dot{u}+b_{2}\\dot{u}+b_{3}u-\\beta_{0}\\tilde{u}-\\beta_{1}\\dot{u}-\\beta_{2}\\dot{u}}\\\\ {=-a_{1}(\\dot{y}-\\beta_{0}\\dot{u}-\\beta_{1}\\dot{u}-\\beta_{2}u)-a_{1}\\beta_{0}\\dot{u}-a_{1}\\beta_{1}\\dot{u}-a_{1}\\beta_{2}u}\\\\ {-a_{2}(\\dot{y}-\\beta_{0}\\dot{u}-\\beta_{1}u)-a_{2}\\beta_{0}\\dot{u}-a_{2}\\beta_{1}u-a_{3}(\\dot{y}-\\beta_{0}u)-a_{3}\\beta_{0}u}\\\\ {+b_{0}\\ddot{u}+b_{1}\\dot{u}+b_{2}\\dot{u}+b_{3}u-\\beta_{0}\\ddot{u}-\\beta_{1}\\dot{u}-\\beta_{2}\\dot{u}}\\\\ {=-a_{1}x_{3}-a_{2}x_{2}-a_{3}x_{1}+(b_{0}-\\beta_{0})\\tilde{u}+(b_{1}-\\beta_{1}-a_{1}b_{0})\\dot{u}}\\\\ {+(b_{2}-\\beta_{2}-a_{1}\\beta_{1}-a_{2}\\beta_{0})\\dot{u}+(b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0})u}\\\\ {=-a_{1}x_{3}-a_{2}x_{2}-a_{3}x_{1}+(b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0})u}\\\\ {=-a_{1}x_{3}-a_{2}x_{2}-a_{3}x_{1}+(b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0})u}\\\\ {=-a_{1}x_{3}-a_{2}x_{2}-a_{3}x_{1}+\\beta_{3}u}\\end{array}\n$$  \n\nHence, we get  \n\n$$\n\\dot{x}_{3}=-a_{3}x_{1}\\,-\\,a_{2}x_{2}\\,-\\,a_{1}x_{3}\\,+\\,\\beta_{3}u\n$$  \n\nCombining Equations (2–61), (2–62), and (2–63) into a vector-matrix equation, we obtain Equation (2–59). Also, from the definition of state variable $x_{1}$ ,we get the output equation given by Equation (2–60).  \n\nA–2–7. Obtain a state-space equation and output equation for the system defined by  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{2s^{3}\\,+\\,s^{2}\\,+\\,s\\,+\\,2}{s^{3}\\,+\\,4s^{2}\\,+\\,5s\\,+\\,2}}\n$$  \n\nSolution. From the given transfer function, the differential equation for the system is  \n\n$$\n\\dddot{y}\\,+\\,4\\ddot{y}\\,+\\,5\\dot{y}\\,+\\,2y\\,=\\,2\\ddot{u}\\,+\\,\\dot{u}\\,+\\,\\dot{u}\\,+\\,2u\n$$  \n\nComparing this equation with the standard equation given by Equation (2–33), rewritten  \n\n$$\n\\dddot{y}\\,+\\,a_{1}\\ddot{y}\\,+\\,a_{2}\\dot{y}\\,+\\,a_{3}y\\,=\\,b_{0}\\ddot{u}\\,+\\,b_{1}\\ddot{u}\\,+\\,b_{2}\\dot{u}\\,+\\,b_{3}u\n$$  \n\nwe find  \n\n$$\n\\begin{array}{l l l}{{a_{1}=4,\\quad}}&{{a_{2}=5,\\quad}}&{{a_{3}=2}}\\\\ {{\\quad}}&{{b_{0}=2,\\quad}}&{{b_{1}=1,\\quad}}&{{b_{2}=1,\\quad}}&{{b_{3}=2}}\\end{array}\n$$  \n\nReferring to Equation (2–35), we have  \n\n$$\n\\begin{array}{r l}&{\\beta_{0}=b_{0}=2}\\\\ &{\\beta_{1}=b_{1}-a_{1}\\beta_{0}=1-4\\times2=-7}\\\\ &{\\beta_{2}=b_{2}-a_{1}\\beta_{1}-a_{2}\\beta_{0}=1-4\\times(-7)-5\\times2=19}\\\\ &{\\beta_{3}=b_{3}-a_{1}\\beta_{2}-a_{2}\\beta_{1}-a_{3}\\beta_{0}}\\\\ &{\\quad=2-4\\times19-5\\times(-7)-2\\times2=-43}\\end{array}\n$$  \n\nReferring to Equation (2–34), we define  \n\n$$\n\\begin{array}{l}{{x_{1}=y\\,-\\,\\beta_{0}u=y\\,-\\,2u}}\\\\ {{\\ }}\\\\ {{x_{2}=\\dot{x}_{1}-\\,\\beta_{1}u=\\dot{x}_{1}+\\,7u}}\\\\ {{\\ }}\\\\ {{x_{3}=\\dot{x}_{2}-\\,\\beta_{2}u=\\dot{x}_{2}-\\,19u}}\\end{array}\n$$  \n\nThen referring to Equation (2–36),  \n\n$$\n\\begin{array}{l}{\\dot{x}_{1}=\\,x_{2}\\,-\\,7u}\\\\ {\\dot{x}_{2}=\\,x_{3}+\\,19u}\\\\ {\\dot{x}_{3}=-a_{3}x_{1}\\,-\\,a_{2}x_{2}\\,-\\,a_{1}x_{3}\\,+\\,\\beta_{3}u}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=-2x_{1}\\,-\\,5x_{2}\\,-\\,4x_{3}\\,-\\,43u}\\end{array}\n$$  \n\nHence, the state-space representation of the system is  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-2}&{-5}&{-4}{\\mathord{\\left[\\!\\!{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}+{\\left[\\begin{array}{l}{-7}\\\\ {19}\\\\ {-43}\\end{array}\\right]}}\\end{array}\\right]}}\\\\ {y={\\left[\\!\\!1\\right.}}&{0}&{0{\\mathord{\\left[\\!\\!{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}+2u}}\\end{array}\\right]}\\end{array}}\n$$  \n\nThis is one possible state-space representation of the system. There are many (infinitely many) others. If we use MATLAB, it produces the following state-space representation:  \n\n$$\n\\begin{array}{r l}{\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\hat{x}_{1}\\right]=\\left[\\phantom{\\frac{1}{\\hbar_{2}}}\\!\\!\\!\\!\\!-4\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!-5\\!\\!\\!\\!}&{-2\\left]\\!\\!\\!\\!\\left[\\phantom{\\frac{1}{\\hbar_{2}}}\\!\\!\\!\\!\\!\\left[x_{1}\\right]\\right]+\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\right]_{\\phantom{\\frac{1}{\\hbar_{2}}}}}\\\\ {\\phantom{\\frac{1}{\\hbar_{3}}}\\!\\!\\!\\!\\!\\!-\\vdots\\!\\!\\!\\!\\!}&{+\\left[\\phantom{\\frac{1}{\\hbar_{3}}}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\right]\\!\\!\\!\\!\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\right]\\!\\!\\!\\!\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!1\\right]}\\\\ {\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\!\\!\\!\\!\\!\\!}&{-2\\left[\\phantom{\\frac{1}{\\hbar_{2}}}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\right]\\!\\!\\!\\!\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!x_{1}\\right]+2u}\\\\ {\\phantom{\\frac{1}{\\hbar_{2}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\!\\!\\!\\!\\left[\\phantom{\\frac{1}{\\hbar_{2}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\right]\\!\\!\\!\\!\\!}&{+\\left[\\phantom{\\frac{1}{\\hbar_{1}}}\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!0\\right]}\\end{array}\n$$  \n\nSee MATLAB Program 2-4. (Note that all state-space representations for the same system are equivalent.)  \n\n![](images/d5e447b4ac0608dfdaddd65f04130507a9d7573de8fafe8c15a86bd48321dced.jpg)  \n\nA–2–8. Obtain a state-space model of the system shown in Figure 2–26.  \n\nSolution. The system involves one integrator and two delayed integrators. The output of each integrator or delayed integrator can be a state variable. Let us define the output of the plant as $x_{1}$ ,the output of the controller as $x_{2}$ ,and the output of the sensor as $x_{3}$ .Then we obtain  \n\n![](images/5774e4a4ff6fd4b674634a93df8ce45a36f95518ab35b5d24a6b92603d5632c0.jpg)  \n\nFigure 2–26 Control system.  \n\nwhich can be rewritten as  \n\n$$\n\\begin{array}{r l}&{s X_{1}(s)\\,=\\,-5X_{1}(s)\\,+\\,10X_{2}(s)}\\\\ &{s X_{2}(s)\\,=\\,-X_{3}(s)\\,+\\,U(s)}\\\\ &{s X_{3}(s)\\,=\\,X_{1}(s)\\,-\\,X_{3}(s)}\\\\ &{~~~\\,Y(s)\\,=\\,X_{1}(s)}\\end{array}\n$$  \n\nBy taking the inverse Laplace transforms of the preceding four equations, we obtain  \n\n$$\n\\begin{array}{r l}&{\\dot{x}_{1}=-5x_{1}+10x_{2}}\\\\ &{\\dot{x}_{2}=-x_{3}+u}\\\\ &{\\dot{x}_{3}=x_{1}-x_{3}}\\\\ &{\\dot{y}=x_{1}}\\end{array}\n$$  \n\nThus, a state-space model of the system in the standard form is given by  \n\n$$\n{\\begin{array}{r l}&{\\left[{\\begin{array}{c}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}}\\right]={\\left[\\begin{array}{c c c}{-5}&{10}&{0}\\\\ {0}&{0}&{-1}\\\\ {1}&{0}&{-1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {1}\\\\ {0}\\end{array}\\right]}u}\\\\ &{\\qquad\\qquad y=[1}&{0}&{0]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\end{array}}\n$$  \n\nIt is important to note that this is not the only state-space representation of the system. Infinitely many other state-space representations are possible. However, the number of state variables is the same in any state-space representation of the same system. In the present system, the number of state variables is three, regardless of what variables are chosen as state variables.  \n\nA–2–9. Obtain a state-space model for the system shown in Figure 2–27(a).  \n\nSolution. First, notice that $(a s\\,+\\,b)/s^{2}$ involves a derivative term. Such a derivative term may be avoided if we modify $(a s\\,+\\,b)/s^{2}$ as  \n\n$$\n{\\frac{a s\\,+\\,b}{s^{2}}}=\\left(a\\,+\\,{\\frac{b}{s}}\\right){\\frac{1}{s}}\n$$  \n\nUsing this modification, the block diagram of Figure 2–27(a) can be modified to that shown in Figure 2–27(b).  \n\nDefine the outputs of the integrators as state variables, as shown in Figure 2–27(b).Then from Figure 2–27(b) we obtain  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{X_{1}(s)}{X_{2}(s)\\ +\\ a\\big[U(s)\\ -\\ X_{1}(s)\\big]}=\\frac{1}{s}}}\\\\ &{}&{\\displaystyle\\frac{X_{2}(s)}{U(s)\\ -\\ X_{1}(s)}=\\frac{b}{s}}\\\\ &{}&{\\displaystyle Y(s)\\ =\\ X_{1}(s)=\\ X_{1}(s)}\\end{array}\n$$  \n\nwhich may be modified to  \n\n$$\n\\begin{array}{l}{{s X_{1}(s)\\,=\\,X_{2}(s)\\,+\\,a\\big[U(s)\\,-\\,X_{1}(s)\\big]}}\\\\ {{s X_{2}(s)\\,=\\,-b X_{1}(s)\\,+\\,b U(s)}}\\\\ {{Y(s)\\,=\\,X_{1}(s)}}\\end{array}\n$$  \n\n![](images/2228f2a3ddef942a505e0e84934cfa3a9d4e441766d9a3a2ebec9ecca6c54e2f.jpg)  \nFigure 2–27 (a) Control system; (b) modified block diagram.   \n(b)  \n\nTaking the inverse Laplace transforms of the preceding three equations, we obtain  \n\n$$\n\\begin{array}{r l}&{\\dot{x}_{1}=-a x_{1}+\\,x_{2}\\,+\\,a u}\\\\ &{\\dot{x}_{2}=-b x_{1}+\\,b u}\\\\ &{\\;\\;y=\\,x_{1}}\\end{array}\n$$  \n\nRewriting the state and output equations in the standard vector-matrix form, we obtain  \n\n$$\n\\begin{array}{r l}{\\bigg[\\dot{x}_{1}\\bigg]=\\bigg[\\!\\!-a\\!\\!}&{{}1\\!\\!\\bigg]\\!\\!\\bigg[\\!\\!\\!-x_{1}\\!\\!\\bigg]+\\bigg[\\!\\!\\!a\\!\\bigg]u}\\\\ {\\dot{x}_{2}\\!\\!\\bigg]=\\bigg[\\!\\!\\!-b\\!\\!\\!}&{{}0\\!\\!\\bigg]\\!\\!\\bigg[\\!\\!\\!-x_{2}\\!\\!\\bigg]}\\\\ {y=[1\\!\\!\\!}&{{}0]\\!\\!\\bigg[\\!\\!\\!x_{1}\\!\\!\\bigg]}\\end{array}\n$$  \n\nA–2–10. Obtain a state-space representation of the system shown in Figure 2–28(a).  \n\nSolution. In this problem, first expand $(s\\,+\\,z)/(s\\,+\\,p)$ into partial fractions.  \n\n$$\n\\frac{s\\,+\\,z}{s\\,+\\,p}=1+\\frac{z\\,-\\,p}{s\\,+\\,p}\n$$  \n\nNext,convert $K/[s(s+a)]$ into the product of $K/s$ and $1/(s+a)$ .Then redraw the block diagram, as shown in Figure 2–28(b). Defining a set of state variables, as shown in Figure 2–28(b), we obtain the following equations:  \n\n$$\n\\begin{array}{r l}&{\\dot{x}_{1}=-a x_{1}+\\,x_{2}}\\\\ &{\\dot{x}_{2}=-K x_{1}+\\,K x_{3}+\\,K u}\\\\ &{\\dot{x}_{3}=-(z\\,-\\,p)x_{1}-\\,p x_{3}+\\,(z\\,-\\,p)u}\\\\ &{\\dot{y}=\\,x_{1}}\\end{array}\n$$  \n\n![](images/8b73aee6c10f5395f13fc456144d3c7ceb55d3c8eb6243a43ac7cab5599cb938.jpg)  \nFigure 2–28 (a) Control system; (b) block diagram defining state variables for the system.  \n\nRewriting gives  \n\n$$\n{\\begin{array}{r}{\\left[{\\begin{array}{c}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {-{\\dot{x}}_{3}}\\end{array}}\\right]={\\left[\\begin{array}{c c c}{-a}&{1}&{0}\\\\ {-K}&{0}&{K}\\\\ {-(z-p)}&{0}&{-p}\\end{array}\\right]}{\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{c}{0}\\\\ {K}\\\\ {z-p}\\end{array}\\right]}u}\\\\ {y=[1}&{0}&{0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nNotice that the output of the integrator and the outputs of the first-order delayed integrators $\\big[1/(s+a)$ and $(z\\,-\\,p)/(s\\,+\\,p)]$ osen as state variables. It is important to remember that the output of the block (s+z)/(s+p) in Figure 2–28(a) cannot be a state variable, because this block involves a derivative term, s+z.  \n\nA–2–11. Obtain the transfer function of the system defined by  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{c c c}{-1}&{1}&{0}\\\\ {0}&{-1}&{1}\\\\ {0}&{0}&{-2}\\end{array}\\right]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]\\!\\!u}\\\\ {y=[1}&{0}&{0]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\end{array}\n$$  \n\nSolution. Referring to Equation (2–29), the transfer function $G(s)$ is given by  \n\n$$\nG(s)\\,=\\,{\\bf C}(s{\\bf I}\\,-\\,{\\bf A})^{-1}{\\bf B}\\,+\\,D\n$$  \n\nIn this problem, matrices $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ , and $D$ are  \n\n$$\n\\begin{array}{r l}&{\\mathbf{A}=\\left[\\begin{array}{c c c}{-1}&{1}&{0}\\\\ {0}&{-1}&{1}\\\\ {0}&{0}&{-2}\\end{array}\\right],\\qquad\\mathbf{B}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {1}\\end{array}\\right],\\qquad\\mathbf{C}=[1}&{0}&{0],\\qquad D=0}\\end{array}\n$$  \n\nHence  \n\n$$\nG(s)=[1\\quad0\\quad0]{\\binom{\\textstyle s+1\\quad-1\\quad\\;\\;0}{\\textstyle0}}^{\\textstyle-1}{\\binom{\\textstyle0}{\\textstyle0}}^{\\textstyle-1}{\\binom{\\textstyle0}{\\textstyle0}}^{\\textstyle-1}{\\binom{\\textstyle0}{\\textstyle0}}^{\\textstyle-1}\n$$  \n\n$$\n\\begin{array}{r l}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{{}=[1\\!\\!\\!\\!\\!\\!\\!1\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!0\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$  \n\n$$\n={\\frac{1}{(s\\,+\\,1)^{2}(s\\,+\\,2)}}={\\frac{1}{s^{3}\\,+\\,4s^{2}\\,+\\,5s\\,+\\,2}}\n$$  \n\nA–2–12. Consider a system with multiple inputs and multiple outputs.When the system has more than one output, the MATLAB command  \n\n$$\n[\\mathsf{N U M},\\mathsf{d e n}]=\\mathsf{s s2t f}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{i}\\mathsf{u})\n$$  \n\nproduces transfer functions for all outputs to each input. (The numerator coefficients are returned   \nto matrix NUM with as many rows as there are outputs.) Consider the system defined by  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\;\\;\\;0}&{\\;\\;1{\\Biggl]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThis system involves two inputs and two outputs.Four transfer functions are involved: $Y_{1}(s)/U_{1}(s)$ ,$Y_{2}(s)/U_{1}(s),Y_{1}(s)/U_{2}(s)$ ,and $Y_{2}(s)/U_{2}(s)$ .(When considering input $u_{1}$ ,we assume that input $u_{2}$ is zero and vice versa.)  \n\nSolution. MATLAB Program 2-5 produces four transfer functions. This is the MATLAB representation of the following four transfer functions:  \n\n$$\n\\frac{Y_{1}(s)}{U_{1}(s)}=\\frac{s\\,+\\,4}{s^{2}\\,+\\,4s\\,+\\,25},\\qquad\\quad\\frac{Y_{2}(s)}{U_{1}(s)}=\\frac{-25}{s^{2}\\,+\\,4s\\,+\\,25}\n$$  \n\n$$\n\\frac{Y_{1}(s)}{U_{2}(s)}=\\frac{s\\,+\\,5}{s^{2}\\,+\\,4s\\,+\\,25},\\qquad\\quad\\frac{Y_{2}(s)}{U_{2}(s)}=\\frac{s\\,-\\,25}{s^{2}\\,+\\,4s\\,+\\,25}\n$$  \n\nA–2–13. Linearize the nonlinear equation  \n\n$$\nz\\,=\\,x^{2}\\,+\\,4x y\\,+\\,6y^{2}\n$$  \n\nin the region defined by $8\\leq{x}\\leq10,2\\leq{y}\\leq4$ .  \n\nSolution. Define  \n\n$$\nf(x,y)\\,=\\,z\\,=\\,x^{2}\\,+\\,4x y\\,+\\,6y^{2}\n$$  \n\nThen  \n\n$$\nz=f(x,y)=f(\\bar{x},\\bar{y})\\,+\\,{\\left[{\\frac{\\partial f}{\\partial x}}\\,(x\\,-\\,\\bar{x})\\,+\\,{\\frac{\\partial f}{\\partial y}}\\,(y\\,-\\,\\bar{y})\\,\\right]}_{x=\\bar{x},\\,y=\\bar{y}}\\,+\\,\\cdots\n$$  \n\nwhere we choose $\\Bar{x}\\,=\\,9,\\,\\Bar{y}\\,=\\,3$ .  \n\nSince the higher-order terms in the expanded equation are small, neglecting these higherorder terms, we obtain  \n\n$$\nz\\,-\\,{\\bar{z}}\\,=\\,K_{1}(\\,x\\,-\\,{\\bar{x}}\\,)\\,+\\,K_{2}(\\,y\\,-\\,{\\bar{y}}\\,)\n$$  \n\nwhere  \n\n$$\nK_{1}=\\frac{\\partial f}{\\partial x}\\left|_{x=\\bar{x},\\,y=\\bar{y}}\\,=2\\bar{x}\\,+\\,4\\bar{y}\\,=2\\times\\,9\\,+\\,4\\times\\,3\\,=\\,30\n$$  \n\n$$\nK_{2}=\\frac{\\partial f}{\\partial y}\\left|_{x=\\bar{x},\\,y=\\bar{y}}\\,=4\\bar{x}\\,+\\,12\\bar{y}\\,=4\\,\\times\\,9\\,+\\,12\\,\\times\\,3\\,=\\,72\n$$  \n\n$$\n\\bar{z}\\,=\\,\\bar{x}^{2}\\,+\\,4\\bar{x}\\bar{y}\\,+\\,6\\bar{y}^{2}\\,=\\,9^{2}\\,+\\,4\\,\\times\\,9\\,\\times\\,3\\,+\\,6\\,\\times\\,9\\,=\\,243\n$$  \n\nThus  \n\n$$\nz\\mathrm{~-~}243\\mathrm{~=~}30(x\\mathrm{~-~}9)\\mathrm{~+~}72(y\\mathrm{~-~}3)\n$$  \n\nHence a linear approximation of the given nonlinear equation near the operating point is  \n\n$$\nz\\,-\\,30x\\,-\\,72y\\,+\\,243\\,=\\,0\n$$  \n\n# PROBLEMS  \n\nB–2–1. Simplify the block diagram shown in Figure 2–29 and obtain the closed-loop transfer function $C(s)/R(s)$ .  \n\n![](images/f9c1505b6a26b1af45cb55e95404c91e3bd99ac009aeb648c95fb523d5fcc14c.jpg)  \nFigure 2–29 Block diagram of a system.  \n\nB–2–2. Simplify the block diagram shown in Figure 2–30 and obtain the closed-loop transfer function $C(s)/R(s)$ .  \n\nB–2–3. Simplify the block diagram shown in Figure 2–31 and obtain the closed-loop transfer function $C(s)/R(s)$ .  \n\n![](images/d287e24d68fa7bcdb53629ba6508c5c44cb68ea4a2962c477d6a5752c39037cd.jpg)  \n\nFigure 2–30 Block diagram of a system.  \n\n![](images/9b5f14675048f686b3c14b546af593363b77843f10014a8971cf01fba7818711.jpg)  \n\nFigure 2–31 Block diagram of a system.  \n\nB–2–4. Consider industrial automatic controllers whose control actions are proportional,integral,proportional-plusintegral,proportional-plus-derivative,and proportional-plusintegral-plus-derivative. The transfer functions of these controllers can be given, respectively, by  \n\n$$\n{\\begin{array}{r l}&{{\\frac{U(s)}{E(s)}}=K_{p}}\\\\ &{{\\frac{U(s)}{E(s)}}={\\frac{K_{i}}{s}}}\\\\ &{{\\frac{U(s)}{E(s)}}=K_{p}\\!\\left(1+{\\frac{1}{T_{i}s}}\\right)}\\\\ &{{\\frac{U(s)}{E(s)}}=K_{p}(1+T_{i}s)}\\\\ &{{\\frac{U(s)}{E(s)}}=K_{p}\\!\\left(1+{\\frac{1}{T_{i}s}}+T_{i}s\\right)}\\end{array}}\n$$  \n\ning error signal. Sketch $u(t)$ -versus$\\cdot t$ curves for each of the five types of controllers when the actuating error signal is  \n\n(a) $e(t)=$ unit-step function (b) $e(t)=$ unit-ramp function  \n\nIn sketching curves, assume that the numerical values of $K_{p}$ ,$K_{i},T_{i}$ ,and $T_{d}$ are given as  \n\n$$\n\\begin{array}{l}{K_{p}=\\mathrm{proportional\\;gain}\\,=\\,4}\\\\ {K_{i}=\\mathrm{integral\\;gain}\\,=\\,2}\\\\ {T_{i}=\\mathrm{integral\\;time}\\,=\\,2\\;\\mathrm{sec}}\\\\ {T_{d}=\\mathrm{derivative\\;time}\\,=\\,0.8\\;\\mathrm{sec}}\\end{array}\n$$  \n\nwhere $U(s)$ is the Laplace transform of $u(t)$ ,the controller output, and $E(s)$ the Laplace transform of $e(t)$ ,the actuatB–2–5. Figure 2–32 shows a closed-loop system with a reference input and disturbance input. Obtain the expression for the output $C(s)$ when both the reference input and disturbance input are present.  \n\nB–2–6. Consider the system shown in Figure 2–33. Derive the expression for the steady-state error when both the reference input $R(s)$ and disturbance input $D(s)$ are present.  \n\nB–2–7. Obtain the transfer functions $C(s)/R(s)$ and $C(s)/D(s)$ of the system shown in Figure 2–34.  \n\n![](images/e026df8c57d701cb27324269b2a2a66e18f067e38629757fea9182e2dcf6232d.jpg)  \nFigure 2–32 Closed-loop system.  \n\n![](images/a2096789718aa563df27cd625108486cce400041d62197b8286888bfb1bbb7ad.jpg)  \n\nFigure 2–33 Control system.  \n\n![](images/3bfa1972240a6e7426776c4a4ef7fa3e7952e8eb2dbba10732deaffd04b2e60f.jpg)  \n\nFigure 2–34 Control system.  \n\nB–2–8. Obtain a state-space representation of the system shown in Figure 2–35.  \n\n![](images/636334b27655820bf1d6ccb404d1962b3ded321d0d818df1a8c2f95e58cf04e3.jpg)  \nFigure 2–35 Control system.  \n\nB–2–9. Consider the system described by  \n\n$$\n\\dddot{y}\\,+\\,3\\ddot{y}\\,+\\,2\\dot{y}\\,=\\,u\n$$  \n\nDerive a state-space representation of the system.  \n\nB–2–10. Consider the system described by  \n\n$$\n\\begin{array}{r l}{\\left[\\dot{x}_{1}\\right]=\\left[-4\\!\\!\\!}&{{}\\!\\!\\!-1\\!\\!\\!}\\\\ {\\dot{x}_{2}\\right]=\\left[\\!\\!\\!}&{{}\\!\\!\\!3\\!\\!\\!}&{{}\\!\\!\\!\\!-1\\right]\\!\\!\\!\\left[\\phantom{\\frac{x_{1}}{x_{2}}}\\!\\!\\!\\!}\\\\ {\\qquad\\!\\!}&{{}\\!\\!\\!\\!y=[1\\!\\!\\!\\!}&{{}\\!\\!\\!\\!0]\\!\\!\\!\\left[\\phantom{\\frac{x_{1}}{x_{1}}}\\!\\!\\!\\!\\right]}\\end{array}\\right]+\\left[\\!\\!\\!\\!1\\right]\\!\\!\\!u}\\\\ {y=[1\\!\\!\\!\\!}&{{}\\!\\!\\!\\!0]\\!\\!\\left[\\phantom{\\frac{x_{1}}{x_{2}}}\\!\\!\\!\\!\\!}\\\\ {x_{2}\\right]}\\end{array}\n$$  \n\nObtain the transfer function of the system.  \n\nB–2–11. Consider a system defined by the following statespace equations:  \n\n$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{r}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{r r}{-5}&{-1}\\\\ {3}&{-1}\\\\ {0}&{0}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\!\\begin{array}{r}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{r}{2}\\\\ {5}\\end{array}\\!\\!\\right]\\!\\!u}\\\\ {y=[1}&{2]\\!\\!\\left[\\!\\!\\begin{array}{r}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}\\end{array}\n$$  \n\nObtain the transfer function $G(s)$ of the system.  \n\nB–2–12. Obtain the transfer matrix of the system defined by  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]=\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-2}&{-4}&{-6}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{1}\\\\ {1}&{0}\\end{array}\\right]\\!\\!\\left[\\!u_{1}\\!\\right]}\\\\ &{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]=\\left[\\!\\!\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {-2}&{-4}&{-6}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\end{array}\n$$  \n\nB–2–13. Linearize the nonlinear equation  \n\n$$\nz\\,=\\,x^{2}\\,+\\,8x y\\,+\\,3y^{2}\n$$  \n\nin the region defined by $2\\leq x\\leq4,10\\leq y\\leq12.$  \n\nB–2–14. Find a linearized equation for  \n\n$$\ny=0.2x^{3}\n$$  \n\nabout a point $x\\,=\\,2$ .  \n\n# Mathematical Modeling of Mechanical Systems and Electrical Systems  \n\n3–1 INTRODUCTION  \n\nThis chapter presents mathematical modeling of mechanical systems and electrical systems. In Chapter 2 we obtained mathematical models of a simple electrical circuit and a simple mechanical system. In this chapter we consider mathematical modeling of a variety of mechanical systems and electrical systems that may appear in control systems.  \n\nThe fundamental law govering mechanical systems is Newton’s second law. In Section 3–2 we apply this law to various mechanical systems and derive transferfunction models and state-space models.  \n\nThe basic laws governing electrical circuits are Kirchhoff’s laws. In Section 3–3 we obtain transfer-function models and state-space models of various electrical circuits and operational amplifier systems that may appear in many control systems.  \n\n# 3–2 MATHEMATICAL MODELING OF MECHANICAL SYSTEMS  \n\nThis section first discusses simple spring systems and simple damper systems. Then we derive transfer-function models and state-space models of various mechanical systems.  \n\n![](images/d24c20ac03d9cc63e324f00b1fa0c91a3c2433213aa955476923be05e9509bd8.jpg)  \nFigure 3–1 (a) System consisting of two springs in parallel; (b) system consisting of two springs in series.  \n\n# EXAMPLE 3–1  \n\nLet us obtain the equivalent spring constants for the systems shown in Figures 3–1(a) and (b), respectively.  \n\nFor the springs in parallel [Figure 3–1(a)] the equivalent spring constant $k_{\\mathrm{eq}}$ is obtained from  \n\n$$\nk_{1}x\\,+\\,k_{2}x\\,=\\,F\\,=\\,k_{\\mathrm{eq}}x\n$$  \n\nor  \n\n$$\nk_{\\mathrm{eq}}=k_{1}+k_{2}\n$$  \n\nFor the springs in series [Figure–3–1(b)], the force in each spring is the same.Thus  \n\n$$\nk_{1}y\\,=\\,F,\\qquad k_{2}(x\\,-\\,y)\\,=\\,F\n$$  \n\nElimination of $y$ from these two equations results in  \n\n$$\nk_{2}\\bigg(x\\,-\\frac{F}{k_{1}}\\bigg)\\,=\\,F\n$$  \n\nor  \n\n$$\nk_{2}x\\,=\\,F\\,+\\frac{k_{2}}{k_{1}}\\,F\\,=\\frac{k_{1}\\,+\\,k_{2}}{k_{1}}\\,F\n$$  \n\nThe equivalent spring constant $k_{\\mathrm{eq}}$ for this case is then found as  \n\n$$\nk_{\\mathrm{eq}}={\\frac{F}{x}}={\\frac{k_{1}k_{2}}{k_{1}+k_{2}}}={\\frac{1}{{\\frac{1}{k_{1}}}+{\\frac{1}{k_{2}}}}}\n$$  \n\n# EXAMPLE 3–2  \n\nLet us obtain the equivalent viscous-friction coefficient $b_{\\mathrm{eq}}$ for each of the damper systems shown in Figures 3–2(a) and (b).An oil-filled damper is often called a dashpot.A dashpot is a device that provides viscous friction,or damping.It consists of a piston and oil-filled cylinder.Any relative motion between the piston rod and the cylinder is resisted by the oil because the oil must flow around the piston (or through orifices provided in the piston) from one side of the piston to the other.The dashpot essentially absorbs energy.This absorbed energy is dissipated as heat,and the dashpot does not store any kinetic or potential energy.  \n\n![](images/c90e8f2da89c0342bf20df420adb7b3996bc0087e5a3be6b1f31c3b54e9a4a88.jpg)  \nFigure 3–2 (a) Two dampers connected in parallel; (b) two dampers connected in series.  \n\n(a) The force $f$ due to the dampers is  \n\n$$\nf=b_{1}\\,(\\dot{y}\\,-\\,\\dot{x})\\,+\\,b_{2}(\\dot{y}\\,-\\,\\dot{x})\\,=\\,\\bigl(b_{1}\\,+\\,b_{2}\\bigr)(\\dot{y}\\,-\\,\\dot{x})\n$$  \n\nIn terms of the equivalent viscous-friction coefficient $b_{\\mathrm{eq}}$ , force $f$ is given by  \n\n$$\nf\\,=\\,b_{\\mathrm{eq}}(\\dot{y}\\,-\\,\\dot{x})\n$$  \n\nHence  \n\n$$\nb_{\\mathrm{eq}}\\,=\\,b_{1}\\,+\\,b_{2}\n$$  \n\n(b) The force $f$ due to the dampers is  \n\n$$\nf\\,=\\,b_{1}(\\dot{z}\\,-\\,\\dot{x})\\,=\\,b_{2}\\,(\\dot{y}\\,-\\,\\dot{z})\n$$  \n\nwhere $z$ is the displacement of a point between damper $b_{1}$ and damper $b_{2}$ .(Note that the same force is transmitted through the shaft.) From Equation (3–1), we have  \n\n$$\n\\bigl(b_{1}\\,+\\,b_{2}\\bigr)\\dot{z}\\,=\\,b_{2}\\dot{y}\\,+\\,b_{1}\\dot{x}\n$$  \n\nor  \n\n$$\n\\dot{z}\\,=\\,\\frac{1}{b_{1}\\,+\\,b_{2}}\\left(b_{2}\\dot{y}\\,+\\,b_{1}\\dot{x}\\right)\n$$  \n\nIn terms of the equivalent viscous-friction coefficient $b_{\\mathrm{eq}}$ ,force $f$ is given by  \n\n$$\nf\\,=\\,b_{\\mathrm{eq}}\\big(\\dot{y}\\,-\\,\\dot{x}\\big)\n$$  \n\nBy substituting Equation (3–2) into Equation (3–1), we have  \n\n$$\n\\begin{array}{l}{\\displaystyle{f=b_{2}\\big(\\dot{y}\\,-\\,\\dot{z}\\big)\\,=\\,b_{2}\\bigg[\\dot{y}\\,-\\,\\frac{1}{b_{1}\\,+\\,b_{2}}\\big(b_{2}\\dot{y}\\,+\\,b_{1}\\dot{x}\\big)\\bigg]}}\\\\ {\\displaystyle{=\\frac{b_{1}b_{2}}{b_{1}\\,+\\,b_{2}}\\,(\\dot{y}\\,-\\,\\dot{x})}}\\end{array}\n$$  \n\nThus,  \n\n$$\nf\\,=\\,b_{\\mathrm{eq}}(\\dot{y}\\,-\\,\\dot{x})\\,=\\frac{b_{1}b_{2}}{b_{1}\\,+\\,b_{2}}\\,(\\dot{y}\\,-\\,\\dot{x})\n$$  \n\nHence,  \n\n$$\nb_{\\mathrm{eq}}={\\frac{b_{1}b_{2}}{b_{1}+b_{2}}}={\\frac{1}{{\\frac{1}{b_{1}}}+{\\frac{1}{b_{2}}}}}\n$$  \n\nConsider the spring-mass-dashpot system mounted on a massless cart as shown in Figure 3–3. Let us obtain mathematical models of this system by assuming that the cart is standing still for $t<0$ and the spring-mass-dashpot system on the cart is also standing still for $t<0,$ .In this system, $u(t)$ is the displacement of the cart and is the input to the system. $\\mathrm{At}\\,t\\,=\\,0$ ,the cart is moved at a constant speed, or $\\dot{u}=$ #constant.The displacement $y(t)$ of the mass is the output. (The displacement is relative to the ground.) In this system, $_m$ denotes the mass, $^b$ denotes the viscous-friction coefficient,and $k$ denotes the spring constant.We assume that the friction force of the dashpot is proportional to $\\dot{y}\\mathrm{~-~}\\dot{u}$ and that the spring is a linear spring; that is, the spring force is proportional to $y\\mathrm{~-~}u$ .  \n\nFor translational systems, Newton’s second law states that  \n\n$$\nm a=\\sum F\n$$  \n\nwhere $_m$ is a mass, $a$ is the acceleration of the mass, and $\\Sigma F$ is the sum of the forces acting on the mass in the direction of the acceleration $a$ .Applying Newton’s second law to the present system and noting that the cart is massless, we obtain  \n\n$$\nm\\,{\\frac{d^{2}y}{d t^{2}}}=-b\\biggl({\\frac{d y}{d t}}-{\\frac{d u}{d t}}\\biggr)\\,-\\,k(y\\,-\\,u)\n$$  \n\nor  \n\n$$\nm\\,{\\frac{d^{2}y}{d t^{2}}}+\\,b\\,{\\frac{d y}{d t}}+\\,k y\\,=\\,b\\,{\\frac{d u}{d t}}+\\,k u\n$$  \n\nThis equation represents a mathematical model of the system considered. Taking the Laplace transform of this last equation, assuming zero initial condition, gives  \n\n$$\n(m s^{2}+b s\\,+\\,k)Y(s)\\,=\\,(b s\\,+\\,k)U(s)\n$$  \n\nTaking the ratio of $Y(s)$ to $U(s)$ , we find the transfer function of the system to be  \n\n$$\n{\\mathrm{Transfer~function}}=G(s)={\\frac{Y(s)}{U(s)}}={\\frac{b s\\,+\\,k}{m s^{2}\\,+\\,b s\\,+\\,k}}\n$$  \n\nSuch a transfer-function representation of a mathematical model is used very frequently in control engineering.  \n\n![](images/54afe08bda6f6f1227e833f38069a8402138ecaa5c1ebcf794582a42a1edffc3.jpg)  \nFigure 3–3 Spring-massdashpot system mounted on a cart.   \nChapter 3 /Mathematical Modeling of Mechanical Systems and Electrical Systems  \n\nNext we shall obtain a state-space model of this system. We shall first compare the differential equation for this system  \n\n$$\n{\\ddot{y}}\\,+{\\frac{b}{m}}\\,{\\dot{y}}\\,+{\\frac{k}{m}}\\,y\\,={\\frac{b}{m}}\\,{\\dot{u}}\\,+{\\frac{k}{m}}\\,u\n$$  \n\nwith the standard form  \n\n$$\n\\ddot{y}\\,+\\,a_{1}\\dot{y}\\,+\\,a_{2}y\\,=\\,b_{0}\\dot{u}\\,+\\,b_{1}\\dot{u}\\,+\\,b_{2}u\n$$  \n\nand identify $a_{1},a_{2},b_{0},b_{1}$ ,and $b_{2}$ as follows:  \n\n$$\na_{1}=\\frac{b}{m},\\qquad a_{2}=\\frac{k}{m},\\qquad b_{0}=0,\\qquad b_{1}=\\frac{b}{m},\\qquad b_{2}=\\frac{k}{m}\n$$  \n\nReferring to Equation (3–35), we have  \n\n$$\n\\begin{array}{l}{\\displaystyle\\beta_{0}=b_{0}=0}\\\\ {\\displaystyle\\beta_{1}=b_{1}-a_{1}\\beta_{0}=\\frac{b}{m}}\\\\ {\\displaystyle\\beta_{2}=b_{2}-a_{1}\\beta_{1}-a_{2}\\beta_{0}=\\frac{k}{m}-\\left(\\frac{b}{m}\\right)^{2}}\\end{array}\n$$  \n\nThen, referring to Equation (2–34), define  \n\n$$\n\\begin{array}{l}{{x_{1}=y\\,-\\,\\beta_{0}u\\,=\\,y}}\\\\ {{\\ }}\\\\ {{x_{2}=\\,\\dot{x}_{1}-\\,\\beta_{1}u=\\,\\dot{x}_{1}-\\frac{b}{m}\\,u}}\\end{array}\n$$  \n\nFrom Equation (2–36) we have  \n\n$$\n\\begin{array}{l}{{\\dot{x}}_{1}=\\,x_{2}\\,+\\,\\beta_{1}u\\,=\\,x_{2}\\,+\\frac{b}{m}\\,u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {{\\dot{x}}_{2}=-a_{2}x_{1}\\,-\\,a_{1}x_{2}\\,+\\,\\beta_{2}u=-\\frac{k}{m}\\,x_{1}\\,-\\frac{b}{m}\\,x_{2}\\,+\\,\\left[\\frac{k}{m}\\,-\\,\\left(\\frac{b}{m}\\right)^{2}\\right]u}\\end{array}\n$$  \n\nand the output equation becomes  \n\n$$\ny\\,=\\,x_{1}\n$$  \n\nor  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {\\displaystyle-{\\frac{k}{m}}}&{\\displaystyle-{\\frac{b}{m}}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{c}{\\displaystyle{\\frac{b}{m}}}\\\\ {\\displaystyle{\\frac{k}{m}}-\\left({\\frac{b}{m}}\\right)^{2}}\\end{array}\\right]}u\n$$  \n\nand  \n\n$$\ny\\,=\\,[1\\qquad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nEquations (3–3) and (3–4) give a state-space representation of the system. (Note that this is not the only state-space representation.There are infinitely many state-space representations for the system.)  \n\n![](images/7b15223154e87f6e5985844475d5674348e1f2bc4f4f7cfad11254066b72fec5.jpg)  \nFigure 3–4 Mechanical system.  \n\n# EXAMPLE 3–4  \n\nObtain the transfer functions $X_{1}(s)/U(s)$ and $X_{2}(s)/U(s)$ of the mechanical system shown in   \nFigure 3–4. The equations of motion for the system shown in Figure 3–4 are  \n\n$$\n\\begin{array}{r l}&{m_{1}\\ddot{x}_{1}=-k_{1}x_{1}\\,-\\,k_{2}\\big(x_{1}\\,-\\,x_{2}\\big)\\,-\\,b\\big(\\dot{x}_{1}\\,-\\,\\dot{x}_{2}\\big)\\,+\\,u}\\\\ &{m_{2}\\ddot{x}_{2}=-k_{3}x_{2}\\,-\\,k_{2}\\big(x_{2}\\,-\\,x_{1}\\big)\\,-\\,b\\big(\\dot{x}_{2}\\,-\\,\\dot{x}_{1}\\big)}\\end{array}\n$$  \n\nSimplifying, we obtain  \n\n$$\n\\begin{array}{r l}&{m_{1}\\ddot{x}_{1}\\,+\\,b\\dot{x}_{1}\\,+\\,\\bigl(k_{1}\\,+\\,k_{2}\\bigr)x_{1}\\,=\\,b\\dot{x}_{2}\\,+\\,k_{2}x_{2}\\,+\\,u}\\\\ &{m_{2}\\ddot{x}_{2}\\,+\\,b\\dot{x}_{2}\\,+\\,\\bigl(k_{2}\\,+\\,k_{3}\\bigr)x_{2}\\,=\\,b\\dot{x}_{1}\\,+\\,k_{2}x_{1}}\\end{array}\n$$  \n\nTaking the Laplace transforms of these two equations, assuming zero initial conditions, we obtain  \n\n$$\n\\begin{array}{r l}&{\\big[m_{1}s^{2}\\,+\\,b s\\,+\\,\\big(k_{1}\\,+\\,k_{2}\\big)\\big]X_{1}(s)\\,=\\,\\big(b s\\,+\\,k_{2}\\big)X_{2}(s)\\,+\\,U(s)}\\\\ &{\\big[m_{2}s^{2}\\,+\\,b s\\,+\\,\\big(k_{2}\\,+\\,k_{3}\\big)\\big]X_{2}(s)\\,=\\,\\big(b s\\,+\\,k_{2}\\big)X_{1}(s)}\\end{array}\n$$  \n\nSolving Equation (3–6) for $X_{2}(s)$ and substituting it into Equation (3–5) and simplifying, we get  \n\n$$\n\\begin{array}{c}{{{}[(m_{1}s^{2}\\,+\\,b s\\,+\\,k_{1}\\,+\\,k_{2})(m_{2}s^{2}\\,+\\,b s\\,+\\,k_{2}\\,+\\,k_{3})\\,-\\,(b s\\,+\\,k_{2})^{2}]X_{1}(s)}}\\\\ {{{}}}\\\\ {{=(m_{2}s^{2}\\,+\\,b s\\,+\\,k_{2}\\,+\\,k_{3})U(s)}}\\end{array}\n$$  \n\nfrom which we obtain  \n\n$$\n{\\frac{X_{1}(s)}{U(s)}}={\\frac{m_{2}s^{2}+b s+k_{2}+k_{3}}{{\\bigl(}m_{1}s^{2}+b s+k_{1}+k_{2}{\\bigr)}{\\bigl(}m_{2}s^{2}+b s+k_{2}+k_{3}{\\bigr)}-{\\bigl(}b s+k_{2}{\\bigr)}^{2}}}\n$$  \n\nFrom Equations (3–6) and (3–7) we have  \n\n$$\n\\frac{X_{2}(s)}{U(s)}=\\frac{b s\\:+\\:k_{2}}{\\left(m_{1}s^{2}\\:+\\:b s\\:+\\:k_{1}\\:+\\:k_{2}\\right)\\!\\left(m_{2}s^{2}\\:+\\:b s\\:+\\:k_{2}\\:+\\:k_{3}\\right)\\:-\\:\\left(b s\\:+\\:k_{2}\\right)^{2}}\n$$  \n\nEquations (3–7) and (3–8) are the transfer functions $X_{1}(s)/U(s)$ and $X_{2}(s)/U(s)$ ,respectively.  \n\n# EXAMPLE 3–5  \n\nAn inverted pendulum mounted on a motor-driven cart is shown in Figure 3–5(a).This is a model of the attitude control of a space booster on takeoff. (The objective of the attitude control problem is to keep the space booster in a vertical position.) The inverted pendulum is unstable in that it may fall over any time in any direction unless a suitable control force is applied.Here we consider  \n\nFigure 3–5   \n(a) Inverted   \npendulum system; (b) free-body   \ndiagram.  \n\n![](images/92d5cadbac3f197ed75274c933f7865b9e3db3bd671b7c4459a75e354c1935e1.jpg)  \n(b)  \n\nonly a two-dimensional problem in which the pendulum moves only in the plane of the page.The control force $u$ is applied to the cart.Assume that the center of gravity of the pendulum rod is at its geometric center. Obtain a mathematical model for the system.  \n\nDefine the angle of the rod from the vertical line as $\\theta$ . Define also the $\\left({x,y}\\right)$ coordinates of the center of gravity of the pendulum rod as $\\left(x_{G},y_{G}\\right)$ .Then  \n\n$$\n\\begin{array}{l}{{x_{G}=x+l\\sin\\theta}}\\\\ {{{}}}\\\\ {{y_{G}=l\\cos\\theta}}\\end{array}\n$$  \n\nTo derive the equations of motion for the system, consider the free-body diagram shown in Figure 3–5(b). The rotational motion of the pendulum rod about its center of gravity can be described by  \n\n$$\nI\\ddot{\\theta}\\,=\\,V l\\sin\\theta\\,-\\,H l\\cos\\theta\n$$  \n\nwhere $I$ is the moment of inertia of the rod about its center of gravity.  \n\nThe horizontal motion of center of gravity of pendulum rod is given by  \n\n$$\nm\\,{\\frac{d^{2}}{d t^{2}}}\\,(x\\,+\\,l\\sin\\theta)\\,=\\,H\n$$  \n\nThe vertical motion of center of gravity of pendulum rod is  \n\n$$\nm\\,{\\frac{d^{2}}{d t^{2}}}\\left(l\\cos\\theta\\right)\\,=\\,V\\,-\\,m g\n$$  \n\nThe horizontal motion of cart is described by  \n\n$$\nM\\,{\\frac{d^{2}x}{d t^{2}}}=u\\,-\\,H\n$$  \n\nsmall quantities such that Since we must keep the inverted pendulum vertical, we can assume that $\\sin\\theta\\,\\triangleq\\,\\theta,\\cos\\theta\\,=\\,1,$ , and $\\dot{\\theta\\theta}^{2}=0$ #Then, Equations (3–9) through (3–11) $\\theta(t)$ and ${\\dot{\\theta}}(t)$ are can be linearized.The linearized equations are  \n\n$$\n\\begin{array}{r l}&{I\\ddot{\\theta}=V l\\theta-H l}\\\\ &{m(\\ddot{x}\\,+\\,l\\ddot{\\theta})=H}\\\\ &{0=V-m g}\\end{array}\n$$  \n\nFrom Equations (3–12) and (3–14), we obtain  \n\n$$\n(M\\,+\\,m){\\ddot{x}}\\,+\\,m{\\dot{l}}{\\ddot{\\theta}}\\,=\\,u\n$$  \n\nFrom Equations (3–13), (3–14), and (3–15), we have  \n\n$$\n\\begin{array}{l}{{I\\ddot{\\theta}\\,=\\,m g l\\theta\\,-\\,H l}}\\\\ {{\\ }}\\\\ {{\\ }}\\\\ {{\\qquad=\\,m g l\\theta\\,-\\,l(m\\ddot{x}\\,+\\,m l\\ddot{\\theta})}}\\\\ {{\\ }}\\\\ {{\\left(I\\,+\\,m l^{2}\\right)\\ddot{\\theta}\\,+\\,m l\\ddot{x}\\,=\\,m g l\\theta}}\\end{array}\n$$  \n\nor  \n\nEquations (3–16) and (3–17) describe the motion of the inverted-pendulum-on-the-cart system.   \nThey constitute a mathematical model of the system.  \n\n# EXAMPLE 3–6  \n\nConsider the inverted-pendulum system shown in Figure 3–6. Since in this system the mass is concentrated at the top of the rod, the center of gravity is the center of the pendulum ball. For this case, the moment of inertia of the pendulum about its center of gravity is small, and we assume $I\\,=\\,0$ in Equation (3–17).Then the mathematical model for this system becomes as follows:  \n\n$$\n\\begin{array}{l}{(M+m)\\ddot{x}\\,+\\,m l\\ddot{\\theta}\\,=\\,u}\\\\ {m l^{2}\\ddot{\\theta}\\,+\\,m l\\ddot{x}\\,=\\,m g l\\theta}\\end{array}\n$$  \n\nEquations (3–18) and (3–19) can be modified to  \n\n$$\n\\begin{array}{l}{{M{\\ddot{\\theta}}=(M+m)g\\theta-u}}\\\\ {{{\\cal M}{\\ddot{x}}=u-m g\\theta}}\\end{array}\n$$  \n\nFigure 3–6   \nInverted-pendulum system.  \n\n![](images/069ad0fa4f9aab5014cf83c8102f668f62a383c64191ef94e6158ec209bb8fa2.jpg)  \n\nEquation (3–20) was obtained by eliminating (3–21) was obtained by eliminating $\\ddot{\\theta}$ \\$ from Equations (3–18) and (3–19). From Equation (3–20) $\\dot{x}$ from Equations (3–18) and (3–19). Equation we obtain the plant transfer function to be  \n\n$$\n\\frac{\\theta(s)}{-U(s)}=\\frac{1}{M l s^{2}\\ –\\ (M+m)g}}\\\\ =\\frac{1}{M l\\biggl(s\\nonumber\\ +\\sqrt{\\frac{M\\nonumber\\ +\\ m}{M l}g}\\biggr)\\biggl(s\\nonumber\\ -\\sqrt{\\frac{M\\nonumber\\ +\\ m}{M l}g}\\biggr)}\n$$  \n\nThe inverted-pendulum plant has one pole on the negative real axis CA BD$\\left[s=-(\\sqrt{M+m}/\\sqrt{M l})\\sqrt{g}\\right]$ CA BDand another on the positive real axis $\\left[s=\\left({\\sqrt{M+m}}/{\\sqrt{M l}}\\right)\\sqrt{g}\\right]$ .Hence,the plant is open-loop unstable. Define state variables $x_{1},x_{2},x_{3}$ and $x_{4}$ by  \n\n$$\n\\begin{array}{l}{x_{1}=\\theta}\\\\ {x_{2}=\\dot{\\theta}}\\\\ {x_{3}=x}\\\\ {x_{4}=\\dot{x}}\\end{array}\n$$  \n\nNote that angle $\\theta$ indicates the rotation of the pendulum rod about point $P$ , and $x$ is the location of the cart. If we consider $\\theta$ and $x$ as the outputs of the system, then  \n\n$$\n\\mathbf{y}={\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l}{\\theta}\\\\ {x}\\end{array}\\right]}={\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\n(Notice that both $\\theta$ and $x$ are easily measurable quantities.) Then, from the definition of the state variables and Equations (3–20) and (3–21), we obtain  \n\n$$\n\\begin{array}{l}{\\displaystyle\\dot{x}_{1}=\\,x_{2}}\\\\ {\\displaystyle\\dot{x}_{2}=\\frac{M+\\,m}{M l}\\,g x_{1}-\\frac{1}{M l}\\,u}\\\\ {\\displaystyle\\dot{x}_{3}=\\,x_{4}}\\\\ {\\displaystyle\\dot{x}_{4}=-\\,\\frac{m}{M}\\,g x_{1}+\\frac{1}{M}\\,u}\\end{array}\n$$  \n\n$$\n{\\begin{array}{r}{{\\left[\\begin{array}{l}{}\\\\ {{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\\\ {{\\dot{x}}_{4}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l}{0}&{1}&{0}&{0}\\\\ {{\\cfrac{M+m}{M l}}\\,g}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {-{\\cfrac{m}{M}}\\,g}&{0}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {-{\\cfrac{1}{M l}}}\\\\ {0}\\\\ {{\\cfrac{1}{M}}}\\end{array}\\right]}u}\\end{array}}\n$$  \n\n$$\n{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]}\n$$  \n\n# 3–3 MATHEMATICAL MODELING OF ELECTRICAL SYSTEMS  \n\nEquations (3–22) and (3–23) give a state-space representation of the inverted-pendulum system. (Note that state-space representation of the system is not unique.There are infinitely many such representations for this system.)  \n\nBasic laws governing electrical circuits are Kirchhoff’s current law and voltage law. Kirchhoff’s current law (node law) states that the algebraic sum of all currents entering and leaving a node is zero. (This law can also be stated as follows:The sum of currents entering a node is equal to the sum of currents leaving the same node.) Kirchhoff’s voltage law (loop law) states that at any given instant the algebraic sum of the voltages around any loop in an electrical circuit is zero. (This law can also be stated as follows:The sum of the voltage drops is equal to the sum of the voltage rises around a loop.) A mathematical model of an electrical circuit can be obtained by applying one or both of Kirchhoff’s laws to it.  \n\nThis section first deals with simple electrical circuits and then treats mathematical modeling of operational amplifier systems.  \n\nLRC Circuit. Consider the electrical circuit shown in Figure 3–7. The circuit consists of an inductance $L$ (henry), a resistance $R$ (ohm), and a capacitance $C$ (farad). Applying Kirchhoff’s voltage law to the system, we obtain the following equations:  \n\n$$\n\\begin{array}{r}{{\\cal L}\\displaystyle\\frac{d i}{d t}+\\,R i+\\displaystyle\\frac{1}{C}\\int_{}^{}i\\,d t=e_{i}}\\\\ {\\displaystyle\\frac{1}{C}\\int_{}^{}i\\,d t=e_{o}}\\end{array}\n$$  \n\nFigure 3–7 Electrical circuit.  \n\n![](images/77b2ee7ecb17acd3c74f61dd2685ab76286cb03bb0f1b79686c08846706a71b4.jpg)  \n\nEquations (3–24) and (3–25) give a mathematical model of the circuit.  \n\nA transfer-function model of the circuit can also be obtained as follows:Taking the Laplace transforms of Equations (3–24) and (3–25), assuming zero initial conditions, we obtain  \n\n$$\n\\begin{array}{r}{{L s I(s)\\,+\\,R I(s)\\,+\\,{\\displaystyle\\frac{1}{C}{\\displaystyle\\frac{1}{s}}\\,I(s)\\,=\\,E_{i}(s)}}}\\\\ {{{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\frac{1}{C}{\\displaystyle\\frac{1}{s}}\\,I(s)\\,=\\,E_{o}(s)}}}\\end{array}\n$$  \n\nIf $e_{i}$ is assumed to be the input and $e_{o}$ the output,then the transfer function of this system is found to be  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{1}{L C s^{2}\\,+\\,R C s\\,+\\,1}\n$$  \n\nA state-space model of the system shown in Figure 3–7 may be obtained as follows: First, note that the differential equation for the system can be obtained from Equation (3–26) as  \n\n$$\n\\ddot{e}_{o}+\\frac{R}{L}\\,\\dot{e}_{o}+\\frac{1}{L C}\\,e_{o}=\\frac{1}{L C}\\,e_{i}\n$$  \n\nThen by defining state variables by  \n\n$$\n\\begin{array}{l}{x_{1}=e_{o}}\\\\ {x_{2}=\\dot{e}_{o}}\\end{array}\n$$  \n\nand the input and output variables by  \n\n$$\n\\begin{array}{l}{u\\,=\\,e_{i}}\\\\ {y\\,=\\,e_{o}\\,=\\,x_{1}}\\end{array}\n$$  \n\nwe obtain  \n\n$$\n\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{c c}{0}&{1}\\\\ {\\displaystyle-\\frac{1}{L C}}&{\\displaystyle-\\frac{R}{L}\\right]\\!\\!\\left[\\frac{x_{1}}{x_{2}}\\right]+\\left[\\begin{array}{c}{0}\\\\ {\\displaystyle\\frac{1}{L C}}\\end{array}\\right]\\!\\!u\n$$  \n\nand  \n\n$$\ny\\,=\\,[1\\qquad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nThese two equations give a mathematical model of the system in state space.  \n\nTransfer Functions of Cascaded Elements. Many feedback systems have components that load each other. Consider the system shown in Figure 3–8.Assume that $e_{i}$ is the input and $e_{o}$ is the output. The capacitances $C_{1}$ and $C_{2}$ are not charged initially.  \n\n![](images/d4861390f56745c9996893b94b30a7ffad3a5a27b76fc9aa7e90a04508c57259.jpg)  \nFigure 3–8 Electrical system.  \n\nIt will be shown that the second stage of the circuit ( $R_{2}C_{2}$ portion) produces a loading effect on the first stage ( $[R_{1}C_{1}$ portion).The equations for this system are  \n\n$$\n{\\frac{1}{C_{1}}}\\int(i_{1}\\,-\\,i_{2})\\,d t\\,+\\,R_{1}i_{1}\\,=\\,e_{i}\n$$  \n\nand  \n\n$$\n\\begin{array}{r}{\\displaystyle{\\frac{1}{C_{1}}\\int(i_{2}-\\,i_{1})\\,d t\\,+\\,R_{2}i_{2}+\\frac{1}{C_{2}}\\int i_{2}\\,d t=0}}\\\\ {\\displaystyle{\\frac{1}{C_{2}}\\int_{}^{}i_{2}\\,d t=e_{o}}}\\end{array}\n$$  \n\nTaking the Laplace transforms of Equations (3–27) through (3–29), respectively, using zero initial conditions, we obtain  \n\n$$\n\\begin{array}{c}{{\\displaystyle\\frac{1}{C_{1}s}[I_{1}(s)\\,-\\,I_{2}(s)]\\,+\\,R_{1}I_{1}(s)\\,=\\,E_{i}(s)}}\\\\ {{{}}}\\\\ {{\\displaystyle\\frac1{C_{1}s}[I_{2}(s)\\,-\\,I_{1}(s)]\\,+\\,R_{2}I_{2}(s)\\,+\\,\\frac1{C_{2}s}\\,I_{2}(s)\\,=\\,0}}\\\\ {{{}}}\\\\ {{\\displaystyle\\frac1{C_{2}s}\\,I_{2}(s)\\,=\\,E_{o}(s)}}\\end{array}\n$$  \n\nEliminating $I_{1}(s)$ from Equations (3–30) and (3–31) and writing $E_{i}(s)$ in terms of $I_{2}(s)$ ,we find the transfer function between $E_{o}(s)$ and $E_{i}(\\mathbf{s})$ to be  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{1}{\\left(R_{1}C_{1}s\\,+\\,1\\right)\\left(R_{2}C_{2}s\\,+\\,1\\right)\\,+\\,R_{1}C_{2}s}}\\\\ {\\displaystyle\\,=\\frac{1}{R_{1}C_{1}R_{2}C_{2}s^{2}\\,+\\,\\left(R_{1}C_{1}\\,+\\,R_{2}C_{2}\\,+\\,R_{1}C_{2}\\right)s\\,+\\,1}}\\end{array}\n$$  \n\nThe term $R_{1}C_{2}s$ in the denominator of the transfer function represents the interaction of two simple $R C$ circuits. Since $\\left(R_{1}C_{1}\\,+\\,R_{2}C_{2}\\,+\\,R_{1}C_{2}\\right)^{2}>4\\dot{R_{1}}C_{1}R_{2}C_{2}$ ,the two roots of the denominator of Equation (3–33) are real.  \n\nThe present analysis shows that, if two $R C$ circuits are connected in cascade so that the output from the first circuit is the input to the second, the overall transfer function is not the product of $1/\\big(R_{1}C_{1}s\\,+\\,1\\big)$ and $1/\\big(R_{2}C_{2}s\\,+\\,1\\big)$ .The reason for this is that, when we derive the transfer function for an isolated circuit, we implicitly assume that the output is unloaded. In other words, the load impedance is assumed to be infinite, which means that no power is being withdrawn at the output.When the second circuit is connected to the output of the first, however, a certain amount of power is withdrawn, and thus the assumption of no loading is violated.Therefore, if the transfer function of this system is obtained under the assumption of no loading, then it is not valid. The degree of the loading effect determines the amount of modification of the transfer function.  \n\nComplex Impedances. In deriving transfer functions for electrical circuits, we frequently find it convenient to write the Laplace-transformed equations directly, without writing the differential equations. Consider the system shown in Figure 3–9(a). In this system, $Z_{1}$ and $Z_{2}$ represent complex impedances. The complex impedance $Z(s)$ of a two-terminal circuit is the ratio of $E(s)$ , the Laplace transform of the voltage across the terminals, to $I(s)$ ,the Laplace transform of the current through the element, under the assumption that the initial conditions are zero, so that $Z(s)\\,=\\,E(s)/{I(s)}$ .If the two-terminal element is a resistance $R$ , capacitance $C$ , or inductance $L$ , then the complex impedance is given by $R,1/C s$ , or $L s$ , respectively. If complex impedances are connected in series, the total impedance is the sum of the individual complex impedances.  \n\nRemember that the impedance approach is valid only if the initial conditions involved are all zeros. Since the transfer function requires zero initial conditions, the impedance approach can be applied to obtain the transfer function of the electrical circuit. This approach greatly simplifies the derivation of transfer functions of electrical circuits.  \n\nConsider the circuit shown in Figure 3–9(b).Assume that the voltages $e_{i}$ and $e_{o}$ are the input and output of the circuit, respectively. Then the transfer function of this circuit is  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}={\\frac{Z_{2}(s)}{Z_{1}(s)\\,+\\,Z_{2}(s)}}\n$$  \n\nFor the system shown in Figure 3–7,  \n\n$$\nZ_{1}=L s\\,+\\,R,\\ \\ \\ \\ \\ Z_{2}={\\frac{1}{C s}}\n$$  \n\nHence the transfer function $E_{o}(s)/E_{i}(s)$ can be found as follows:  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{\\displaystyle\\frac{1}{C s}}{L s\\,+\\,R\\,+\\,\\displaystyle\\frac{1}{C s}}=\\frac{1}{L C s^{2}\\,+\\,R C s\\,+\\,1}\n$$  \n\nwhich is, of course, identical to Equation (3–26).  \n\n![](images/c2710474d4aee56b02c0661ef13d4b8b53ad475696ec1e85becab632316b4f06.jpg)  \nFigure 3–9 Electrical circuits.  \n\nConsider again the system shown in Figure 3–8. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ by use of the complex impedance approach. (Capacitors $C_{1}$ and $C_{2}$ are not charged initially.)  \n\nThe circuit shown in Figure 3–8 can be redrawn as that shown in Figure 3–10(a), which can be further modified to Figure 3–10(b).  \n\nIn the system shown in Figure 3–10(b) the current $I$ is divided into two currents $I_{1}$ and $I_{2}$ .Noting that  \n\n$$\nZ_{2}I_{1}=(Z_{3}+\\,Z_{4})I_{2},\\qquad I_{1}+\\,I_{2}=\\,I\n$$  \n\nwe obtain  \n\n$$\nI_{1}={\\frac{Z_{3}\\,+\\,Z_{4}}{Z_{2}\\,+\\,Z_{3}\\,+\\,Z_{4}}}\\,I,\\qquad I_{2}={\\frac{Z_{2}}{Z_{2}\\,+\\,Z_{3}\\,+\\,Z_{4}}}\\,I\n$$  \n\nNoting that  \n\n$$\n\\begin{array}{l}{{\\displaystyle{E_{i}(s)=Z_{1}I+Z_{2}I_{1}=\\left[Z_{1}+\\frac{Z_{2}\\left(Z_{3}+\\,Z_{4}\\right)}{Z_{2}+\\,Z_{3}+\\,Z_{4}}\\right]I}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{E_{o}(s)=Z_{4}I_{2}=\\frac{Z_{2}Z_{4}}{Z_{2}+\\,Z_{3}+\\,Z_{4}}I}}}\\end{array}\n$$  \n\nwe obtain  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{Z_{2}Z_{4}}{Z_{1}(Z_{2}\\mathrm{~+~}Z_{3}\\mathrm{~+~}Z_{4})\\mathrm{~+~}Z_{2}(Z_{3}\\mathrm{~+~}Z_{4})}\n$$  \n\nSubstituting $Z_{1}=R_{1},Z_{2}=1/(C_{1}s),Z_{3}=R_{2}$ ,and $Z_{4}=1/\\bigl(C_{2}s\\bigr)$ into this last equation, we get  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{\\displaystyle\\frac{1}{C_{1}s}\\frac{1}{C_{2}s}}{R_{1}\\Big(\\frac{1}{C_{1}s}+R_{2}+\\frac{1}{C_{2}s}\\Big)+\\frac{1}{C_{1}s}\\Big(R_{2}+\\frac{1}{C_{2}s}\\Big)}}}\\\\ &{}&{=\\frac{1}{R_{1}C_{1}R_{2}C_{2}s^{2}+\\big(R_{1}C_{1}+R_{2}C_{2}+R_{1}C_{2}\\big)s+1}}\\end{array}\n$$  \n\nwhich is the same as that given by Equation (3–33).  \n\n![](images/cc67d5b43f148dfc42aefc082c68e0448a1bf42b3ecdb2cc3df727b7644459f3.jpg)  \nFigure 3–10 (a) The circuit of Figure 3–8 shown in terms of impedances; (b) equivalent circuit diagram.  \n\n![](images/3d76db5a88d316e55619cb9079edaab5ecb22d0c9cb1cad29d2525bb5234ab27.jpg)  \nFigure 3–11 (a) System consisting of two nonloading cascaded elements; (b) an equivalent system.  \n\nTransfer Functions of Nonloading Cascaded Elements. The transfer function of a system consisting of two nonloading cascaded elements can be obtained by eliminating the intermediate input and output. For example, consider the system shown in Figure 3–11(a).The transfer functions of the elements are  \n\n$$\nG_{1}(s)=\\frac{X_{2}(s)}{X_{1}(s)}\\qquad\\mathrm{and}\\qquad G_{2}(s)\\,=\\frac{X_{3}(s)}{X_{2}(s)}\n$$  \n\nIf the input impedance of the second element is infinite, the output of the first element is not affected by connecting it to the second element.Then the transfer function of the whole system becomes  \n\n$$\nG(s)=\\frac{X_{3}(s)}{X_{1}(s)}=\\frac{X_{2}(s)X_{3}(s)}{X_{1}(s)X_{2}(s)}=G_{1}(s)G_{2}(s)~\n$$  \n\nThe transfer function of the whole system is thus the product of the transfer functions of the individual elements.This is shown in Figure 3–11(b).  \n\nAs an example,consider the system shown in Figure 3–12.The insertion of an isolating amplifier between the circuits to obtain nonloading characteristics is frequently used in combining circuits. Since amplifiers have very high input impedances, an isolation amplifier inserted between the two circuits justifies the nonloading assumption.  \n\nThe two simple $R C$ circuits, isolated by an amplifier as shown in Figure 3–12, have negligible loading effects, and the transfer function of the entire circuit equals the product of the individual transfer functions.Thus, in this case,  \n\n$$\n\\begin{array}{c}{\\displaystyle\\frac{E_{o}(s)}{E_{i}(s)}=\\bigg(\\frac{1}{R_{1}C_{1}s\\,+\\,1}\\bigg)(K)\\bigg(\\frac{1}{R_{2}C_{2}s\\,+\\,1}\\bigg)}\\\\ {\\displaystyle=\\frac{K}{\\big(R_{1}C_{1}s\\,+\\,1\\big)\\big(R_{2}C_{2}s\\,+\\,1\\big)}}\\end{array}\n$$  \n\nElectronic Controllers. In what follows we shall discuss electronic controllers using operational amplifiers.We begin by deriving the transfer functions of simple operationalamplifier circuits.Then we derive the transfer functions of some of the operational-amplifier controllers.Finally,we give operational-amplifier controllers and their transfer functions in the form of a table.  \n\n![](images/cd5757d4151579fc6c626f68f12dbbc89dee34fcb847686e3a0ebe37e72fe2e7.jpg)  \nFigure 3–12 Electrical system.  \n\n![](images/1b05152f1d325b0f651b3c79b64f8ed60d23df8248774b5538a9a8b87cb0d52e.jpg)  \nFigure 3–13 Operational amplifier.  \n\nOperational Amplifiers. Operational amplifiers, often called op amps, are frequently used to amplify signals in sensor circuits. $\\mathrm{Op}$ amps are also frequently used in filters used for compensation purposes. Figure 3–13 shows an op amp. It is a common practice to choose the ground as 0 volt and measure the input voltages $e_{1}$ and $e_{2}$ relative to the ground. The input $e_{1}$ to the minus terminal of the amplifier is inverted, and the input $e_{2}$ to the plus terminal is not inverted.The total input to the amplifier thus becomes $\\boldsymbol{e}_{2}\\mathrm{~-~}\\boldsymbol{e}_{1}$ .Hence, for the circuit shown in Figure 3–13, we have  \n\n$$\ne_{o}=K(e_{2}\\,-\\,e_{1})=-K(e_{1}\\,-\\,e_{2})\n$$  \n\nwhere the inputs $e_{1}$ and $e_{2}$ may be dc or ac signals and $K$ is the differential gain (voltage gain).The magnitude of $K$ is approximately $10^{5}\\sim10^{6}$ for dc signals and ac signals with frequencies less than approximately $10\\,\\mathrm{Hz}$ . (The differential gain $K$ decreases with the signal frequency and becomes about unity for frequencies of $\\begin{array}{r}{1~\\mathrm{MHz}\\sim50~\\mathrm{MHz}.}\\end{array}$ )Note that the op amp amplifies the difference in voltages $e_{1}$ and $e_{2}$ .Such an amplifier is commonly called a differential amplifier. Since the gain of the op amp is very high, it is necessary to have a negative feedback from the output to the input to make the amplifier stable. (The feedback is made from the output to the inverted input so that the feedback is a negative feedback.)  \n\nIn the ideal op amp, no current flows into the input terminals, and the output voltage is not affected by the load connected to the output terminal. In other words, the input impedance is infinity and the output impedance is zero. In an actual op amp, a very small (almost negligible) current flows into an input terminal and the output cannot be loaded too much. In our analysis here, we make the assumption that the op amps are ideal.  \n\nInverting Amplifier. Consider the operational-amplifier circuit shown in Figure 3–14. Let us obtain the output voltage $e_{o}$ .  \n\n![](images/3db6429687967f328b76500ae756ba796f2f5e33e5350c8df85a59b6ab90ae03.jpg)  \nFigure 3–14 Inverting amplifier.   \nChapter 3 /Mathematical Modeling of Mechanical Systems and Electrical Systems  \n\nThe equation for this circuit can be obtained as follows: Define  \n\n$$\ni_{1}={\\frac{e_{i}-e^{\\prime}}{R_{1}}},\\ \\ \\ \\ \\ i_{2}={\\frac{e^{\\prime}-e_{o}}{R_{2}}}\n$$  \n\nSince only a negligible current flows into the amplifier, the current $i_{1}$ must be equal to current $i_{2}$ .Thus  \n\n$$\n\\frac{e_{i}\\,-\\,e^{\\prime}}{R_{1}}=\\frac{e^{\\prime}\\,-\\,e_{o}}{R_{2}}\n$$  \n\nSince $K(0\\,-\\,e^{\\prime})\\,=\\,e_{0}$ and $K\\!\\gg\\!1,e^{\\prime}$ must be almost zero, or $e^{\\prime}\\doteq0$ .Hence we have  \n\n$$\n\\frac{e_{i}}{R_{1}}=\\frac{-e_{o}}{R_{2}}\n$$  \n\nor  \n\n$$\n\\boldsymbol{e}_{o}=-\\,\\frac{R_{2}}{R_{1}}\\,\\boldsymbol{e}_{i}\n$$  \n\nThus the circuit shown is an inverting amplifier. If $R_{1}=R_{2}$ ,then the op-amp circuit shown acts as a sign inverter.  \n\nNoninverting Amplifier. Figure 3–15(a) shows a noninverting amplifier.A circuit equivalent to this one is shown in Figure 3–15(b). For the circuit of Figure 3–15(b), we have  \n\n$$\ne_{o}=K\\bigg(e_{i}-\\frac{R_{1}}{R_{1}+R_{2}}\\,e_{o}\\bigg)\n$$  \n\nwhere $K$ is the differential gain of the amplifier. From this last equation, we get  \n\n$$\ne_{i}=\\Bigg(\\frac{R_{1}}{R_{1}+R_{2}}+\\frac{1}{K}\\Bigg)e_{o}\n$$  \n\nSince $K\\gg1$ if $R_{1}/{\\left(R_{1}\\,+\\,R_{2}\\right)}\\gg1/K$ ,then  \n\n$$\ne_{o}=\\bigg(1+\\frac{R_{2}}{R_{1}}\\bigg)e_{i}\n$$  \n\nThis equation gives the output voltage $e_{o}$ .Since $e_{o}$ and $e_{i}$ have the same signs,the op-amp circuit shown in Figure 3–15(a) is noninverting.  \n\n![](images/0e657422c7b5aea563eb7aab82e6671655fb7f2cba579a5e0d586e58370b614a.jpg)  \nFigure 3–15 (a) Noninverting operational amplifier; (b) equivalent circuit.   \n(b)  \n\nFigure 3–16 shows an electrical circuit involving an operational amplifier. Obtain the output $e_{o}$ .Let us define  \n\n$$\ni_{1}=\\frac{e_{i}\\,-\\,e^{\\prime}}{R_{1}},\\qquad i_{2}=C\\,\\frac{d(e^{\\prime}\\,-\\,e_{o})}{d t},\\qquad i_{3}=\\frac{e^{\\prime}\\,-\\,e_{o}}{R_{2}}\n$$  \n\nNoting that the current flowing into the amplifier is negligible, we have  \n\n$$\ni_{1}=i_{2}+i_{3}\n$$  \n\nHence  \n\n$$\n\\frac{e_{i}\\,-\\,e^{\\prime}}{R_{1}}=C\\,\\frac{d\\bigl(e^{\\prime}\\,-\\,e_{o}\\bigr)}{d t}+\\frac{e^{\\prime}\\,-\\,e_{o}}{R_{2}}\n$$  \n\nSince $e^{\\prime}\\doteq0$ ,we have  \n\n$$\n{\\frac{e_{i}}{R_{1}}}=-C\\,{\\frac{d e_{o}}{d t}}-{\\frac{e_{o}}{R_{2}}}\n$$  \n\nTaking the Laplace transform of this last equation, assuming the zero initial condition, we have  \n\n$$\n\\frac{E_{i}(s)}{R_{1}}=-\\,\\frac{R_{2}C s\\,+\\,1}{R_{2}}\\,E_{o}(s)\n$$  \n\nwhich can be written as  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=-\\,\\frac{R_{2}}{R_{1}}\\frac{1}{R_{2}C s\\,+\\,1}\n$$  \n\nThe op-amp circuit shown in Figure 3–16 is a first-order lag circuit.(Several other circuits involving op amps are shown in Table 3–1 together with their transfer functions. Table 3–1 is given on page 85.)  \n\n![](images/8a28e8573ba4211f76a8c343da90cae4c840e98b87315d2cc53084a8f3c2e9de.jpg)  \nFigure 3–16 First-order lag circuit using operational amplifier.  \n\n![](images/365edd253e14bc389b4d126ac43c2380bf318501e215941e1dcb38850e7821e1.jpg)  \nFigure 3–17 Operationalamplifier circuit.  \n\nImpedance Approach to Obtaining Transfer Functions. Consider the op-amp circuit shown in Figure 3–17. Similar to the case of electrical circuits we discussed earlier, the impedance approach can be applied to op-amp circuits to obtain their transfer functions. For the circuit shown in Figure 3–17, we have  \n\n$$\n\\frac{E_{i}(s)\\,-\\,E^{\\prime}(s)}{Z_{1}}=\\frac{E^{\\prime}(s)\\,-\\,E_{o}(s)}{Z_{2}}\n$$  \n\nSince $E^{\\prime}(s)\\doteq0$ ,we have  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}=-\\,{\\frac{Z_{2}(s)}{Z_{1}(s)}}\n$$  \n\n# EXAMPLE 3–9  \n\nReferring to the op-amp circuit shown in Figure 3–16, obtain the transfer function $E_{o}(s)/E_{i}(s)$ by   \nuse of the impedance approach. The complex impedances $Z_{1}(s)$ and $Z_{2}(s)$ for this circuit are  \n\n$$\nZ_{1}(s)\\,=\\,R_{1}\\qquad\\mathrm{and}\\qquad Z_{2}(s)\\,=\\frac{1}{C s\\,+\\,\\frac{1}{R_{2}}}=\\frac{R_{2}}{R_{2}C s\\,+\\,1}\n$$  \n\nThe transfer function $E_{o}(s)/E_{i}(s)$ is, therefore, obtained as  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}=-{\\frac{Z_{2}(s)}{Z_{1}(s)}}=-{\\frac{R_{2}}{R_{1}}}{\\frac{1}{R_{2}C s\\,+\\,1}}\n$$  \n\nwhich is, of course, the same as that obtained in Example 3-8.  \n\nLead or Lag Networks Using Operational Amplifiers. Figure 3–18(a) shows an electronic circuit using an operational amplifier. The transfer function for this circuit can be obtained as follows: Define the input impedance and feedback impedance as $Z_{1}$ and $Z_{2}$ ,respectively.Then  \n\n$$\nZ_{1}=\\frac{R_{1}}{R_{1}C_{1}s\\,+\\,1},\\qquad Z_{2}=\\frac{R_{2}}{R_{2}C_{2}s\\,+\\,1}\n$$  \n\nHence, referring to Equation (3–34), we have  \n\n$$\n\\frac{E(s)}{E_{i}(s)}=-\\frac{Z_{2}}{Z_{1}}=-\\,\\frac{R_{2}}{R_{1}}\\frac{R_{1}C_{1}s\\,+\\,1}{R_{2}C_{2}s\\,+\\,1}=-\\,\\frac{C_{1}}{C_{2}}\\frac{s+\\frac{1}{R_{1}C_{1}}}{s\\,+\\,\\frac{1}{R_{2}C_{2}}}\n$$  \n\nNotice that the transfer function in Equation (3–35) contains a minus sign.Thus,this circuit is sign inverting. If such a sign inversion is not convenient in the actual application, a sign inverter may be connected to either the input or the output of the circuit of Figure 3–18(a). An example is shown in Figure 3–18(b).The sign inverter has the transfer function of  \n\n$$\n{\\frac{E_{o}(s)}{E(s)}}=-\\,{\\frac{R_{4}}{R_{3}}}\n$$  \n\nThe sign inverter has the gain of $-R_{4}/R_{3}$ .Hence the network shown in Figure 3–18(b) has the following transfer function:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{R_{2}R_{4}}{R_{1}R_{3}}\\frac{R_{1}C_{1}s+1}{R_{2}C_{2}s+1}=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\\frac{s+\\frac{1}{R_{1}C_{1}}}{s+\\frac{1}{R_{2}C_{2}}}}\\\\ &{\\quad=K_{c}\\alpha\\frac{T s+1}{\\alpha T s+1}=K_{c}\\frac{s+\\frac{1}{T}}{s+\\frac{1}{\\alpha T}}}\\end{array}\n$$  \n\n![](images/de69d15cfb8cf773a1a9b17a650998c8b5c6cbb287a428db49c9814d261aea0f.jpg)  \nFigure 3–18 (a) Operational-amplifier circuit; (b) operational-amplifier circuit used as a lead or lag compensator.  \n\nwhere  \n\n$$\nT\\,=\\,R_{1}C_{1},\\qquad\\alpha T\\,=\\,R_{2}C_{2},\\qquad K_{c}=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\n$$  \n\nNotice that  \n\n$$\nK_{c}\\alpha=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\\frac{R_{2}C_{2}}{R_{1}C_{1}}=\\frac{R_{2}R_{4}}{R_{1}R_{3}},\\qquad\\alpha=\\frac{R_{2}C_{2}}{R_{1}C_{1}}\n$$  \n\nThis network has a dc gain of $K_{c}\\alpha\\,=\\,R_{2}R_{4}/\\bigl(R_{1}R_{3}\\bigr)$ .  \n\nNote that this network, whose transfer function is given by Equation (3–36), is a lead network if $R_{1}C_{1}>R_{2}C_{2}$ ,or $\\alpha<1$ .It is a lag network if $R_{1}C_{1}<R_{2}C_{2}$ .  \n\nPID Controller Using Operational Amplifiers. Figure 3–19 shows an electronic proportional-plus-integral-plus-derivative controller (a PID controller) using operational amplifiers.The transfer function $E(s)/E_{i}(s)$ is given by  \n\n$$\n{\\frac{E(s)}{E_{i}(s)}}=-\\,{\\frac{Z_{2}}{Z_{1}}}\n$$  \n\nwhere  \n\n$$\nZ_{1}=\\frac{R_{1}}{R_{1}C_{1}s\\,+\\,1},\\qquad Z_{2}=\\frac{R_{2}C_{2}s\\,+\\,1}{C_{2}s}\n$$  \n\nThus  \n\n$$\n\\frac{E(s)}{E_{i}(s)}=-\\bigg(\\frac{R_{2}C_{2}s\\,+\\,1}{C_{2}s}\\bigg)\\bigg(\\frac{R_{1}C_{1}s\\,+\\,1}{R_{1}}\\bigg)\n$$  \n\nNoting that  \n\n$$\n{\\frac{E_{o}(s)}{E(s)}}=-\\,{\\frac{R_{4}}{R_{3}}}\n$$  \n\n![](images/8e0cc49b4f560d3c7956b6104b2629a7ff22c6d38511790b4c1c5588442fa5b2.jpg)  \nFigure 3–19 Electronic PID controller.  \n\nwe have  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{E_{o}(s)}{E(s)}\\frac{E(s)}{E_{i}(s)}=\\frac{R_{4}R_{2}}{R_{3}R_{1}}\\frac{\\big(R_{1}C_{1}s\\,+\\,1\\big)\\big(R_{2}C_{2}s\\,+\\,1\\big)}{R_{2}C_{2}s}}}\\\\ &{}&{\\qquad=\\frac{R_{4}R_{2}}{R_{3}R_{1}}\\bigg(\\frac{R_{1}C_{1}\\,+\\,R_{2}C_{2}}{R_{2}C_{2}}+\\frac{1}{R_{2}C_{2}s}+R_{1}C_{1}s\\bigg)}\\\\ &{}&{\\qquad=\\frac{R_{4}\\big(R_{1}C_{1}\\,+\\,\\,R_{2}C_{2}\\big)}{R_{3}R_{1}C_{2}}\\bigg[1\\,\\ensuremath{\\mathrm{~+~}}\\,\\frac{1}{\\big(R_{1}C_{1}\\,+\\,\\,R_{2}C_{2}\\big)s}\\,+\\frac{R_{1}C_{1}R_{2}C_{2}}{R_{1}C_{1}\\,+\\,\\,R_{2}C_{2}}s\\bigg]}\\end{array}\n$$  \n\nNotice that the second operational-amplifier circuit acts as a sign inverter as well as a gain adjuster.  \n\nWhen a PID controller is expressed as  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\,K_{p}\\bigg(1\\,+\\frac{T_{i}}{s}+\\,T_{d}s\\bigg)\n$$  \n\n$K_{p}$ is called the proportional gain, $T_{i}$ is called the integral time, and $T_{d}$ is called the derivative time. From Equation (3–37) we obtain the proportional gain $K_{p}$ ,integral time $T_{i}$ ,and derivative time $T_{d}$ to be  \n\n$$\n\\begin{array}{l}{{K_{p}=\\displaystyle\\frac{R_{4}\\big(R_{1}C_{1}+R_{2}C_{2}\\big)}{R_{3}R_{1}C_{2}}}}\\\\ {{\\displaystyle\\quad T_{i}=\\displaystyle\\frac{1}{R_{1}C_{1}+R_{2}C_{2}}}}\\\\ {{\\displaystyle\\quad T_{d}=\\displaystyle\\frac{R_{1}C_{1}R_{2}C_{2}}{R_{1}C_{1}+R_{2}C_{2}}}}\\end{array}\n$$  \n\nWhen a PID controller is expressed as  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=K_{p}+\\frac{K_{i}}{s}+K_{d}s\n$$  \n\n$K_{p}$ is called the proportional gain, $K_{i}$ is called the integral gain, and $K_{d}$ is called the derivative gain. For this controller  \n\n$$\n\\begin{array}{l}{{K_{p}=\\displaystyle\\frac{R_{4}\\bigl(R_{1}C_{1}+R_{2}C_{2}\\bigr)}{R_{3}R_{1}C_{2}}}}\\\\ {{\\mathrm{}}}\\\\ {{K_{i}=\\displaystyle\\frac{R_{4}}{R_{3}R_{1}C_{2}}}}\\\\ {{\\mathrm{}}}\\\\ {{K_{d}=\\displaystyle\\frac{R_{4}R_{2}C_{1}}{R_{3}}}}\\end{array}\n$$  \n\nTable 3–1 shows a list of operational-amplifier circuits that may be used as controllers or compensators.  \n\n![](images/36b5e40caad5223dd373ff276a300ae6d10a7fc01d0442bf5fc278ac9d192c93.jpg)  \n\nA–3–1. Figure 3–20(a) shows a schematic diagram of an automobile suspension system.As the car moves along the road, the vertical displacements at the tires act as the motion excitation to the automobile suspension system.The motion of this system consists of a translational motion of the center of mass and a rotational motion about the center of mass. Mathematical modeling of the complete system is quite complicated.  \n\nA very simplified version of the suspension system is shown in Figure 3–20(b).Assuming that the motion $x_{i}$ at point $P$ is the input to the system and the vertical motion $x_{o}$ of the body is the output,obtain the transfer function $X_{o}(s)/X_{i}(s)$ .(Consider the motion of the body only in the vertical direction.) Displacement $x_{o}$ is measured from the equilibrium position in the absence of input $x_{i}$ .  \n\nSolution. The equation of motion for the system shown in Figure 3–20(b) is  \n\n$$\nm\\ddot{x}_{o}+b\\big(\\dot{x}_{o}\\,-\\,\\dot{x}_{i}\\big)\\,+\\,k\\big(x_{o}\\,-\\,x_{i}\\big)\\,=\\,0\n$$  \n\nor  \n\n$$\nm\\ddot{x}_{o}\\,+\\,b\\dot{x}_{o}\\,+\\,k x_{o}\\,=\\,b\\dot{x}_{i}\\,+\\,k x_{i}\n$$  \n\nTaking the Laplace transform of this last equation, assuming zero initial conditions, we obtain  \n\n$$\n(m s^{2}+b s\\,+\\,k)X_{o}(s)\\,=\\,(b s\\,+\\,k)X_{i}(s)\n$$  \n\nHence the transfer function $X_{o}(s)/X_{i}(s)$ is given by  \n\n$$\n{\\frac{X_{o}(s)}{X_{i}(s)}}={\\frac{b s\\,+\\,k}{m s^{2}\\,+\\,b s\\,+\\,k}}\n$$  \n\n![](images/12a30b2e84e9b777d1c496663501ce6c96945bc519dd01b7219de9aa23462786.jpg)  \nFigure 3–20 (a) Automobile suspension system; (b) simplified suspension system.  \n\nA–3–2. Obtain the transfer function $Y(s)/U(s)$ of the system shown in Figure 3–21. The input $u$ is a displacement input. (Like the system of Problem A–3–1 , this is also a simplified version of an automobile or motorcycle suspension system.)  \n\nSolution. Assume that displacements $x$ and $y$ are measured from respective steady-state positions in the absence of the input $u$ .Applying the Newton’s second law to this system, we obtain  \n\n$$\n\\begin{array}{l}{{m_{1}\\ddot{x}=k_{2}(y\\mathrm{~-~}x)\\,+\\,b(\\dot{y}\\mathrm{~-~}\\dot{x})\\,+\\,k_{1}(u\\mathrm{~-~}x)}}\\\\ {{{}}}\\\\ {{m_{2}\\ddot{y}=-k_{2}(y\\mathrm{~-~}x)\\,-\\,b(\\dot{y}\\mathrm{~-~}\\dot{x})}}\\end{array}\n$$  \n\nHence, we have  \n\n$$\n\\begin{array}{c}{{m_{1}\\ddot{x}\\,+\\,b\\dot{x}\\,+\\,\\bigl(k_{1}\\,+\\,k_{2}\\bigr)x\\,=\\,b\\dot{y}\\,+\\,k_{2}y\\,+\\,k_{1}u}}\\\\ {{{}}}\\\\ {{m_{2}\\ddot{y}\\,+\\,b\\dot{y}\\,+\\,k_{2}y\\,=\\,b\\dot{x}\\,+\\,k_{2}x}}\\end{array}\n$$  \n\nTaking Laplace transforms of these two equations, assuming zero initial conditions, we obtain  \n\n$$\n\\begin{array}{c}{{\\big[m_{1}s^{2}\\,+\\,b s\\,+\\,\\big(k_{1}\\,+\\,k_{2}\\big)\\big]X(s)\\,=\\,\\big(b s\\,+\\,k_{2}\\big)Y(s)\\,+\\,k_{1}U(s)}}\\\\ {{{}}}\\\\ {{\\big[m_{2}s^{2}\\,+\\,b s\\,+\\,k_{2}\\big]Y(s)\\,=\\,\\big(b s\\,+\\,k_{2}\\big)X(s)}}\\end{array}\n$$  \n\nEliminating $X(s)$ from the last two equations, we have  \n\n$$\n\\left(m_{1}s^{2}+b s+k_{1}+k_{2}\\right){\\frac{m_{2}s^{2}+b s+k_{2}}{b s+k_{2}}}Y(s)\\,=\\bigl(b s\\,+\\,k_{2}\\bigr)Y(s)\\,+\\,k_{1}U(s)\n$$  \n\nwhich yields  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{k_{1}(b s\\,+\\,k_{2})}{m_{1}m_{2}s^{4}\\,+\\,(m_{1}\\,+\\,m_{2})b s^{3}\\,+\\,[k_{1}m_{2}\\,+\\,(m_{1}\\,+\\,m_{2})k_{2}]s^{2}\\,+\\,k_{1}b s\\,+\\,k_{1}k_{2}}\n$$  \n\nFigure 3–21 Suspension system.  \n\n![](images/964e1eb0976832e7783d1f01d8176bc2622b9346d6539fb8931d02a2ca74b7b7.jpg)  \n\n![](images/ec789565a6e16f878f921ed76efe0b1444138fb42783517796b4200ea24ada0f.jpg)  \nFigure 3–22 Mechanical system.  \n\nA–3–3. Obtain a state-space representation of the system shown in Figure 3–22. Solution. The system equations are  \n\n$$\n\\begin{array}{r}{m_{1}\\ddot{y}_{1}\\,+\\,b\\dot{y}_{1}\\,+\\,k\\big(y_{1}\\,-\\,y_{2}\\big)=0}\\\\ {m_{2}\\ddot{y}_{2}\\,+\\,k\\big(y_{2}\\,-\\,y_{1}\\big)=u}\\end{array}\n$$  \n\nThe output variables for this system are $y_{1}$ and $y_{2}$ .Define state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=y_{1}}\\\\ {x_{2}=\\dot{y}_{1}}\\\\ {x_{3}=y_{2}}\\\\ {x_{4}=\\dot{y}_{2}}\\end{array}\n$$  \n\nThen we obtain the following equations:  \n\n$$\n\\begin{array}{l}{\\dot{x}_{1}=x_{2}}\\\\ {\\dot{x}_{2}=\\displaystyle\\frac{1}{m_{1}}\\big[-b\\dot{y}_{1}-\\,k\\big(y_{1}-\\,y_{2}\\big)\\big]=-\\displaystyle\\frac{k}{m_{1}}\\,x_{1}-\\displaystyle\\frac{b}{m_{1}}\\,x_{2}+\\displaystyle\\frac{k}{m_{1}}\\,x_{3}}\\\\ {\\dot{x}_{3}=x_{4}}\\\\ {\\dot{x}_{4}=\\displaystyle\\frac{1}{m_{2}}\\big[-k\\big(y_{2}-\\,y_{1}\\big)\\,+\\,u\\big]=\\displaystyle\\frac{k}{m_{2}}\\,x_{1}-\\displaystyle\\frac{k}{m_{2}}\\,x_{3}+\\displaystyle\\frac{1}{m_{2}}\\,u}\\end{array}\n$$  \n\nHence, the state equation is  \n\n$$\n{\\begin{array}{r l}&{{\\overline{{\\mathbf{\\Lambda}}}}_{\\dot{x}_{1}}{\\overline{{\\mathbf{\\Lambda}}}}}\\\\ &{{\\overline{{\\mathbf{\\Lambda}}}}_{\\dot{x}_{2}}}\\\\ &{{\\overline{{\\mathbf{\\Lambda}}}}_{\\dot{x}_{3}}}\\\\ {{\\overline{{\\mathbf{\\Lambda}}}}_{\\dot{x}_{4}}}\\end{array}}\\left[{\\begin{array}{c c c c}{0}&{1}&{0}&{0}\\\\ {{-{\\frac{k}{m_{1}}}}}&{{-{\\frac{b}{m_{1}}}}}&{{{\\frac{k}{m_{1}}}}}&{{0}}\\\\ {0}&{0}&{0}&{1}\\\\ {{\\overline{{\\mathbf{\\Lambda}}}}}&{0}&{{-{\\frac{k}{m_{2}}}}}&{{0}}\\end{array}}\\right]{\\left[{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\\\ {{\\frac{1}{m_{2}}}}\\\\ {{m_{3}}}\\end{array}\\right]}u\n$$  \n\nand the output equation is  \n\n$$\n{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]}\n$$  \n\nA–3–4. Obtain the transfer function $X_{o}(s)/X_{i}(s)$ of the mechanical system shown in Figure 3–23(a). Also obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the electrical system shown in Figure 3–23(b). Show that these transfer functions of the two systems are of identical form and thus they are analogous systems.  \n\n![](images/3442b1cf09580d2d82d0a1d1460b2e5e8c1306b5be65a3158e9b3fa252ebb6e2.jpg)  \nFigure 3–23 (a) Mechanical system; (b) analogous electrical system.   \n(b)  \n\nSolution. In Figure 3–23(a) we assume that displacements $x_{i}$ ,$x_{o}$ ,and $y$ are measured from their respective steady-state positions.Then the equations of motion for the mechanical system shown in Figure 3–23(a) are  \n\n$$\n\\begin{array}{r l}&{b_{1}\\big(\\dot{x}_{i}\\,-\\,\\dot{x}_{o}\\big)\\,+\\,k_{1}\\big(x_{i}\\,-\\,x_{o}\\big)\\,=\\,b_{2}\\big(\\dot{x}_{o}\\,-\\,\\dot{y}\\big)}\\\\ &{b_{2}\\big(\\dot{x}_{o}\\,-\\,\\dot{y}\\big)\\,=\\,k_{2}y}\\end{array}\n$$  \n\nBy taking the Laplace transforms of these two equations,assuming zero initial conditions,we have  \n\n$$\n\\begin{array}{r}{b_{1}\\big[s X_{i}(s)\\,-\\,s X_{o}(s)\\big]\\,+\\,k_{1}\\big[X_{i}(s)\\,-\\,X_{o}(s)\\big]\\,=\\,b_{2}\\big[s X_{o}(s)\\,-\\,s Y(s)\\big]\\,}\\\\ {b_{2}\\big[s X_{o}(s)\\,-\\,s Y(s)\\big]\\,=\\,k_{2}Y(s)\\qquad\\qquad\\qquad}\\end{array}\n$$  \n\nIf we eliminate $Y(s)$ from the last two equations, then we obtain  \n\n$$\nb_{1}[s X_{i}(s)\\,-\\,s X_{o}(s)]\\,+\\,k_{1}\\bigl[X_{i}(s)\\,-\\,X_{o}(s)\\bigr]\\,=\\,b_{2}s X_{o}(s)\\,-\\,b_{2}s\\frac{b_{2}s X_{o}(s)}{b_{2}s\\,+\\,k_{2}}\n$$  \n\nor  \n\n$$\n\\big(b_{1}s\\,+\\,k_{1}\\big)X_{i}(s)\\,=\\,\\bigg(b_{1}s\\,+\\,k_{1}\\,+\\,b_{2}s\\,-\\,b_{2}s\\,\\frac{b_{2}s}{b_{2}s\\,+\\,k_{2}}\\bigg)X_{o}(s)\n$$  \n\nHence the transfer function $X_{o}(s)/X_{i}(s)$ can be obtained as  \n\n$$\n\\frac{X_{o}(s)}{X_{i}(s)}=\\frac{\\bigg(\\displaystyle\\frac{b_{1}}{k_{1}}\\,s\\,+\\,1\\bigg)\\bigg(\\displaystyle\\frac{b_{2}}{k_{2}}\\,s\\,+\\,1\\bigg)}{\\bigg(\\displaystyle\\frac{b_{1}}{k_{1}}\\,s\\,+\\,1\\bigg)\\bigg(\\displaystyle\\frac{b_{2}}{k_{2}}\\,s\\,+\\,1\\bigg)+\\displaystyle\\frac{b_{2}}{k_{1}}\\,s}\n$$  \n\nFor the electrical system shown in Figure 3–23(b), the transfer function $E_{o}(s)/E_{i}(s)$ is found to be  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{R_{1}+\\displaystyle\\frac{1}{C_{1}s}}{\\displaystyle\\frac{1}{(1/R_{2})+C_{2}s}+R_{1}+\\displaystyle\\frac{1}{C_{1}s}}}}\\\\ &{}&{=\\frac{\\left(R_{1}C_{1}s+1\\right)\\!\\left(R_{2}C_{2}s+1\\right)}{\\left(R_{1}C_{1}s+1\\right)\\!\\left(R_{2}C_{2}s+1\\right)+\\ R_{2}C_{1}s}}\\end{array}\n$$  \n\nA comparison of the transfer functions shows that the systems shown in Figures 3–23(a) and (b) are analogous.  \n\nA–3–5. Obtain the transfer functions $E_{o}(s)/E_{i}(s)$ of the bridged T networks shown in Figures 3–24(a) and (b).  \n\nSolution. The bridged $T$ networks shown can both be represented by the network of Figure 3–25(a), where we used complex impedances.This network may be modified to that shown in Figure 3–25(b).  \n\nIn Figure 3–25(b), note that  \n\n$$\nI_{1}={I_{2}}+{I_{3}},\\qquad{I_{2}}{Z_{1}}=\\bigl({Z_{3}}+{Z_{4}}\\bigr){I_{3}}\n$$  \n\nFigure 3–24 Bridged $T$ networks.  \n\n![](images/88cb02d39f30fd86bb0fbf6a75e3ecd8dc3a1139faab97976a61991e5fe4d733.jpg)  \n\n![](images/52d26adef2cbb44662b43e3dd672037c4791dbab07615d2ff692bb192329d2e2.jpg)  \nFigure 3–25 (a) Bridged $T$ network in terms of complex impedances; (b) equivalent network.   \nChapter 3 /Mathematical Modeling of Mechanical Systems and Electrical Systems  \n\nHence  \n\n$$\nI_{2}={\\frac{Z_{3}\\,+\\,Z_{4}}{Z_{1}\\,+\\,Z_{3}\\,+\\,Z_{4}}}\\,I_{1},\\qquad I_{3}={\\frac{Z_{1}}{Z_{1}\\,+\\,Z_{3}\\,+\\,Z_{4}}}\\,I_{1}\n$$  \n\nThen the voltages $E_{i}(s)$ and $E_{o}(s)$ can be obtained as  \n\n$$\n\\begin{array}{r l}&{E_{i}(s)=Z_{1}I_{2}+Z_{2}I_{1}}\\\\ &{\\qquad=\\biggl[Z_{2}+\\cfrac{Z_{1}\\left(Z_{3}\\,+\\,Z_{4}\\right)}{Z_{1}\\,+\\,Z_{3}\\,+\\,Z_{4}}\\biggr]I_{1}}\\\\ &{\\qquad=\\cfrac{Z_{2}\\left(Z_{1}\\,+\\,Z_{3}\\,+\\,Z_{4}\\right)\\,+\\,Z_{1}\\,\\left(Z_{3}\\,+\\,Z_{4}\\right)}{Z_{1}\\,+\\,Z_{3}\\,+\\,Z_{4}}\\,I_{1}}\\end{array}\n$$  \n\n$$\n\\begin{array}{r l}&{E_{o}(s)=Z_{3}I_{3}+Z_{2}I_{1}}\\\\ &{\\qquad\\quad=\\cfrac{Z_{3}Z_{1}}{Z_{1}\\,+\\,\\,Z_{3}\\,+\\,\\,Z_{4}}\\,I_{1}+\\,Z_{2}I_{1}}\\\\ &{\\qquad\\quad=\\cfrac{Z_{3}Z_{1}\\,+\\,\\,Z_{2}\\left(Z_{1}\\,+\\,Z_{3}\\,+\\,\\,Z_{4}\\right)}{Z_{1}\\,+\\,\\,Z_{3}\\,+\\,\\,Z_{4}}\\,I_{1}}\\end{array}\n$$  \n\nHence, the transfer function $E_{o}(s)/E_{i}(s)$ of the network shown in Figure 3–25(a) is obtained as  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{Z_{3}Z_{1}+Z_{2}\\left(Z_{1}\\mathrm{~+~}Z_{3}\\mathrm{~+~}Z_{4}\\right)}{Z_{2}\\!\\left(Z_{1}\\mathrm{~+~}Z_{3}\\mathrm{~+~}Z_{4}\\right)\\mathrm{~+~}Z_{1}Z_{3}\\mathrm{~+~}Z_{1}Z_{4}}\n$$  \n\nFor the bridged T network shown in Figure 3–24(a), substitute  \n\n$$\nZ_{1}={\\cal R},\\ \\ \\ \\ \\ Z_{2}=\\frac{1}{C_{1}s},\\ \\ \\ \\ \\ Z_{3}={\\cal R},\\ \\ \\ \\ \\ Z_{4}=\\frac{1}{C_{2}s}\n$$  \n\ninto Equation (3–38).Then we obtain the transfer function $E_{o}(s)/E_{i}(s)$ to be  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{R^{2}+\\displaystyle\\frac{1}{C_{1}s}\\left(R+R+\\frac{1}{C_{2}s}\\right)}{\\displaystyle\\frac{1}{C_{1}s}\\left(R+R+\\frac{1}{C_{2}s}\\right)+R^{2}+R\\frac{1}{C_{2}s}}}}\\\\ &{}&{=\\frac{R C_{1}R C_{2}s^{2}+2R C_{2}s+1}{R C_{1}R C_{2}s^{2}+\\left(2R C_{2}+R C_{1}\\right)s+1}}\\end{array}\n$$  \n\nSimilarly, for the bridged T network shown in Figure 3–24(b), we substitute  \n\n$$\nZ_{1}={\\frac{1}{C s}},\\ \\ \\ \\ \\ Z_{2}=R_{1},\\ \\ \\ \\ \\ Z_{3}={\\frac{1}{C s}},\\ \\ \\ \\ \\ Z_{4}=R_{2}\n$$  \n\ninto Equation (3–38).Then the transfer function $E_{o}(s)/E_{i}(s)$ can be obtained as follows:  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{\\displaystyle\\frac{1}{C s}\\frac{1}{C s}+{\\cal R}_{1}\\left(\\frac{1}{C s}+\\frac{1}{C s}+{\\cal R}_{2}\\right)}{{\\cal R}_{1}\\left(\\frac{1}{C s}+\\frac{1}{C s}+{\\cal R}_{2}\\right)+\\frac{1}{C s}\\frac{1}{C s}+{\\cal R}_{2}\\frac{1}{C s}}}\\\\ &{}&{=\\frac{{\\cal R}_{1}C{\\cal R}_{2}C s^{2}+2{\\cal R}_{1}C s+1}{{\\cal R}_{1}C{\\cal R}_{2}C s^{2}+(2{\\cal R}_{1}C+{\\cal R}_{2}C)s+1}}\\end{array}\n$$  \n\n![](images/0644b14293618498bb5526ed73c9de14c27c6e66440d4398745e6e7757f27e7d.jpg)  \nFigure 3–26 Operationalamplifier circuit.  \n\nA–3–6. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp circuit shown in Figure 3–26. Solution. The voltage at point $A$ is  \n\n$$\ne_{A}={\\frac{1}{2}}\\left(e_{i}\\,-\\,e_{o}\\right)\\,+\\,e_{o}\n$$  \n\nThe Laplace-transformed version of this last equation is  \n\n$$\nE_{A}(s)\\,=\\frac{1}{2}\\left[E_{i}(s)\\,+\\,E_{o}(s)\\right]\n$$  \n\nThe voltage at point $B$ is  \n\n$$\nE_{B}(s)=\\frac{\\displaystyle\\frac{1}{C s}}{R_{2}+\\frac{1}{C s}}E_{i}(s)=\\frac{1}{R_{2}C s\\,+\\,1}\\,E_{i}(s)\n$$  \n\nSince $\\big[E_{B}(s)\\,-\\,E_{A}(s)\\big]K\\,=\\,E_{o}(s)$ and $K\\gg1$ we must have $E_{A}(s)\\,=\\,E_{B}(s)$ .Thus  \n\n$$\n{\\frac{1}{2}}\\bigl[E_{i}(s)\\,+\\,E_{o}(s)\\bigr]={\\frac{1}{R_{2}C s\\,+\\,1}}\\,E_{i}(s)\n$$  \n\nHence  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}=-{\\frac{R_{2}C s\\,-\\,1}{R_{2}C s\\,+\\,1}}=-{\\frac{s-{\\frac{1}{R_{2}C}}}{s\\,+\\,{\\frac{1}{R_{2}C}}}}\n$$  \n\nA–3–7. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp system shown in Figure 3–27 in terms of complex impedances $Z_{1},Z_{2},Z_{3}$ , and $Z_{4}$ . Using the equation derived, obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp system shown in Figure 3–26.  \n\nSolution. From Figure 3–27, we find  \n\n$$\n\\frac{E_{i}(s)\\,-\\,E_{A}(s)}{Z_{3}}=\\frac{E_{A}(s)\\,-\\,E_{o}(s)}{Z_{4}}\n$$  \n\nFigure 3–27 Operationalamplifier circuit.  \n\n![](images/0a099c0ec4bde94d2ff6375b8d453825e7bcdb5ce28229a0a28f2a9bdde90a31.jpg)  \n\nor  \n\n$$\nE_{i}(s)\\,-\\,\\biggl(1\\,+\\frac{Z_{3}}{Z_{4}}\\biggr)E_{A}(s)\\,=-\\,\\frac{Z_{3}}{Z_{4}}\\,E_{o}(s)\n$$  \n\nSince  \n\n$$\nE_{A}(s)\\,=\\,E_{B}(s)\\,=\\,\\frac{Z_{1}}{Z_{1}\\,+\\,Z_{2}}\\,E_{i}(s)\n$$  \n\nby substituting Equation (3–40) into Equation (3–39), we obtain  \n\n$$\n\\Bigg[\\frac{Z_{4}Z_{1}\\,+\\,Z_{4}Z_{2}\\,-\\,Z_{4}Z_{1}\\,-\\,Z_{3}Z_{1}}{Z_{4}\\big(Z_{1}\\,+\\,Z_{2}\\big)}\\Bigg]E_{i}(s)\\,=\\,-\\,\\frac{Z_{3}}{Z_{4}}\\,E_{o}(s)\n$$  \n\nfrom which we get the transfer function $E_{o}(s)/E_{i}(s)$ to be  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=-\\,\\frac{Z_{4}Z_{2}\\,-\\,Z_{3}Z_{1}}{Z_{3}(Z_{1}\\,+\\,Z_{2})}\n$$  \n\nTo find the transfer function $E_{o}(s)/E_{i}(s)$ of the circuit shown in Figure 3–26, we substitute  \n\n$$\nZ_{1}=\\frac{1}{C s},\\ \\ \\ \\ \\ Z_{2}=R_{2},\\ \\ \\ \\ \\ Z_{3}=R_{1},\\ \\ \\ \\ \\ Z_{4}=R_{1}\n$$  \n\ninto Equation (3–41).The result is  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}=-{\\frac{R_{1}R_{2}-\\,R_{1}{\\frac{1}{C s}}}{R_{1}\\bigg({\\frac{1}{C s}}+\\,R_{2}\\bigg)}}=-\\,{\\frac{R_{2}C s\\,-\\,1}{R_{2}C s\\,+\\,1}}\n$$  \n\nwhich is, as a matter of course, the same as that obtained in Problem A–3–6 .  \n\nA–3–8. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the operational-amplifier circuit shown in Figure 3–28.  \n\nSolution. We will first obtain currents $i_{1},i_{2},i_{3},i_{4}$ , and $i_{5}$ .Then we will use node equations at nodes $A$ and $B$ .  \n\n$$\n\\begin{array}{l l}{{\\displaystyle i_{1}=\\frac{e_{i}-e_{A}}{R_{1}};\\quad}}&{{\\displaystyle i_{2}=\\frac{e_{A}-e_{o}}{R_{3}},\\quad}}&{{\\displaystyle i_{3}=C_{1}\\frac{d e_{A}}{d t}}}\\\\ {{\\displaystyle i_{4}=\\frac{e_{A}}{R_{2}},\\quad}}&{{\\displaystyle i_{5}=C_{2}\\frac{-d e_{o}}{d t}}}\\end{array}\n$$  \n\nAt node $A$ , we have $i_{1}=i_{2}+i_{3}+i_{4}$ ,or  \n\n$$\n\\frac{e_{i}-e_{A}}{R_{1}}=\\frac{e_{A}-e_{o}}{R_{3}}+C_{1}\\frac{d e_{A}}{d t}+\\frac{e_{A}}{R_{2}}\n$$  \n\nAt node $B$ , we get $i_{4}=i_{5}$ ,or  \n\n$$\n\\frac{e_{A}}{R_{2}}=C_{2}\\frac{-d e_{o}}{d t}\n$$  \n\nBy rewriting Equation (3–42), we have  \n\n$$\nC_{1}\\frac{d e_{A}}{d t}+\\bigg(\\frac{1}{R_{1}}+\\frac{1}{R_{2}}+\\frac{1}{R_{3}}\\bigg)e_{A}=\\frac{e_{i}}{R_{1}}+\\frac{e_{o}}{R_{3}}\n$$  \n\nFrom Equation (3–43), we get  \n\n$$\ne_{A}=-R_{2}C_{2}\\frac{d e_{o}}{d t}\n$$  \n\nBy substituting Equation (3–45) into Equation (3–44), we obtain  \n\n$$\nC_{1}\\bigg(-R_{2}C_{2}\\frac{d^{2}e_{o}}{d t^{2}}\\bigg)\\:+\\:\\bigg(\\frac{1}{R_{1}}+\\frac{1}{R_{2}}+\\frac{1}{R_{3}}\\bigg)\\big(-R_{2}C_{2}\\big)\\frac{d e_{o}}{d t}=\\frac{e_{i}}{R_{1}}+\\frac{e_{o}}{R_{3}}\n$$  \n\nTaking the Laplace transform of this last equation, assuming zero initial conditions, we obtain  \n\n$$\n-C_{1}C_{2}R_{2}s^{2}E_{o}(s)\\:+\\:\\biggl(\\frac{1}{R_{1}}+\\frac{1}{R_{2}}+\\frac{1}{R_{3}}\\biggr)(-R_{2}C_{2})s E_{o}(s)\\:-\\:\\frac{1}{R_{3}}\\:E_{o}(s)\\:=\\:\\frac{E_{i}(s)}{R_{1}}\n$$  \n\nfrom which we get the transfer function $E_{o}(s)/E_{i}(s)$ as follows:  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=-\\,\\frac{1}{R_{1}C_{1}R_{2}C_{2}s^{2}\\,+\\,\\big[R_{2}C_{2}\\,+\\,R_{1}C_{2}\\,+\\,\\big(R_{1}/R_{3}\\big)R_{2}C_{2}\\big]s\\,+\\,\\big(R_{1}/R_{3}\\big)}\n$$  \n\n![](images/b536cfc222f4bb774e313f509501673cccf339b766e6b3cacbfabd9cba75f032.jpg)  \nFigure 3–28 Operationalamplifier circuit.  \n\nA–3–9. Consider the servo system shown in Figure 3–29(a).The motor shown is a servomotor,a dc motor designed specifically to be used in a control system.The operation of this system is as follows:A pair of potentiometers acts as an error-measuring device.They convert the input and output positions into proportional electric signals. The command input signal determines the angular position $r$ of the wiper arm of the input potentiometer.The angular position $r$ is the reference input to the system,and the electric potential of the arm is proportional to the angular position of the arm.The output shaft position determines the angular position $c$ of the wiper arm of the output potentiometer.The difference between the input angular position $r$ and the output angular position $c$ is the error signal $e$ , or  \n\n$$\ne\\,=\\,r\\,-\\,c\n$$  \n\nThe potential difference $e_{r}\\mathrm{~-~}e_{c}=e_{v}$ is the error voltage, where $e_{r}$ is proportional to $r$ and $e_{c}$ is proportional to $c$ ; that is, $e_{r}=K_{0}r$ and $e_{c}=K_{0}c$ ,where $K_{0}$ is a proportionality constant.The error voltage that appears at the potentiometer terminals is amplified by the amplifier whose gain constant is $K_{1}$ .The output voltage of this amplifier is applied to the armature circuit of the dc motor.A fixed voltage is applied to the field winding. If an error exists, the motor develops a torque to rotate the output load in such a way as to reduce the error to zero. For constant field current, the torque developed by the motor is  \n\n$$\nT\\,=\\,K_{2}i_{a}\n$$  \n\nwhere $K_{2}$ is the motor torque constant and $i_{a}$ is the armature current.  \n\nWhen the armature is rotating, a voltage proportional to the product of the flux and angular velocity is induced in the armature. For a constant flux, the induced voltage $e_{b}$ is directly proportional to the angular velocity $d\\theta/d t$ ,or  \n\n$$\ne_{b}=K_{3}\\frac{d\\theta}{d t}\n$$  \n\nwhere $e_{b}$ is the back emf, $K_{3}$ is the back emf constant of the motor, and $\\theta$ is the angular displacement of the motor shaft.  \n\n![](images/bf6b2afa96693a9778a9d270eb527f0a953e996d2cdce11dbc8bffa804d28b81.jpg)  \nFigure 3–29 (a) Schematic diagram of servo system; (b) block diagram for the system; (c) simplified block diagram.  \n\nObtain the transfer function between the motor shaft angular displacement $\\theta$ and the error voltage $e_{v}$ .Obtain also a block diagram for this system and a simplified block diagram when $L_{a}$ is negligible.  \n\nSolution. The speed of an armature-controlled dc servomotor is controlled by the armature voltage $e_{a}$ .(The armature voltage $e_{a}=K_{1}e_{v}$ is the output of the amplifier.) The differential equation for the armature circuit is  \n\n$$\nL_{a}\\frac{d i_{a}}{d t}+R_{a}i_{a}\\,+\\,e_{b}=\\,e_{a}\n$$  \n\nor  \n\n$$\nL_{a}\\frac{d i_{a}}{d t}+R_{a}i_{a}+K_{3}\\frac{d\\theta}{d t}=K_{1}e_{v}\n$$  \n\nThe equation for torque equilibrium is  \n\n$$\nJ_{0}\\frac{d^{2}\\theta}{d t^{2}}+b_{0}\\frac{d\\theta}{d t}=T=K_{2}i_{a}\n$$  \n\nwhere $J_{0}$ is the inertia of the combination of the motor, load, and gear train referred to the motor shaft and $b_{0}$ is the viscous-friction coefficient of the combination of the motor, load, and gear train referred to the motor shaft.  \n\nBy eliminating $i_{a}$ from Equations (3–46) and (3–47), we obtain  \n\n$$\n\\frac{\\Theta(s)}{E_{v}(s)}=\\frac{K_{1}K_{2}}{s(L_{a}s\\,+\\,R_{a})(J_{0}s\\,+\\,b_{0})\\,+\\,K_{2}K_{3}s}\n$$  \n\nWe assume that the gear ratio of the gear train is such that the output shaft rotates $n$ times for each revolution of the motor shaft.Thus,  \n\n$$\nC(s)\\,=\\,n\\theta(s)\n$$  \n\nThe relationship among $E_{v}(s),R(s)$ ,and $C(s)$ is  \n\n$$\nE_{v}(s)\\,=\\,K_{0}\\big[R(s)\\,-\\,C(s)\\big]\\,=\\,K_{0}{\\cal E}(s)\n$$  \n\nThe block diagram of this system can be constructed from Equations (3–48), (3–49), and (3–50), as shown in Figure 3–29(b).The transfer function in the feedforward path of this system is  \n\n$$\nG(s)\\,=\\frac{C(s)}{\\theta(s)}\\frac{\\,\\theta(s)\\,}{{E_{v}}(s)}\\frac{{E_{v}}(s)\\,}{{E(s)}}=\\frac{{K_{0}K_{1}K_{2}n}}{s[\\bigl({L_{a}s\\,+\\,R_{a}}\\bigr)\\bigl({J_{0}s\\,+\\,b_{0}}\\bigr)\\,+\\,{K_{2}K_{3}}]}\n$$  \n\nWhen $L_{a}$ is small, it can be neglected, and the transfer function $G(s)$ in the feedforward path becomes  \n\n$$\n\\begin{array}{c}{{G(s)=\\displaystyle\\frac{K_{0}K_{1}K_{2}n}{s\\big[R_{a}(J_{0}s~+~b_{0})~+~K_{2}K_{3}\\big]}}}\\\\ {{=\\displaystyle\\frac{K_{0}K_{1}K_{2}n/R_{a}}{J_{0}s^{2}~+~\\Big(b_{0}~+~\\displaystyle\\frac{K_{2}K_{3}}{R_{a}}\\Big)s}}}\\end{array}\n$$  \n\nThe term $\\big[b_{0}\\,+\\,\\big(K_{2}K_{3}/R_{a}\\big)\\big]s$ indicates that the back emf of the motor effectively increases the viscous friction of the system. The inertia $J_{0}$ and viscous friction coefficient $b_{0}\\,+\\,\\dot{\\left(K_{2}K_{3}/R_{a}\\right)}$ are  \n\nreferred to the motor shaft. When $J_{0}$ and $b_{0}\\,+\\,\\left(K_{2}K_{3}/R_{a}\\right)$ A Bare multiplied by $1/n^{2}$ ,the inertia and viscous-friction coefficient are expressed in terms of the output shaft. Introducing new parameters defined by  \n\n$$\n\\begin{array}{r l}&{J=J_{0}/n^{2}=\\mathrm{moment~of~inertia~referred~to~the~output~shaft}}\\\\ &{B=\\big[b_{0}+\\big(K_{2}K_{3}/R_{a}\\big)\\big]/n^{2}=\\mathrm{~viscous-friction~coefficient~referred~to~the~output~s~}}\\\\ &{K=K_{0}K_{1}K_{2}/n R_{a}}\\end{array}\n$$  \n\nthe transfer function $G(s)$ given by Equation (3–51) can be simplified, yielding  \n\n$$\nG(s)={\\frac{K}{J s^{2}\\,+\\,B s}}\n$$  \n\nor  \n\n$$\nG(s)=\\frac{K_{m}}{s(T_{m}s\\,+\\,1)}\n$$  \n\nwhere  \n\n$$\nK_{m}=\\frac{K}{B},\\ \\ \\ \\ \\ T_{m}=\\frac{J}{B}=\\frac{R_{a}J_{0}}{R_{a}b_{0}+K_{2}K_{3}}\n$$  \n\nThe block diagram of the system shown in Figure 3–29(b) can thus be simplified as shown in Figure 3–29(c).  \n\n# PROBLEMS  \n\nB–3–1. Obtain the equivalent viscous-friction coefficient $b_{e q}$ of the system shown in Figure 3–30.  \n\n![](images/ec17c6680a2c3314f575e6ac22ad800e62c837511a05ff3ca22a7c59e85108f3.jpg)  \n\nFigure 3–30 Damper system.  \n\nB–3–2. Obtain mathematical models of the mechanical systems shown in Figures 3–31(a) and (b).  \n\n![](images/01fdfe921c8b8b7c99b2cc3490ea5ca5ce0537e30e05cabcb136a12c873b256f.jpg)  \nFigure 3–31 Mechanical systems.  \n\nB–3–3. Obtain a state-space representation of the mechanical system shown in Figure 3–32, where $u_{1}$ and $u_{2}$ are the inputs and $y_{1}$ and $y_{2}$ are the outputs.  \n\n![](images/98c4927639c05419895c086b9f56dbeb3a94eca2184c32d66f891fccdaebb8c0.jpg)  \nFigure 3–34 Inverted-pendulum system.  \n\n![](images/4942e6633833d1ff0b3dc26b1b6b6cc7bcc40d7f8ab9474c96f79606ee380c18.jpg)  \nFigure 3–32 Mechanical system.  \n\nB–3–4. Consider the spring-loaded pendulum system shown in Figure 3–33. Assume that the spring force acting on the pendulum is zero when the pendulum is vertical, or $\\theta\\,=\\,0$ .Assume also that the friction involved is negligible and the angle of oscillation $\\theta$ is small. Obtain a mathematical model of the system.  \n\nB–3–6. Obtain the transfer functions $X_{1}(s)/U(s)$ and $X_{2}(s)/U(s)$ of the mechanical system shown in Figure 3–35.  \n\n![](images/0c7ab20ad315c8271b749c781004e968b6b664b7436bb53a16187941a025411a.jpg)  \nFigure 3–33 Spring-loaded pendulum system.  \n\nB–3–5. Referring to Examples 3–5 and 3–6, consider the inverted-pendulum system shown in Figure 3–34. Assume that the mass of the inverted pendulum is $_m$ and is evenly distributed along the length of the rod. (The center of gravity of the pendulum is located at the center of the rod.) Assuming that $\\theta$ is small, derive mathematical models for the system in the forms of differential equations, transfer functions, and state-space equations.  \n\n![](images/12175073a5ad3a5e3a66796c0bc051393a54eb33f28d6f39dc1bb9e5d5a157fd.jpg)  \nFigure 3–35 Mechanical system.  \n\nB–3–7. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the electrical circuit shown in Figure 3–36.  \n\n![](images/a879b8b9d9986b6663e28732647992c268330ad9c7a49ad34bbf339718114e08.jpg)  \nFigure 3–36 Electrical circuit.  \n\nB–3–8. Consider the electrical circuit shown in Figure 3–37. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ by use of the block diagram approach.  \n\n![](images/303f93ae066b0960c807c6aed682ba4233b6ec08417f4c81815a9660ee382c09.jpg)  \nFigure 3–37 Electrical circuit.  \n\nB–3–9. Derive the transfer function of the electrical circuit shown in Figure 3–38. Draw a schematic diagram of an analogous mechanical system.  \n\n![](images/6c1d5990bfeb5664751b8e3e8f97d57441eccb595df46541a6ee33ef21b414c5.jpg)  \nFigure 3–38 Electrical circuit.  \n\nB–3–10. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp circuit shown in Figure 3–39.  \n\n![](images/dab514c5c3c664d12c0a39ae6d52a01d31c85cbc0c465b6dfbd23082dbb91834.jpg)  \nFigure 3–39 Operational-amplifier circuit.  \n\nB–3–11. Obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp circuit shown in Figure 3–40.  \n\n![](images/566d48815b69b96307be60ee21a8641cb5ad1350bc1ec8e92a61f7b72a20fe7c.jpg)  \nFigure 3–40 Operational-amplifier circuit.  \n\nB–3–12. Using the impedance approach, obtain the transfer function $E_{o}(s)/E_{i}(s)$ of the op-amp circuit shown in Figure 3–41.  \n\n![](images/f5c5030801cd4ddbb27114e6bc2001f3868476a4ac31a68a3e32caf2dc31b344.jpg)  \nFigure 3–41 Operational-amplifier circuit.  \n\nB–3–13. Consider the system shown in Figure 3–42. An armature-controlled dc servomotor drives a load consisting of the moment of inertia $J_{L}$ . The torque developed by the motor is $T.$ The moment of inertia of the motor rotor is $J_{m}$ .The angular displacements of the motor rotor and the load element are $\\theta_{m}$ and $\\theta$ , respectively. The gear ratio is $n=\\theta/\\theta_{m}$ .Obtain the transfer function $\\theta(s)/E_{i}(s)$ .  \n\n![](images/65e352e9df75c9c53361af274ac5f4004fe2cc26a21fb067226a490f61373140.jpg)  \nFigure 3–42 Armature-controlled dc servomotor system.  \n\n# Mathematical Modeling of Fluid Systems and Thermal Systems  \n\n4–1 INTRODUCTION  \n\nThis chapter treats mathematical modeling of fluid systems and thermal systems.As the most versatile medium for transmitting signals and power, fluids—liquids and gases— have wide usage in industry.Liquids and gases can be distinguished basically by their relative incompressibilities and the fact that a liquid may have a free surface, whereas a gas expands to fill its vessel. In the engineering field the term pneumatic describes fluid systems that use air or gases and hydraulic applies to those using oil.  \n\nWe first discuss liquid-level systems that are frequently used in process control. Here we introduce the concepts of resistance and capacitance to describe the dynamics of such systems. Then we treat pneumatic systems. Such systems are extensively used in the automation of production machinery and in the field of automatic controllers. For instance, pneumatic circuits that convert the energy of compressed air into mechanical energy enjoy wide usage.Also,various types of pneumatic controllers are widely used in industry.Next, we present hydraulic servo systems.These are widely used in machine tool systems,aircraft control systems, etc. We discuss basic aspects of hydraulic servo systems and hydraulic controllers.Both pneumatic systems and hydraulic systems can be modeled easily by using the concepts of resistance and capacitance. Finally, we treat simple thermal systems. Such systems involve heat transfer from one substance to another. Mathematical models of such systems can be obtained by using thermal resistance and thermal capacitance.  \n\nOutline of the Chapter. Section 4–1 has presented introductory material for the chapter. Section 4–2 discusses liquid-level systems. Section 4–3 treats pneumatic systems—in particular, the basic principles of pneumatic controllers. Section 4–4 first discusses hydraulic servo systems and then presents hydraulic controllers. Finally, Section 4–5 analyzes thermal systems and obtains mathematical models of such systems.  \n\nIn analyzing systems involving fluid flow, we find it necessary to divide flow regimes into laminar flow and turbulent flow, according to the magnitude of the Reynolds number. If the Reynolds number is greater than about 3000 to 4000, then the flow is turbulent.The flow is laminar if the Reynolds number is less than about 2000. In the laminar case, fluid flow occurs in streamlines with no turbulence. Systems involving laminar flow may be represented by linear differential equations.  \n\nIndustrial processes often involve flow of liquids through connecting pipes and tanks. The flow in such processes is often turbulent and not laminar. Systems involving turbulent flow often have to be represented by nonlinear differential equations. If the region of operation is limited, however, such nonlinear differential equations can be linearized. We shall discuss such linearized mathematical models of liquid-level systems in this section.Note that the introduction of concepts of resistance and capacitance for such liquidlevel systems enables us to describe their dynamic characteristics in simple forms.  \n\nResistance and Capacitance of Liquid-Level Systems. Consider the flow through a short pipe connecting two tanks. The resistance $R$ for liquid flow in such a pipe or restriction is defined as the change in the level difference (the difference of the liquid levels of the two tanks) necessary to cause a unit change in flow rate; that is,  \n\n$$\nR={\\frac{{\\mathrm{change~in~level~difference}},\\,\\mathrm{m}}{{\\mathrm{change~in~flow~rate}},\\,\\mathrm{m}^{3}/{\\mathrm{sec}}}}\n$$  \n\nSince the relationship between the flow rate and level difference differs for the laminar flow and turbulent flow, we shall consider both cases in the following.  \n\nConsider the liquid-level system shown in Figure 4–1(a). In this system the liquid spouts through the load valve in the side of the tank. If the flow through this restriction is laminar, the relationship between the steady-state flow rate and steady-state head at the level of the restriction is given by  \n\n$$\nQ\\,=\\,K H\n$$  \n\n![](images/24c0a9a132c835fe2e884e55cd5f04fb3e2c3d4b293fc172d03691598f686952.jpg)  \nFigure 4–1 (a) Liquid-level system; (b) headversus-flow-rate curve.  \n\nwhere $Q=$ steady-state liquid flow rate, $\\mathrm{m}^{3}$ 'sec  \n\n$K=$ coefficient, $\\mathbf{m}^{2}.$ 'sec $H=$ steady-state head, m  \n\nFor laminar flow, the resistance $R_{l}$ is obtained as  \n\n$$\nR_{l}=\\frac{d H}{d Q}=\\frac{H}{Q}\n$$  \n\nThe laminar-flow resistance is constant and is analogous to the electrical resistance.  \n\nIf the flow through the restriction is turbulent, the steady-state flow rate is given by  \n\n$$\nQ=K{\\sqrt{H}}\n$$  \n\nwhere $Q=$ steady-state liquid flow rate, $\\mathrm{m}^{3}/\\mathrm{sec}$  \n\n$K=$ coefficient, $\\mathrm{m}^{2.5}/\\mathrm{sec}$ $H=$ steady-state head, m  \n\nThe resistance $R_{t}$ for turbulent flow is obtained from  \n\n$$\nR_{t}=\\frac{d H}{d Q}\n$$  \n\nSince from Equation (4–1) we obtain  \n\n$$\nd Q={\\frac{K}{2{\\sqrt{H}}}}\\,d H\n$$  \n\nwe have  \n\n$$\n{\\frac{d H}{d Q}}={\\frac{2{\\sqrt{H}}}{K}}={\\frac{2{\\sqrt{H}}\\,{\\sqrt{H}}}{Q}}={\\frac{2H}{Q}}\n$$  \n\nThus,  \n\n$$\nR_{t}=\\frac{2H}{Q}\n$$  \n\nThe value of the turbulent-flow resistance $R_{t}$ depends on the flow rate and the head.The value of $R_{t}$ , however, may be considered constant if the changes in head and flow rate are small.  \n\nBy use of the turbulent-flow resistance, the relationship between $Q$ and $H$ can be given by  \n\n$$\nQ=\\frac{2H}{R_{t}}\n$$  \n\nSuch linearization is valid, provided that changes in the head and flow rate from their respective steady-state values are small.  \n\nIn many practical cases,the value of the coefficient $K$ in Equation (4–1),which depends on the flow coefficient and the area of restriction, is not known.Then the resistance may be determined by plotting the head-versus-flow-rate curve based on experimental data and measuring the slope of the curve at the operating condition.An example of such a plot is shown in Figure 4–1(b).In the figure,point $P$ is the steady-state operating point.The tangent line to the curve at point $P$ intersects the ordinate at point $\\left({\\bar{0}},-{\\bar{H}}\\right)$ .Thus, the slope of this tangent line is $2\\bar{H}/\\bar{Q}$ .Since the resistance $R_{t}$ at the operating point $P$ is given by $2\\bar{H}/\\bar{Q}$ ,the resistance $R_{t}$ is the slope of the curve at the operating point.  \n\nConsider the operating condition in the neighborhood of point $P$ . Define a small deviation of the head from the steady-state value as $h$ and the corresponding small change of the flow rate as $q$ .Then the slope of the curve at point $P$ can be given by  \n\n$$\n{\\mathrm{Slope~of~curve~at~point~}}P={\\frac{h}{q}}={\\frac{2\\overline{{H}}}{\\overline{{Q}}}}=R_{t}\n$$  \n\nThe linear approximation is based on the fact that the actual curve does not differ much from its tangent line if the operating condition does not vary too much.  \n\nThe capacitance $C$ of a tank is defined to be the change in quantity of stored liquid necessary to cause a unit change in the potential (head). (The potential is the quantity that indicates the energy level of the system.)  \n\n$$\nC={\\frac{{\\mathrm{change}}\\;{\\mathrm{in}}\\;{\\mathrm{liquid}}\\;{\\mathrm{stored}},{\\mathrm{m}}^{3}}{{\\mathrm{change}}\\;{\\mathrm{in}}\\;{\\mathrm{head}},{\\mathrm{m}}}}\n$$  \n\nIt should be noted that the capacity $(\\mathrm{m}^{3})$ and the capacitance $(\\mathbf{m}^{2})$ are different. The capacitance of the tank is equal to its cross-sectional area. If this is constant, the capacitance is constant for any head.  \n\nLiquid-Level Systems. Consider the system shown in Figure 4–1(a). The variables are defined as follows:  \n\n$\\bar{Q}=$ steady-state flow rate (before any change has occurred), $\\mathrm{m}^{3}/\\mathrm{sec}$ $q_{i}=$ small deviation of inflow rate from its steady-state value, $\\mathrm{m}^{3}$ 'sec $q_{o}=$ small deviation of outflow rate from its steady-state value, $\\mathrm{m}^{3}/\\mathrm{sec}$ $\\bar{H}\\,=$ steady-state head (before any change has occurred), m $h=$ small deviation of head from its steady-state value, m  \n\nAs stated previously, a system can be considered linear if the flow is laminar. Even if the flow is turbulent, the system can be linearized if changes in the variables are kept small.Based on the assumption that the system is either linear or linearized,the differential equation of this system can be obtained as follows:Since the inflow minus outflow during the small time interval $d t$ is equal to the additional amount stored in the tank, we see that  \n\n$$\nC\\:d h\\,=\\,\\bigl(q_{i}\\,-\\,q_{o}\\bigr)\\,d t\n$$  \n\nFrom the definition of resistance, the relationship between $q_{o}$ and $h$ is given by  \n\n$$\nq_{o}=\\frac{h}{R}\n$$  \n\nThe differential equation for this system for a constant value of $R$ becomes  \n\n$$\nR C\\,{\\frac{d h}{d t}}+h\\,=\\,R q_{i}\n$$  \n\nNote that $R C$ is the time constant of the system.Taking the Laplace transforms of both sides of Equation (4–2), assuming the zero initial condition, we obtain  \n\n$$\n(R C s\\,+\\,1)H(s)\\,=\\,R Q_{i}(s)\n$$  \n\nwhere  \n\n$$\nH(s)\\,=\\,{\\mathcal{L}}[h]\\qquad{\\mathrm{and}}\\qquad Q_{i}(s)\\,=\\,{\\mathcal{L}}[q_{i}]\n$$  \n\nIf $q_{i}$ is considered the input and $h$ the output, the transfer function of the system is  \n\n$$\n{\\frac{H(s)}{Q_{i}(s)}}={\\frac{R}{R C s\\,+\\,1}}\n$$  \n\nIf, however, $q_{o}$ is taken as the output, the input being the same, then the transfer function is  \n\n$$\n\\frac{Q_{o}(s)}{Q_{i}(s)}=\\frac{1}{R C s\\,+\\,1}\n$$  \n\nwhere we have used the relationship  \n\n$$\nQ_{o}(s)={\\frac{1}{R}}\\,H(s)\n$$  \n\nLiquid-Level Systems with Interaction. Consider the system shown in Figure 4–2. In this system, the two tanks interact.Thus the transfer function of the system is not the product of two first-order transfer functions.  \n\nIn the following, we shall assume only small variations of the variables from the steady-state values. Using the symbols as defined in Figure 4–2, we can obtain the following equations for this system:  \n\n$$\n\\begin{array}{c}{{\\frac{l_{1}-\\dot{h}_{2}}{R_{1}}=q_{1}}}\\\\ {{{}}}\\\\ {{C_{1}\\frac{d h_{1}}{d t}=q-q_{1}}}\\\\ {{{}}}\\\\ {{\\frac{h_{2}}{R_{2}}=q_{2}}}\\\\ {{{}}}\\\\ {{C_{2}\\frac{d h_{2}}{d t}=q_{1}-q_{2}}}\\end{array}\n$$  \n\nIf $q$ is considered the input and $q_{2}$ the output, the transfer function of the system is  \n\n$$\n\\frac{Q_{2}(s)}{Q(s)}=\\frac{1}{R_{1}C_{1}R_{2}C_{2}s^{2}\\,+\\,\\left(R_{1}C_{1}\\,+\\,R_{2}C_{2}\\,+\\,R_{2}C_{1}\\right)s\\,+\\,1}\n$$  \n\n![](images/3eda815b869e1446b6cc7acd65ce36e675e52c5e98dea2d8487f58b506f74cb4.jpg)  \n$\\bar{Q}$ :Steady-state flow rate $\\dot{\\overline{{H}}}_{1}$ :Steady-state liquid level of tank 1 $\\overline{{H}}_{2}$ :Steady-state liquid level of tank 2  \n\n![](images/63b8189f71cb97d11bef7c8cdf3271538fbe89d32b60e526a190eb5d8630097f.jpg)  \nFigure 4–3 (a) Elements of the block diagram of the system shown in Figure 4–2; (b) block diagram of the system; (c)–(e) successive reductions of the block diagram.  \n\nIt is instructive to obtain Equation (4–7), the transfer function of the interacted system, by block diagram reduction. From Equations (4–3) through (4–6), we obtain the elements of the block diagram, as shown in Figure 4–3(a). By connecting signals properly, we can construct a block diagram, as shown in Figure 4–3(b). This block diagram can be simplified, as shown in Figure 4–3(c). Further simplifications result in Figures 4–3(d) and (e). Figure 4–3(e) is equivalent to Equation (4–7).  \n\nNotice the similarity and difference between the transfer function given by Equation (4–7) and that given by Equation (3–33).The term $R_{2}C_{1}s$ that appears in the denominator of Equation (4–7) exemplifies the interaction between the two tanks. Similarly, the term $R_{1}C_{2}s$ in the denominator of Equation (3–33) represents the interaction between the two $R C$ circuits shown in Figure 3–8.  \n\n# 4–3 PNEUMATIC SYSTEMS  \n\nIn industrial applications pneumatic systems and hydraulic systems are frequently compared.Therefore, before we discuss pneumatic systems in detail, we shall give a brief comparison of these two kinds of systems.  \n\nComparison Between Pneumatic Systems and Hydraulic Systems. The fluid generally found in pneumatic systems is air; in hydraulic systems it is oil. And it is primarily the different properties of the fluids involved that characterize the differences between the two systems.These differences can be listed as follows:  \n\n1. Air and gases are compressible,whereas oil is incompressible (except at high pressure).   \n2. Air lacks lubricating property and always contains water vapor. Oil functions as a hydraulic fluid as well as a lubricator.   \n3. The normal operating pressure of pneumatic systems is very much lower than that of hydraulic systems.   \n4. Output powers of pneumatic systems are considerably less than those of hydraulic systems.   \n5. Accuracy of pneumatic actuators is poor at low velocities, whereas accuracy of hydraulic actuators may be made satisfactory at all velocities.   \n6. In pneumatic systems, external leakage is permissible to a certain extent, but internal leakage must be avoided because the effective pressure difference is rather small. In hydraulic systems internal leakage is permissible to a certain extent, but external leakage must be avoided.   \n7. No return pipes are required in pneumatic systems when air is used, whereas they are always needed in hydraulic systems.   \n8. Normal operating temperature for pneumatic systems is $5^{\\circ}$ to $60^{\\circ}\\mathrm{C}$ ($41^{\\circ}$ to $140^{\\circ}\\mathrm{F}$ ). The pneumatic system, however, can be operated in the $0^{\\circ}$ to $200^{\\circ}\\mathrm{C}$ ($32^{\\circ}$ to $392^{\\circ}\\mathrm{F},$ range. Pneumatic systems are insensitive to temperature changes, in contrast to hydraulic systems, in which fluid friction due to viscosity depends greatly on temperature. Normal operating temperature for hydraulic systems is $20^{\\circ}$ to $70^{\\circ}\\mathrm{C}$ ($68^{\\circ}$ to $158^{\\circ}\\mathrm{F}$ ).   \n9. Pneumatic systems are fire- and explosion-proof, whereas hydraulic systems are not, unless nonflammable liquid is used.   \nIn what follows we begin with a mathematical modeling of pneumatic systems.Then  \n\nWe shall first give detailed discussions of the principle by which proportional controllers operate.Then we shall treat methods for obtaining derivative and integral control actions. Throughout the discussions, we shall place emphasis on the fundamental principles, rather than on the details of the operation of the actual mechanisms.  \n\nPneumatic Systems. The past decades have seen a great development in lowpressure pneumatic controllers for industrial control systems, and today they are used extensively in industrial processes. Reasons for their broad appeal include an explosionproof character, simplicity, and ease of maintenance.  \n\nResistance and Capacitance of Pressure Systems. Many industrial processes and pneumatic controllers involve the flow of a gas or air through connected pipelines and pressure vessels.  \n\nConsider the pressure system shown in Figure 4–4(a). The gas flow through the restriction is a function of the gas pressure difference $p_{i}\\mathrm{~-~}p_{o}$ . Such a pressure system may be characterized in terms of a resistance and a capacitance.  \n\nThe gas flow resistance $R$ may be defined as follows:  \n\n$$\nR={\\frac{\\mathrm{change\\in\\gas\\pressure\\difference,lb_{f}/f t^{2}}}{\\mathrm{change\\in\\gas\\flow\\rate,lb/sec}}}\n$$  \n\nor  \n\n$$\nR=\\frac{d(\\Delta P)}{d q}\n$$  \n\nwhere $d(\\Delta P)$ is a small change in the gas pressure difference and $d q$ is a small change in the gas flow rate. Computation of the value of the gas flow resistance $R$ may be quite time consuming. Experimentally, however, it can be easily determined from a plot of the pressure difference versus flow rate by calculating the slope of the curve at a given operating condition, as shown in Figure 4–4(b).  \n\nThe capacitance of the pressure vessel may be defined by  \n\n$$\nC=\\frac{\\mathrm{change\\;in\\;gas\\;stored,lb}}{\\mathrm{change\\;in\\;gas\\;pressure,lb_{f}/f t^{2}}}\n$$  \n\nor  \n\n$$\nC={\\frac{d m}{d p}}=V\\,{\\frac{d\\rho}{d p}}\n$$  \n\nFigure 4–4 (a) Schematic diagram of a pressure system; (b) pressuredifference-versusflow-rate curve.  \n\n![](images/1fbf714bb22d12efe254662cd95af913df416591ca10a8351b304bd6c5f99863.jpg)  \n\n$$\n\\begin{array}{r l}&{C=\\mathrm{capacitance,lb.ft^{2}/l b_{f}}}\\\\ &{m=\\mathrm{mass~of~gas~in~vessel,lb}}\\\\ &{p=\\mathrm{gas~pressure,lb_{f}/f t^{2}}}\\\\ &{V=\\mathrm{volume~of~vessel,ft^{3}}}\\\\ &{\\rho=\\mathrm{density,lb/ft^{3}}}\\end{array}\n$$  \n\nThe capacitance of the pressure system depends on the type of expansion process involved. The capacitance can be calculated by use of the ideal gas law. If the gas expansion process is polytropic and the change of state of the gas is between isothermal and adiabatic, then  \n\n$$\np\\bigg(\\frac{V}{m}\\bigg)^{n}=\\frac{p}{\\rho^{n}}=\\mathrm{constant}=K\n$$  \n\nwhere $n=$ polytropic exponent.  \n\nFor ideal gases,  \n\n$$\np{\\bar{v}}={\\bar{R}}T\\qquad{\\mathrm{or}}\\qquad p v={\\frac{{\\bar{R}}}{M}}T\n$$  \n\nwhere $p=$ absolute pressure, $\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{ft}^{2}$  \n\n$\\bar{v}=$ volume occupied by 1 mole of a gas, $\\mathrm{ft}^{3}/\\mathrm{lb}$ -mole   \n$\\textstyle{\\overline{{R}}}\\,=$ universal gas constant, ft$\\mathbf{\\nabla}\\cdot\\mathbf{lb}_{\\mathrm{f}}$ 'lb-mole ${}^{\\circ}\\mathbf{R}$   \n$T=$ absolute temperature, $^{\\circ}\\mathbf{R}$   \n$v=$ specific volume of gas, $\\mathrm{ft}^{3}/\\mathrm{lb}$   \n$M=$ molecular weight of gas per mole, lb 'lb-mole  \n\nThus  \n\n$$\np v\\,=\\frac{p}{\\rho}=\\frac{\\bar{R}}{M}\\,T\\,=\\,R_{\\mathrm{gas}}T\n$$  \n\nwhere $R_{\\mathrm{gas}}\\,=\\,\\mathrm{gas}$ constant, ft $\\cdot\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{lb}\\ ^{\\circ}\\mathrm{R}$ .  \n\nThe polytropic exponent $n$ is unity for isothermal expansion.For adiabatic expansion, $n$ is equal to the ratio of specific heats $c_{p}/c_{v}$ ,where $c_{p}$ is the specific heat at constant pressure and $c_{v}$ is the specific heat at constant volume. In many practical cases, the value of $n$ is approximately constant, and thus the capacitance may be considered constant.  \n\nThe value of $d\\rho/d p$ is obtained from Equations (4–10) and (4–11). From Equation (4–10) we have  \n\n$$\nd p\\,=\\,K n\\rho^{n-1}\\,d\\rho\n$$  \n\nor  \n\n$$\n\\frac{d\\rho}{d p}=\\frac{1}{K n\\rho^{n-1}}=\\frac{\\rho^{n}}{p n\\rho^{n-1}}=\\frac{\\rho}{p n}\n$$  \n\nSubstituting Equation (4–11) into this last equation, we get  \n\n$$\n{\\frac{d\\rho}{d p}}={\\frac{1}{n R_{\\mathrm{gas}}T}}\n$$  \n\nThe capacitance $C$ is then obtained as  \n\n$$\nC=\\frac{V}{n R_{\\mathrm{gas}}T}\n$$  \n\nThe capacitance of a given vessel is constant if the temperature stays constant. (In many practical cases, the polytropic exponent $n$ is approximately $1.0\\sim1.2$ for gases in uninsulated metal vessels.)  \n\nPressure Systems. Consider the system shown in Figure 4–4(a). If we assume only small deviations in the variables from their respective steady-state values, then this system may be considered linear.  \n\nLet us define  \n\n${\\overline{{P}}}\\,=$ gas pressure in the vessel at steady state (before changes in pressure have occurred), $\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{ft}^{2}$   \n$p_{i}=$ small change in inflow gas pressure, $\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{ft}^{2}$   \n$p_{o}=$ small change in gas pressure in the vessel, $\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{ft}^{2}$   \n$V=$ volume of the vessel, ft 3   \n$m=$ mass of gas in the vessel, lb   \n$q=$ gas flow rate, lb 'sec   \n$\\rho=$ density of gas, lb/ft 3  \n\nFor small values of $p_{i}$ and $p_{o}$ ,the resistance $R$ given by Equation (4–8) becomes constant and may be written as  \n\n$$\nR=\\frac{p_{i}-p_{o}}{q}\n$$  \n\nThe capacitance $C$ is given by Equation (4–9), or  \n\n$$\nC={\\frac{d m}{d p}}\n$$  \n\nSince the pressure change $d p_{o}$ times the capacitance $C$ is equal to the gas added to the vessel during $d t$ seconds, we obtain  \n\n$$\nC\\:d p_{o}=q\\:d t\n$$  \n\nor  \n\n$$\nC\\,{\\frac{d p_{o}}{d t}}={\\frac{p_{i}\\,-\\,p_{o}}{R}}\n$$  \n\nwhich can be written as  \n\n$$\nR C\\,{\\frac{d p_{o}}{d t}}+\\,p_{o}=\\,p_{i}\n$$  \n\nIf $p_{i}$ and $p_{o}$ are considered the input and output, respectively, then the transfer function of the system is  \n\n$$\n{\\frac{P_{o}(s)}{P_{i}(s)}}={\\frac{1}{R C s\\,+\\,1}}\n$$  \n\nwhere $R C$ has the dimension of time and is the time constant of the system.  \n\nPneumatic Nozzle–Flapper Amplifiers. A schematic diagram of a pneumatic nozzle–flapper amplifier is shown in Figure 4–5(a).The power source for this amplifier is a supply of air at constant pressure. The nozzle–flapper amplifier converts small changes in the position of the flapper into large changes in the back pressure in the nozzle.Thus a large power output can be controlled by the very little power that is needed to position the flapper.  \n\nIn Figure 4–5(a), pressurized air is fed through the orifice, and the air is ejected from the nozzle toward the flapper. Generally, the supply pressure $P_{s}$ for such a controller is 20 psig $(1.4~\\mathrm{kg_{f}}/\\mathrm{cm}^{2}$ gage). The diameter of the orifice is on the order of 0.01 in. $(0.25\\,\\mathrm{mm})$ and that of the nozzle is on the order of 0.016 in. $(0.4\\,\\mathrm{mm})$ ).To ensure proper functioning of the amplifier, the nozzle diameter must be larger than the orifice diameter.  \n\nIn operating this system, the flapper is positioned against the nozzle opening. The nozzle back pressure $P_{b}$ is controlled by the nozzle–flapper distance $X$ . As the flapper approaches the nozzle,the opposition to the flow of air through the nozzle increases,with the result that the nozzle back pressure $P_{b}$ increases. If the nozzle is completely closed by the flapper, the nozzle back pressure $P_{b}$ becomes equal to the supply pressure $P_{s}$ .If the flapper is moved away from the nozzle, so that the nozzle–flapper distance is wide (on the order of 0.01 in.), then there is practically no restriction to flow, and the nozzle back pressure $P_{b}$ takes on a minimum value that depends on the nozzle–flapper device. (The lowest possible pressure will be the ambient pressure $P_{a}$ .)  \n\nNote that, because the air jet puts a force against the flapper, it is necessary to make the nozzle diameter as small as possible.  \n\nA typical curve relating the nozzle back pressure $P_{b}$ to the nozzle–flapper distance $X$ is shown in Figure 4–5(b).The steep and almost linear part of the curve is utilized in the actual operation of the nozzle–flapper amplifier. Because the range of flapper displacements is restricted to a small value, the change in output pressure is also small, unless the curve is very steep.  \n\n![](images/efdccd5e860717ec8388752aa5b4960ebe48688e347296ae3e49339f6bc93cef.jpg)  \nFigure 4–5 (a) Schematic diagram of a pneumatic nozzle– flapper amplifier; (b) characteristic curve relating nozzle back pressure and nozzle–flapper distance.  \n\nThe nozzle–flapper amplifier converts displacement into a pressure signal. Since industrial process control systems require large output power to operate large pneumatic actuating valves, the power amplification of the nozzle–flapper amplifier is usually insufficient. Consequently, a pneumatic relay is often needed as a power amplifier in connection with the nozzle–flapper amplifier.  \n\nPneumatic Relays. In practice, in a pneumatic controller, a nozzle–flapper amplifier acts as the first-stage amplifier and a pneumatic relay as the secondstage amplifier. The pneumatic relay is capable of handling a large quantity of airflow.  \n\nA schematic diagram of a pneumatic relay is shown in Figure 4–6(a).As the nozzle back pressure $P_{b}$ increases, the diaphragm valve moves downward. The opening to the atmosphere decreases and the opening to the pneumatic valve increases, thereby increasing the control pressure $P_{c}$ .When the diaphragm valve closes the opening to the atmosphere, the control pressure $P_{c}$ becomes equal to the supply pressure $P_{s}$ .When the nozzle back pressure $P_{b}$ decreases and the diaphragm valve moves upward and shuts off the air supply, the control pressure $P_{c}$ drops to the ambient pressure $P_{a}$ .The control pressure $P_{c}$ can thus be made to vary from 0 psig to full supply pressure, usually 20 psig.  \n\nThe total movement of the diaphragm valve is very small. In all positions of the valve, except at the position to shut off the air supply, air continues to bleed into the atmosphere, even after the equilibrium condition is attained between the nozzle back pressure and the control pressure. Thus the relay shown in Figure 4–6(a) is called a bleed-type relay.  \n\nThere is another type of relay, the nonbleed type. In this one the air bleed stops when the equilibrium condition is obtained and, therefore, there is no loss of pressurized air at steady-state operation. Note, however, that the nonbleed-type relay must have an atmospheric relief to release the control pressure $P_{c}$ from the pneumatic actuating valve.A schematic diagram of a nonbleed-type relay is shown in Figure 4–6(b).  \n\nIn either type of relay, the air supply is controlled by a valve, which is in turn controlled by the nozzle back pressure.Thus, the nozzle back pressure is converted into the control pressure with power amplification.  \n\nSince the control pressure $P_{c}$ changes almost instantaneously with changes in the nozzle back pressure $P_{b}$ ,the time constant of the pneumatic relay is negligible compared with the other larger time constants of the pneumatic controller and the plant.  \n\n![](images/391ba62cc8c716b4be225d0f741bcfc011fd6a806626e6783b773e4767a1c835.jpg)  \nFigure 4–6 (a) Schematic diagram of a bleed-type relay; (b) schematic diagram of a nonbleed-type relay.  \n\n![](images/c0296ee0738f3c640b76847148040efebac2c593975e2765178248eda639171d.jpg)  \n\nIt is noted that some pneumatic relays are reverse acting. For example, the relay shown in Figure 4–7 is a reverse-acting relay. Here, as the nozzle back pressure $P_{b}$ increases, the ball valve is forced toward the lower seat, thereby decreasing the control pressure $P_{c}$ .Thus, this relay is a reverse-acting relay.  \n\nPneumatic Proportional Controllers (Force-Distance Type). Two types of pneumatic controllers,one called the force-distance type and the other the force-balance type, are used extensively in industry.Regardless of how differently industrial pneumatic controllers may appear, careful study will show the close similarity in the functions of the pneumatic circuit.Here we shall consider the force-distance type of pneumatic controllers.  \n\nFigure 4–8(a) shows a schematic diagram of such a proportional controller.The nozzle– flapper amplifier constitutes the first-stage amplifier, and the nozzle back pressure is controlled by the nozzle–flapper distance.The relay-type amplifier constitutes the secondstage amplifier.The nozzle back pressure determines the position of the diaphragm valve for the second-stage amplifier, which is capable of handling a large quantity of airflow.  \n\nIn most pneumatic controllers, some type of pneumatic feedback is employed. Feedback of the pneumatic output reduces the amount of actual movement of the flapper. Instead of mounting the flapper on a fixed point, as shown in Figure 4–8(b), it is often pivoted on the feedback bellows, as shown in Figure 4–8(c).The amount of feedback can be regulated by introducing a variable linkage between the feedback bellows and the flapper connecting point.The flapper then becomes a floating link. It can be moved by both the error signal and the feedback signal.  \n\nThe operation of the controller shown in Figure 4–8(a) is as follows. The input signal to the two-stage pneumatic amplifier is the actuating error signal. Increasing the actuating error signal moves the flapper to the left.This move will, in turn, increase the nozzle back pressure, and the diaphragm valve moves downward. This results in an increase of the control pressure. This increase will cause bellows $F$ to expand and move the flapper to the right, thus opening the nozzle. Because of this feedback, the nozzle– flapper displacement is very small, but the change in the control pressure can be large.  \n\nIt should be noted that proper operation of the controller requires that the feedback bellows move the flapper less than that movement caused by the error signal alone. (If these two movements were equal, no control action would result.)  \n\nEquations for this controller can be derived as follows. When the actuating error is zero,or $e\\,=\\,0$ ,an equilibrium state exists with the nozzle–flapper distance equal to $\\bar{X}$ ,the displacement of bellows equal to nozzle back pressure equal to $\\bar{P}_{b}$ –,and the control pressure equal to $\\bar{Y}$ ,the displacement of the diaphragm equal to $\\bar{P}_{c}$ .When an actuating $\\bar{Z}$ ,the error exists,the nozzle–flapper distance,the displacement of the bellows,the displacement of the diaphragm,the nozzle back pressure,and the control pressure deviate from their respective equilibrium values.Let these deviations be $x,y,z,p_{b}$ ,and $p_{c}$ ,respectively.(The positive direction for each displacement variable is indicated by an arrowhead in the diagram.)  \n\n![](images/209cdb91d10b9e2eab48c17a2ba1b7c034ebc1c4231813c65eeaf02561194e1f.jpg)  \nFigure 4–8 (a) Schematic diagram of a force-distance type of pneumatic proportional controller; (b) flapper mounted on a fixed point; (c) flapper mounted on a feedback bellows; (d) displacement $x$ as a result of addition of two small displacements; (e) block diagram for the controller; (f) simplified block diagram for the controller.  \n\nAssuming that the relationship between the variation in the nozzle back pressure and the variation in the nozzle–flapper distance is linear, we have  \n\n$$\np_{b}=K_{1}x\n$$  \n\nwhere $K_{1}$ is a positive constant. For the diaphragm valve,  \n\n$$\np_{b}=K_{2}z\n$$  \n\nwhere $K_{2}$ is a positive constant. The position of the diaphragm valve determines the control pressure. If the diaphragm valve is such that the relationship between $p_{c}$ and $z$ is linear, then  \n\n$$\np_{c}=K_{3}z\n$$  \n\nwhere $K_{3}$ is a positive constant. From Equations (4–13), (4–14), and (4–15), we obtain  \n\n$$\np_{c}=\\frac{K_{3}}{K_{2}}\\,p_{b}=\\frac{K_{1}K_{3}}{K_{2}}\\,x\\,=\\,K x\n$$  \n\nwhere $K\\,=\\,K_{1}K_{3}/K_{2}$ is a positive constant. For the flapper, since there are two small movements ( $e$ and $y$ ) in opposite directions, we can consider such movements separately and add up the results of two movements into one displacement $x$ . See Figure 4–8(d). Thus, for the flapper movement, we have  \n\n$$\nx=\\frac{b}{a\\,+\\,b}\\,e\\,-\\,\\frac{a}{a\\,+\\,b}\\,y\n$$  \n\nThe bellows acts like a spring, and the following equation holds true:  \n\n$$\nA p_{c}=k_{s}y\n$$  \n\nwhere $A$ is the effective area of the bellows and $k_{s}$ is the equivalent spring constant— that is, the stiffness due to the action of the corrugated side of the bellows.  \n\nAssuming that all variations in the variables are within a linear range, we can obtain a block diagram for this system from Equations (4–16), (4–17), and (4–18) as shown in Figure 4–8(e). From Figure 4–8(e), it can be clearly seen that the pneumatic controller shown in Figure 4–8(a) itself is a feedback system.The transfer function between $p_{c}$ and $e$ is given by  \n\n$$\n{\\frac{P_{c}(s)}{E(s)}}={\\frac{\\displaystyle{\\frac{b}{a\\,+\\,b}}\\,K}{\\displaystyle{1\\,+\\,K\\,{\\frac{a}{a\\,+\\,b}}{\\frac{A}{k_{s}}}}}}=K_{p}\n$$  \n\nA simplified block diagram is shown in Figure 4–8(f). Since $p_{c}$ and $e$ are proportional, the pneumatic controller shown in Figure 4–8(a) is a pneumatic proportional controller .As seen from Equation (4–19), the gain of the pneumatic proportional controller can be widely varied by adjusting the flapper connecting linkage. [The flapper connecting linkage is not shown in Figure 4–8(a).] In most commercial proportional controllers an adjusting knob or other mechanism is provided for varying the gain by adjusting this linkage.  \n\nAs noted earlier, the actuating error signal moved the flapper in one direction, and the feedback bellows moved the flapper in the opposite direction,but to a smaller degree.  \n\n![](images/bcaa11acbce88572c369d3ea678d1e114bacec5a7f761bf2a170ae46a315e952.jpg)  \nFigure 4–9 (a) Pneumatic controller without a feedback mechanism; (b) curves $P_{b}$ versus $X$ and $P_{c}$ versus $X$ .  \n\nThe effect of the feedback bellows is thus to reduce the sensitivity of the controller.The principle of feedback is commonly used to obtain wide proportional-band controllers.  \n\nPneumatic controllers that do not have feedback mechanisms [which means that one end of the flapper is fixed, as shown in Figure 4–9(a)] have high sensitivity and are called pneumatic two-position controllers or pneumatic on–off controllers . In such a controller, only a small motion between the nozzle and the flapper is required to give a complete change from the maximum to the minimum control pressure. The curves relating $P_{b}$ to $X$ and $P_{c}$ to $X$ are shown in Figure 4–9(b). Notice that a small change in $X$ can cause a large change in $P_{b}$ ,which causes the diaphragm valve to be completely open or completely closed.  \n\nPneumatic Proportional Controllers (Force-Balance Type). Figure 4–10 shows a schematic diagram of a force-balance type pneumatic proportional controller. Forcebalance type controllers are in extensive use in industry.Such controllers are called stack controllers.The basic principle of operation does not differ from that of the force-distance type controller.The main advantage of the force-balance type controller is that it eliminates many mechanical linkages and pivot joints,thereby reducing the effects of friction.  \n\nIn what follows, we shall consider the principle of the force-balance type controller. In the controller shown in Figure 4–10, the reference input pressure $P_{r}$ and the output pressure $P_{o}$ are fed to large diaphragm chambers. Note that a force-balance type pneumatic controller operates only on pressure signals.Therefore, it is necessary to convert the reference input and system output to corresponding pressure signals.  \n\n![](images/5001fa9f99cf27823d98b6df53a8934f485df85f2e08e1bc81da5f0287f386ef.jpg)  \nFigure 4–10 Schematic diagram of a force-balance type pneumatic proportional controller.  \n\nAs in the case of the force-distance type controller, this controller employs a flapper, nozzle, and orifices. In Figure 4–10, the drilled opening in the bottom chamber is the nozzle.The diaphragm just above the nozzle acts as a flapper.  \n\nThe operation of the force-balance type controller shown in Figure $4{-}10\\ \\mathrm{may}$ be summarized as follows: 20-psig air from an air supply flows through an orifice, causing a reduced pressure in the bottom chamber. Air in this chamber escapes to the atmosphere through the nozzle. The flow through the nozzle depends on the gap and the pressure drop across it. An increase in the reference input pressure $P_{r}$ ,while the output pressure $P_{o}$ remains the same, causes the valve stem to move down, decreasing the gap between the nozzle and the flapper diaphragm.This causes the control pressure $P_{c}$ to increase. Let  \n\n$$\np_{e}=P_{r}-P_{o}\n$$  \n\nIf the control pressure equal to $\\boldsymbol{p}_{e}=0$ ,there is an equilibrium state with the nozzle–flapper distance equal to $\\bar{P}_{c}$ –.At this equilibrium state, $\\hat{P_{1}}=\\overline{{P}}_{c}k$ k ( where $k<1$ $\\bar{X}$ )and and  \n\n$$\n\\bar{X}\\,=\\,\\alpha\\big(\\bar{P}_{c}A_{1}\\,-\\,\\bar{P}_{c}k A_{1}\\big)\n$$  \n\nwhere $\\alpha$ is a constant.  \n\nLet us assume that $p_{e}\\neq0$ and define small variations in the nozzle–flapper distance and control pressure as $x$ and $p_{c}$ , respectively.Then we obtain the following equation:  \n\n$$\n\\bar{X}\\,+\\,x\\,=\\,\\alpha\\big[\\big(\\bar{P}_{c}\\,+\\,p_{c}\\big)A_{1}\\,-\\,\\big(\\bar{P}_{c}\\,+\\,p_{c}\\big)k A_{1}\\,-\\,p_{e}\\big(A_{2}\\,-\\,A_{1}\\big)\\big]\n$$  \n\nFrom Equations (4–21) and (4–22), we obtain  \n\n$$\nx\\,=\\,\\alpha\\big[p_{c}(1\\,-\\,k)A_{1}\\,-\\,p_{e}\\big(A_{2}\\,-\\,A_{1}\\big)\\big]\n$$  \n\nAt this point, we must examine the quantity $x$ . In the design of pneumatic controllers, the nozzle–flapper distance is made quite small. In view of the fact that $x/\\alpha$ is very much smaller than $p_{c}(1\\,-\\,k)A_{1}$ or $p_{e}(A_{2}\\,\\,\\dot{-}\\,A_{1}).$ —that is, for $p_{e}\\neq0$  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{x}{\\alpha}\\ll\\,p_{c}(1\\,-\\,k)A_{1}}}\\\\ {\\displaystyle{\\frac{x}{\\alpha}\\ll\\,p_{e}(A_{2}\\,-\\,A_{1})}}\\end{array}\n$$  \n\nwe may neglect the term $x$ in our analysis. Equation (4–23) can then be rewritten to reflect this assumption as follows:  \n\n$$\np_{c}(1\\,-\\,k)A_{1}=\\,p_{e}\\!(A_{2}\\,-\\,A_{1})\n$$  \n\nand the transfer function between $p_{c}$ and $p_{e}$ becomes  \n\n$$\n{\\frac{P_{c}(s)}{P_{e}(s)}}={\\frac{A_{2}\\,-\\,A_{1}}{A_{1}}}{\\frac{1}{1\\,-\\,k}}=K_{p}\n$$  \n\nwhere $p_{e}$ is defined by Equation (4–20). The controller shown in Figure 4–10 is a proportional controller.The value of gain $K_{p}$ increases as $k$ approaches unity. Note that the value of $k$ depends on the diameters of the orifices in the inlet and outlet pipes of the feedback chamber. (The value of $k$ approaches unity as the resistance to flow in the orifice of the inlet pipe is made smaller.)  \n\nPneumatic Actuating Valves. One characteristic of pneumatic controls is that they almost exclusively employ pneumatic actuating valves.A pneumatic actuating valve can provide a large power output. (Since a pneumatic actuator requires a large power input to produce a large power output, it is necessary that a sufficient quantity of pressurized air be available.) In practical pneumatic actuating valves, the valve characteristics may not be linear; that is, the flow may not be directly proportional to the valve stem position, and also there may be other nonlinear effects, such as hysteresis.  \n\nConsider the schematic diagram of a pneumatic actuating valve shown in Figure 4–11. Assume that the area of the diaphragm is $A$ .Assume also that when the actuating error is zero, the control pressure is equal to $\\overline{{P}}_{c}$ and the valve displacement is equal to $\\bar{X}$ .  \n\nIn the following analysis, we shall consider small variations in the variables and linearize the pneumatic actuating valve. Let us define the small variation in the control pressure and the corresponding valve displacement to be $p_{c}$ and $x$ , respectively. Since a small change in the pneumatic pressure force applied to the diaphragm repositions the load, consisting of the spring, viscous friction, and mass, the force-balance equation becomes  \n\n$$\nA p_{c}\\,=\\,m\\ddot{x}\\,+\\,b\\dot{x}\\,+\\,k x\n$$  \n\nwhere $m={\\mathrm{mass}}$ of the valve and valve stem  \n\n$b=$ viscous-friction coefficient $k=$ spring constant  \n\nIf the force due to the mass and viscous friction are negligibly small, then this last equation can be simplified to  \n\n$$\nA p_{c}=k x\n$$  \n\nThe transfer function between $x$ and $p_{c}$ thus becomes  \n\n$$\n{\\frac{X(s)}{P_{c}(s)}}={\\frac{A}{k}}=K_{c}\n$$  \n\n![](images/a9d5a8795822f79b1c61be22744fba5fd6cac3a2feef9e22cc60da727ca33feb.jpg)  \nFigure 4–11 Schematic diagram of a pneumatic actuating valve.  \n\nwhere $X(s)\\,=\\,{\\mathcal{L}}[x]$ and $P_{c}(s)\\,=\\,\\mathcal{L}\\big[p_{c}\\big]$ .If $q_{i}$ , the change in flow through the pneumatic actuating valve, is proportional to $x$ , the change in the valve-stem displacement, then  \n\n$$\n{\\frac{Q_{i}(s)}{X(s)}}=K_{q}\n$$  \n\nwhere $Q_{i}(s)\\,=\\,{\\mathcal{L}}[q_{i}]$ and $K_{q}$ is a constant. The transfer function between $q_{i}$ and $p_{c}$ becomes  \n\n$$\n{\\frac{Q_{i}(s)}{P_{c}(s)}}=K_{c}K_{q}=K_{v}\n$$  \n\nwhere $K_{v}$ is a constant.  \n\nThe standard control pressure for this kind of a pneumatic actuating valve is between 3 and 15 psig. The valve-stem displacement is limited by the allowable stroke of the diaphragm and is only a few inches. If a longer stroke is needed, a piston–spring combination may be employed.  \n\nIn pneumatic actuating valves, the static-friction force must be limited to a low value so that excessive hysteresis does not result. Because of the compressibility of air, the control action may not be positive; that is, an error may exist in the valve-stem position. The use of a valve positioner results in improvements in the performance of a pneumatic actuating valve.  \n\nBasic Principle for Obtaining Derivative Control Action. We shall now present methods for obtaining derivative control action. We shall again place the emphasis on the principle and not on the details of the actual mechanisms.  \n\nThe basic principle for generating a desired control action is to insert the inverse of the desired transfer function in the feedback path. For the system shown in Figure 4–12, the closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)H(s)}\n$$  \n\nIf $\\left|G(s)H(s)\\right|\\gg1$ , then $C(s)/R(s)$ can be modified to  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{1}{H(s)}}\n$$  \n\nThus, if proportional-plus-derivative control action is desired, we insert an element having the transfer function $1/(T s\\,+\\,1)$ in the feedback path.  \n\n![](images/3bd5cf3ec85b457c70e75e8b3298878e7aaf0f30cc2d9648fc14f6f8ffb4d05e.jpg)  \nFigure 4–12 Control system.  \n\n![](images/739ec327b789544ad27ddfef572a5fc9c8ffa899a135e3f7635cc622b3337854.jpg)  \nFigure 4–13 (a) Pneumatic proportional controller; (b) block diagram of the controller.  \n\nConsider the pneumatic controller shown in Figure 4–13(a).Considering small changes in the variables,we can draw a block diagram of this controller as shown in Figure 4–13(b). From the block diagram we see that the controller is of proportional type.  \n\nWe shall now show that the addition of a restriction in the negative feedback path will modify the proportional controller to a proportional-plus-derivative controller, or a PD controller.  \n\nConsider the pneumatic controller shown in Figure 4–14(a).Assuming again small changes in the actuating error, nozzle–flapper distance, and control pressure, we can summarize the operation of this controller as follows: Let us first assume a small step change in e.  \n\n![](images/026d23854b7b0d6e421609a82626673f0ef993a7e3dcfbc541fb6eb94d0c6514.jpg)  \nFigure 4–14 (a) Pneumatic proportional-plusderivative controller; (b) step change in $e$ and the corresponding changes in $x$ and $p_{c}$ plotted versus $t$ ; (c) block diagram of the controller.  \n\nThen the change in the control pressure $p_{c}$ will be instantaneous.The restriction $R$ will momentarily prevent the feedback bellows from sensing the pressure change $p_{c}$ .Thus the feedback bellows will not respond momentarily, and the pneumatic actuating valve will feel the full effect of the movement of the flapper.As time goes on,the feedback bellows will expand. The change in the nozzle–flapper distance $x$ and the change in the control pressure $p_{c}$ can be plotted against time $t$ , as shown in Figure 4–14(b).At steady state, the feedback bellows acts like an ordinary feedback mechanism.The curve $p_{c}$ versus $t$ clearly shows that this controller is of the proportional-plus-derivative type.  \n\nA block diagram corresponding to this pneumatic controller is shown in Figure 4–14(c). In the block diagram, $K$ is a constant, $A$ is the area of the bellows, and $k_{s}$ is the equivalent spring constant of the bellows.The transfer function between $p_{c}$ and $e$ can be obtained from the block diagram as follows:  \n\n$$\n\\frac{P_{c}(s)}{E(s)}=\\frac{\\cfrac{b}{a+b}\\,K}{1+\\cfrac{K a}{a+b}\\frac{A}{k_{s}}\\frac{1}{R C s\\,+\\,1}}\n$$  \n\nIn such a controller the loop gain $\\big|K a A/\\big[(a\\,+\\,b)k_{s}(R C s\\,+\\,1)\\big]\\big|$ is made much greater than unity.Thus the transfer function $P_{c}(s)/E(s)$ can be simplified to give  \n\n$$\n\\frac{P_{c}(s)}{E(s)}=K_{p}(1\\,+\\,T_{d}s)\n$$  \n\nwhere  \n\n$$\nK_{p}=\\frac{b k_{s}}{a A},\\qquad T_{d}=R C\n$$  \n\nThus, delayed negative feedback, or the transfer function $1/(R C s\\,+\\,1)$ in the feedback path, modifies the proportional controller to a proportional-plus-derivative controller.  \n\nNote that if the feedback valve is fully opened, the control action becomes proportional. If the feedback valve is fully closed, the control action becomes narrow-band proportional (on–off).  \n\nObtaining Pneumatic Proportional-Plus-Integral Control Action. Consider the proportional controller shown in Figure 4–13(a). Considering small changes in the variables, we can show that the addition of delayed positive feedback will modify this proportional controller to a proportional-plus-integral controller, or a PI controller.  \n\nConsider the pneumatic controller shown in Figure 4–15(a).The operation of this controller is as follows:The bellows denoted by I is connected to the control pressure source without any restriction. The bellows denoted by II is connected to the control pressure source through a restriction.Let us assume a small step change in the actuating error.This will cause the back pressure in the nozzle to change instantaneously.Thus a change in the control pressure $p_{c}$ also occurs instantaneously. Due to the restriction of the valve in the path to bellows II, there will be a pressure drop across the valve.As time goes on, air will flow across the valve in such a way that the change in pressure in bellows II attains the value $p_{c}$ .Thus bellows II will expand or contract as time elapses in such a way as to move the flapper an additional amount in the direction of the original displacement $e$ .This will cause the back pressure $p_{c}$ in the nozzle to change continuously, as shown in Figure 4–15(b).  \n\n![](images/dca791f57db19b37243cdd099da4b46ae64b1df91dd26f3a8d9f2aba2fbdd18b.jpg)  \nFigure 4–15 (a) Pneumatic proportional-plusintegral controller; (b) step change in $e$ and the corresponding changes in $x$ and $p_{c}$ plotted versus $t$ ; (c) block diagram of the controller; (d) simplified block diagram.  \n\nNote that the integral control action in the controller takes the form of slowly canceling the feedback that the proportional control originally provided.  \n\nA block diagram of this controller under the assumption of small variations in the variables is shown in Figure 4–15(c). A simplification of this block diagram yields Figure 4–15(d).The transfer function of this controller is  \n\n$$\n\\frac{P_{c}(s)}{E(s)}=\\frac{\\cfrac{b}{a+b}\\,K}{1+\\cfrac{K a}{a+b}\\frac{A}{k_{s}}\\left(1-\\cfrac{1}{R C s\\,+\\,1}\\right)}\n$$  \n\nwhere $K$ is a constant, $A$ is the area of the bellows,and @C$k_{s}$ is the equivalent spring constant D @ of the combined bellows.If $\\left|K a A R C s/[(a\\,+\\,b)k_{s}(R C s\\,+\\,1)\\right]\\right|\\,\\hat{\\gg}\\,1,$ which is usually the case, the transfer function can be simplified to  \n\n$$\n{\\frac{P_{c}(s)}{E(s)}}=K_{p}\\bigg(1+{\\frac{1}{T_{i}s}}\\bigg)\n$$  \n\nwhere  \n\n$$\nK_{p}=\\frac{b k_{s}}{a A},\\qquad T_{i}=R C\n$$  \n\n![](images/8edbc241564e34ae3a735bfdb7542cd2154524862b06ed87107761607e5a0bc9.jpg)  \nFigure 4–16 (a) Pneumatic proportional-plusintegral-plusderivative controller; (b) block diagram of the controller.  \n\nObtaining Pneumatic Proportional-Plus-Integral-Plus-Derivative Control Action. A combination of the pneumatic controllers shown in Figures 4–14(a) and 4–15(a) yields a proportional-plus-integral-plus-derivative controller, or a PID controller. Figure 4–16(a) shows a schematic diagram of such a controller. Figure 4–16(b) shows a block diagram of this controller under the assumption of small variations in the variables.  \n\nThe transfer function of this controller is  \n\n$$\n\\frac{P_{c}(s)}{E(s)}=\\frac{\\cfrac{b K}{a+b}}{1+\\frac{K a}{a+b}\\frac{A}{k_{s}}\\frac{\\bigl(R_{i}C-R_{d}C\\bigr)s}{\\bigl(R_{d}C s\\,+\\,1\\bigr)\\bigl(R_{i}C s\\,+\\,1\\bigr)}}\n$$  \n\nBy defining  \n\n$$\nT_{i}=R_{i}C,\\qquad T_{d}=R_{d}C\n$$  \n\nand noting that under normal operation $\\big|K a A\\big(T_{i}-T_{d}\\big)s/\\big[(a+b)k_{s}\\big(T_{d}s+1\\big)\\big(T_{i}s+1\\big)\\big]\\big|\\gg1$ and $T_{i}\\gg T_{d}$ ,we obtain  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\frac{P_{c}(s)}{E(s)}\\div\\frac{b k_{s}}{a A}\\frac{(T_{d}s\\mathrm{~+~}1)\\left(T_{i}s\\mathrm{~+~}1\\right)}{(T_{i}\\mathrm{~-~}T_{d})s}}}\\\\ &{\\nrightarrow\\frac{b k_{s}}{a A}\\frac{T_{d}T_{i}s^{2}\\mathrm{~+~}T_{i}s\\mathrm{~+~}1}{T_{i}s}}\\\\ &{=K_{p}\\bigg(1+\\frac{1}{T_{i}s}+T_{d}s\\bigg)}\\end{array}\n$$  \n\nwhere  \n\n$$\nK_{p}={\\frac{b k_{s}}{a A}}\n$$  \n\nEquation (4–24) indicates that the controller shown in Figure 4–16(a) is a proportionalplus-integral-plus-derivative controller or a PID controller.  \n\n# 4–4 HYDRAULIC SYSTEMS  \n\nExcept for low-pressure pneumatic controllers,compressed air has seldom been used for the continuous control of the motion of devices having significant mass under external load forces. For such a case, hydraulic controllers are generally preferred.  \n\nHydraulic Systems. The widespread use of hydraulic circuitry in machine tool applications, aircraft control systems, and similar operations occurs because of such factors as positiveness, accuracy, flexibility, high horsepower-to-weight ratio, fast starting, stopping, and reversal with smoothness and precision, and simplicity of operations.  \n\nThe operating pressure in hydraulic systems is somewhere between 145 and $5000\\,\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{in}.$ .(between 1 and $35\\,\\mathrm{MPa}]$ ). In some special applications, the operating pressure may go up to $10{,}000\\;\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{in}$ .$(70\\;\\mathrm{MPa})$ ). For the same power requirement, the weight and size of the hydraulic unit can be made smaller by increasing the supply pressure. With highpressure hydraulic systems, very large force can be obtained. Rapid-acting, accurate positioning of heavy loads is possible with hydraulic systems. A combination of electronic and hydraulic systems is widely used because it combines the advantages of both electronic control and hydraulic power.  \n\nAdvantages and Disadvantages of Hydraulic Systems. There are certain advantages and disadvantages in using hydraulic systems rather than other systems. Some of the advantages are the following:  \n\n1. Hydraulic fluid acts as a lubricant, in addition to carrying away heat generated in the system to a convenient heat exchanger.   \n2. Comparatively small-sized hydraulic actuators can develop large forces or torques.   \n3. Hydraulic actuators have a higher speed of response with fast starts, stops, and speed reversals.   \n4. Hydraulic actuators can be operated under continuous, intermittent, reversing, and stalled conditions without damage.   \n5. Availability of both linear and rotary actuators gives flexibility in design.   \n6. Because of low leakages in hydraulic actuators, speed drop when loads are applied is small.  \n\nOn the other hand, several disadvantages tend to limit their use.  \n\n1. Hydraulic power is not readily available compared to electric power.   \n2. Cost of a hydraulic system may be higher than that of a comparable electrical system performing a similar function.   \n3. Fire and explosion hazards exist unless fire-resistant fluids are used.   \n4. Because it is difficult to maintain a hydraulic system that is free from leaks, the system tends to be messy.   \n5. Contaminated oil may cause failure in the proper functioning of a hydraulic system.   \n6. As a result of the nonlinear and other complex characteristics involved, the design of sophisticated hydraulic systems is quite involved.   \n7. Hydraulic circuits have generally poor damping characteristics.If a hydraulic circuit is not designed properly, some unstable phenomena may occur or disappear, depending on the operating condition.  \n\nComments. Particular attention is necessary to ensure that the hydraulic system is stable and satisfactory under all operating conditions. Since the viscosity of hydraulic fluid can greatly affect damping and friction effects of the hydraulic circuits, stability tests must be carried out at the highest possible operating temperature.  \n\nNote that most hydraulic systems are nonlinear. Sometimes, however, it is possible to linearize nonlinear systems so as to reduce their complexity and permit solutions that are sufficiently accurate for most purposes.A useful linearization technique for dealing with nonlinear systems was presented in Section 2–7.  \n\nHydraulic Servo System. Figure 4–17(a) shows a hydraulic servomotor. It is essentially a pilot-valve-controlled hydraulic power amplifier and actuator. The pilot valve is a balanced valve,in the sense that the pressure forces acting on it are all balanced. A very large power output can be controlled by a pilot valve, which can be positioned with very little power.  \n\nIn practice, the ports shown in Figure 4–17(a) are often made wider than the corresponding valves. In such a case, there is always leakage through the valves. Such leakage improves both the sensitivity and the linearity of the hydraulic servomotor. In the following analysis we shall make the assumption that the ports are made wider than the valves—that is, the valves are underlapped. [Note that sometimes a dither signal, a high-frequency signal of very small amplitude (with respect to the maximum displacement of the valve), is superimposed on the motion of the pilot valve. This also improves the sensitivity and linearity.In this case also there is leakage through the valve.]  \n\n![](images/145f03b21fdb92f7cad004cc4392304ff47f6447a8b18e7ef94327ba08c6f851.jpg)  \nFigure 4–17 (a) Hydraulic servo system; (b) enlarged diagram of the valve orifice area.   \n(b)  \n\nWe shall apply the linearization technique presented in Section 2–7 to obtain a linearized mathematical model of the hydraulic servomotor. We assume that the valve is underlapped and symmetrical and admits hydraulic fluid under high pressure into a power cylinder that contains a large piston, so that a large hydraulic force is established to move a load.  \n\nIn Figure 4–17(b) we have an enlarged diagram of the valve orifice area. Let us define the valve orifice areas of ports 1, 2, 3, 4 as $A_{1},A_{2},A_{3},A_{4}$ ,respectively.Also, define the flow rates through ports $1,2,3,4$ as $q_{1},q_{2},q_{3},q_{4}$ ,respectively. Note that, since the valve is symmetrical, $A_{1}=A_{3}$ and $A_{2}=A_{4}$ .Assuming the displacement $x$ to be small, we obtain  \n\n$$\n\\begin{array}{l}{{A_{1}=A_{3}=k\\bigg(\\displaystyle\\frac{x_{0}}{2}+x\\bigg)}}\\\\ {{\\mathrm{}}}\\\\ {{A_{2}=A_{4}=k\\bigg(\\displaystyle\\frac{x_{0}}{2}-x\\bigg)}}\\end{array}\n$$  \n\nwhere $k$ is a constant.  \n\nFurthermore, we shall assume that the return pressure 1 $p_{o}$ in the return line is small and thus can be neglected. Then, referring to Figure 4–17(a), flow rates through valve orifices are  \n\n$$\n\\begin{array}{r l}&{q_{1}=c_{1}A_{1}\\sqrt{\\frac{2g}{\\gamma}\\left(p_{s}-p_{1}\\right)}=C_{1}\\sqrt{p_{s}-p_{1}}\\left(\\frac{x_{0}}{2}+x\\right)}\\\\ &{q_{2}=c_{2}A_{2}\\sqrt{\\frac{2g}{\\gamma}\\left(p_{s}-p_{2}\\right)}=C_{2}\\sqrt{p_{s}-p_{2}}\\left(\\frac{x_{0}}{2}-x\\right)}\\\\ &{q_{3}=c_{1}A_{3}\\sqrt{\\frac{2g}{\\gamma}\\left(p_{2}-p_{0}\\right)}=C_{1}\\sqrt{p_{2}-p_{0}}\\left(\\frac{x_{0}}{2}+x\\right)=C_{1}\\sqrt{p_{2}}\\left(\\frac{x_{0}}{2}+x\\right)}\\\\ &{q_{4}=c_{2}A_{4}\\sqrt{\\frac{2g}{\\gamma}\\left(p_{1}-p_{0}\\right)}=C_{2}\\sqrt{p_{1}-p_{0}}\\left(\\frac{x_{0}}{2}-x\\right)=C_{2}\\sqrt{p_{1}}\\left(\\frac{x_{0}}{2}-x\\right)}\\end{array}\n$$  \n\nwhere $C_{1}=c_{1}k\\sqrt{2g/\\gamma}$ and $C_{2}=c_{2}k\\sqrt{2g/\\gamma}$ 1 ,and $\\gamma$ is the specific weight and is given by 1 $\\gamma=\\rho g$ , where $\\rho$ is mass density and $g$ is the acceleration of gravity. The flow rate $q$ to the left-hand side of the power piston is  \n\n$$\nq\\,=\\,q_{1}\\,-\\,q_{4}\\,=\\,C_{1}\\sqrt{p_{s}\\,-\\,p_{1}}\\left({\\frac{x_{0}}{2}}\\,+\\,x\\right)\\,-\\,C_{2}\\sqrt{p_{1}}\\left({\\frac{x_{0}}{2}}\\,-\\,x\\right)\n$$  \n\nThe flow rate from the right-hand side of the power piston to the drain is the same as this $q$ and is given by  \n\n$$\nq\\,=\\,q_{3}\\,-\\,q_{2}=C_{1}{\\sqrt{p_{2}}}\\left({\\frac{x_{0}}{2}}\\,+\\,x\\right)\\,-\\,C_{2}{\\sqrt{p_{s}-\\,p_{2}}}\\left({\\frac{x_{0}}{2}}\\,-\\,x\\right)\n$$  \n\nIn the present analysis we assume that the fluid is incompressible. Since the valve is symmetrical, we have $q_{1}=q_{3}$ and $q_{2}=q_{4}$ .By equating $q_{1}$ and $q_{3}$ ,we obtain  \n\n$$\np_{s}\\,-\\,p_{1}\\,=\\,p_{2}\n$$  \n\nor  \n\n$$\np_{s}=\\,p_{1}\\,+\\,p_{2}\n$$  \n\nIf we define the pressure difference across the power piston as $\\Delta p$ or  \n\n$$\n\\Delta p\\,=\\,p_{1}\\,-\\,p_{2}\n$$  \n\nthen  \n\n$$\np_{1}=\\frac{p_{s}+\\Delta p}{2},\\qquad p_{2}=\\frac{p_{s}-\\Delta p}{2}\n$$  \n\nFor the symmetrical valve shown in Figure 4–17(a), the pressure in each side of the power piston is $(1/2)p_{s}$ when no load is applied, or $\\Delta p=0.$ .As the spool valve is displaced, the pressure in one line increases as the pressure in the other line decreases by the same amount.  \n\nIn terms of $p_{s}$ and $\\Delta p$ ,we can rewrite the flow rate $q$ given by Equation (4–25) as  \n\n$$\nq=q_{1}-q_{4}=C_{1}\\sqrt{\\frac{p_{s}-\\,\\Delta p}{2}}\\bigg(\\frac{x_{0}}{2}+\\,x\\bigg)\\,-\\,C_{2}\\sqrt{\\frac{p_{s}+\\,\\Delta p}{2}}\\bigg(\\frac{x_{0}}{2}-\\,x\\bigg)\n$$  \n\nNoting that the supply pressure $p_{s}$ is constant. the flow rate $q$ can be written as a function of the valve displacement $x$ and pressure difference $\\Delta p$ ,or  \n\n$$\nq=C_{1}\\sqrt{\\frac{p_{s}-\\,\\Delta p}{2}}\\bigg(\\frac{x_{0}}{2}+\\,x\\bigg)\\,-\\,C_{2}\\sqrt{\\frac{p_{s}+\\,\\Delta p}{2}}\\bigg(\\frac{x_{0}}{2}-\\,x\\bigg)\\,=\\,f(x,\\Delta p)\n$$  \n\nBy applying the linearization technique presented in Section 3–10 to this case,the linearized equation about point $x\\,=\\,\\bar{x}$ ,$\\Delta p\\,=\\,\\Delta\\bar{p},q\\,=\\,\\bar{q}$ is  \n\n$$\nq\\,-\\,{\\bar{q}}\\,=\\,a(x\\,-\\,{\\bar{x}}\\,)\\,+\\,b(\\Delta p\\,-\\,\\Delta{\\bar{p}})\n$$  \n\nwhere  \n\n$$\n\\begin{array}{r l}{\\bar{q}=f(\\bar{x},\\Delta\\bar{p})}&{}\\\\ {a=\\frac{\\partial f}{\\partial x}\\bigg|_{x=\\bar{x},\\,\\Delta p=\\Delta p}}&{=C_{1}\\sqrt{\\frac{p_{s}-\\Delta\\bar{p}}{2}}+C_{2}\\sqrt{\\frac{p_{s}+\\Delta\\bar{p}}{2}}}\\\\ {b=\\frac{\\partial f}{\\partial\\Delta p}\\bigg|_{x=\\bar{x},\\,\\Delta p=\\Delta\\bar{p}}=-\\bigg[\\frac{C_{1}}{2\\sqrt{2}\\,\\sqrt{p_{s}-\\Delta\\bar{p}}}\\,\\bigg(\\frac{x_{0}}{2}+\\bar{x}\\bigg)}\\\\ &{\\,\\,+\\,\\frac{C_{2}}{2\\sqrt{2}\\,\\sqrt{p_{s}+\\Delta\\bar{p}}}\\,\\bigg(\\frac{x_{0}}{2}-\\bar{x}\\bigg)\\bigg]<0}\\end{array}\n$$  \n\nCoefficients $a$ and $^b$ here are called valve coefficients . Equation (4–26) is a linearized mathematical model of the spool valve near an operating point $x\\,=\\,\\overline{{x}}$ –,$\\Delta p\\,=\\,\\Delta\\bar{p},q\\,=\\,\\bar{q}$ .The values of valve coefficients $a$ and $^b$ vary with the operating point. Note that $\\partial f/\\partial\\Delta p$ is negative and so $^b$ is negative.  \n\nSince the normal operating point is the point where $\\bar{x}\\,=\\,0,\\,\\Delta\\bar{p}\\,=\\,0,\\,\\bar{q}\\,=\\,0$ ,near the normal operating point Equation (4–26) becomes  \n\n$$\nq\\,=\\,K_{1}x\\,-\\,K_{2}\\,\\Delta p\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{K_{1}=\\left(C_{1}+C_{2}\\right)\\sqrt{\\frac{p_{s}}{2}}>0}}\\\\ {{\\ }}\\\\ {{K_{2}=\\left(C_{1}+C_{2}\\right)\\frac{x_{0}}{4\\sqrt{2}\\,\\sqrt{p_{s}}}>0}}\\end{array}\n$$  \n\nEquation (4–27) is a linearized mathematical model of the spool valve near the origin $\\acute{\\bar{x}}\\,=\\,0,\\,\\Delta\\bar{p}\\,=\\,0,\\,\\bar{q}\\,=\\,0.$ )Note that the region near the origin is most important in this kind of system, because the system operation usually occurs near this point.  \n\nFigure 4–18 shows this linearized relationship among $q,x$ ,and $\\Delta P$ .The straight lines shown are the characteristic curves of the linearized hydraulic servomotor.This family of curves consists of equidistant parallel straight lines, parametrized by $x$ .  \n\nIn the present analysis we assume that the load reactive forces are small, so that the leakage flow rate and oil compressibility can be ignored.  \n\nReferring to Figure 4–17(a), we see that the rate of flow of oil $q$ times $d t$ is equal to the power-piston displacement $d y$ times the piston area $A$ times the density of oil $\\rho$ .Thus, we obtain  \n\n$$\nA\\rho\\,d y\\,=\\,q\\,d t\n$$  \n\nNotice that for a given flow rate $q$ the larger the piston area $A$ is, the lower will be the velocity $d y/d t$ . Hence, if the piston area $A$ is made smaller, the other variables remaining constant, the velocity $d y/d t$ will become higher.Also, an increased flow rate $q$ will cause an increased velocity of the power piston and will make the response time shorter.  \n\nEquation (4–27) can now be written as  \n\n$$\n\\Delta P={\\frac{1}{K_{2}}}\\left(K_{1}x\\,-\\,A\\rho\\,{\\frac{d y}{d t}}\\right)\n$$  \n\nThe force developed by the power piston is equal to the pressure difference $\\Delta P$ times the piston area $A$ or  \n\n$$\n{\\begin{array}{r l}&{{\\mathrm{Force~developed~by~the~power~piston}}=A\\ \\Delta P}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad={\\cfrac{A}{K_{2}}}\\left(K_{1}x\\,-\\,A\\rho\\,{\\cfrac{d y}{d t}}\\right)}\\end{array}}\n$$  \n\n![](images/dec2e1a4fc4384fb7ff1ff7ad07bd6f44dc91dbd040f4527351674a9657b68a5.jpg)  \nFigure 4–18 Characteristic curves of the linearized hydraulic servomotor.  \n\nFor a given maximum force, if the pressure difference is sufficiently high, the piston area, or the volume of oil in the cylinder, can be made small. Consequently, to minimize the weight of the controller, we must make the supply pressure sufficiently high.  \n\nAssume that the power piston moves a load consisting of a mass and viscous friction. Then the force developed by the power piston is applied to the load mass and friction, and we obtain  \n\n$$\nm\\ddot{y}\\,+\\,b\\dot{y}\\,=\\frac{A}{K_{2}}\\big(K_{1}x\\,-\\,A\\rho\\dot{y}\\big)\n$$  \n\nor  \n\n$$\nm\\ddot{y}\\,+\\,\\biggl(b\\,+\\frac{A^{2}\\rho}{K_{2}}\\biggr)\\dot{y}\\,=\\frac{A K_{1}}{K_{2}}\\,x\n$$  \n\nwhere $_m$ is the mass of the load and $^b$ is the viscous-friction coefficient.  \n\nAssuming that the pilot-valve displacement $x$ is the input and the power-piston displacement $y$ is the output, we find that the transfer function for the hydraulic servomotor is, from Equation (4–28),  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{Y(s)}{X(s)}=\\displaystyle\\frac{1}{s\\Big[\\Big(\\displaystyle\\frac{m K_{2}}{A K_{1}}\\Big)s+\\displaystyle\\frac{b K_{2}}{A K_{1}}+\\displaystyle\\frac{A\\rho}{K_{1}}\\Big]}}\\\\ {\\displaystyle=\\frac{K}{s(T s\\,+\\,1)}}\\end{array}\n$$  \n\nwhere  \n\n$$\nK={\\frac{1}{{\\frac{b K_{2}}{A K_{1}}}+{\\frac{A\\rho}{K_{1}}}}}\\qquad{\\mathrm{and}}\\qquad T={\\frac{m K_{2}}{b K_{2}+A^{2}\\rho}}\n$$  \n\nFrom Equation (4–29) we see that this transfer function is of the second order.If the ratio $m K_{2}/\\bigl(b\\hat{K}_{2}\\,+\\,A^{2}\\rho\\bigr)$ is negligibly small or the time constant $T$ is negligible, the transfer function $Y(s)/X(s)$ can be simplified to give  \n\n$$\n{\\frac{Y(s)}{X(s)}}={\\frac{K}{s}}\n$$  \n\nIt is noted that a more detailed analysis shows that if oil leakage, compressibility (including the effects of dissolved air), expansion of pipelines, and the like are taken into consideration, the transfer function becomes  \n\n$$\n{\\frac{Y(s)}{X(s)}}={\\frac{K}{s(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}}\n$$  \n\nwhere $T_{1}$ and $T_{2}$ are time constants.As a matter of fact, these time constants depend on the volume of oil in the operating circuit.The smaller the volume, the smaller the time constants.  \n\nHydraulic Integral Controller. The hydraulic servomotor shown in Figure 4–19 is a pilot-valve-controlled hydraulic power amplifier and actuator. Similar to the hydraulic servo system shown in Figure 4–17, for negligibly small load mass the servomotor shown in Figure 4–19 acts as an integrator or an integral controller. Such a servomotor constitutes the basis of the hydraulic control circuit.  \n\nIn the hydraulic servomotor shown in Figure 4–19, the pilot valve (a four-way valve) has two lands on the spool. If the width of the land is smaller than the port in the valve sleeve,the valve is said to be underlapped .Overlapped valves have a land width greater than the port width.A zero-lapped valve has a land width that is identical to the port width. (If the pilot valve is a zero-lapped valve,analyses of hydraulic servomotors become simpler.)  \n\nIn the present analysis, we assume that hydraulic fluid is incompressible and that the inertia force of the power piston and load is negligible compared to the hydraulic force at the power piston.We also assume that the pilot valve is a zero-lapped valve, and the oil flow rate is proportional to the pilot valve displacement.  \n\nOperation of this hydraulic servomotor is as follows. If input $x$ moves the pilot valve to the right, port II is uncovered, and so high-pressure oil enters the right-hand side of the power piston. Since port I is connected to the drain port, the oil in the left-hand side of the power piston is returned to the drain. The oil flowing into the power cylinder is at high pressure; the oil flowing out from the power cylinder into the drain is at low pressure. The resulting difference in pressure on both sides of the power piston will cause it to move to the left.  \n\nNote that the rate of flow of oil $q$ $(\\mathrm{kg/sec})$ times dt (sec )is equal to the power-piston displacement dy (m)times the piston area $A$ $(\\mathbf{m}^{2})$ )times the density of oil $\\rho$ $(\\mathrm{kg}/\\mathrm{m}^{3})$ .Therefore,  \n\n$$\nA\\rho\\,d y\\,=\\,q\\,d t\n$$  \n\nBecause of the assumption that the oil flow rate $q$ is proportional to the pilot-valve displacement $x$ , we have  \n\n$$\nq\\,=\\,K_{1}x\n$$  \n\nwhere $K_{1}$ is a positive constant. From Equations (4–30) and (4–31) we obtain  \n\n$$\nA\\rho\\,{\\frac{d y}{d t}}=K_{1}x\n$$  \n\n![](images/7f8adc0610e0ab5c0d514cdb38ff6cd9de49f5683bbb2730278f57bf54b1d52b.jpg)  \nFigure 4–19 Hydraulic servomotor.   \nChapter 4 /Mathematical Modeling of Fluid Systems and Thermal Systems  \n\nThe Laplace transform of this last equation, assuming a zero initial condition, gives  \n\n$$\nA\\rho s Y(s)\\,=\\,K_{1}X(s)\n$$  \n\nor  \n\n$$\n{\\frac{Y(s)}{X(s)}}={\\frac{K_{1}}{A\\rho s}}={\\frac{K}{s}}\n$$  \n\nwhere $K\\,=\\,K_{1}/(A\\rho)$ . Thus the hydraulic servomotor shown in Figure 4–19 acts as an integral controller.  \n\nHydraulic Proportional Controller. It has been shown that the servomotor in Figure 4–19 acts as an integral controller. This servomotor can be modified to a proportional controller by means of a feedback link. Consider the hydraulic controller shown in Figure 4–20(a).The left-hand side of the pilot valve is joined to the left-hand side of the power piston by a link ABC .This link is a floating link rather than one moving about a fixed pivot.  \n\nThe controller here operates in the following way. If input emoves the pilot valve to the right, port II will be uncovered and high-pressure oil will flow through port II into the right-hand side of the power piston and force this piston to the left.The power piston, in moving to the left, will carry the feedback link $A B C$ with it, thereby moving the pilot valve to the left.This action continues until the pilot piston again covers ports I and II.A block diagram of the system can be drawn as in Figure 4–20(b).The transfer function between $Y(s)$ and $E(s)$ is given by  \n\n$$\n{\\frac{Y(s)}{E(s)}}={\\frac{\\cfrac{b}{a+b}}{1+{\\frac{K}{s}}{\\frac{a}{a+b}}}}\n$$  \n\nNoting that under the normal operating conditions we have $\\left|K a/\\!\\left[s(a\\,+\\,b)\\right]\\right|\\,\\gg\\,1$ this last equation can be simplified to  \n\n$$\n{\\frac{Y(s)}{E(s)}}={\\frac{b}{a}}=K_{p}\n$$  \n\n![](images/4bdd7b663e72275d5fda92a74554b4709269e1f5fc803eb9e9f990267fd7e1c6.jpg)  \nFigure 4–20 (a) Servomotor that acts as a proportional controller; (b) block diagram of the servomotor.  \n\n![](images/b08366beac5f0e43369dad4182da7b7dc83f3f29ce774262191fe667b1b730bb.jpg)  \n\nThe transfer function between $y$ and $e$ becomes a constant.Thus,the hydraulic controller shown in Figure 4–20(a) acts as a proportional controller,the gain of which is $K_{p}$ .This gain can be adjusted by effectively changing the lever ratio $b/a$ . (The adjusting mechanism is not shown in the diagram.)  \n\nWe have thus seen that the addition of a feedback link will cause the hydraulic servomotor to act as a proportional controller.  \n\nDashpots. The dashpot (also called a damper) shown in Figure 4–21(a) acts as a differentiating element.Suppose that we introduce a step displacement to the piston position $y$ .Then the displacement $z$ becomes equal to $y$ momentarily.Because of the spring force,however,the oil will flow through the resistance $R$ and the cylinder will come back to the original position.The curves $y$ versus $t$ and $z$ versus $t$ are shown in Figure 4–21(b).  \n\nLet us derive the transfer function between the displacement $z$ and displacement $y$ .Define the pressures existing on the right and left sides of the piston as $P_{1}(\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{in}.^{2})$ and $P_{2}(\\mathrm{lb}_{\\mathrm{f}}/\\mathrm{in}.^{2})$ ,respectively. Suppose that the inertia force involved is negligible.Then the force acting on the piston must balance the spring force.Thus  \n\n$$\nA\\big(P_{1}\\,-\\,P_{2}\\big)\\,=\\,k z\n$$  \n\nwhere $A=$ piston area, in.  \n\n$k=$ spring constant, $\\mathrm{1b_{f}/i n}$ .  \n\nThe flow rate $q$ is given by  \n\n$$\nq=\\frac{P_{1}-P_{2}}{R}\n$$  \n\nwhere $q=$ flow rate through the restriction, lb 'sec  \n\n$R=$ resistance to flow at the restriction, lb f-sec 'in. -lb  \n\nSince the flow through the restriction during dt seconds must equal the change in the mass of oil to the left of the piston during the same dt seconds, we obtain  \n\n$$\nq\\,d t=\\,A\\rho(d y\\,-\\,d z)\n$$  \n\nwhere $\\rho=$ density, $1\\mathrm{b}/\\mathrm{in}.^{3}$ . (We assume that the fluid is incompressible or $\\rho=$ constant.) This last equation can be rewritten as  \n\n$$\n{\\frac{d y}{d t}}-{\\frac{d z}{d t}}={\\frac{q}{A\\rho}}={\\frac{P_{1}-P_{2}}{R A\\rho}}={\\frac{k z}{R A^{2}\\rho}}\n$$  \n\n![](images/448a522dd75e6b8e1b8831cafe83f65879b0bca0bfd1333934f9b1b70cbe0f13.jpg)  \nFigure 4–21 (a) Dashpot; (b) step change in $y$ and the corresponding change in $z$ plotted versus $t$ ; (c) block diagram of the dashpot.  \n\nor  \n\n$$\n\\frac{d y}{d t}=\\frac{d z}{d t}+\\frac{k z}{R A^{2}\\rho}\n$$  \n\nTaking the Laplace transforms of both sides of this last equation, assuming zero initial conditions, we obtain  \n\n$$\ns Y(s)\\,=\\,s Z(s)\\,+\\,{\\frac{k}{R A_{\\rho}^{2}}}\\,Z(s)\n$$  \n\nThe transfer function of this system thus becomes  \n\n$$\n{\\frac{Z(s)}{Y(s)}}={\\frac{s}{\\displaystyle s+{\\frac{k}{R A^{2}\\rho}}}}\n$$  \n\nLet us define $R A^{2}\\rho/k=T$ .(Note that $R A^{2}\\rho/k$ has the dimension of time.) Then  \n\n$$\n\\frac{Z(s)}{Y(s)}=\\frac{T s}{T s\\,+\\,1}=\\frac{1}{1+\\frac{1}{T s}}\n$$  \n\nClearly, the dashpot is a differentiating element. Figure 4–21(c) shows a block diagram representation for this system.  \n\nObtaining Hydraulic Proportional-Plus-Integral Control Action. Figure 4–22(a) shows a schematic diagram of a hydraulic proportional-plus-integral controller.A block diagram of this controller is shown in Figure 4–22(b). The transfer function $Y(s)/E(s)$ is given by  \n\n$$\n{\\frac{Y(s)}{E(s)}}={\\frac{\\displaystyle{\\frac{b}{a+b}}{\\frac{K}{s}}}{\\displaystyle{1+{\\frac{K a}{a+b}}{\\frac{T}{T s+1}}}}}\n$$  \n\n![](images/dd7520430594a5b940494bb21f2f2f5cd10f7ce0e43ef761a9ab1ec55a8719d7.jpg)  \nFigure 4–22 (a) Schematic diagram of a hydraulic proportional-plus-integral controller; (b) block diagram of the controller.  \n\nIn such a controller, under normal operation $\\left|K a T/[(a\\,+\\,b)(T s\\,+\\,1)\\right]\\right|\\,\\gg\\,1$ ,with the result that  \n\n$$\n{\\frac{Y(s)}{E(s)}}=\\,K_{p}\\bigg(1\\,+\\,{\\frac{1}{T_{i}s}}\\bigg)\n$$  \n\nwhere  \n\n$$\nK_{p}=\\frac{b}{a},~~~~~T_{i}=T=\\frac{R A^{2}\\rho}{k}\n$$  \n\nThus the controller shown in Figure 4–22(a) is a proportional-plus-integral controller (PI controller).  \n\nObtaining Hydraulic Proportional-Plus-Derivative Control Action. Figure 4–23(a) shows a schematic diagram of a hydraulic proportional-plus-derivative controller. The cylinders are fixed in space and the pistons can move. For this system, notice that  \n\n$$\n\\begin{array}{c}{{k(y\\,-\\,z)\\,=\\,A\\big(P_{2}\\,-\\,P_{1}\\big)}}\\\\ {{\\,}}\\\\ {{q=\\displaystyle{\\frac{P_{2}\\,-\\,P_{1}}{R}}}}\\\\ {{\\,}}\\\\ {{q\\,d t=\\rho A\\,d z}}\\end{array}\n$$  \n\nHence  \n\n$$\ny\\,=\\,z\\,+\\frac{A}{k}\\,q R\\,=\\,z\\,+\\frac{R A^{2}\\rho}{k}\\frac{d z}{d t}\n$$  \n\nor  \n\n$$\n{\\frac{Z(s)}{Y(s)}}={\\frac{1}{T s\\,+\\,1}}\n$$  \n\n![](images/800d80c62e7d0e6d9b520d5f5f158409a403e1e4eaa3f63aaa4335b877a38fd3.jpg)  \nFigure 4–23 (a) Schematic diagram of a hydraulic proportional-plus-derivative controller; (b) block diagram of the controller.  \n\nwhere  \n\n$$\nT={\\frac{R A^{2}\\rho}{k}}\n$$  \n\nA block diagram for this system is shown in Figure 4–23(b). From the block diagram the transfer function $Y(s)/E(s)$ can be obtained as  \n\n$$\n{\\frac{Y(s)}{E(s)}}={\\frac{\\cfrac{b}{a+b}{\\cfrac{K}{s}}}{1+{\\cfrac{a}{a+b}}{\\frac{K}{s}}{\\frac{1}{T s+1}}}}\n$$  \n\nUnder normal operation we have $\\left|a K/[(a\\,+\\,b)s(T s\\,+\\,1)\\right]\\right|\\,\\gg\\,1.$ Hence  \n\n$$\n{\\frac{Y(s)}{E(s)}}=K_{p}(1\\,+\\,T s)\n$$  \n\nwhere  \n\n$$\nK_{p}=\\frac{b}{a},\\;\\;\\;\\;\\;\\;T=\\frac{R A^{2}\\rho}{k}\n$$  \n\nThus the controller shown in Figure 4–23(a) is a proportional-plus-derivative controller (PD controller).  \n\nObtaining Hydraulic Proportional-Plus-Integral-Plus-Derivative Control Action. Figure 4–24 shows a schematic diagram of a hydraulic proportional-plus-integral-plusderivative controller. It is a combination of the proportional-plus-integral controller and proportional-plus derivative controller.  \n\nIf the two dashpots are identical except the piston shafts, the transfer function $Z(s)/Y(s)$ can be obtained as follows:  \n\n$$\n\\frac{Z(s)}{Y(s)}=\\frac{T_{1}s}{T_{1}T_{2}s^{2}\\:+\\:(T_{1}\\:+\\:2T_{2})s\\:+\\:1}\n$$  \n\n(For the derivation of this transfer function, refer to Problem A–4–9 .)  \n\n![](images/4f5e8f4a902e5329c700b9b489341333ddda0dba21bce61ffd96477c9b7c50a7.jpg)  \nFigure 4–24 Schematic diagram of a hydraulic proportional-plusintegral-plusderivative controller.  \n\n![](images/1819495fd0db191d10428879eaf6777f9f0de33aea65ded15892f73490fa7995.jpg)  \nFigure 4–25 Block diagram for the system shown in Figure 4–24.  \n\nA block diagram for this system is shown in Figure 4–25. The transfer function $Y(s)/E(s)$ can be obtained as  \n\n$$\n\\frac{Y(s)}{E(s)}=\\frac{b}{a\\,+\\,b}\\frac{\\displaystyle\\frac{K}{s}}{1\\,+\\,\\frac{a}{a\\,+\\,b}\\frac{K}{s}\\frac{T_{1}s}{T_{1}T_{2}s^{2}\\,+\\,\\bigl(T_{1}\\,+\\,2T_{2}\\bigr)s\\,+\\,1}}\n$$  \n\nUnder normal circumstances we design the system such that  \n\n$$\n\\left|\\frac{a}{a\\,+\\,b}\\frac{K}{s}\\frac{T_{1}s}{T_{1}T_{2}s^{2}\\,+\\,\\left(T_{1}\\,+\\,2T_{2}\\right)s\\,+\\,1}\\right|\\,\\gg\\,1\n$$  \n\nthen  \n\n$$\n\\begin{array}{c}{\\displaystyle\\frac{Y(s)}{E(s)}=\\frac{b}{a}\\frac{T_{1}T_{2}s^{2}+\\left(T_{1}+2T_{2}\\right)s\\,+\\,1}{T_{1}s}}\\\\ {\\,=\\,K_{p}\\,+\\,\\frac{K_{i}}{s}\\,+\\,K_{d}s}\\end{array}\n$$  \n\nwhere  \n\n$$\nK_{p}={\\frac{b}{a}}{\\frac{T_{1}+2T_{2}}{T_{1}}},\\ \\ \\ \\ \\ K_{i}={\\frac{b}{a}}{\\frac{1}{T_{1}}},\\ \\ \\ \\ \\ K_{d}={\\frac{b}{a}}T_{2}\n$$  \n\nThus, the controller shown in Figure 4–24 is a proportional-plus-integral-plus-derivative controller (PID controller).  \n\n# 4–5 THERMAL SYSTEMS  \n\nThermal systems are those that involve the transfer of heat from one substance to another. Thermal systems may be analyzed in terms of resistance and capacitance, although the thermal capacitance and thermal resistance may not be represented accurately as lumped parameters, since they are usually distributed throughout the substance. For precise analysis, distributed-parameter models must be used. Here, however, to simplify the analysis we shall assume that a thermal system can be represented by a lumped-parameter model, that substances that are characterized by resistance to heat flow have negligible heat capacitance, and that substances that are characterized by heat capacitance have negligible resistance to heat flow.  \n\nThere are three different ways heat can flow from one substance to another: conduction, convection, and radiation. Here we consider only conduction and convection. (Radiation heat transfer is appreciable only if the temperature of the emitter is very high compared to that of the receiver.Most thermal processes in process control systems do not involve radiation heat transfer.)  \n\nFor conduction or convection heat transfer,  \n\n$$\nq\\,=\\,K\\;\\Delta\\theta\n$$  \n\nwhere $q=$ heat flow rate, kcal 'sec $\\Delta\\theta\\,=$ temperature difference, $^{\\circ}\\mathbf{C}$ $K=$ coefficient, kcal 'sec $^\\circ C$  \n\nThe coefficient $K$ is given by  \n\n$$\n{\\begin{array}{r l}{K={\\frac{k A}{\\Delta X}},\\quad}&{{\\mathrm{for~conduction}}}\\\\ {=H A,\\quad}&{{\\mathrm{for~convection}}}\\end{array}}\n$$  \n\nwhere $k=$ thermal conductivity, kcal 'm sec $^\\circ C$ $A=$ area normal to heat flow, $\\ensuremath{\\mathbf{m}}^{2}$ $\\Delta X=$ thickness of conductor, m $H=$ convection coefficient, kcal '$\\mathrm{~m}^{2}$ sec $^\\circ C$  \n\nThermal Resistance and Thermal Capacitance. The thermal resistance $R$ for heat transfer between two substances may be defined as follows:  \n\n$$\nR={\\frac{{\\mathrm{change~in~temperature~difference}},{\\mathrm{}}^{\\circ}C}{{\\mathrm{change~in~heat~flow~rate}},{\\mathrm{kcal}}/{\\mathrm{sec}}}}\n$$  \n\nThe thermal resistance for conduction or convection heat transfer is given by  \n\n$$\nR=\\frac{d(\\Delta\\theta)}{d q}=\\frac{1}{K}\n$$  \n\nSince the thermal conductivity and convection coefficients are almost constant, the thermal resistance for either conduction or convection is constant.  \n\nThe thermal capacitance $C$ is defined by  \n\n$$\nC={\\frac{\\mathrm{change~in~heat~stored,~kcal}}{\\mathrm{change~in~temperature,~}^{\\circ}C}}\n$$  \n\nor  \n\n$$\nC\\,=\\,m c\n$$  \n\nwhere $m\\,=\\,{\\mathrm{mass}}$ of substance considered, kg $c=$ specific heat of substance, $\\mathrm{kcal/kg\\,^{\\circ}C}$  \n\nThermal System. Consider the system shown in Figure 4–26(a). It is assumed that the tank is insulated to eliminate heat loss to the surrounding air. It is also assumed that there is no heat storage in the insulation and that the liquid in the tank is perfectly mixed so that it is at a uniform temperature.Thus,a single temperature is used to describe the temperature of the liquid in the tank and of the outflowing liquid.  \n\nLet us define  \n\n$$\n\\begin{array}{r l}&{\\bar{\\theta}_{i}=\\mathrm{steady-state~temperature~of~inflowing~liquid,\\,of~}}\\\\ &{\\bar{\\theta}_{o}=\\mathrm{steady-state~temperature~of~outflowing~liquid,\\,of~}}\\\\ &{\\alpha=\\mathrm{steady-state~liquid~flow~rate,~kg/sec~}}\\\\ &{M=\\mathrm{~mass~of~liquid~in~tank,kg}}\\\\ &{c=\\mathrm{specific~heat~of~liquid,~kcal/kg~^cC~}}\\\\ &{R=\\mathrm{~thermal~resistance,\\!~\\!c~sec/kcal}}\\\\ &{C=\\mathrm{~thermal~capacitance,~kcal/\\circC~}}\\\\ &{\\bar{H}=\\mathrm{~steady-state~heat~input~rate,kcal/sec~}}\\end{array}\n$$  \n\nAssume that the temperature of the inflowing liquid is kept constant and that the heat input rate to the system (heat supplied by the heater) is suddenly changed from $\\bar{H}$ to $\\bar{H}^{\\ }+\\,h_{i}$ will then change gradually from uid will also be changed from ,where $h_{i}$ represents a small change in the heat input rate.The heat outflow rate $\\bar{\\theta}_{o}$ $\\bar{H}$ to to $\\overline{{\\theta}}_{o}+\\theta$ –$\\bar{H}^{\\ \\ +}\\ h_{o}$ .For this case, .The temperature of the outflowing liq$h_{o}$ ,$C$ , and $R$ are obtained, respectively, as  \n\n$$\n\\begin{array}{l}{{h_{o}=G c\\theta}}\\\\ {{\\mathrm{~}}}\\\\ {{C=M c}}\\\\ {{\\mathrm{~}}}\\\\ {{R=\\displaystyle\\frac{\\theta}{h_{o}}=\\frac{1}{G c}}}\\end{array}\n$$  \n\nThe heat-balance equation for this system is  \n\n$$\nC\\,d\\theta=\\left(h_{i}\\,-\\,h_{o}\\right)d t\n$$  \n\n![](images/1e1eb6fe7b3c9dd7d5b9c433666907b814c10438701a33d044d8c1219b327a76.jpg)  \nFigure 4–26 (a) Thermal system: (b) block diagram of the system.  \n\nor  \n\n$$\nC\\,{\\frac{d\\theta}{d t}}=h_{i}\\,-\\,h_{o}\n$$  \n\nwhich may be rewritten as  \n\n$$\nR C\\,{\\frac{d\\theta}{d t}}+\\,\\theta\\,=\\,R h_{i}\n$$  \n\nNote that the time constant of the system is equal to $R C$ or $M/G$ seconds.The transfer function relating $\\theta$ and $h_{i}$ is given by  \n\n$$\n{\\frac{\\theta(s)}{H_{i}(s)}}={\\frac{R}{R C s\\,+\\,1}}\n$$  \n\nwhere $\\Theta(s)\\,=\\,\\mathcal{L}\\big[\\theta(t)\\big]$ and $H_{i}(s)\\,=\\,\\mathcal{L}\\big[h_{i}(t)\\big]$ .  \n\nIn practice, the temperature of the inflowing liquid may fluctuate and may act as a load disturbance. (If a constant outflow temperature is desired, an automatic controller may be installed to adjust the heat inflow rate to compensate for the fluctuations in the temperature of the inflowing liquid.) If the temperature of the inflowing liquid is suddenly changed from $\\overline{{\\Theta}}_{i}$ to $\\bar{\\theta}_{i}+\\bar{\\theta}_{i}$ while the heat input rate $H$ and the liquid flow rate $G$ are kept constant, then the heat outflow rate will be changed from $\\bar{H}$ to $\\bar{H}^{\\mathrm{~+~}}h_{o}$ ,and the temperature of the outflowing liquid will be changed from $\\overline{{\\theta}}_{o}$ to $\\overline{{\\Theta}}_{o}+\\,\\theta.$ .The heatbalance equation for this case is  \n\n$$\nC\\;d\\theta=\\left(G c\\theta_{i}\\;-\\;h_{o}\\right)d t\n$$  \n\nor  \n\n$$\nC\\,{\\frac{d\\theta}{d t}}=G c\\theta_{i}\\,-\\,h_{o}\n$$  \n\nwhich may be rewritten  \n\n$$\nR C\\,{\\frac{d\\theta}{d t}}+\\,\\theta\\,=\\,\\theta_{i}\n$$  \n\nThe transfer function relating $\\theta$ and $\\theta_{i}$ is given by  \n\n$$\n\\frac{\\theta(s)}{\\theta_{i}(s)}=\\frac{1}{R C s\\,+\\,1}\n$$  \n\nwhere $\\Theta(s)\\,=\\,\\mathcal{L}\\big[\\theta(t)\\big]$ and $\\Theta_{i}(s)\\,=\\,\\mathcal{L}\\big[\\theta_{i}(t)\\big]$ .  \n\nIf the present thermal system is subjected to changes in both the temperature of the inflowing liquid and the heat input rate, while the liquid flow rate is kept constant, the change $\\theta$ in the temperature of the outflowing liquid can be given by the following equation:  \n\n$$\nR C\\,{\\frac{d\\theta}{d t}}+\\,\\theta\\,=\\,\\theta_{i}\\,+\\,R h_{i}\n$$  \n\nA block diagram corresponding to this case is shown in Figure 4–26(b). Notice that the system involves two inputs.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–4–1. In the liquid-level system of Figure 4–27 assume that the outflow rate $Q\\,\\mathrm{m}^{3}/\\mathrm{sec}$ through the outflow valve is related to the head $H\\textrm{m}$ by  \n\n$$\nQ\\,=\\,K{\\sqrt{H}}\\,=\\,0.01{\\sqrt{H}}\n$$  \n\nAssume also that when the inflow rate A $Q_{i}$ is $0.015\\;\\mathrm{m}^{3}/\\mathrm{sec}$ Bthe head stays constant. For $t<0$ the system is at tate $Q_{i}=0.015\\;\\mathrm{m}^{3}/\\mathrm{sec})$ . At $t\\,=\\,0$ the inflow valve is closed and so there is no inflow for $t\\geq0$ 0. Find the time necessary to empty the tank to half the original head. The capacitance Cof the tank is 2 m .  \n\nSolution. When the head is stationary, the inflow rate equals the outflow rate. Thus head $H_{o}$ at $t\\,=\\,0$ is obtained from  \n\n$$\n0.015\\,=\\,0.01\\sqrt{H_{o}}\n$$  \n\nor  \n\n$$\nH_{o}=2.25\\:\\mathrm{m}\n$$  \n\nThe equation for the system for $t>0$ is  \n\n$$\n-C\\,d H=Q\\,d t\n$$  \n\nor  \n\n$$\n{\\frac{d H}{d t}}=-{\\frac{Q}{C}}={\\frac{-0.01{\\sqrt{H}}}{2}}\n$$  \n\nHence  \n\n$$\n\\frac{d H}{\\sqrt{H}}=-0.005\\,d t\n$$  \n\nAssume that, at $t=t_{1},H=1.125\\,\\mathrm{m}.$ . Integrating both sides of this last equation, we obtain  \n\n$$\n\\int_{2.25}^{1.125}\\frac{d H}{\\sqrt{H}}=\\,\\int_{0}^{t_{1}}(-0.005)\\,d t=-0.005t_{1}\n$$  \n\nIt follows that  \n\n$$\n2\\sqrt{H}\\left|_{2.25}^{1.125}=2\\sqrt{1.125}\\,-\\,2\\sqrt{2.25}=-0.005t_{1}\n$$  \n\nor  \n\n$$\nt_{1}=175.7\n$$  \n\nThus, the head becomes half the original value $\\left(2.25\\;\\mathrm{m}\\right)$ ) in 175.7 sec.  \n\nFigure 4–27 Liquid-level system.  \n\n![](images/8de1a814ef7378de7841f341f5447d5cce4b5cbd78f8deab6c7ee9814f5d22fd.jpg)  \n\nA–4–2. Consider the liquid-level system shown in Figure 4–28. In the system, $\\overline{{Q}}_{1}$ and $\\bar{Q}_{2}$ –are steady-state inflow rates and $\\boldsymbol{\\bar{H}}_{1}$ and $\\boldsymbol{\\bar{H}_{2}}$ –are steady-state heads.The quantities $q_{i1},q_{i2},h_{1},h_{2},q_{1},$ , and $q_{o}$ are considered small. Obtain a state-space representation for the system when $h_{1}$ and $h_{2}$ are the outputs and $q_{i1}$ and $q_{i2}$ are the inputs.  \n\nSolution. The equations for the system are  \n\n$$\n\\begin{array}{r l}&{\\quad C_{1}\\,d h_{1}=\\left(q_{i1}\\,-\\,q_{1}\\right)d t}\\\\ &{\\quad\\displaystyle\\frac{h_{1}-h_{2}}{R_{1}}=q_{1}}\\\\ &{\\quad C_{2}\\,d h_{2}=\\left(q_{1}+\\,q_{i2}\\,-\\,q_{o}\\right)d t}\\\\ &{\\quad\\quad\\displaystyle\\frac{h_{2}}{R_{2}}=q_{o}}\\end{array}\n$$  \n\nElimination of $q_{1}$ from Equation (4–32) using Equation (4–33) results in  \n\n$$\n{\\frac{d h_{1}}{d t}}={\\frac{1}{C_{1}}}\\left(q_{i1}-{\\frac{h_{1}-h_{2}}{R_{1}}}\\right)\n$$  \n\nEliminating $q_{1}$ and $q_{o}$ from Equation (4–34) by using Equations (4–33) and (4–35) gives  \n\n$$\n\\frac{d h_{2}}{d t}=\\frac{1}{C_{2}}\\left(\\frac{h_{1}-h_{2}}{R_{1}}+q_{i2}-\\frac{h_{2}}{R_{2}}\\right)\n$$  \n\nDefine state variables $x_{1}$ and $x_{2}$ by  \n\n$$\n\\begin{array}{l}{x_{1}=h_{1}}\\\\ {x_{2}=h_{2}}\\end{array}\n$$  \n\nthe input variables $u_{1}$ and $u_{2}$ by  \n\n$$\n\\begin{array}{r}{u_{1}=q_{i1}}\\\\ {u_{2}=q_{i2}}\\end{array}\n$$  \n\nand the output variables $y_{1}$ and $y_{2}$ by  \n\n$$\n\\begin{array}{l}{y_{1}=h_{1}=x_{1}}\\\\ {y_{2}=h_{2}=x_{2}}\\end{array}\n$$  \n\nThen Equations (4–36) and (4–37) can be written as  \n\n$$\n\\begin{array}{l}{{\\dot{x}}_{1}=-\\displaystyle\\frac{1}{R_{1}C_{1}}\\,x_{1}+\\displaystyle\\frac{1}{R_{1}C_{1}}\\,x_{2}+\\displaystyle\\frac{1}{C_{1}}\\,u_{1}}\\\\ {{\\dot{x}}_{2}=\\displaystyle\\frac{1}{R_{1}C_{2}}\\,x_{1}-\\left(\\displaystyle\\frac{1}{R_{1}C_{2}}+\\displaystyle\\frac{1}{R_{2}C_{2}}\\right)x_{2}+\\displaystyle\\frac{1}{C_{2}}\\,u_{2}}\\end{array}\n$$  \n\n![](images/bf951f524a91272162f555eca009d7d7f2453f8820bdb8906754c407b54292f7.jpg)  \nQ1+q1  \n\nFigure 4–28 Liquid-level system.  \n\nIn the form of the standard vector-matrix representation, we have  \n\n$$\n\\left[\\!\\!{\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}}\\right]={\\left[\\!\\!{\\begin{array}{c c}{-{\\cfrac{1}{R_{1}C_{1}}}}&{{\\cfrac{1}{R_{1}C_{1}}}}\\\\ {{\\cfrac{1}{R_{1}C_{2}}}}&{-\\left({\\cfrac{1}{R_{1}C_{2}}}+{\\cfrac{1}{R_{2}C_{2}}}\\right)}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+\\left[\\!\\!{\\begin{array}{c c}{1}&{0}\\\\ {\\overline{{C_{1}}}}&{0}\\\\ {0}&{{\\cfrac{1}{C_{2}}}}\\end{array}}\\!\\!\\right]{\\left[\\!\\!{\\begin{array}{c}{u_{1}}\\\\ {u_{2}}\\end{array}}\\right]}\n$$  \n\nwhich is the state equation, and  \n\n$$\n{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0{\\top}{\\Big[}{\\bar{x}}_{1}{\\Big]}}\\\\ {0}&{1{\\rfloor}{\\bar{x}}_{2}}\\end{array}\\right]}\n$$  \n\nwhich is the output equation.  \n\nA–4–3. The value of the gas constant for any gas may be determined from accurate experimental observations of simultaneous values of $p,v.$ , and $T$ .  \n\nObtain the gas constant $R_{\\mathrm{air}}$ for air. Note that at $32^{\\circ}\\mathrm{F}$ and 14.7 psia the specific volume of air is 12.39 ft 3 'lb.Then obtain the capacitance of a $20{-}\\mathrm{ft}^{3}$ pressure vessel that contains air at $160^{\\circ}\\mathrm{F}.$ Assume that the expansion process is isothermal.  \n\n# Solution.  \n\n$$\nR_{\\mathrm{air}}={\\frac{p v}{T}}={\\frac{14.7\\times144\\times12.39}{460\\times32}}=53.3\\,{\\mathrm{ft}}{\\mathrm{-}}{\\mathrm{lb}}_{\\mathrm{f}}/{\\mathrm{lb}}^{\\mathrm{o}}{\\mathrm{R}}\n$$  \n\nReferring to Equation (4–12), the capacitance of a 20-ft 3 pressure vessel is  \n\n$$\nC={\\frac{V}{n R_{\\mathrm{air}}T}}={\\frac{20}{1\\times53.3\\times620}}=6.05\\times10^{-4}{\\frac{\\mathrm{lb}}{{\\mathrm{lb}}_{\\mathrm{t}}/{\\mathrm{ft}}^{2}}}\n$$  \n\nNote that in terms of SI units, $R_{\\mathrm{air}}$ is given by  \n\n$$\nR_{\\mathrm{air}}\\,=\\,287\\,\\mathrm{N{-}m/k g\\,K}\n$$  \n\nA–4–4. In the pneumatic pressure system of Figure 4–29(a), assume that, for $t<0$ , the system is at steady 2 will change from state and that the pressure of the entire system is cal.At $t\\,=\\,0$ , the input pressure is changed from $\\bar{P}$ –to $\\bar{P}+\\,p_{1}$ –and from $\\bar{P}$ to $\\bar{P}\\,+\\,p_{2}$ $\\bar{P}$ $\\bar{P}$ to .Also, assume that the two bellows are identi$\\bar{P}+\\,p_{i}$ ,–respectively.The capacity (volume) of each .Then the pressures in bellows 1 and bellows is $5\\times10^{-4}\\,\\mathrm{m}^{3}$ , and the operating-pressure difference $\\Delta p$ (difference between $p_{i}$ and $p_{1}$ or difference between $p_{i}$ and $p_{2}$ ) is between $\\bar{-0.5}\\times10^{5}\\,\\mathrm{N}/\\mathrm{m}^{2}$ and $\\bar{0}.5\\times10^{5}\\,\\mathrm{N}/\\mathrm{m}^{2}.$ The corresponding mass flow rates $(\\mathrm{kg/sec})$ through the valves are shown in Figure 4–29(b).Assume that the bellows expand or contract linearly with the air pressures applied to them, that the equivalent spring constant of the bellows system is $k=1\\times10^{5}\\,\\mathrm{N/m}$ ,and that each bellows has area $A=15\\times10^{-4}\\,\\mathrm{m}^{2}$ .  \n\n![](images/35ae95a1b4600d70da99520677f22b4471e1cb73558964435294c4b446cfe284.jpg)  \nFigure 4–29 (a) Pneumatic pressure system; (b) pressuredifference-versusmass-flow-rate curves.  \n\n![](images/c2585789e18e6a2db8d6912ca8f2b0fe622801cc50b986e916d28927194fce15.jpg)  \n\nDefining the displacement of the midpoint of the rod that connects two bellows as $x$ , find the transfer function $X(s)/P_{i}(s)$ .Assume that the expansion process is isothermal and that the temperature of the entire system stays at $30^{\\circ}\\mathbf{C}.$ Assume also that the polytropic exponent $n$ is $1$ .  \n\nSolution. Referring to Section 4–3, transfer function $P_{1}(s)/P_{i}(s)$ can be obtained as  \n\n$$\n{\\frac{P_{1}(s)}{P_{i}(s)}}={\\frac{1}{R_{1}C s\\,+\\,1}}\n$$  \n\nSimilarly, transfer function $P_{2}(s)/P_{i}(s)$ is  \n\n$$\n{\\frac{P_{2}(s)}{P_{i}(s)}}={\\frac{1}{R_{2}C s\\,+\\,1}}\n$$  \n\nThe force acting on bellows 1 in the $x$ direction is $A\\big(\\Bar{P}\\,+\\,p_{1}\\big)$ ,and the force acting on bellows 2 in the negative $x$ direction is $A\\!\\left(\\overline{{P}}\\,+\\,p_{2}\\right)$ .The resultant force balances with $k x$ , the equivalent spring force of the corrugated sides of the bellows.  \n\n$$\nA\\!\\left(p_{1}\\,-\\,p_{2}\\right)=k x\n$$  \n\nor  \n\n$$\nA\\big[P_{1}(s)\\,-\\,P_{2}(s)\\big]\\,=\\,k X(s)\n$$  \n\nReferring to Equations (4–38) and (4–39), we see that  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{P_{1}(s)\\,-\\,P_{2}(s)\\,=\\,\\bigg(\\frac{1}{R_{1}C s\\,+\\,1}-\\frac{1}{R_{2}C s\\,+\\,1}\\bigg)P_{i}(s)\\,}}\\\\ &{}&{\\,=\\frac{R_{2}C s\\,-\\,R_{1}C s}{\\big(R_{1}C s\\,+\\,1\\big)\\big(R_{2}C s\\,+\\,1\\big)}\\,P_{i}(s)}\\end{array}\n$$  \n\nBy substituting this last equation into Equation (4–40) and rewriting, the transfer function $X(s)/P_{i}(s)$ is obtained as  \n\n$$\n\\frac{X(s)}{P_{i}(s)}=\\frac{A}{k}\\frac{\\left(R_{2}C\\mathrm{~-~}R_{1}C\\right)\\!s}{\\left(R_{1}C s\\mathrm{~+~}1\\right)\\left(R_{2}C s\\mathrm{~+~}1\\right)}\n$$  \n\nThe numerical values of average resistances $R_{1}$ and $R_{2}$ are  \n\n$$\n\\begin{array}{l}{{R_{1}={\\displaystyle\\frac{d\\,\\Delta p}{d q_{1}}}={\\frac{0.5\\times10^{5}}{3\\times10^{-5}}}=0.167\\times10^{10}{\\frac{\\mathrm{N/m^{2}}}{\\mathrm{kg/sec}}}}}\\\\ {{R_{2}={\\displaystyle\\frac{d\\,\\Delta p}{d q_{2}}}={\\frac{0.5\\times10^{5}}{1.5\\times10^{-5}}}=0.333\\times10^{10}{\\frac{\\mathrm{N/m^{2}}}{\\mathrm{kg/sec}}}}}\\end{array}\n$$  \n\nThe numerical value of capacitance $C$ of each bellows is  \n\n$$\nC={\\frac{V}{n R_{\\mathrm{air}}T}}={\\frac{5\\times10^{-4}}{1\\times287\\times(273+30)}}=5.75\\times10^{-9}{\\frac{\\mathrm{kg}}{{\\mathrm{N/m^{2}}}}}\n$$  \n\nwhere $R_{\\mathrm{air}}\\,=\\,287\\,\\,\\mathrm{N{-}m/k g\\,K}.$ . (See Problem A–4–3 .) Consequently,  \n\n$$\n\\begin{array}{c}{{R_{1}C=0.167\\times10^{10}\\times5.75\\times10^{-9}=9.60\\,\\mathrm{sec}}}\\\\ {{R_{2}C=0.333\\times10^{10}\\times5.75\\times10^{-9}=19.2\\,\\mathrm{sec}}}\\end{array}\n$$  \n\nBy substituting the numerical values for $A,k,R_{1}C$ , and $R_{2}C$ into Equation (4–41), we obtain  \n\n$$\n\\frac{X(s)}{P_{i}(s)}=\\frac{1.44\\times10^{-7}s}{(9.6s\\,+\\,1)(19.2s\\,+\\,1)}\n$$  \n\nA–4–5. Draw a block diagram of the pneumatic controller shown in Figure 4–30.Then derive the transfer function of this controller.Assume that $R_{d}\\,\\ll\\,R_{i}$ .Assume also that the two bellows are identical. If the resistance $R_{d}$ is removed (replaced by the line-sized tubing),what control action do we get? If the resistance $R_{i}$ is removed (replaced by the line-sized tubing), what control action do we get?  \n\nSolution. Let us assume that when $e=0$ the nozzle–flapper distance is equal to $\\bar{X}$ and the control pressure is equal to $\\bar{P}_{c}$ .In the present analysis, we shall assume small deviations from the respective reference values as follows:  \n\n$e=$ small error signal   \n$x=$ small change in the nozzle–flapper distance   \n$p_{c}=$ small change in the control pressure   \n$p_{\\mathrm{{I}}}=$ small pressure change in bellows I due to small change in the control pressure   \n$p_{\\mathrm{{II}}}=$ small pressure change in bellows II due to small change in the control pressure   \n$y=$ small displacement at the lower end of the flapper  \n\nIn this controller, $p_{c}$ is transmitted to bellows I through the resistance $R_{d}$ . Similarly, $p_{c}$ is transmitted to bellows II through the series of resistances $R_{d}$ and $R_{i}$ .The relationship between $p_{I}$ and $p_{c}$ is  \n\n$$\n\\frac{P_{I}(s)}{P_{c}(s)}=\\frac{1}{R_{d}C s\\,+\\,1}=\\frac{1}{T_{d}s\\,+\\,1}\n$$  \n\nwhere $T_{d}\\,=\\,R_{d}C\\,=$ derivative time. Similarly, $p_{\\mathrm{II}}$ and $p_{\\mathrm{I}}$ are related by the transfer function  \n\n$$\n{\\frac{P_{\\mathrm{II}}(s)}{P_{\\mathrm{I}}(s)}}={\\frac{1}{R_{i}C s\\,+\\,1}}={\\frac{1}{T_{i}s\\,+\\,1}}\n$$  \n\nwhere $T_{i}\\,=\\,R_{i}C\\,=$ integral time.The force-balance equation for the two bellows is  \n\n$$\n\\big(p_{\\mathrm{I}}\\,-\\,p_{\\mathrm{II}}\\big)A\\,=\\,k_{s}y\n$$  \n\nwhere $k_{s}$ is the stiffness of the two connected bellows and $A$ is the cross-sectional area of the bellows.The relationship among the variables $e,x$ , and $y$ is  \n\n$$\nx=\\frac{b}{a\\,+\\,b}\\,e\\,-\\,\\frac{a}{a\\,+\\,b}\\,y\n$$  \n\nThe relationship between $p_{c}$ and $x$ is  \n\n$$\np_{c}=K x\\qquad(K>0)\n$$  \n\n![](images/b423fc4a281673d9796d4e232958f2fe58d105b5e9d8204bde99371a125c161b.jpg)  \nFigure 4–30 Schematic diagram of a pneumatic controller.  \n\nFrom the equations just derived, a block diagram of the controller can be drawn, as shown in Figure 4–31(a). Simplification of this block diagram results in Figure 4–31(b). The transfer function between $P_{c}(s)$ and $E(s)$ is  \n\n$$\n{\\frac{P_{c}(s)}{E(s)}}={\\frac{\\displaystyle{\\frac{b}{a+b}}\\,K}{\\displaystyle{1+\\,K\\,{\\frac{a}{a+b}}{\\frac{A}{k_{s}}}\\left({\\frac{T_{i}s}{T_{i}s\\,+\\,1}}\\right)\\left({\\frac{1}{T_{d}s\\,+\\,1}}\\right)}}}\n$$  \n\nFor a practical controller, under normal operation $\\left|K a A T_{i}s/\\right[(a\\,+\\,b)k_{s}(T_{i}s\\,+\\,1)(T_{d}s\\,+\\,1)\\right]\\,\\,$ is very much greater than unity and $T_{i}\\gg T_{d}$ .Therefore, the transfer function can be simplified as follows:  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{P_{c}(s)}{E(s)}\\div\\frac{b k_{s}\\bigl(T_{i}s\\,+\\,1\\bigr)\\bigl(T_{d}s\\,+\\,1\\bigr)}{a A T_{i}s}}\\\\ {\\displaystyle=\\frac{b k_{s}}{a A}\\,\\biggl(\\frac{T_{i}+T_{d}}{T_{i}}+\\frac{1}{T_{i}s}+\\,T_{d}s\\biggr)}\\\\ {\\displaystyle\\doteq\\,K_{p}\\biggl(1\\,+\\,\\frac{1}{T_{i}s}+\\,T_{d}s\\biggr)}\\end{array}\n$$  \n\nwhere  \n\n$$\nK_{p}={\\frac{b k_{s}}{a A}}\n$$  \n\n![](images/1e4d6238c3fb8eb66426da99efaae58ffa131098c5c0d2f66ccbee2ef49e2e40.jpg)  \nFigure 4–31 (a) Block diagram of the pneumatic controller shown in Figure 4–30; (b) simplified block diagram.  \n\nThus the controller shown in Figure 4–30 is a proportional-plus-integral-plus-derivative one.  \n\nIf the resistance $R_{d}$ is removed, or $R_{d}=0$ , the action becomes that of a proportional-plusintegral controller.  \n\nFigure 4–32   \n(a) Overlapped spool valve;   \n(b) underlapped   \nspool valve.  \n\n![](images/809af2e895091de4d0e57b02e49a97e85ae460702954f197c03a3fdd7e9c54a2.jpg)  \n\n![](images/86480451f7fe59d92a8cde7b0b95df32dc5a72e7770bc1efe98955da27d003f0.jpg)  \n\nIf the resistance $R_{i}$ is removed, or $R_{i}=0$ , the action becomes that of a narrow-band proportional, or two-position, controller. (Note that the actions of two feedback bellows cancel each other, and there is no feedback.)  \n\nA–4–6. Actual spool valves are either overlapped or underlapped because of manufacturing tolerances. Consider the overlapped and underlapped spool valves shown in Figures 4–32(a) and (b). Sketch curves relating the uncovered port area $A$ versus displacement $x$ .  \n\nSolution. For the overlapped valve,a dead zone exists between $-{\\textstyle\\frac{1}{2}}x_{0}$ and $\\scriptstyle{\\frac{1}{2}}x_{0}$ ,or $-{\\textstyle\\frac{1}{2}}x_{0}<x<{\\textstyle\\frac{1}{2}}x_{0}$ .The curve for uncovered port area $A$ versus displacement $x$ is shown in Figure 4–33(a). Such an overlapped valve is unfit as a control valve.  \n\nFor the underlapped valve, the curve for port area $A$ versus displacement $x$ is shown in Figure 4–33(b). The effective curve for the underlapped region has a higher slope, meaning a higher sensitivity. Valves used for controls are usually underlapped.  \n\nA–4–7. Figure 4–34 shows a hydraulic jet-pipe controller. Hydraulic fluid is ejected from the jet pipe. If the jet pipe is shifted to the right from the neutral position, the power piston moves to the left, and vice versa. The jet-pipe valve is not used as much as the flapper valve because of large null flow, slower response, and rather unpredictable characteristics. Its main advantage lies in its insensitivity to dirty fluids.  \n\nSuppose that the power piston is connected to a light load so that the inertia force of the load element is negligible compared to the hydraulic force developed by the power piston.What type of control action does this controller produce?  \n\nSolution. Define the displacement of the jet nozzle from the neutral position as $x$ and the displacement of the power piston as $y.$ If the jet nozzle is moved to the right by a small displace  \n\n# Figure 4–33  \n\n(a) Uncovered-portarea- $A$ -versus  \ndisplacement$^{x}$ curve for the overlapped   \nvalve; (b) uncoveredport-area$A$ -versusdisplacement$^{x}$ curve for the underlapped valve. Figure 4–34   \nHydraulic jet-pipe controller.  \n\n![](images/f29c1fc8ef41818a9138b9d595443623f79100fa910d8765548d20c7fbcabd11.jpg)  \n\n![](images/f70a1d1e0b63a8614d2312c5679029c86294e200946b55934dc95cf42c5084c2.jpg)  \n\nment $x$ , the oil flows to the right side of the power piston, and the oil in the left side of the power piston is returned to the drain.The oil flowing into the power cylinder is at high pressure; the oil flowing out from the power cylinder into the drain is at low pressure. The resulting pressure difference causes the power piston to move to the left.  \n\nFor a small jet-nozzle displacement $x$ , the flow rate $q$ to the power cylinder is proportional to $x$ ; that is,  \n\n$$\nq\\,=\\,K_{1}x\n$$  \n\nFor the power cylinder,  \n\n$$\nA\\rho\\,d y\\,=\\,q\\,d t\n$$  \n\nwhere $A$ is the power-piston area and $\\rho$ is the density of oil. Hence  \n\n$$\n\\frac{d y}{d t}=\\frac{q}{A\\rho}=\\frac{K_{1}}{A\\rho}\\,x\\,=\\,K x\n$$  \n\nwhere $K=K_{1}/(A\\rho)\\,=$ constant.The transfer function $Y(s)/X(s)$ is thus  \n\n$$\n{\\frac{Y(s)}{X(s)}}={\\frac{K}{s}}\n$$  \n\nThe controller produces the integral control action.  \n\nFigure 4–35 Speed control system.  \n\n![](images/94bceefda3626d7daa03817877b47f1e1af447f238a0d1dd4b86d795d30d2e58.jpg)  \nA–4–8. Explain the operation of the speed control system shown in Figure 4–35.  \n\nSolution. If the engine speed increases, the sleeve of the fly-ball governor moves upward. This movement acts as the input to the hydraulic controller.A positive error signal (upward motion of the sleeve) causes the power piston to move downward, reduces the fuel-valve opening, and decreases the engine speed.A block diagram for the system is shown in Figure 4–36. From the block diagram the transfer function $Y(s)/E(s)$ can be obtained as  \n\n$$\n{\\frac{Y(s)}{E(s)}}={\\frac{a_{2}}{a_{1}\\,+\\,a_{2}}}{\\frac{\\displaystyle{\\frac{K}{s}}}{1\\,+\\,{\\frac{a_{1}}{a_{1}\\,+\\,a_{2}}}{\\frac{b s}{b s\\,+\\,k}}{\\frac{K}{s}}}}\n$$  \n\nIf the following condition applies,  \n\n$$\n\\left|\\frac{a_{1}}{a_{1}\\,+\\,a_{2}}\\frac{b s}{b s\\,+\\,k}\\frac{K}{s}\\right|\\,\\gg\\,1\n$$  \n\nthe transfer function $Y(s)/E(s)$ becomes  \n\n$$\n\\frac{Y(s)}{E(s)}\\div\\frac{a_{2}}{a_{1}\\,+\\,a_{2}}\\frac{a_{1}\\,+\\,a_{2}}{a_{1}}\\frac{b s\\,+\\,k}{b s}=\\frac{a_{2}}{a_{1}}\\left(1\\,+\\,\\frac{k}{b s}\\right)\n$$  \n\n![](images/cbd219f672b10c90c0c0401f678c1d8b383d973688b32b1145faa44bac96812d.jpg)  \nFigure 4–36 Block diagram for the speed control system shown in Figure 4–35.  \n\nThe speed controller has proportional-plus-integral control action.  \n\nA–4–9. Derive the transfer function $Z(s)/Y(s)$ of the hydraulic system shown in Figure 4–37.Assume that the two dashpots in the system are identical ones except the piston shafts.  \n\nSolution. In deriving the equations for the system, we assume that force $F$ is applied at the right end of the shaft causing displacement $y$ . (All displacements $y,\\,w$ ,and $z$ are measured from respective equilibrium positions when no force is applied at the right end of the shaft.) When force $F$ is applied, pressure $P_{1}$ becomes higher than pressure $P_{1}^{\\prime}$ ,or $P_{1}>P_{1}^{\\prime}$ .Similarly, $P_{2}>P_{2}^{\\prime}$ .  \n\nFor the force balance, we have the following equation:  \n\n$$\nk_{2}(y\\,-\\,w)\\,=\\,A\\big(P_{1}\\,-\\,P_{1}^{\\prime}\\big)\\,+\\,A\\big(P_{2}\\,-\\,P_{2}^{\\prime}\\big)\n$$  \n\nSince  \n\n$$\nk_{1}z\\,=\\,A\\big(P_{1}\\,-\\,P_{1}^{\\prime}\\big)\n$$  \n\nand  \n\n$$\nq_{1}={\\frac{P_{1}\\,-\\,P_{1}^{\\prime}}{R}}\n$$  \n\nwe have  \n\n$$\nk_{1}z\\,=\\,A R q_{1}\n$$  \n\nAlso, since  \n\n$$\nq_{1}\\,d t=A(d w\\,-\\,d z)\\rho\n$$  \n\nwe have  \n\nor  \n\n$$\n\\dot{w}\\,-\\,\\dot{z}\\,=\\frac{k_{1}z}{A^{2}R\\rho}\n$$  \n\nDefine $A^{2}R\\rho\\,=\\,B$ .($B$ is the viscous-friction coefficient.) Then  \n\n$$\n\\dot{w}\\,-\\,\\dot{z}\\,=\\frac{k_{1}}{B}\\,z\n$$  \n\nAlso, for the right-hand-side dashpot we have  \n\n$$\nq_{2}\\,d t=A\\rho\\:d w\n$$  \n\nSince $q_{2}\\,=\\,(P_{2}\\,-\\,P_{2}^{\\prime})/R$ , we obtain  \n\n$$\n\\dot{w}\\,=\\frac{q_{2}}{A\\rho}=\\frac{A\\bigl(P_{2}\\,-\\,P_{2}^{\\prime}\\bigr)}{A^{2}R\\rho}\n$$  \n\nor  \n\n$$\nA\\big(P_{2}\\,-\\,P_{2}^{\\prime}\\big)\\,=\\,B\\dot{w}\n$$  \n\nSubstituting Equations (4–43) and (4–45) into Equation (4–42), we have  \n\n$$\nk_{2}y\\,-\\,k_{2}w\\,=\\,k_{1}z\\,+\\,B\\dot{w}\n$$  \n\nTaking the Laplace transform of this last equation, assuming zero initial condition, we obtain  \n\n$$\nk_{2}Y(s)\\,=\\,\\bigl(k_{2}\\,+\\,B s\\bigr)W(s)\\,+\\,k_{1}Z(s)\n$$  \n\n$$\ng\\lim\\limits_{x\\rightarrow\\frac{1}{2}}\\sqrt{\\frac{\\sum\\limits_{i=1}^{R}}{\\sum\\limits_{i=1}^{P_{1}}\\frac{\\sum\\limits_{j=1}^{R}}{P_{1}^{\\prime}}-\\frac{\\sum\\limits_{i=1}^{R}}{w}\\frac{\\prod\\limits_{i=1}^{R}}{\\sum\\limits_{j=1}^{P_{2}}\\sum\\limits_{i=1}^{P_{2}}\\cdots\\sum\\limits_{n}^{P_{2}}}}}+\\cdots\n$$  \n\nFigure 4–37 Hydraulic system.  \n\nTaking the Laplace transform of Equation (4–44), assuming zero initial condition, we obtain  \n\n$$\nW(s)={\\frac{k_{1}+B s}{B s}}\\,Z(s)\n$$  \n\nBy using Equation (4–47) to eliminate $W(s)$ from Equation (4–46), we obtain  \n\n$$\nk_{2}Y(s)\\,=\\,\\bigl(k_{2}\\,+\\,B s\\bigr)\\frac{k_{1}\\,+\\,B s}{B s}\\,Z(s)\\,+\\,k_{1}Z(s)\n$$  \n\nfrom which we obtain the transfer function $Z(s)/Y(s)$ to be  \n\n$$\n\\frac{Z(s)}{Y(s)}=\\frac{k_{2}s}{B s^{2}+(2k_{1}+k_{2})s+\\frac{k_{1}k_{2}}{B}}\n$$  \n\nMultiplying $B/(k_{1}k_{2})$ to both the numerator and denominator of this last equation, we get  \n\n$$\n\\frac{Z(s)}{Y(s)}=\\frac{\\displaystyle\\frac{B}{k_{1}}s}{\\displaystyle\\frac{B^{2}}{k_{1}k_{2}}\\,s^{2}\\,+\\,\\left(\\frac{2B}{k_{2}}+\\frac{B}{k_{1}}\\right)s\\,+\\,1}\n$$  \n\nDefine $B/k_{1}=T_{1},B/k_{2}=T_{2}.$ .Then the transfer function $Z(s)/Y(s)$ becomes as follows:  \n\n$$\n\\frac{Z(s)}{Y(s)}=\\frac{T_{1}s}{T_{1}T_{2}s^{2}\\:+\\:\\left(T_{1}\\:+\\:2T_{2}\\right)\\!s\\:+\\:1}\n$$  \n\nA–4–10. Considering small deviations from steady-state operation, draw a block diagram of the air heating system shown in Figure 4–38. Assume that the heat loss to the surroundings and the heat capacitance of the metal parts of the heater are negligible.  \n\nSolution. Let us define  \n\n$\\bar{\\theta}_{i}=$ steady-state temperature of inlet air, $^\\circ C$   \n$\\bar{\\theta}_{o}=$ steady-state temperature of outlet air, $^\\circ C$   \n$G=$ mass flow rate of air through the heating chamber, kg 'sec   \n$M=$ mass of air contained in the heating chamber, kg $c=$ specific heat of air, $\\mathrm{kcal/kg\\,^{\\circ}C}$   \n$R=$ thermal resistance, $^\\circ C$ sec 'kcal   \n$C=$ thermal capacitance of air contained in the heating chamber =Mc , kcal '$^\\circ C$   \n$\\bar{H}\\,=$ steady-state heat input, kcal 'sec  \n\nLet us assume that the heat input is suddenly changed from $\\bar{H}$ to $\\bar{H}\\,+\\,h$ and the inlet air temperature is suddenly changed from $\\bar{\\theta}_{i}$ to $\\overline{{\\theta}}_{i}^{\\,\\mathrm{~\\,~}}+\\,\\theta_{i}$ .Then the outlet air temperature will be changed from $\\overline{{\\theta}}_{o}$ to $\\overline{{\\theta}}_{o}+\\,\\theta_{o}$ .  \n\nThe equation describing the system behavior is  \n\n$$\nC\\,d\\theta_{o}=\\left[h\\,+\\,G c(\\theta_{i}\\,-\\,\\theta_{o})\\right]d t\n$$  \n\n![](images/6e82c3b0b3e593776db569fee1383168f8fcb5644f843f985d5f5b1d70229289.jpg)  \nFigure 4–38 Air heating system.   \nChapter 4 /Mathematical Modeling of Fluid Systems and Thermal Systems  \n\n![](images/2824110f9aa090871225ce7c261703e68f6a1e8332041395a1f50136f8d9f35d.jpg)  \nFigure 4–39 Block diagram of the air heating system shown in Figure 4–38.  \n\nor  \n\n$$\nC\\,\\frac{d\\theta_{o}}{d t}=h\\,+\\,G c\\big(\\theta_{i}\\,-\\,\\theta_{o}\\big)\n$$  \n\nNoting that  \n\n$$\nG c=\\frac{1}{R}\n$$  \n\nwe obtain  \n\n$$\nC\\,\\frac{d\\theta_{o}}{d t}=h\\,+\\,\\frac{1}{R}\\left(\\theta_{i}\\,-\\,\\theta_{o}\\right)\n$$  \n\nor  \n\n$$\nR C\\,\\frac{d\\theta_{o}}{d t}+\\,\\theta_{o}\\,=\\,R h\\,+\\,\\theta_{i}\n$$  \n\nTaking the Laplace transforms of both sides of this last equation and substituting the initial condition that $\\theta_{0}(0)\\,=\\,0$ , we obtain  \n\n$$\n\\theta_{o}(s)={\\frac{R}{R C s\\,+\\,1}}\\,H(s)\\,+{\\frac{1}{R C s\\,+\\,1}}\\,\\theta_{i}(s)\n$$  \n\nThe block diagram of the system corresponding to this equation is shown in Figure 4–39.  \n\nA–4–11. Consider the thin, glass-wall, mercury thermometer system shown in Figure 4–40.Assume that the thermometer is at a uniform temperature $\\bar{\\Theta}$ (ambient temperature) and that at $t\\,=\\,0$ it is immersed in a bath of temperature $\\overline{{\\Theta}}^{}+\\,\\theta_{b}$ ,where $\\theta_{b}$ is the bath temperature (which may be constant or changing) measured from the ambient temperature $\\bar{\\Theta}$ .Define the instantaneous thermometer temperature by $\\overline{{\\theta}}+\\,\\theta$ ,so that $\\theta$ is the change in the thermometer temperature satisfying the condition that $\\theta(0)=0.$ Obtain a mathematical model for the system.Also obtain an electrical analog of the thermometer system.  \n\n![](images/ad448596e7ab844d2b8ae8e79929b2c5e08c1290573d192b7d93eccf71aab1e8.jpg)  \nFigure 4–40 Thin, glass-wall, mercury thermometer system.  \n\nSolution. A mathematical model for the system can be derived by considering heat balance as follows:The heat entering the thermometer during $d t$ sec is $q\\,d t$ , where $q$ is the heat flow rate to the thermometer.This heat is stored in the thermal capacitance $C$ of the thermometer, thereby raising its temperature by $d\\theta.$ .Thus the heat-balance equation is  \n\n$$\nC\\,d\\theta\\,=\\,q\\,d t\n$$  \n\nFigure 4–41   \nElectrical analog of the thermometer   \nsystem shown in   \nFigure 4–40.  \n\n![](images/f914f9cc1fb07dc929dbe06e0e73ebfb48455f5c92d6dec429d5068fd218f427.jpg)  \n\nSince thermal resistance $R$ may be written as  \n\n$$\nR=\\frac{d(\\Delta\\theta)}{d q}=\\frac{\\Delta\\theta}{q}\n$$  \n\nheat flow rate $q$ may be given, in terms of thermal resistance $R$ , as  \n\n$$\nq=\\frac{(\\bar{\\Theta}+\\,\\theta_{b})\\,-\\,(\\bar{\\Theta}+\\,\\theta)}{R}=\\frac{\\theta_{b}\\,-\\,\\theta}{R}\n$$  \n\nwhere $\\overline{{\\theta}}+\\,\\theta_{b}$ is the bath temperature and $\\overline{{\\theta}}+\\,\\theta$ is the thermometer temperature. Hence, we can rewrite Equation (4–48) as  \n\n$$\nC\\,{\\frac{d\\theta}{d t}}={\\frac{\\theta_{b}\\,-\\,\\theta}{R}}\n$$  \n\nor  \n\n$$\nR C\\,\\frac{d\\theta}{d t}+\\,\\theta\\,=\\,\\theta_{b}\n$$  \n\nEquation (4–49) is a mathematical model of the thermometer system.  \n\nReferring to Equation (4–49), an electrical analog for the thermometer system can be written as  \n\n$$\nR C\\,{\\frac{d e_{o}}{d t}}+\\,e_{o}=\\,e_{i}\n$$  \n\nAn electrical circuit represented by this last equation is shown in Figure 4–41.  \n\n# PROBLEMS  \n\nB–4–1. Consider the conical water-tank system shown in 1 Figure 4–42. The flow through the valve is turbulent and is related to the head $H$ by  \n\n$$\nQ\\,=\\,0.005{\\sqrt{H}}\n$$  \n\nwhere $Q$ is the flow rate measured in $\\mathrm{m}^{3}/\\mathrm{sec}$ and $H$ is in meters.  \n\nSuppose that the head is $2\\;\\mathrm{m}$ at $t\\,=\\,0$ . What will be the head at $t\\,=\\,60$ sec?  \n\n![](images/b5895f57a530952c233bb7ccd278af0a9740cbdb909e19d540b3450ab7325350.jpg)  \nFigure 4–42 Conical water-tank system.  \n\nB–4–2. Consider the liquid-level control system shown in Figure 4–43.The controller is of the proportional type.The set point of the controller is fixed.  \n\nDraw a block diagram of the system, assuming that changes in the variables are small. Obtain the transfer function between the level of the second tank and the disturbance input $q_{d}$ . Obtain the steady-state error when the disturbance $q_{d}$ is a unit-step function.  \n\nB–4–3. For the pneumatic system shown in Figure 4–44, assume that steady-state values of the air pressure and the displacement of the bellows are $\\bar{P}$ and $\\bar{X}$ ,respectively. Assume also that the input pressure is changed from $\\bar{P}$ to $\\bar{P}+\\ p_{i}$ ,where $p_{i}$ is a small change in the input pressure.This change will cause the displacement of the bellows to change a small amount $x$ .Assuming that the capacitance of the bellows is $C$ and the resistance of the valve is $R$ , obtain the transfer function relating $x$ and $p_{i}$ .  \n\n![](images/f2d0970c2302c8580c814e7ce79fb7c59b9d71cd89b51db1cb9ca57432a9eabd.jpg)  \n\nFigure 4–43 Liquid-level control system.  \n\n![](images/5fa3e19ba992fedd2fbe4194f9309e6d904a9da772c0a81becc83f801007a957.jpg)  \n\nFigure 4–44 Pneumatic system.  \n\nB–4–4. Figure 4–45 shows a pneumatic controller.The pneumatic relay has the characteristic that $p_{c}=K p_{b}$ ,where $K>0$ .What kind of control action does this controller produce? Derive the transfer function $P_{c}(s)/E(s)$ .  \n\nB–4–5. Consider the pneumatic controller shown in Figure 4–46.Assuming that the pneumatic relay has the characteristics that $p_{c}=K p_{b}$ (where $K>0$ ),determine the control action of this controller.The input to the controller is $e$ and the output is $p_{c}$ .  \n\n![](images/a1071f9840b8b667d9b51ae8f3392827daf878db8a7df6c9036520469436b614.jpg)  \n\nFigure 4–45 Pneumatic controller.  \n\n![](images/05621c0c6f7c3b19a7fddf330faac18b8d143d9dea32ed3a9e6ff847e34689c8.jpg)  \n\nFigure 4–46 Pneumatic controller.  \n\nB–4–6. Figure 4–47 shows a pneumatic controller.The signal $e$ is the input and the change in the control pressure $p_{c}$ is the output. Obtain the transfer function $P_{c}(s)/E(s)$ .Assume that the pneumatic relay has the characteristics that $p_{c}=K p_{b}$ ,where $K>0$ .  \n\nB–4–7. Consider the pneumatic controller shown in Figure 4–48. What control action does this controller produce? Assume that the pneumatic relay has the characteristics that $p_{c}=K p_{b}$ ,where $K>0$ .  \n\n![](images/d859a932b48ceb46067dc2dc1c163cd324cf34f89a8719999ee60356b715a505.jpg)  \n\n![](images/47ced7d2f3a05a5ed705fa2e9e2cd4d1ac17166f07ac877e7a0f96f9dce155b5.jpg)  \n\nB–4–8. Figure 4–49 shows a flapper valve. It is placed between two opposing nozzles.If the flapper is moved slightly to the right, the pressure unbalance occurs in the nozzles and the power piston moves to the left, and vice versa. Such a device is frequently used in hydraulic servos as the firststage valve in two-stage servovalves. This usage occurs because considerable force may be needed to stroke larger spool valves that result from the steady-state flow force.To reduce or compensate this force, two-stage valve configuration is often employed; a flapper valve or jet pipe is used as the first-stage valve to provide a necessary force to stroke the second-stage spool valve.  \n\n![](images/3df677e442d0c0f43afb3fc901febf914b4a90b2ea24facd03ac334269e5163b.jpg)  \nFigure 4–49 Flapper valve.  \n\nFigure 4–50 shows a schematic diagram of a hydraulic servomotor in which the error signal is amplified in two stages using a jet pipe and a pilot valve. Draw a block diagram of the system of Figure 4–50 and then find the transfer function between $y$ and $x$ , where $x$ is the air pressure and $y$ is the displacement of the power piston.  \n\n![](images/2ae1a0b85dfa2f032a3537e3f9558b9ee219ce18a6191aece3996812eca0f75c.jpg)  \n\nB–4–9. Figure 4–51 is a schematic diagram of an aircraft elevator control system. The input to the system is the deflection angle $\\theta$ of the control lever, and the output is the elevator angle $\\phi$ . Assume that angles $\\theta$ and $\\phi$ are relatively small. Show that for each angle $\\theta$ of the control lever there is a corresponding (steady-state) elevator angle $\\phi$ .  \n\n![](images/a0e6a50d7fbb1cb7e465322e8a46eaf449b057363c042c93884706215ccf56b7.jpg)  \nFigure 4–51 Aircraft elevator control system.  \n\nB–4–10. Consider the liquid-level control system shown in Figure 4–52. The inlet valve is controlled by a hydraulic integral controller.Assume that the steady-state inflow rate is $\\bar{Q}$ –and steady-state outflow rate is also $\\bar{Q}$ ,the steady-state head is $\\bar{H}$ ,steady-state pilot valve displacement is $\\dot{\\bar{X}}\\,=\\,0$ ,and steady-state valve position is $\\bar{Y}$ .We assume that the set point $\\bar{R}$ corresponds to the steady-state head $\\bar{H}$ .The set point is fixed.Assume also that the disturbance inflow rate $q_{d}$ $t\\,=\\,0,$ , which is a small quantity, is applied to the water tank at .This disturbance causes the head to change from $\\bar{H}$ –to $\\bar{H}\\,+\\,h$ .This change results in a change in the outflow rate by causes a change in the inflow rate from $q_{o}$ .Through the hydraulic controller, the change in head $\\bar{Q}$ –to $\\bar{Q}+q_{i}$ .(The integral controller tends to keep the head constant as much as possible in the presence of disturbances.) We assume that all changes are of small quantities.  \n\nWe assume that the velocity of the power piston (valve) is proportional to pilot-valve displacement $x$ , or  \n\n$$\n{\\frac{d y}{d t}}=K_{1}x\n$$  \n\nwhere $K_{1}$ is a positive constant. We also assume that the change in the inflow rate $q_{i}$ is negatively proportional to the change in the valve opening $y$ , or  \n\n$$\nq_{i}=-K_{v}y\n$$  \n\nwhere $K_{v}$ is a positive constant.  \n\nAssuming the following numerical values for the system,  \n\n$$\n\\begin{array}{l l l}{{C=2\\,\\mathrm{m}^{2},\\quad}}&{{R=0.5\\,\\mathrm{sec}/\\mathrm{m}^{2},\\quad}}&{{K_{v}=1\\,\\mathrm{m}^{2}/\\mathrm{sec}}}\\\\ {{a=0.25\\,\\mathrm{m},\\quad}}&{{b=0.75\\,\\mathrm{m},\\quad}}&{{K_{1}=4\\,\\mathrm{sec}^{-1}}}\\end{array}\n$$  \n\nobtain the transfer function $H(s)/Q_{d}(s)$ .  \n\n![](images/8194d83abd426d5fa848d1e4ccd7bd94521be9c32bf4b784ab0f27170b225f14.jpg)  \n(Resistance)  \n\nFigure 4–52 Liquid-level control system.  \n\nB–4–11. Consider the controller shown in Figure 4–53.The input is the air pressure reference pressure $\\bar{P}$ –and the output is the displacement $p_{i}$ measured from some steady-state $y$ of the power piston. Obtain the transfer function $Y(s)/P_{i}(s)$ .  \n\n![](images/24aefe3421954d5fe51110a4c9ecb12f77b9a858979c79b9bc4af979e815f966.jpg)  \n\ny(Output)  \n\nFigure 4–53 Controller.  \n\nB–4–12. A thermocouple has a time constant of 2 sec. A thermal well has a time constant of 30 sec.When the thermocouple is inserted into the well, this temperaturemeasuring device can be considered a two-capacitance system.  \n\nDetermine the time constants of the combined thermocouple–thermal-well system.Assume that the weight of the thermocouple is $8~\\mathrm{g}$ and the weight of the thermal well is $40\\,\\mathrm{g}$ .Assume also that the specific heats of the thermocouple and thermal well are the same.  \n\n# Transient and Steady-State Response Analyses  \n\nIn early chapters it was stated that the first step in analyzing a control system was to derive a mathematical model of the system. Once such a model is obtained, various methods are available for the analysis of system performance.  \n\nIn practice, the input signal to a control system is not known ahead of time but is random in nature, and the instantaneous input cannot be expressed analytically. Only in some special cases is the input signal known in advance and expressible analytically or by curves, such as in the case of the automatic control of cutting tools.  \n\nIn analyzing and designing control systems, we must have a basis of comparison of performance of various control systems.This basis may be set up by specifying particular test input signals and by comparing the responses of various systems to these input signals.  \n\nMany design criteria are based on the response to such test signals or on the response of systems to changes in initial conditions (without any test signals).The use of test signals can be justified because of a correlation existing between the response characteristics of a system to a typical test input signal and the capability of the system to cope with actual input signals.  \n\nTypical Test Signals. The commonly used test input signals are step functions, ramp functions, acceleration functions, impulse functions, sinusoidal functions, and white noise. In this chapter we use test signals such as step, ramp, acceleration and impulse signals.With these test signals, mathematical and experimental analyses of control systems can be carried out easily, since the signals are very simple functions of time.  \n\nWhich of these typical input signals to use for analyzing system characteristics may be determined by the form of the input that the system will be subjected to most frequently under normal operation. If the inputs to a control system are gradually changing functions of time, then a ramp function of time may be a good test signal. Similarly, if a system is subjected to sudden disturbances, a step function of time may be a good test signal; and for a system subjected to shock inputs, an impulse function may be best. Once a control system is designed on the basis of test signals, the performance of the system in response to actual inputs is generally satisfactory. The use of such test signals enables one to compare the performance of many systems on the same basis.  \n\nTransient Response and Steady-State Response. The time response of a control system consists of two parts:the transient response and the steady-state response. By transient response, we mean that which goes from the initial state to the final state. By steady-state response, we mean the manner in which the system output behaves as $t$ approaches infinity.Thus the system response $c(t)$ may be written as  \n\n$$\nc(t)\\,=\\,c_{\\mathrm{{tr}}}(t)\\,+\\,c_{\\mathrm{{ss}}}(t)\n$$  \n\nwhere the first term on the right-hand side of the equation is the transient response and the second term is the steady-state response.  \n\nAbsolute Stability, Relative Stability, and Steady-State Error. In designing a control system, we must be able to predict the dynamic behavior of the system from a knowledge of the components. The most important characteristic of the dynamic behavior of a control system is absolute stability—that is,whether the system is stable or unstable.A control system is in equilibrium if,in the absence of any disturbance or input, the output stays in the same state.A linear time-invariant control system is stable if the output eventually comes back to its equilibrium state when the system is subjected to an initial condition. A linear time-invariant control system is critically stable if oscillations of the output continue forever. It is unstable if the output diverges without bound from its equilibrium state when the system is subjected to an initial condition.Actually, the output of a physical system may increase to a certain extent but may be limited by mechanical “stops,” or the system may break down or become nonlinear after the output exceeds a certain magnitude so that the linear differential equations no longer apply.  \n\nImportant system behavior (other than absolute stability) to which we must give careful consideration includes relative stability and steady-state error. Since a physical control system involves energy storage, the output of the system, when subjected to an input, cannot follow the input immediately but exhibits a transient response before a steady state can be reached. The transient response of a practical control system often exhibits damped oscillations before reaching a steady state. If the output of a system at steady state does not exactly agree with the input, the system is said to have steadystate error.This error is indicative of the accuracy of the system. In analyzing a control system, we must examine transient-response behavior and steady-state behavior.  \n\nOutline of the Chapter. This chapter is concerned with system responses to aperiodic signals (such as step, ramp, acceleration, and impulse functions of time). The outline of the chapter is as follows: Section 5–1 has presented introductory material for the chapter. Section 5–2 treats the response of first-order systems to aperiodic inputs. Section 5–3 deals with the transient response of the second-order systems. Detailed analyses of the step response, ramp response, and impulse response of the second-order systems are presented. Section 5–4 discusses the transient-response analysis of higherorder systems.Section 5–5 gives an introduction to the MATLAB approach to the solution of transient-response problems. Section 5–6 gives an example of a transient-response problem solved with MATLAB. Section 5–7 presents Routh’s stability criterion. Section 5–8 discusses effects of integral and derivative control actions on system performance. Finally, Section 5–9 treats steady-state errors in unity-feedback control systems.  \n\n# 5–2 FIRST-ORDER SYSTEMS  \n\nConsider the first-order system shown in Figure 5–1(a). Physically, this system may represent an $R C$ circuit, thermal system, or the like.A simplified block diagram is shown in Figure 5–1(b).The input-output relationship is given by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{1}{T s\\,+\\,1}}\n$$  \n\nIn the following, we shall analyze the system responses to such inputs as the unit-step, unit-ramp, and unit-impulse functions.The initial conditions are assumed to be zero.  \n\nNote that all systems having the same transfer function will exhibit the same output in response to the same input. For any given physical system, the mathematical response can be given a physical interpretation.  \n\nUnit-Step Response of First-Order Systems. Since the Laplace transform of the unit-step function is $1/s$ , substituting $R(s)\\,=\\,1/s$ into Equation (5–1), we obtain  \n\n$$\nC(s)=\\frac{1}{T s\\,+\\,1\\,\\d s}\\frac{1}{s}\n$$  \n\nExpanding $C(s)$ into partial fractions gives  \n\n$$\nC(s)\\,={\\frac{1}{s}}\\,-\\,{\\frac{T}{T s\\,+\\,1}}={\\frac{1}{s}}\\,-\\,{\\frac{1}{s\\,+\\,(1/T)}}\n$$  \n\nTaking the inverse Laplace transform of Equation (5–2), we obtain  \n\n$$\nc(t)\\,=\\,1\\,-\\,e^{-t/T},\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\n![](images/46820475f17acad9dc471b622ad6294b65289957e539ea2cfedfee20475336f0.jpg)  \nFigure 5–1 (a) Block diagram of a first-order system; (b) simplified block diagram.  \n\nEquation (5–3) states that initially the output $c(t)$ is zero and finally it becomes unity. One important characteristic of such an exponential response curve $c(t)$ is that at $t\\,=\\,T$ the value of $c(t)$ is 0.632, or the response $c(t)$ has reached $63.2\\%$ of its total change.This may be easily seen by substituting $t\\,=\\,T$ in $c(t)$ .That is,  \n\n$$\nc(T)\\,=\\,1\\,-\\,e^{-1}\\,=\\,0.632\n$$  \n\n![](images/5fe7f092e268cfb149201f018522d293be929198324ba44c94b17465171ecef8.jpg)  \nFigure 5–2 Exponential response curve.  \n\nNote that the smaller the time constant $T$ , the faster the system response. Another important characteristic of the exponential response curve is that the slope of the tangent line at $t\\,=\\,0$ is $1/T$ , since  \n\n$$\n\\left.\\frac{d c}{d t}\\right|_{t=0}=\\frac{1}{T}\\,e^{-t/T}\\,\\bigg|_{t=0}=\\frac{1}{T}\n$$  \n\nThe output would reach the final value at $t\\,=\\,T$ if it maintained its initial speed of response.From Equation (5–4) we see that the slope of the response curve $c(t)$ decreases monotonically from $1/T$ at $t\\,=\\,0$ to zero at $t\\,=\\infty$ .  \n\nThe exponential response curve $c(t)$ given by Equation (5–3) is shown in Figure 5–2. In one time constant,the exponential response curve has gone from 0 to $63.2\\%$ of the final value.In two time constants,the response reaches $86.5\\%$ of the final value.At $t\\,=\\,3T,4T_{\\mathrm{{c}}}$ ,and $5T$ , the response reaches $95\\%$ ,$98.2\\%$ , and $99.3\\%$ , respectively, of the final value.Thus, for $t\\geq4T$ , the response remains within $2\\%$ of the final value. As seen from Equation (5–3), the steady state is reached mathematically only after an infinite time. In practice, however, a reasonable estimate of the response time is the length of time the response curve needs to reach and stay within the $2\\%$ line of the final value, or four time constants.  \n\nUnit-Ramp Response of First-Order Systems. Since the Laplace transform of the unit-ramp function is $1/s^{2}$ , we obtain the output of the system of Figure 5–1(a) as  \n\n$$\nC(s)={\\frac{1}{T s\\,+\\,1}}{\\frac{1}{s^{2}}}\n$$  \n\nExpanding $C(s)$ into partial fractions gives  \n\n$$\nC(s)=\\frac{1}{s^{2}}-\\frac{T}{s}+\\frac{T^{2}}{T s\\,+\\,1}\n$$  \n\nTaking the inverse Laplace transform of Equation (5–5), we obtain  \n\n$$\nc(t)\\,=\\,t\\,-\\,T\\,+\\,T e^{-t/T},\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\nThe error signal $e(t)$ is then  \n\n$$\n\\begin{array}{l}{{e(t)=r(t)-c(t)}}\\\\ {{\\qquad=T(1-e^{-t/T})}}\\end{array}\n$$  \n\n![](images/810e44bbf5cada202d5f577cf064d5207a2e1a9072d06cfed3000450643d7e5c.jpg)  \nFigure 5–3 Unit-ramp response of the system shown in Figure 5–1(a).  \n\nAs $t$ approaches infinity, $e^{-t/T}$ approaches zero, and thus the error signal $e(t)$ approaches $T$ or  \n\n$$\ne(\\infty)=T\n$$  \n\nThe unit-ramp input and the system output are shown in Figure 5–3. The error in following the unit-ramp input is equal to $T$ for sufficiently large $t,$ .The smaller the time constant $T$ , the smaller the steady-state error in following the ramp input.  \n\nUnit-Impulse Response of First-Order Systems. For the unit-impulse input, $R(s)\\,=\\,1$ and the output of the system of Figure 5–1(a) can be obtained as  \n\n$$\nC(s)={\\frac{1}{T s\\,+\\,1}}\n$$  \n\nThe inverse Laplace transform of Equation (5–7) gives  \n\n$$\nc(t)={\\frac{1}{T}}\\,e^{-t/T},\\qquad\\mathrm{for}\\,t\\geq0\n$$  \n\nThe response curve given by Equation (5–8) is shown in Figure 5–4.  \n\n![](images/088893f48e7466207cbd1883e03f3a049a1ab21db3288f7d14174440463b7331.jpg)  \nFigure 5–4 Unit-impulse response of the system shown in Figure 5–1(a).  \n\nAn Important Property of Linear Time-Invariant Systems. In the analysis above, it has been shown that for the unit-ramp input the output $c(t)$ is  \n\n$$\nc(t)\\,=\\,t\\,-\\,T\\,+\\,T e^{-t/T},\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\nFor the unit-step input, which is the derivative of unit-ramp input, the output $c(t)$ is  \n\n$$\nc(t)\\,=\\,1\\,-\\,e^{-t/T},\\qquad\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\nFinally, for the unit-impulse input, which is the derivative of unit-step input, the output $c(t)$ is  \n\n$$\nc(t)={\\frac{1}{T}}\\,e^{-t/T},\\qquad\\qquad\\mathrm{~for~}t\\geq0\\qquad[\\mathrm{See~Equation~}(5\\mathrm{-}8).]\n$$  \n\nComparing the system responses to these three inputs clearly indicates that the response to the derivative of an input signal can be obtained by differentiating the response of the system to the original signal. It can also be seen that the response to the integral of the original signal can be obtained by integrating the response of the system to the original signal and by determining the integration constant from the zero-output initial condition.This is a property of linear time-invariant systems. Linear time-varying systems and nonlinear systems do not possess this property.  \n\n# 5–3 SECOND-ORDER SYSTEMS  \n\nIn this section, we shall obtain the response of a typical second-order control system to a step input, ramp input, and impulse input. Here we consider a servo system as an example of a second-order system.  \n\nServo System. The servo system shown in Figure 5–5(a) consists of a proportional controller and load elements (inertia and viscous-friction elements). Suppose that we wish to control the output position $c$ in accordance with the input position $r$ .  \n\nThe equation for the load elements is  \n\n$$\nJ{\\ddot{c}}\\,+\\,B{\\dot{c}}\\,=\\,T\n$$  \n\nwhere $T$ is the torque produced by the proportional controller whose gain is $K$ . By taking Laplace transforms of both sides of this last equation, assuming the zero initial conditions, we obtain  \n\n$$\nJ s^{2}C(s)\\,+\\,B s C(s)\\,=\\,T(s)\n$$  \n\nSo the transfer function between $C(s)$ and $T(s)$ is  \n\n$$\n{\\frac{C(s)}{T(s)}}={\\frac{1}{s(J s\\,+\\,B)}}\n$$  \n\nBy using this transfer function, Figure 5–5(a) can be redrawn as in Figure 5–5(b), which can be modified to that shown in Figure 5–5(c).The closed-loop transfer function is then obtained as  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{J s^{2}\\,+\\,B s\\,+\\,K}=\\frac{K/J}{s^{2}\\,+\\,(B/J)s\\,+\\,(K/J)}\n$$  \n\nSuch a system where the closed-loop transfer function possesses two poles is called a second-order system. (Some second-order systems may involve one or two zeros.)  \n\n![](images/bd9a5b3cd4bda04399b96367ebc36b711d4454d763bba70904a4ae0175ae0c15.jpg)  \nFigure 5–5 (a) Servo system; (b) block diagram; (c) simplified block diagram.  \n\nStep Response of Second-Order System. The closed-loop transfer function of the system shown in Figure 5–5(c) is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{J s^{2}\\,+\\,B s\\,+\\,K}\n$$  \n\nwhich can be rewritten as  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{\\displaystyle\\frac{K}{J}}{\\left[s+\\frac{B}{2J}+\\sqrt{\\left(\\frac{B}{2J}\\right)^{2}-\\frac{K}{J}}\\right]\\left[s+\\frac{B}{2J}-\\sqrt{\\left(\\frac{B}{2J}\\right)^{2}-\\frac{K}{J}}\\right]}\n$$  \n\nThe closed-loop poles are complex conjugates if $B^{2}\\mathrm{~-~}4J K\\mathrm{~<~}0$ and they are real if $B^{2}\\mathrm{~-~}4J K\\mathrm{~\\geq~}0.$ . In the transient-response analysis, it is convenient to write  \n\n$$\n\\frac{K}{J}=\\omega_{n}^{2},\\qquad\\frac{B}{J}=2\\zeta\\omega_{n}=2\\sigma\n$$  \n\nwhere $\\sigma$ is called the attenuation ;$\\omega_{n}$ , the undamped natural frequency 1 ; and $\\zeta$ , the damping ratio of the system. The damping ratio $\\zeta$ is the ratio of the actual damping $B$ to the critical damping $B_{c}=2\\sqrt{J K}$ or  \n\n$$\n\\zeta=\\frac{B}{B_{c}}=\\frac{B}{2\\sqrt{J K}}\n$$  \n\nFigure 5–6Second-order system.  \n\n![](images/2889a25edc968668905a70a5fbcc15034285a4bdc7224e3fd508f58cec6ba1f9.jpg)  \n\nIn terms of $\\zeta$ and $\\omega_{n}$ , the system shown in Figure 5–5(c) can be modified to that shown in Figure 5–6, and the closed-loop transfer function $C(s)/R(s)$ given by Equation (5–9) can be written  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nThis form is called the standard form of the second-order system.  \n\nThe dynamic behavior of the second-order system can then be described in terms of two parameters $\\zeta$ and $\\omega_{n}$ . If $0<\\zeta<1$ , the closed-loop poles are complex conjugates and lie in the left-half $s$ plane. The system is then called underdamped, and the transient response is oscillatory. If $\\zeta=0$ , the transient response does not die out. If $\\zeta=1$ ,the system is called critically damped. Overdamped systems correspond to $\\zeta>1$ .  \n\nWe shall now solve for the response of the system shown in Figure 5–6 to a unit-step input.We shall consider three different cases: the underdamped ($0<\\zeta<1]$ ), critically damped ($\\chi=1$ ), and overdamped ($[\\zeta>1$ )cases.  \n\n(1) Underdamped case ($0<\\zeta<1$ ):In this case, $C(s)/R(s)$ can be written  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{\\omega_{n}^{2}}{(s\\:+\\:\\zeta\\omega_{n}\\:+\\:j\\omega_{d})(s\\:+\\:\\zeta\\omega_{n}\\:-\\:j\\omega_{d})}\n$$  \n\nwhere $\\omega_{d}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}$ .The frequency $\\omega_{d}$ is called the damped natural frequency . For a unit-step input, $C(s)$ can be written  \n\n$$\nC(s)\\,=\\frac{\\omega_{n}^{2}}{\\left(s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}\\right)s}\n$$  \n\nThe inverse Laplace transform of Equation (5–11) can be obtained easily if $C(s)$ is written in the following form:  \n\n$$\n\\begin{array}{l}{\\displaystyle C(s)\\,=\\frac{1}{s}-\\frac{s\\,+\\,2\\zeta\\omega_{n}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\\\\ {\\displaystyle\\,=\\frac{1}{s}-\\frac{s\\,+\\,\\zeta\\omega_{n}}{\\left(s\\,+\\,\\zeta\\omega_{n}\\right)^{2}\\,+\\,\\omega_{d}^{2}}-\\frac{\\zeta\\omega_{n}}{\\left(s\\,+\\,\\zeta\\omega_{n}\\right)^{2}\\,+\\,\\omega_{d}^{2}}}\\end{array}\n$$  \n\nReferring to the Laplace transform table in Appendix A, it can be shown that  \n\n$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}^{-1}\\bigg[\\frac{s\\,+\\,\\zeta\\omega_{n}}{\\left(s\\,+\\,\\zeta\\omega_{n}\\right)^{2}\\,+\\,\\omega_{d}^{2}}\\bigg]=e^{-\\zeta\\omega_{n}t}\\cos\\omega_{d}t}\\\\ {\\displaystyle\\mathcal{L}^{-1}\\bigg[\\frac{\\omega_{d}}{\\left(s\\,+\\,\\zeta\\omega_{n}\\right)^{2}\\,+\\,\\omega_{d}^{2}}\\bigg]=e^{-\\zeta\\omega_{n}t}\\sin\\omega_{d}t}\\end{array}\n$$  \n\nHence the inverse Laplace transform of Equation (5–11) is obtained as  \n\n$$\n{\\begin{array}{r l r l}&{{\\mathcal{L}}^{-1}{\\bigl[}C(s){\\bigr]}=c(t)}\\\\ &{\\qquad\\qquad=1\\,-\\,e^{-\\zeta\\omega_{n}t}{\\biggl(}\\cos\\omega_{d}t\\,+\\,{\\frac{\\zeta}{\\sqrt{1-\\zeta^{2}}}}\\sin\\omega_{d}t{\\biggr)}}\\\\ &{\\qquad\\qquad=1\\,-\\,{\\frac{e^{-\\zeta\\omega_{n}t}}{\\sqrt{1-\\zeta^{2}}}}\\sin\\biggl(\\omega_{d}t\\,+\\,\\tan^{-1}{\\frac{\\sqrt{1-\\zeta^{2}}}{\\zeta}}\\biggr),}&&{{\\mathrm{for}}\\,t\\geq0}\\end{array}}\n$$  \n\nFrom Equation (5–12), it can be seen that the frequency of transient oscillation is the damped natural frequency $\\omega_{d}$ and thus varies with the damping ratio $\\zeta$ .The error signal for this system is the difference between the input and output and is  \n\n$$\n\\begin{array}{l}{{\\displaystyle e(t)\\,=\\,r(t)\\,-\\,c(t)}}\\\\ {{\\qquad=\\,e^{-\\zeta\\omega_{n}t}\\bigg(\\cos\\omega_{d}t\\,+\\,\\frac{\\zeta}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\omega_{d}t\\bigg),\\quad\\mathrm{~for~}t\\ge0}}\\end{array}\n$$  \n\nThis error signal exhibits a damped sinusoidal oscillation. At steady state, or at $t\\,=\\infty$ ,no error exists between the input and output.  \n\nIf the damping ratio $\\zeta$ is equal to zero, the response becomes undamped and oscillations continue indefinitely. The response $c(t)$ for the zero damping case may be obtained by substituting $\\zeta=0$ in Equation (5–12), yielding  \n\n$$\nc(t)\\,=\\,1\\,-\\,\\cos\\omega_{n}t,\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\nThus, from Equation (5–13), we see that $\\omega_{n}$ represents the undamped natural frequen2 cy of the system.That is, $\\omega_{n}$ is that frequency at which the system output would oscillate if the damping were decreased to zero. If the linear system has any amount of damping, the undamped natural frequency cannot be observed experimentally. The frequency that may be observed is the damped natural frequency $\\omega_{d}$ ,which is equal to $\\omega_{n}{\\sqrt{1-\\zeta^{2}}}$ .This frequency is always lower than the undamped natural frequency.An increase in $\\zeta$ would reduce the damped natural frequency $\\omega_{d}$ . If $\\zeta$ is increased beyond unity, the response becomes overdamped and will not oscillate.  \n\n(2) Critically damped case $(\\zeta=1)$ ):If the two poles of $C(s)/R(s)$ are equal,the system is said to be a critically damped one.  \n\nFor a unit-step input, $R(s)\\,=\\,1/s$ and $C(s)$ can be written  \n\n$$\nC(s)\\,=\\,\\frac{\\omega_{n}^{2}}{\\left(s\\,+\\,\\omega_{n}\\right)^{2}s}\n$$  \n\nThe inverse Laplace transform of Equation (5–14) may be found as  \n\n$$\nc(t)\\,=\\,1\\,-\\,e^{-\\omega_{n}t}\\bigl(1\\,+\\,\\omega_{n}t\\bigr),\\qquad\\mathrm{for}\\,t\\ge0\n$$  \n\nThis result can also be obtained by letting $\\zeta$ approach unity in Equation (5–12) and by using the following limit:  \n\n$$\n\\operatorname*{lim}_{\\zeta\\rightarrow1}\\frac{\\sin\\omega_{d}t}{\\sqrt{1-\\zeta^{2}}}=\\operatorname*{lim}_{\\zeta\\rightarrow1}\\frac{\\sin\\omega_{n}\\sqrt{1-\\zeta^{2}}t}{\\sqrt{1-\\zeta^{2}}}=\\omega_{n}t\n$$  \n\n(3) Overdamped case $(\\zeta>1)$ ):In this case, the two poles of $C(s)/R(s)$ are negative real and unequal. For a unit-step input, $R(s)\\,=\\,1/s$ and $C(s)$ can be written  \n\n$$\nC(s)=\\frac{\\omega_{n}^{2}}{(s\\,+\\,\\zeta\\omega_{n}\\,+\\,\\omega_{n}\\sqrt{\\zeta^{2}\\,-\\,1})(s\\,+\\,\\zeta\\omega_{n}\\,-\\,\\omega_{n}\\sqrt{\\zeta^{2}\\,-\\,1})s}\n$$  \n\nThe inverse Laplace transform of Equation (5–16) is  \n\n$$\n\\begin{array}{l}{\\displaystyle c(t)\\,=\\,1\\,+\\,\\frac{1}{2\\sqrt{\\zeta^{2}-1}\\big(\\zeta+\\sqrt{\\zeta^{2}-1}\\big)}\\,e^{-\\big(\\zeta+\\sqrt{\\zeta^{2}-1}\\big)\\omega_{n}t}}\\\\ {\\displaystyle\\qquad-\\,\\frac{1}{2\\sqrt{\\zeta^{2}-1}\\big(\\zeta-\\sqrt{\\zeta^{2}-1}\\big)}\\,e^{-\\big(\\zeta-\\sqrt{\\zeta^{2}-1}\\big)\\omega_{n}t}}\\\\ {\\,=\\,1\\,+\\,\\frac{\\omega_{n}}{2\\sqrt{\\zeta^{2}-1}}\\,\\bigg(\\frac{e^{-s_{1}t}}{s_{1}}-\\frac{e^{-s_{2}t}}{s_{2}}\\bigg),\\qquad\\qquad\\qquad\\qquad\\qquad\\,\\mathrm{for}\\,t\\ge0}\\end{array}\n$$  \n\nwhere $s_{1}=(\\zeta\\,+\\,\\sqrt{\\zeta^{2}\\,-\\,1})\\omega_{n}$ and $s_{2}=(\\zeta\\,-\\,\\sqrt{\\zeta^{2}\\,-\\,1})\\omega_{n}$ .Thus, the response $c(t)$ includes two decaying exponential terms.  \n\nWhen $\\zeta$ is appreciably greater than unity, one of the two decaying exponentials decreases much faster than the other, so the faster-decaying exponential term (which corresponds to a smaller time constant) may be neglected.That is, if A B$-s_{2}$ is located very much closer to the $j\\omega$ a th $-s_{1}$ which means $\\left|s_{2}\\right|\\ll\\left|s_{1}\\right|.$ , then an approximate solution we may neglect $-s_{1}$ .This is permissible because the effect of $-s_{1}$ on the response is much smaller than that of much faster than the term involving $-s_{2}$ , since the term involving $s_{2}$ . Once the faster-decaying exponential term has 2 $s_{1}$ in Equation (5–17) decays disappeared, the response is similar to that of a first-order system, and $C(s)/R(s)$ may be approximated by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{\\zeta\\omega_{n}-\\,\\omega_{n}\\sqrt{\\zeta^{2}-1}}{s\\,+\\,\\zeta\\omega_{n}-\\,\\omega_{n}\\sqrt{\\zeta^{2}-1}}=\\frac{s_{2}}{s\\,+\\,s_{2}}\n$$  \n\nThis approximate form is a direct consequence of the fact that the initial values and final values of both the original $C(s)/R(s)$ and the approximate one agree with each other.  \n\nWith the approximate transfer function $C(s)/R(s)$ , the unit-step response can be obtained as  \n\n$$\nC(s)=\\frac{\\zeta\\omega_{n}-\\,\\omega_{n}\\sqrt{\\zeta^{2}-1}}{\\left(s\\,+\\,\\zeta\\omega_{n}\\,-\\,\\omega_{n}\\sqrt{\\zeta^{2}\\,-\\,1}\\right)\\!s}\n$$  \n\nThe time response $c(t)$ is then  \n\n$$\nc(t)\\,=\\,1\\,-\\,e^{-(\\zeta-\\sqrt{\\zeta^{2}-1})\\omega_{n}t},\\qquad\\mathrm{for}\\,t\\,\\ge\\,0\n$$  \n\nThis gives an approximate unit-step response when one of the poles of $C(s)/R(s)$ can be neglected.  \n\n![](images/5c378ae2a28259d77b773ad41de66c4604c431027371dfe9ad7aa65e8a4d38fa.jpg)  \nFigure 5–7 Unit-step response curves of the system shown in Figure 5–6.  \n\nA family of unit-step response curves $c(t)$ with various values of $\\zeta$ is shown in Figure 5–7, where the abscissa is the dimensionless variable $\\omega_{n}t$ . The curves are functions only of $\\zeta$ . These curves are obtained from Equations (5–12), (5–15), and (5–17). The system described by these equations was initially at rest.  \n\nNote that two second-order systems having the same $\\zeta$ but different $\\omega_{n}$ will exhibit the same overshoot and the same oscillatory pattern. Such systems are said to have the same relative stability.  \n\nFrom Figure 5–7, we see that an underdamped system with $\\zeta$ between 0.5 and 0.8 gets close to the final value more rapidly than a critically damped or overdamped system. Among the systems responding without oscillation, a critically damped system exhibits the fastest response.An overdamped system is always sluggish in responding to any inputs.  \n\nIt is important to note that, for second-order systems whose closed-loop transfer functions are different from that given by Equation (5–10), the step-response curves may look quite different from those shown in Figure 5–7.  \n\nDefinitions of Transient-Response Specifications. Frequently, the performance characteristics of a control system are specified in terms of the transient response to a unit-step input, since it is easy to generate and is sufficiently drastic. (If the response to a step input is known,it is mathematically possible to compute the response to any input.)  \n\nThe transient response of a system to a unit-step input depends on the initial conditions. For convenience in comparing transient responses of various systems, it is a common practice to use the standard initial condition that the system is at rest initially with the output and all time derivatives thereof zero. Then the response characteristics of many systems can be easily compared.  \n\nThe transient response of a practical control system often exhibits damped oscillations before reaching steady state. In specifying the transient-response characteristics of a control system to a unit-step input, it is common to specify the following:  \n\n1. Delay time, $t_{d}$   \n2. Rise time, $t_{r}$   \n3. Peak time, $t_{p}$   \n4. Maximum overshoot, $M_{p}$   \n5. Settling time, $t_{s}$  \n\nThese specifications are defined in what follows and are shown graphically in Figure 5–8.  \n\n1. Delay time, $t_{d}$ : The delay time is the time required for the response to reach half the final value the very first time.   \n2. Rise time, $t_{r}$ :The rise time is the time required for the response to rise from $10\\%$ to $90\\%$ ,$5\\%$ to $95\\%$ , or $0\\%$ to $100\\%$ of its final value. For underdamped secondorder systems, the $0\\%$ to $100\\%$ rise time is normally used. For overdamped systems, the $10\\%$ to $90\\%$ rise time is commonly used.   \n3. Peak time, $t_{p}$ :The peak time is the time required for the response to reach the first peak of the overshoot.   \n4. Maximum (percent) overshoot, $M_{p}$ : The maximum overshoot is the maximum peak value of the response curve measured from unity. If the final steady-state value of the response differs from unity, then it is common to use the maximum percent overshoot. It is defined by  \n\n$$\n{\\mathrm{Maximum~percent~overshoot}}={\\frac{c(t_{p})\\,-\\,c(\\infty)}{c(\\infty)}}\\times100\\%\n$$  \n\nThe amount of the maximum (percent) overshoot directly indicates the relative stability of the system.  \n\n5. Settling time, $t_{s}$ : The settling time is the time required for the response curve to reach and stay within a range about the final value of size specified by absolute percentage of the final value (usually $2\\%$ or $5\\%$ ). The settling time is related to the largest time constant of the control system.Which percentage error criterion to use may be determined from the objectives of the system design in question.  \n\nThe time-domain specifications just given are quite important, since most control systems are time-domain systems; that is, they must exhibit acceptable time responses. (This means that, the control system must be modified until the transient response is satisfactory.)  \n\n![](images/029957f6107302372d7e3a6f9c96481b50f5a6c96e4d0dea9c55fa922bb1d686.jpg)  \nFigure 5–8 Unit-step response curve showing $t_{d},t_{r}$ ,$t_{p},M_{p}$ , and $t_{s}$ .  \nChapter 5 /Transient and Steady-State Response Analyses  \n\nNote that not all these specifications necessarily apply to any given case. For example, for an overdamped system, the terms peak time and maximum overshoot do not apply. (For systems that yield steady-state errors for step inputs, this error must be kept within a specified percentage level. Detailed discussions of steady-state errors are postponed until Section 5–8.)  \n\nA Few Comments on Transient-Response Specifications. Except for certain applications where oscillations cannot be tolerated, it is desirable that the transient response be sufficiently fast and be sufficiently damped.Thus, for a desirable transient response of a second-order system, the damping ratio must be between 0.4 and 0.8. Small values of $\\zeta$ (that is, $\\zeta<0.4$ )yield excessive overshoot in the transient response, and a system with a large value of $\\zeta$ (that is, $\\zeta>0.81$ )responds sluggishly.  \n\nWe shall see later that the maximum overshoot and the rise time conflict with each other. In other words, both the maximum overshoot and the rise time cannot be made smaller simultaneously. If one of them is made smaller, the other necessarily becomes larger.  \n\nSecond-Order Systems and Transient-Response Specifications. In the following, we shall obtain the rise time, peak time, maximum overshoot, and settling time of the second-order system given by Equation (5–10).These values will be obtained in terms of $\\zeta$ and $\\omega_{n}$ .The system is assumed to be underdamped.  \n\nRise time $t_{r}$ :Referring to Equation (5–12),we obtain the rise time $t_{r}$ by letting $c(t_{r})=1$  \n\n$$\nc(t_{r})\\,=\\,1\\,=\\,1\\,-\\,e^{-\\zeta\\omega_{n}t_{r}}\\bigg(\\cos\\omega_{d}t_{r}\\,+\\frac{\\zeta}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\omega_{d}t_{r}\\bigg)\n$$  \n\nSince $e^{-\\zeta\\omega_{n}t_{r}}\\neq0$ we obtain from Equation (5–18) the following equation:  \n\n$$\n\\cos\\omega_{d}t_{r}\\,+\\frac{\\zeta}{\\sqrt{1-\\zeta^{2}}}\\sin\\omega_{d}t_{r}\\,=\\,0\n$$  \n\nSince $\\omega_{n}\\,\\sqrt{1\\,-\\,\\zeta^{2}}\\,=\\,\\omega_{d}$ and $\\zeta\\omega_{n}=\\sigma$ , we have  \n\n$$\n\\tan\\omega_{d}t_{r}=-\\,{\\frac{\\sqrt{1\\,-\\,\\zeta^{2}}}{\\zeta}}=-\\,{\\frac{\\omega_{d}}{\\sigma}}\n$$  \n\nThus, the rise time $t_{r}$ is  \n\n$$\nt_{r}\\,=\\,\\frac{1}{\\omega_{d}}\\tan^{-1}\\!\\left(\\frac{\\omega_{d}}{-\\sigma}\\right)\\,=\\,\\frac{\\pi\\,-\\,\\beta}{\\omega_{d}}\n$$  \n\nwhere angle $\\beta$ is defined in Figure 5–9. Clearly, for a small value of $t_{r},\\omega_{d}$ must be large.  \n\nFigure 5–9   \nDefinition of the angle $\\beta$ .  \n\n![](images/4b6478991fdc6354e487dc56621eb506a54c34955c2059944cfa65d0cfc164e8.jpg)  \n\nPeak time $t_{p}$ :Referring to Equation (5–12), we may obtain the peak time by differentiating $c(t)$ with respect to time and letting this derivative equal zero. Since  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{d c}{d t}=\\zeta\\omega_{n}e^{-\\zeta\\omega_{n}t}\\bigg(\\cos\\omega_{d}t\\,+\\,\\frac{\\zeta}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\omega_{d}t\\bigg)}\\\\ {\\displaystyle\\,+\\,\\,e^{-\\zeta\\omega_{n}t}\\bigg(\\omega_{d}\\sin\\omega_{d}t\\,-\\,\\frac{\\zeta\\omega_{d}}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\cos\\omega_{d}t\\bigg)}\\end{array}\n$$  \n\nand the cosine terms in this last equation cancel each other, $d c/d t$ , evaluated at $t\\,=\\,t_{p}$ ,can be simplified to  \n\n$$\n\\left.{\\frac{d c}{d t}}\\right|_{t=t_{p}}=\\left(\\sin\\omega_{d}t_{p}\\right){\\frac{\\omega_{n}}{\\sqrt{1\\,-\\,\\zeta^{2}}}}\\,e^{-\\zeta\\omega_{n}t_{p}}=\\,0\n$$  \n\nThis last equation yields the following equation:  \n\n$$\n\\sin\\omega_{d}t_{p}\\,=\\,0\n$$  \n\nor  \n\n$$\n\\omega_{d}t_{p}\\,=\\,0,\\,\\pi,2\\pi,3\\pi,\\ldots\n$$  \n\nSince the peak time corresponds to the first peak overshoot, $\\omega_{d}t_{p}=\\pi$ .Hence  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}\n$$  \n\nThe peak time $t_{p}$ corresponds to one-half cycle of the frequency of damped oscillation.  \n\nMaximum overshoot $M_{p}$ :The maximum overshoot occurs at the peak time or at $t\\,=\\,t_{p}\\,=\\,\\pi/\\omega_{d}.$ .Assuming that the final value of the output is unity, $M_{p}$ is obtained from Equation (5–12) as  \n\n$$\n\\begin{array}{l}{{M_{p}=\\,c(t_{p})\\,-\\,1}}\\\\ {{\\ \\ \\ =\\,-e^{-\\zeta\\omega_{n}\\left(\\pi/\\omega_{d}\\right)}\\Big(\\cos\\pi\\,+\\,\\frac\\zeta{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\pi\\Big)}}\\\\ {{\\ \\ \\ =\\,e^{-\\left(\\sigma/\\omega_{d}\\right)\\pi}\\,=\\,e^{-\\left(\\zeta/\\sqrt{1-\\zeta^{2}}\\right)\\pi}}}\\end{array}\n$$  \n\nThe maximum percent overshoot is $e^{-(\\sigma/\\omega_{d})\\pi}\\times100\\%$ .  \n\nIf the final value $c(\\infty)$ of the output is not unity, then we need to use the following equation:  \n\n$$\nM_{p}=\\frac{c(t_{p})\\,-\\,c(\\infty)}{c(\\infty)}\n$$  \n\nSettling time $t_{s}$ :For an underdamped second-order system, the transient response is obtained from Equation (5–12) as  \n\n$$\nc(t)=1-\\frac{e^{-\\zeta\\omega_{n}t}}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\bigg(\\omega_{d}t\\,+\\,\\tan^{-1}\\frac{\\sqrt{1\\,-\\,\\zeta^{2}}}{\\zeta}\\bigg),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{for}\\,t\\ge0\n$$  \n\n![](images/c00fcb927a902fd706bc1cced62f60f3167d4be1bdd23c522faaca93f0d91ae3.jpg)  \nFigure 5–10 Pair of envelope curves for the unitstep response curve of the system shown in Figure 5–6.  \n\nThe curves $1\\,\\pm\\,\\big(e^{-\\zeta\\omega_{n}t}/\\sqrt{1\\,-\\,\\zeta^{2}}\\big)$ A Bare the envelope curves of the transient response to a unit-step input.The response curve $c(t)$ always remains within a pair of the envelope curves, as shown in Figure 5–10.The time constant of these envelope curves is $1/\\zeta\\omega_{n}$ .  \n\nThe speed of decay of the transient response depends on the value of the time constant $1/\\zeta\\omega_{n}$ . For a given $\\omega_{n}$ , the settling time $t_{s}$ is a function of the damping ratio $\\zeta$ . From Figure 5–7,we see that for the same $\\omega_{n}$ and for a range of $\\zeta$ between 0 and 1 the settling time $t_{s}$ for a very lightly damped system is larger than that for a properly damped system.For an overdamped system, the settling time $t_{s}$ becomes large because of the sluggish response.  \n\nThe settling time corresponding to a $\\pm2\\%$ or $\\pm5\\%$ tolerance band may be measured in terms of the time constant $T=1/\\zeta\\omega_{n}$ from the curves of Figure 5–7 for different values of $\\zeta$ .The results are shown in Figure 5–11. For $0<\\zeta<0.9$ , if the $2\\%$ criterion is used, $t_{s}$ is approximately four times the time constant of the system. If the $5\\%$ criterion is used, then $t_{s}$ is approximately three times the time constant. Note that the settling time reaches a minimum value around $\\zeta=0.76$ (for the $2\\%$ criterion) or $\\zeta\\,=\\,0.68$ (for the $5\\%$ criterion) and then increases almost linearly for large values of $\\zeta$ .The discontinuities in the curves of Figure 5–11 arise because an infinitesimal change in the value of $\\zeta$ can cause a finite change in the settling time.  \n\nFor convenience in comparing the responses of systems, we commonly define the settling time $t_{s}$ to be  \n\nor  \n\n$$\n\\begin{array}{l l}{{t_{s}=4T={\\frac{4}{\\sigma}}={\\frac{4}{\\zeta\\omega_{n}}}\\quad}}&{{\\left(2\\%{\\mathrm{~criterion}}\\right)}}\\\\ {{\\ }}&{{\\ }}\\\\ {{t_{s}=3T={\\frac{3}{\\sigma}}={\\frac{3}{\\zeta\\omega_{n}}}\\quad}}&{{\\left(5\\%{\\mathrm{~criterion}}\\right)}}\\end{array}\n$$  \n\nNote that the settling time is inversely proportional to the product of the damping ratio and the undamped natural frequency of the system. Since the value of $\\zeta$ is usually determined from the requirement of permissible maximum overshoot, the settling time is determined primarily by the undamped natural frequency $\\omega_{n}$ . This means that the duration of the transient period may be varied, without changing the maximum overshoot, by adjusting the undamped natural frequency $\\omega_{n}$ .  \n\n![](images/15403478eaef6d8f73d259dd5624932728d76ed78a1eab41832e6a1a1809f2f9.jpg)  \nFigure 5–11 Settling time $t_{s}$ versus $\\zeta$ curves.  \n\nFrom the preceding analysis,it is evident that for rapid response $\\omega_{n}$ must be large.To limit the maximum overshoot $M_{p}$ and to make the settling time small,the damping ratio $\\zeta$ should not be too small. The relationship between the maximum percent overshoot $M_{p}$ and the damping ratio $\\zeta$ is presented in Figure 5–12. Note that if the damping ratio is between 0.4 and 0.7, then the maximum percent overshoot for step response is between $25\\%$ and $4\\%$ .  \n\n![](images/1d2e7cd38a4b31a5ddc08445bac62d13c0207064a87f7dfa498e84c5961a955e.jpg)  \nFigure 5–12 $M_{p}$ versus $\\zeta$ curve.  \n\nIt is important to note that the equations for obtaining the rise time, peak time, maximum overshoot, and settling time are valid only for the standard second-order system defined by Equation (5–10). If the second-order system involves a zero or two zeros, the shape of the unit-step response curve will be quite different from those shown in Figure 5–7.  \n\nConsider the system shown in Figure 5–6,where $\\zeta=0.6$ and 2 $\\omega_{n}=5$ rad 'sec.Let us obtain the rise time $t_{r}$ , peak time $t_{p}$ , maximum overshoot $M_{p}$ , and settling time $t_{s}$ when the system is subjected to a unit-step input.  \n\nFrom the given values of $\\zeta$ and $\\omega_{n}$ , we obtain $\\omega_{d}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}\\,=\\,4$ and $\\sigma\\,=\\,\\zeta\\omega_{n}\\,=\\,3$ .  \n\nRise time $t_{r}$ :The rise time is  \n\n$$\nt_{r}=\\frac{\\pi-\\beta}{\\omega_{d}}=\\frac{3.14\\,-\\,\\beta}{4}\n$$  \n\nwhere $\\beta$ is given by  \n\n$$\n\\beta=\\tan^{-1}{\\frac{\\omega_{d}}{\\sigma}}=\\tan^{-1}{\\frac{4}{3}}=0.93\\;\\mathrm{rad}\n$$  \n\nThe rise time $t_{r}$ is thus  \n\n$$\nt_{r}={\\frac{3.14\\,-\\,0.93}{4}}=0.55\\;{\\mathrm{sec}}\n$$  \n\nPeak time $t_{p}$ :The peak time is  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}=\\frac{3.14}{4}=0.785\\;\\mathrm{sec}\n$$  \n\nMaximum overshoot $M_{p}$ :The maximum overshoot is  \n\n$$\nM_{p}\\,=\\,e^{-(\\sigma/\\omega_{d})\\pi}\\,=\\,e^{-(3/4)\\times3.14}\\,=\\,0.095\n$$  \n\nThe maximum percent overshoot is thus $9.5\\%$ .  \n\nSettling time $t_{s}$ :For the $2\\%$ criterion, the settling time is  \n\n$$\nt_{s}={\\frac{4}{\\sigma}}={\\frac{4}{3}}=1.33\\;\\mathrm{sec}\n$$  \n\nFor the $5\\%$ criterion,  \n\n$$\nt_{s}={\\frac{3}{\\sigma}}={\\frac{3}{3}}=1\\;\\mathrm{sec}\n$$  \n\nServo System with Velocity Feedback. The derivative of the output signal can be used to improve system performance. In obtaining the derivative of the output position signal,it is desirable to use a tachometer instead of physically differentiating the output signal. (Note that the differentiation amplifies noise effects. In fact, if discontinuous noises are present,differentiation amplifies the discontinuous noises more than the useful signal. For example, the output of a potentiometer is a discontinuous voltage signal because, as the potentiometer brush is moving on the windings, voltages are induced in the switchover turns and thus generate transients.The output of the potentiometer therefore should not be followed by a differentiating element.)  \n\n![](images/c96d2ae56457f077149107271599f61ef2a2a3cbb0560366ffb2812aa0c4388c.jpg)  \nFigure 5–13 (a) Block diagram of a servo system; (b) simplified block diagram.   \n(b)  \n\nThe tachometer, a special dc generator, is frequently used to measure velocity without differentiation process. The output of a tachometer is proportional to the angular velocity of the motor.  \n\nConsider the servo system shown in Figure 5–13(a). In this device, the velocity signal, together with the positional signal, is fed back to the input to produce the actuating error signal. In any servo system, such a velocity signal can be easily generated by a tachometer.The block diagram shown in Figure 5–13(a) can be simplified, as shown in Figure 5–13(b), giving  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{J s^{2}\\,+\\,(B\\,+\\,K K_{h})s\\,+\\,K}\n$$  \n\nComparing Equation (5–24) with Equation (5–9), notice that the velocity feedback has the effect of increasing damping.The damping ratio $\\zeta$ becomes  \n\n$$\n\\zeta=\\frac{B\\,+\\,K K_{h}}{2\\sqrt{K J}}\n$$  \n\nThe undamped natural frequency $\\omega_{n}\\,=\\,\\sqrt{K/J}$ is not affected by velocity feedback.Noting that the maximum overshoot for a unit-step input can be controlled by controlling the value of the damping ratio $\\zeta$ , we can reduce the maximum overshoot by adjusting the velocity-feedback constant $K_{h}$ so that $\\zeta$ is between 0.4 and 0.7.  \n\nIt is important to remember that velocity feedback has the effect of increasing the damping ratio without affecting the undamped natural frequency of the system.  \n\n# EXAMPLE 5–2  \n\nFor the system shown in Figure 5–13(a), determine the values of gain $K$ and velocity-feedback constant $K_{h}$ so that the maximum overshoot in the unit-step response is 0.2 and the peak time is 1 sec. With these values of $K$ and $K_{h}$ ,obtain the rise time and settling time.Assume that $J=1\\,\\mathrm{kg}_{\\mathrm{-m}^{2}}$ and B=1 N-m 'rad 'sec.  \n\nDetermination of the values of $K$ and $K_{h}$ :The maximum overshoot 2 $M_{p}$ is given by Equation (5–21) as  \n\n$$\nM_{p}\\,=\\,e^{-(\\zeta/\\sqrt{1-\\zeta^{2}})\\pi}\n$$  \n\nThis value must be 0.2.Thus,  \n\n$$\ne^{-(\\zeta/\\sqrt{1-\\zeta^{2}})\\pi}=0.2\n$$  \n\nor  \n\n$$\n\\frac{\\zeta\\pi}{\\sqrt{1-\\zeta^{2}}}=1.61\n$$  \n\nwhich yields  \n\n$$\n\\zeta\\,=\\,0.456\n$$  \n\nThe peak time $t_{p}$ is specified as 1 sec; therefore, from Equation (5–20),  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}=1\n$$  \n\nor  \n\n$$\n\\omega_{d}=3.14\n$$  \n\nSince $\\zeta$ is $0.456,\\omega_{n}$ is  \n\n$$\n\\omega_{n}=\\frac{\\omega_{d}}{\\sqrt{1-\\zeta^{2}}}=3.53\n$$  \n\nSince the natural frequency $\\omega_{n}$ is equal to $\\sqrt{K/J}$ ,  \n\n$$\nK\\,=\\,J\\omega_{n}^{2}\\,=\\,\\omega_{n}^{2}\\,=\\,12.5\\,\\mathrm{N{-m}}\n$$  \n\nThen $K_{h}$ is, from Equation (5–25),  \n\n$$\nK_{h}=\\frac{2\\sqrt{K J}\\zeta\\,-\\,B}{K}=\\frac{2\\sqrt{K}\\zeta\\,-\\,1}{K}=0.178\\,\\mathrm{sec}\n$$  \n\nRise time $t_{r}$ :From Equation (5–19), the rise time $t_{r}$ is  \n\n$$\nt_{r}=\\frac{\\pi-\\beta}{\\omega_{d}}\n$$  \n\nwhere  \n\n$$\n\\beta\\,=\\,\\tan^{-1}{\\frac{\\omega_{d}}{\\sigma}}=\\tan^{-1}1.95\\,=\\,1.10\n$$  \n\nThus, $t_{r}$ is  \n\n$$\nt_{r}\\,=\\,0.65\\;\\mathrm{sec}\n$$  \n\nSettling time $t_{s}$ :For the $2\\%$ criterion,  \n\n$$\nt_{s}={\\frac{4}{\\sigma}}=2.48\\operatorname{sec}\n$$  \n\nFor the $5\\%$ criterion,  \n\n$$\nt_{s}=\\frac{3}{\\sigma}=1.86\\,\\mathrm{se}.\n$$  \n\nImpulse Response of Second-Order Systems. For a unit-impulse input $r(t)$ ,the corresponding Laplace transform is unity, or $R(s)\\,=\\,1.$ .The unit-impulse response $C(s)$ of the second-order system shown in Figure 5-6 is  \n\n$$\nC(s)\\,=\\,{\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nThe inverse Laplace transform of this equation yields the time solution for the response $c(t)$ as follows:  \n\nFor $0\\leq\\zeta<1$ ,  \n\n$$\nc(t)={\\frac{\\omega_{n}}{\\sqrt{1-\\zeta^{2}}}}\\,e^{-\\zeta\\omega_{n}t}\\sin\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}\\,t,\\qquad\\mathrm{for}\\,t\\ge0\n$$  \n\nFor $\\zeta\\,=\\,1$ ,  \n\n$$\nc(t)\\,=\\,\\omega_{n}^{2}t e^{-\\omega_{n}t},\\qquad\\mathrm{for}\\,t\\,\\geq\\,0\n$$  \n\nFor $\\zeta>1$ ,  \n\n$$\nc(t)=\\frac{\\omega_{n}}{2\\sqrt{\\zeta^{2}-1}}\\,e^{-(\\zeta-\\sqrt{\\zeta^{2}-1})\\omega_{n}t}-\\frac{\\omega_{n}}{2\\sqrt{\\zeta^{2}-1}}\\,e^{-(\\zeta+\\sqrt{\\zeta^{2}-1})\\omega_{n}t},\\qquad\\mathrm{for}\\,\\,t\\ge0\n$$  \n\nNote that without taking the inverse Laplace transform of $C(s)$ we can also obtain the time response $c(t)$ by differentiating the corresponding unit-step response, since the unit-impulse function is the time derivative of the unit-step function. A family of unit-impulse response curves given by Equations (5–26) and (5–27) with various values of $\\zeta$ is shown in Figure 5–14. The curves $c(t)/\\omega_{n}$ are plotted against the dimensionless variable $\\omega_{n}t$ , and thus they are functions only of $\\zeta$ . For the critically damped and overdamped cases, the unit-impulse response is always positive or zero; that is, $c(t)\\geq0.$ . This can be seen from Equations (5–27) and (5–28). For the underdamped case, the unit-impulse response $c(t)$ oscillates about zero and takes both positive and negative values.  \n\n![](images/2acc0872111fd8dec9457cb3f61266bc6db25337d93c74093bd71f6b792a05ee.jpg)  \nFigure 5–14 Unit-impulse response curves of the system shown in Figure 5–6.   \nChapter 5 /Transient and Steady-State Response Analyses  \n\n![](images/9c8fae9bab68a0d3cfa7b8d3bb8e17893c1a933a48ba24e407c94c01eef52a3d.jpg)  \nFigure 5–15 Unit-impulse response curve of the system shown in Figure 5–6.  \n\nFrom the foregoing analysis, we may conclude that if the impulse response $c(t)$ does not change sign, the system is either critically damped or overdamped, in which case the corresponding step response does not overshoot but increases or decreases monotonically and approaches a constant value.  \n\nThe maximum overshoot for the unit-impulse response of the underdamped system occurs at  \n\n$$\nt={\\frac{\\tan^{-1}{\\frac{\\sqrt{1-\\zeta^{2}}}{\\zeta}}}{\\omega_{n}{\\sqrt{1-\\zeta^{2}}}}},\\qquad{\\mathrm{where~}}0<\\zeta<1\n$$  \n\n[Equation (5–29) can be obtained by equating $d c/d t$ to zero and solving for $t$ .] The maximum overshoot is  \n\n$$\nc(t)_{\\mathrm{max}}=\\,\\omega_{n}\\mathrm{exp}\\biggl(-\\frac{\\zeta}{\\sqrt{1-\\zeta^{2}}}\\tan^{-1}\\frac{\\sqrt{1-\\zeta^{2}}}{\\zeta}\\biggr),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{where}\\,\\,0<\\zeta<1\n$$  \n\n[Equation (5–30) can be obtained by substituting Equation (5–29) into Equation (5–26).]  \n\nSince the unit-impulse response function is the time derivative of the unit-step response function, the maximum overshoot $M_{p}$ for the unit-step response can be found from the corresponding unit-impulse response.That is, the area under the unitimpulse response curve from $t\\,=\\,0$ to the time of the first zero, as shown in Figure 5–15, is $1\\,+\\,M_{p}$ , where $M_{p}$ is the maximum overshoot (for the unit-step response) given by Equation (5–21). The peak time $t_{p}$ (for the unit-step response) given by Equation (5–20) corresponds to the time that the unit-impulse response first crosses the time axis.  \n\n# 5–4 HIGHER-ORDER SYSTEMS  \n\nIn this section we shall present a transient-response analysis of higher-order systems in general terms. It will be seen that the response of a higher-order system is the sum of the responses of first-order and second-order systems.  \n\n# Transient Response of Higher-Order Systems. Consider the system shown in Figure 5–16.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)H(s)}\n$$  \n\nIn general, $G(s)$ and $H(s)$ are given as ratios of polynomials in $s$ , or  \n\n$$\nG(s)=\\frac{p(s)}{q(s)}\\qquad\\mathrm{and}\\qquad H(s)=\\frac{n(s)}{d(s)}\n$$  \n\nwhere $p(s),q(s),n(s)$ , and $d(s)$ are polynomials in $s$ .The closed-loop transfer function given by Equation (5–31) may then be written  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{C(s)}{R(s)}=\\frac{p(s)d(s)}{q(s)d(s)\\,+\\,p(s)n(s)}}}\\\\ &{}&{=\\frac{b_{0}s^{m}\\,+\\,b_{1}s^{m-1}\\,+\\,\\cdots\\,+\\,b_{m-1}s\\,+\\,b_{m}}{a_{0}s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}&{\\,\\,(m\\leq n)}\\end{array}\n$$  \n\nThe transient response of this system to any given input can be obtained by a computer simulation. (See Section 5–5.) If an analytical expression for the transient response is desired, then it is necessary to factor the denominator polynomial. [MATLAB may be used for finding the roots of the denominator polynomial. Use the command roots(den) .] Once the numerator and the denominator have been factored, $C(s)/R(s)$ can be written in the form  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K(s\\,+\\,z_{1})(s\\,+\\,z_{2})\\cdots(s\\,+\\,z_{m})}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})}}\n$$  \n\nLet us examine the response behavior of this system to a unit-step input. Consider first the case where the closed-loop poles are all real and distinct. For a unit-step input, Equation (5–32) can be written  \n\n$$\nC(s)={\\frac{a}{s}}+\\ \\sum_{i=1}^{n}\\,{\\frac{a_{i}}{s\\,+\\,p_{i}}}\n$$  \n\nwhere $a_{i}$ is the residue of the pole at $s\\,=\\,-p_{i}$ . (If the system involves multiple poles, then $C(s)$ will have multiple-pole terms.) [The partial-fraction expansion of $C(s)$ , as given by Equation (5–33), can be obtained easily with MATLAB. Use the residue command. (See Appendix B.)]  \n\nIf all closed-loop poles lie in the left-half $s$ plane, the relative magnitudes of the residues determine the relative importance of the components in the expanded form of $C(s)$ . If there is a closed-loop zero close to a closed-loop pole, then the residue at this pole is small and the coefficient of the transient-response term corresponding to this pole becomes small. A pair of closely located poles and zeros will effectively cancel each other. If a pole is located very far from the origin, the residue at this pole may be small. The transients corresponding to such a remote pole are small and last a short time.Terms in the expanded form of $C(s)$ having very small residues contribute little to the transient response, and these terms may be neglected. If this is done, the higher-order system may be approximated by a lower-order one. (Such an approximation often enables us to estimate the response characteristics of a higher-order system from those of a simplified one.)  \n\n![](images/da889cf3cb488d01901927e36358fd89dfc958f31d1668ccf7aee50565581686.jpg)  \nFigure 5–16 Control system.  \n\ncomplex-conjugate poles.A pair of complex-conjugate poles yields a second-order term Next, consider the case where the poles of $C(s)$ consist of real poles and pairs of 2 in s. Since the factored form of the higher-order characteristic equation consists of firstand second-order terms, Equation (5–33) can be rewritten  \n\n$$\nC(s)={\\frac{a}{s}}+\\ \\sum_{j=1}^{q}\\,{\\frac{a_{j}}{s\\ +\\ p_{j}}}+\\ \\sum_{k=1}^{r}{\\frac{b_{k}\\bigl(s+\\zeta_{k}\\omega_{k}\\bigr)\\ +\\ c_{k}\\omega_{k}\\sqrt{1\\ -\\ \\zeta_{k}^{2}}}{s^{2}\\ +\\ 2\\zeta_{k}\\omega_{k}s\\ +\\ \\omega_{k}^{2}}}\\qquad(q\\ +\\ 2r\\ =\\ m)\\,.\n$$  \n\nwhere we assumed all closed-loop poles are distinct. [If the closed-loop poles involve multiple poles, $C(s)$ must have multiple-pole terms.] From this last equation, we see that the response of a higher-order system is composed of a number of terms involving the 2 simple functions found in the responses of first- and second-order systems. The unitstep response $c(t)$ , the inverse Laplace transform of $C(s)$ , is then  \n\n$$\n\\begin{array}{l l}{{c(t)\\,=\\,a\\,+\\,\\displaystyle\\sum_{j=1}^{q}a_{j}e^{-p_{j}t}\\,+\\,\\displaystyle\\sum_{k=1}^{r}b_{k}e^{-\\zeta_{k}\\omega_{k}t}\\cos\\omega_{k}\\sqrt{1-\\zeta_{k}^{2}}t}}&{{}}\\\\ {{\\displaystyle+\\,\\sum_{k=1}^{r}c_{k}e^{-\\zeta_{k}\\omega_{k}t}\\sin\\omega_{k}\\sqrt{1-\\zeta_{k}^{2}}t,}}&{{\\mathrm{for}\\,t\\ge0}}\\end{array}\n$$  \n\nThus the response curve of a stable higher-order system is the sum of a number of exponential curves and damped sinusoidal curves.  \n\nIf all closed-loop poles lie in the left-half $s$ plane, then the exponential terms and the damped exponential terms in Equation (5–34) will approach zero as time $t$ increases. The steady-state output is then $c(\\infty)\\,=\\,a$ .  \n\nLet us assume that the system considered is a stable one.Then the closed-loop poles that are located far from the $j\\omega$ axis have large negative real parts. The exponential terms that correspond to these poles decay very rapidly to zero. (Note that the horizontal distance from a closed-loop pole to the $j\\omega$ axis determines the settling time of transients due to that pole.The smaller the distance is, the longer the settling time.)  \n\nRemember that the type of transient response is determined by the closed-loop poles, while the shape of the transient response is primarily determined by the closedloop zeros. As we have seen earlier, the poles of the input $R(s)$ yield the steady-state response terms in the solution, while the poles of $C(s)/R(s)$ enter into the exponential transient-response terms and/or damped sinusoidal transient-response terms.The zeros of $C(s)/R(s)$ do not affect the exponents in the exponential terms, but they do affect the magnitudes and signs of the residues.  \n\nDominant Closed-Loop Poles. The relative dominance of closed-loop poles is determined by the ratio of the real parts of the closed-loop poles, as well as by the relative magnitudes of the residues evaluated at the closed-loop poles.The magnitudes of the residues depend on both the closed-loop poles and zeros.  \n\nIf the ratios of the real parts of the closed-loop poles exceed 5 and there are no zeros nearby, then the closed-loop poles nearest the $j\\omega$ axis will dominate in the transientresponse behavior because these poles correspond to transient-response terms that decay slowly. Those closed-loop poles that have dominant effects on the transientresponse behavior are called dominant closed-loop poles. Quite often the dominant closed-loop poles occur in the form of a complex-conjugate pair.The dominant closedloop poles are most important among all closed-loop poles.  \n\nNote that the gain of a higher-order system is often adjusted so that there will exist a pair of dominant complex-conjugate closed-loop poles.The presence of such poles in a stable system reduces the effects of such nonlinearities as dead zone, backlash, and coulomb-friction.  \n\nStability Analysis in the Complex Plane. The stability of a linear closed-loop system can be determined from the location of the closed-loop poles in the $s$ plane. If any of these poles lie in the right-half $s$ plane, then with increasing time they give rise to the dominant mode, and the transient response increases monotonically or oscillates with increasing amplitude. This represents an unstable system. For such a system, as soon as the power is turned on, the output may increase with time. If no saturation takes place in the system and no mechanical stop is provided, then the system may eventually be subjected to damage and fail, since the response of a real physical system cannot increase indefinitely.Therefore, closed-loop poles in the right-half $s$ plane are not permissible in the usual linear control system. If all closed-loop poles lie to the left of the $j\\omega$ axis, any transient response eventually reaches equilibrium. This represents a stable system.  \n\nWhether a linear system is stable or unstable is a property of the system itself and does not depend on the input or driving function of the system.The poles of the input, or driving function, do not affect the property of stability of the system, but they contribute only to steady-state response terms in the solution.Thus, the problem of absolute stability can be solved readily by choosing no closed-loop poles in the right-half $s$ plane, including the jvaxis. (Mathematically, closed-loop poles on the $j\\omega$ axis will yield oscillations, the amplitude of which is neither decaying nor growing with time. In practical cases, where noise is present, however, the amplitude of oscillations may increase at a rate determined by the noise power level. Therefore, a control system should not have closed-loop poles on the $j\\omega$ axis.)  \n\nNote that the mere fact that all closed-loop poles lie in the left-half $s$ plane does not guarantee satisfactory transient-response characteristics.If dominant complex-conjugate closed-loop poles lie close to the $j\\omega$ axis, the transient response may exhibit excessive oscillations or may be very slow.Therefore,to guarantee fast,yet well-damped,transientresponse characteristics, it is necessary that the closed-loop poles of the system lie in a particular region in the complex plane, such as the region bounded by the shaded area in Figure 5–17.  \n\nSince the relative stability and transient-response performance of a closed-loop control system are directly related to the closed-loop pole-zero configuration in the $s$ plane, it is frequently necessary to adjust one or more system parameters in order to obtain suitable configurations.The effects of varying system parameters on the closed-loop poles will be discussed in detail in Chapter 6.  \n\n![](images/9d51bdf03a32fdadd2706d4327cfbd0b0e3195702f651a5ce91eddd45a633941.jpg)  \nFigure 5–17 Region in the complex plane satisfying the conditions $\\zeta>0.4$ and $t_{s}<4/\\sigma$ .  \n\n# 5–5 TRANSIENT-RESPONSE ANALYSIS WITH MATLAB  \n\nIntroduction. The practical procedure for plotting time response curves of systems higher than second order is through computer simulation. In this section we present the computational approach to the transient-response analysis with MATLAB.In particular, we discuss step response,impulse response,ramp response,and responses to other simple inputs.  \n\nMATLAB Representation of Linear Systems. The transfer function of a system is represented by two arrays of numbers. Consider the system  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{2s\\,+\\,25}{s^{2}\\,+\\,4s\\,+\\,25}\n$$  \n\nThis system can be represented as two arrays, each containing the coefficients of the polynomials in decreasing powers of $s$ as follows:  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=[2\\ \\,25]}\\\\ {\\mathsf{d e n}=[1\\ \\ 4\\ \\ 25]}\\end{array}\n$$  \n\nAn alternative representation is  \n\n$$\n\\begin{array}{r}{\\mathsf{n u m}=\\left[0\\mathrm{~}2\\mathrm{~}25\\right]}\\\\ {\\mathsf{d e n}=\\left[1\\mathrm{~}4\\mathrm{~}\\,25\\right]}\\end{array}\n$$  \n\nIn this expression a zero is padded. Note that if zeros are padded, the dimensions of “num ” vector and “ den ” vector become the same.An advantage of padding zeros is that the “ num ” vector and “ den ” vector can be directly added. For example,  \n\n$$\n{\\begin{array}{r l}{\\mathsf{n u m+d e n}=[0\\ \\mathrm{~2~}\\ 25]+[1\\ \\mathrm{~4~}\\ 25]}\\\\ {=[1\\ \\mathrm{~6~}\\ 50]}\\end{array}}\n$$  \n\nIf num and den (the numerator and denominator of the closed-loop transfer function) are known, commands such as  \n\n$$\n\\mathrm{step(num,den)},\\quad\\mathrm{step(num,den,t)}\n$$  \n\nwill generate plots of unit-step responses ( tin the step command is the user-specified time.)  \n\nFor a control system defined in a state-space form, where state matrix A , control matrix B, output matrix C, and direct transmission matrix Dof state-space equations are known, the command  \n\n$$\n\\mathrm{step}(\\mathrm{A},\\mathrm{B},\\mathrm{C},\\mathrm{D}),\\quad\\mathrm{step}(\\mathrm{A},\\mathrm{B},\\mathrm{C},\\mathrm{D},\\mathrm{t})\n$$  \n\nwill generate plots of unit-step responses. When tis not explicitly included in the step commands, the time vector is automatically determined.  \n\nNote that the command step(sys) may be used to obtain the unit-step response of a system. First, define the system by  \n\n$$\n\\mathsf{s y s=t f(n u m,d e n)}\n$$  \n\nor  \n\n$$\n\\mathsf{S Y S}=\\mathsf{s s}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D})\n$$  \n\nThen, to obtain, for example, the unit-step response, enter  \n\ninto the computer.  \n\nWhen step commands have left-hand arguments such as  \n\n$$\n\\begin{array}{r l}&{\\mathrm{[y,x,t]}=\\mathsf{s t e p}(\\mathsf{n u m,d e n,t})}\\\\ &{\\mathrm{[y,x,t]}=\\mathsf{s t e p}(\\mathsf{A,B,C,D,i u})}\\\\ &{\\mathrm{[y,x,t]}=\\mathsf{s t e p}(\\mathsf{A,B,C,D,i u,t})}\\end{array}\n$$  \n\nno plot is shown on the screen. Hence it is necessary to use a plot command to see the response curves.The matrices yand $\\boldsymbol{\\mathrm{x}}$ contain the output and state response of the system, respectively, evaluated at the computation time points t. ( yhas as many columns as outputs and one row for each element in t. $\\boldsymbol{\\mathrm{x}}$ has as many columns as states and one row for each element in t. )  \n\nNote in Equation (5–36) that the scalar iu is an index into the inputs of the system and specifies which input is to be used for the response, and tis the user-specified time. If the system involves multiple inputs and multiple outputs, the step command, such as given by Equation (5–36), produces a series of step-response plots, one for each input and output combination of  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\n(For details, see Example 5–3.)  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\!\\!{\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}}\\!\\!\\right]}={\\left[\\!\\!{\\begin{array}{l l}{-1}&{-1}\\\\ {6.5}&{0}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+{\\left[\\!\\!{\\begin{array}{l l}{1}&{1}\\\\ {1}&{0}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}}\\right]}}\\\\ &{{\\left[\\!\\!{\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}}\\right]}={\\left[\\!\\!{\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+{\\left[\\!\\!{\\begin{array}{l l}{0}&{0}\\\\ {0}&{0}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}}\\right]}}\\end{array}}\n$$  \n\nObtain the unit-step response curves.  \n\nAlthough it is not necessary to obtain the transfer-matrix expression for the system to obtain the unit-step response curves with MATLAB, we shall derive such an expression for reference. For the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\nthe transfer matrix $\\mathbf{G}(s)$ is a matrix that relates $\\mathbf{Y}(s)$ and $\\mathbf{U}(s)$ as follows:  \n\n$$\n\\mathbf{Y}(s)\\,=\\,\\mathbf{G}(s)\\mathbf{U}(s)\n$$  \n\nTaking Laplace transforms of the state-space equations, we obtain  \n\n$$\n\\begin{array}{r}{s\\mathbf{X}(s)\\rightharpoonup\\mathbf{x}(0)=\\mathbf{A}\\mathbf{X}(s)+\\mathbf{B}\\mathbf{U}(s)}\\\\ {\\mathbf{Y}(s)=\\mathbf{C}\\mathbf{X}(s)+\\mathbf{D}\\mathbf{U}(s)}\\end{array}\n$$  \n\nIn deriving the transfer matrix, we assume that $\\mathbf{x}(0)=\\mathbf{0}.$ .Then, from Equation (5–37), we get  \n\n$$\n\\mathbf{X}(s)\\,=\\,(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\mathbf{U}(s)\n$$  \n\nSubstituting Equation (5–39) into Equation (5–38), we obtain  \n\n$$\n\\mathbf{Y}(s)\\,=\\,\\bigl[\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\,+\\,\\mathbf{D}\\bigr]\\mathbf{U}(s)\n$$  \n\nThus the transfer matrix $\\mathbf{G}(s)$ is given by  \n\n$$\n\\mathbf{G}(s)\\,=\\,\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\,+\\,\\mathbf{D}\n$$  \n\nThe transfer matrix $\\mathbf{G}(s)$ for the given system becomes  \n\n$$\n\\begin{array}{r l}{\\mathbf{G}(s)=\\mathbf{C}(s\\mathbf{I}-\\mathbf{A})^{-1}\\mathbf{B}}&{}\\\\ &{\\quad={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l l}{s+1}&{1}\\\\ {-6.5}&{s}\\end{array}\\right]}^{-1}{\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{0}\\end{array}\\right]}}\\\\ &{\\quad={\\frac{1}{s^{2}\\,+\\,s\\,+\\,6.5}}{\\left[\\begin{array}{l l}{s}&{-1}\\\\ {6.5}&{s+1}\\end{array}\\right]}{\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{0}\\end{array}\\right]}}\\\\ &{\\quad={\\frac{1}{s^{2}\\,+\\,s\\,+\\,6.5}}{\\left[\\begin{array}{l l}{s-1}&{s}\\\\ {s+7.5}&{6.5}\\end{array}\\right]}}\\end{array}\n$$  \n\nHence  \n\n$$\n\\begin{array}{r}{\\boxed{Y_{1}(s)}\\biggr]=\\left[\\begin{array}{c c c}{\\frac{s\\mathrm{~-~}1}{s^{2}\\mathrm{~+~}s\\mathrm{~+~}6.5}}&{\\frac{s}{s^{2}\\mathrm{~+~}s\\mathrm{~+~}6.5}}\\\\ {\\frac{s\\mathrm{~+~}7.5}{s^{2}\\mathrm{~+~}s\\mathrm{~+~}6.5}}&{\\frac{6.5}{s^{2}\\mathrm{~+~}s\\mathrm{~+~}6.5}}\\end{array}\\right]\\boxed{U_{1}(s)}\\biggr]}\\end{array}\n$$  \n\nSince this system involves two inputs and two outputs, four transfer functions may be defined, depending on which signals are considered as input and output. Note that, when considering the  \n\nsignal $u_{1}$ as the input, we assume that signal $u_{2}$ is zero, and vice versa.The four transfer functions are  \n\n$$\n\\begin{array}{l l}{\\displaystyle{\\frac{Y_{1}(s)}{U_{1}(s)}=\\frac{s\\ -1}{s^{2}\\ +{s+6.5}},\\quad}}&{\\displaystyle{\\frac{Y_{1}(s)}{U_{2}(s)}=\\frac{s}{s^{2}\\ +{s+6.5}}}}\\\\ {\\displaystyle{\\frac{Y_{2}(s)}{U_{1}(s)}=\\frac{s\\ +7.5}{s^{2}\\ +{s+6.5}},\\quad}}&{\\displaystyle{\\frac{Y_{2}(s)}{U_{2}(s)}=\\frac{6.5}{s^{2}\\ +{s+6.5}}}}\\end{array}\n$$  \n\nAssume that $u_{1}$ and $u_{2}$ are unit-step functions.The four individual step-response curves can then be plotted by use of the command  \n\nMATLAB Program 5–1 produces four such step-response curves.The curves are shown in Figure 5–18. (Note that the time vector tis automatically determined, since the command does not include t.)  \n\n![](images/a351f76691c39d91fcd6d63c32d8c1af08a5c9e3b448d408872f97c9747dc40a.jpg)  \n\n![](images/2764e236077e6169968a0a32b122b9102ea8a215cb4ccdfc6b19bd5d86a51c7e.jpg)  \nStep Response   \nFigure 5–18 Unit-step response curves.   \nChapter 5 /Transient and Steady-State Response Analyses  \n\nTo plot two step-response curves for the input $u_{1}$ in one diagram and two step-response curves for the input $u_{2}$ in another diagram, we may use the commands  \n\nand  \n\nrespectively. MATLAB Program 5–2 is a program to plot two step-response curves for the input $u_{1}$ in one diagram and two step-response curves for the input $u_{2}$ in another diagram. Figure 5–19 shows the two diagrams, each consisting of two step-response curves. (This MATLAB program uses text commands. For such commands, refer to the paragraph following this example.)  \n\n# MATLAB Program 5–2  \n\n![](images/48b82839b154574de102487c2215c85de839f41de5b05af252eea679de24b8b4.jpg)  \n\n![](images/176be58920b99c7a2189807e783b1186043f5d02c5af1fe6b0f9c0864b1a9a75.jpg)  \nFigure 5–19 Unit-step response curves. (a) input $\\left(u_{2}\\right)=0\\mathrm{.}$ $u_{1}$ A is the $u_{2}$ Bis the input ${\\bf\\Big(}u_{1}=0{\\bf\\Big)}$ .  \n\nWriting Text on the Graphics Screen. To write text on the graphics screen, enter, for example, the following statements:  \n\nand  \n\nThe first statement tells the computer to write ‘Y1’ beginning at the coordinates $x\\,=\\,3.4$ ,$y=-0.06.$ . Similarly, the second statement tells the computer to write $\\mathbf{\\hat{Y}}2^{\\bullet}$ beginning at the coordinates $x\\,=\\,3.4$ ,$y=1.4.$ . [See MATLAB Program 5–2 and Figure 5–19(a).]  \n\nAnother way to write a text or texts in the plot is to use the gtext command. The syntax is  \n\nWhen gtext is executed, the computer waits until the cursor is positioned (using a mouse) at the desired position in the screen. When the left mouse button is pressed, the text enclosed in simple quotes is written on the plot at the cursor’s position. Any number of gtext commands can be used in a plot. (See, for example, MATLAB Program 5–15.)  \n\n# MATLAB Description of Standard Second-Order System. As noted earlier,the second-order system  \n\n$$\nG(s)\\,=\\,\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}\n$$  \n\nis called the standard second-order system. Given $\\omega_{n}$ and $\\zeta$ , the command  \n\n$$\n\\mathrm{printsys(num,den)}\\qquad\\mathrm{or}\\qquad\\mathrm{printsys(num,den,s)}\n$$  \n\nprints num/den as a ratio of polynomials in $s$ .  \n\nConsider,for example,the case where $\\omega_{n}=5$ rad 'sec and $\\zeta=0.4.$ .MATLAB Program 5–3 generates the standard second-order system, where $\\omega_{n}\\,=\\,5$ rad 'sec and $\\zeta=0.4$ .Note that in MATLAB Program 5–3,“num $0^{\\circ}$ is 1.  \n\n![](images/63653dfe3c841904e0537605c28ac4d52e1a1c00f508cedc80a2eca0eb12034a.jpg)  \n\n# Obtaining the Unit-Step Response of the Transfer-Function System. Let us consider the unit-step response of the system given by  \n\n$$\nG(s)={\\frac{25}{s^{2}+4s\\,+\\,25}}\n$$  \n\nMATLAB Program 5–4 will yield a plot of the unit-step response of this system.A plot of the unit-step response curve is shown in Figure 5–20.  \n\n![](images/ae73e1a0ab4b330f2226db185e8427e7f5b933dded3b0754b469190602bec573.jpg)  \n\n![](images/6926aa72812784d0cc9ee6ea52827bd03bfb31630268ef0370032efa66fa653e.jpg)  \nFigure 5–20 Unit-step response curve.  \n\nNotice in Figure 5–20 (and many others) that the $x$ -axis and $y$ -axis labels are automatically determined. If it is desired to label the $x$ axis and $y$ axis differently, we need to modify the step command. For example, if it is desired to label the $x$ axis as $^{1}\\colon{\\mathsf{S e c}}^{\\prime}$ and the $y$ axis as ‘Output,’ then use step-response commands with left-hand arguments, such as  \n\n$$\n\\mathsf{c}=\\mathsf{s t e p}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{t})\n$$  \n\nor, more generally,  \n\n$$\n[\\mathsf{y},\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{t})\n$$  \n\nand use $\\mathrm{\\plot(t,\\boldsymbol{v})}$ command. See, for example, MATLAB Program 5–5 and Figure 5–21.  \n\n![](images/b938480015832eebf5fa2830ecc4b1b77db0364eeccd1a3427f8d211c4597115.jpg)  \n\n![](images/2f4819deeb598e8530306adb11b9c15ce7e5b0ce7ef2ee0ca425f0cd14d3e553.jpg)  \nFigure 5–21 Unit-step response curve.   \nObtaining Three-Dimensional Plot of Unit-Step Response Curves with MATLAB. MATLAB enables us to plot three-dimensional plots easily.The commands to obtain three-dimensional plots are “mesh” and “surf.” The difference between the “mesh” plot and “surf” plot is that in the former only the lines are drawn and in the latter the spaces between the lines are filled in by colors. In this book we use only the “mesh” command.  \n\nEXAMPLE 5–4 Consider the closed-loop system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{1}{s^{2}\\,+\\,2\\zeta s\\,+\\,1}}\n$$  \n\n(The undamped natural frequency $\\omega_{n}$ is normalized to 1.) Plot unit-step response curves $c(t)$ when $\\zeta$ assumes the following values:  \n\n$$\n\\zeta=0,\\:0.2,\\:0.4,\\:0.6.\\:0.8,\\:1.0\n$$  \n\nAlso plot a three-dimensional plot.  \n\nAn illustrative MATLAB Program for plotting a two-dimensional diagram and a threedimensional diagram of unit-step response curves of this second-order system is given in MATLAB Program 5–6. The resulting plots are shown in Figures 5–22(a) and (b), respectively. Notice that we used the command mesh(t,zeta,y') to plot the three-dimensional plot.We may use a command mesh(y') to get the same result. [Note that command mesh(t,zeta,y) or mesh(y) will produce a three-dimensional plot the same as Figure 5–22(b), except that $x$ axis and $y$ axis are interchanged. See Problem A–5–15 .]  \n\nWhen we want to solve a problem using MATLAB and if the solution process involves many repetitive computations, various approaches may be conceived to simplify the MATLAB program.A frequently used approach to simplify the computation is to use “for loops.”MATLAB Program 5–6 uses such a “for loop.” In this book many MATLAB programs using “for loops” are presented for solving a variety of problems. Readers are advised to study all those problems carefully to familiarize themselves with the approach.  \n\n# MATLAB Program 5–6  \n\n$\\%$ ------- Two-dimensional plot and three-dimensional plot of unit-step   \n$\\%$ response curves for the standard second-order system with wn $\\l=\\,1$   \n$\\%$ and zeta $=0$ , 0.2, 0.4, 0.6, 0.8, and 1. -------   \n$t=0{:}0.2{:}10$ ;  \nzeta = [0 0.2 0.4 0.6 0.8 1];   \nfor $\\mathsf{n}=1:6$ ;  \nnum $\\boldsymbol{\\mathsf{l}}=\\left[\\boldsymbol{\\mathsf{l}}\\right]$ ;  \n$\\mathsf{d e n}=[1\\quad2^{*}z e t a(n)\\quad1]$ 1];   \n$\\lbrack\\mathsf{y}(1:51\\,,\\mathsf{n}),\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{t});$   \nend   \n$\\%$ To plot a two-dimensional diagram, enter the command plot(t,y).   \nplot(t,y)   \ngrid   \ntitle('Plot of Unit-Step Response Curves with \\omega_n $\\l=1$ and \\zeta $=0$ , 0.2, 0.4, 0.6, 0.8, 1') xlabel('t (sec)')   \nylabel('Response')   \n$\\operatorname{text}(4.1\\,,1\\,.86,\"\\!\\!\\nabla\\mathrm{eta}=0^{\\prime})$   \ntext(3.5,1.5,'0.2')   \ntext(3 .5,1.24,'0.4')   \ntext(3.5,1.08,'0.6')   \ntext(3.5,0.95,'0.8')   \ntext(3.5,0.86,'1.0')   \n$\\%$ To plot a three-dimensional diagram, enter the command mesh(t,zeta,y').   \nmesh(t,zeta,y')   \ntitle('Three-Dimensional Plot of Unit-Step Response Curves')   \nxlabel('t Sec')   \nylabel('\\zeta')   \nzlabel('Response')  \n\n![](images/f9e74c80055243a283aebca2095cfb30d476cf1ae2e396e5347852b852d42b17.jpg)  \nThree-Dimensional Plot of Unit-Step Response Curves  \n\n![](images/f8293372414f48ea131f4375ce861634f8af9cc43602d3470459f89754dbef0a.jpg)  \nFigure 5–22 (a) Two-dimensional plot of unit-step response curves for $\\zeta=0,0.2,0.4,0.6,0.8$ ,and 1.0; (b) threedimensional plot of unit-step response curves.  \n\nObtaining Rise Time, Peak Time, Maximum Overshoot, and Settling Time with MATLAB. MATLAB can conveniently be used to obtain the rise time,peak time, maximum overshoot, and settling time. Consider the system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{25}{s^{2}\\,+\\,6s\\,+\\,25}}\n$$  \n\nMATLAB Program 5–7 yields the rise time,peak time,maximum overshoot,and settling time. A unit-step response curve for this system is given in Figure 5–23 to verify the  \n\nresults obtained with MATLAB Program 5–7. (Note that this program can also be applied to higher-order systems. See Problem A–5–10 .)  \n\n![](images/65b3b13bbe6c5e2038ca2459bb14edd3c15cc8e328d6b3bbdd51a5b9c758949d.jpg)  \n\n![](images/c027756ad494d38c8030dbe2a9d6379f29944904be78085f08c42f7af306881d.jpg)  \nFigure 5–23 Unit-step response curve.  \n\nImpulse Response. The unit-impulse response of a control system may be obtained by using any of the impulse commands such as  \n\n$$\n\\begin{array}{l}{\\{\\mathrm{y},\\mathrm{x},\\mathrm{t}\\}=\\mathsf{i m p u l s e}(\\mathsf{n u m},\\mathsf{d e n})}\\\\ {\\ }\\\\ {\\ }\\\\ {\\ }\\\\ {\\ }\\\\ {\\{\\mathrm{[y},\\mathrm{x},\\mathrm{t}]}=\\mathsf{i m p u l s e}(\\mathsf{n u m},\\mathsf{d e n},\\mathrm{t})}\\\\ {\\ }\\\\ {\\ }\\\\ {\\ }\\\\ {\\ }\\\\ {\\ }\\end{array}\n$$  \n\nThe command impulse(num,den) plots the unit-impulse response on the screen. The command impulse(A,B,C,D) produces a series of unit-impulse-response plots, one for each input and output combination of the system  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}\\mathbf{u}}\\\\ {\\quad}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}+\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\nNote that in Equations (5–42) and (5–43) the scalar iu is an index into the inputs of the system and specifies which input to be used for the impulse response.  \n\nNote also that if the command used does not include “ t” explicitly, the time vector is automatically determined. If the command includes the user-supplied time vector “ t”, as do the commands given by Equations (5–41) and (5–43)], this vector specifies the times at which the impulse response is to be computed.  \n\nIf MATLAB is invoked with the left-hand argument $[\\upgamma,\\uptimes,\\up t]$ , such as in the case of $\\mathrm{[y,x,t]=}$ impulse(A,B,C,D) , the command returns the output and state responses of the system and the time vector t. No plot is drawn on the screen.The matrices yand $\\boldsymbol{\\mathrm{x}}$ contain the output and state responses of the system evaluated at the time points t. ( yhas as many columns as outputs and one row for each element in t.xhas as many columns as state variables and one row for each element in t.) To plot the response curve, we must include a plot command, such as plot (t,y ).  \n\nObtain the unit-impulse response of the following system:  \n\n$$\n\\frac{C(s)}{R(s)}=G(s)\\,=\\,\\frac{1}{s^{2}\\,+\\,0.2s\\,+\\,1}\n$$  \n\nMATLAB Program 5–8 will produce the unit-impulse response. The resulting plot is shown in Figure 5–24.  \n\n![](images/7c6907531bf139db7855d30428bc5558d8d178ef5ef8f0793ffc959ecdb779a4.jpg)  \n\nAlternative Approach to Obtain Impulse Response. Note that when the initial conditions are zero, the unit-impulse response of $G(s)$ is the same as the unit-step response of $s G(s)$ .  \n\nConsider the unit-impulse response of the system considered in Example 5–5. Since $R(s)\\,=\\,1$ for the unit-impulse input, we have  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{C(s)}{R(s)}=C(s)=G(s)=\\frac{1}{s^{2}\\,+\\,0.2s\\,+\\,1}}}\\\\ {\\displaystyle{=\\frac{s}{s^{2}\\,+\\,0.2s\\,+\\,1}\\frac{1}{s}}}\\end{array}\n$$  \n\nWe can thus convert the unit-impulse response of $G(s)$ to the unit-step response of $s G(s)$ .  \n\nIf we enter the following num and den into MATLAB,  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=[0\\ \\mathsf{\\ 1}\\ \\mathsf{\\ 0}]}\\\\ {\\mathsf{d e n}=[1\\ \\ 0.2\\ \\mathsf{\\ 1}]}\\end{array}\n$$  \n\nand use the step-response command; as given in MATLAB Program 5–9, we obtain a plot of the unit-impulse response of the system as shown in Figure 5–25.  \n\n![](images/947c98a052b73eaea0bd0adc19d1b56f46c97754dcce035b28395018c6983d05.jpg)  \n\n![](images/574932d0481fc4fd2dae6d750f673fe39793d546405186107548025e286d9806.jpg)  \nFigure 5–25 Unit-impulseresponse curve obtained as the unitstep response of $s G(s)=$ $s/\\bigl(s^{2}\\,+\\,0.2s\\,+\\,1\\bigr).$ .  \n\nRamp Response. There is no ramp command in MATLAB. Therefore, we need to use the step command or the lsim command (presented later) to obtain the ramp response. Specifically, to obtain the ramp response of the transfer-function system $G(s)$ ,divide $G(s)$ by $s$ and use the step-response command. For example, consider the closedloop system  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{2s\\,+\\,1}{s^{2}\\,+\\,s\\,+\\,1}}\n$$  \n\nFor a unit-ramp input, $R(s)\\,=\\,1/s^{2}$ . Hence  \n\n$$\nC(s)\\,=\\frac{2s\\,+\\,1}{s^{2}\\,+\\,s\\,+\\,1}\\frac{1}{s^{2}}=\\frac{2s\\,+\\,1}{(s^{2}\\,+\\,s\\,+\\,1)s}\\frac{1}{s}\n$$  \n\nTo obtain the unit-ramp response of this system, enter the following numerator and denominator into the MATLAB program:  \n\n$$\n\\begin{array}{l}{{\\mathsf{n u m}=[2\\ \\ 1];}}\\\\ {{\\mathsf{d e n}=[1\\ \\ 1\\ \\ 1\\ \\ 0];}}\\end{array}\n$$  \n\nand use the step-response command. See MATLAB Program 5–10. The plot obtained by using this program is shown in Figure 5–26.  \n\n![](images/2bd9eff1a9fbcc262c002a474c5b5433b70073102e631f9365e5a2b2cf967309.jpg)  \n\n![](images/32d77266ef29ca6f132166e86f4d871f3405ba8bed6912eeb04229b0b2337467.jpg)  \nFigure 5–26 Unit-ramp response curve.  \n\nUnit-Ramp Response of a System Defined in State Space. Next, we shall treat the unit-ramp response of the system in state-space form.Consider the system described by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {y=\\mathbf{C}\\mathbf{x}+D u}\\end{array}\n$$  \n\nwhere $u$ is the unit-ramp function. In what follows, we shall consider a simple example to explain the method. Consider the case where  \n\n$$\n\\begin{array}{r l r l}&{\\mathbf{A}=\\left[\\begin{array}{l l}{\\phantom{-}0}&{\\phantom{-}1}\\\\ {-\\,1}&{-\\,1}\\end{array}\\right],}&&{\\mathbf{B}=\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right],}&&{\\mathbf{x}(0)=\\mathbf{0}}\\\\ &{\\mathbf{C}=[1}&{0],}&&{D=[0]}\\end{array}\n$$  \n\nWhen the initial conditions are zeros, the unit-ramp response is the integral of the unitstep response. Hence the unit-ramp response can be given by  \n\n$$\nz\\,=\\,\\int_{0}^{t}y\\,d t\n$$  \n\nFrom Equation (5–44), we obtain  \n\n$$\n{\\dot{z}}\\,=\\,y\\,=\\,x_{1}\n$$  \n\nLet us define  \n\n$$\nz=x_{3}\n$$  \n\nThen Equation (5–45) becomes  \n\n$$\n\\dot{x}_{3}\\,=\\,x_{1}\n$$  \n\nCombining Equation (5–46) with the original state-space equation, we obtain  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l l}{\\;\\,0}&{\\;\\,1}&{0}\\\\ {-\\,1}&{-\\,1}&{0}\\\\ {\\;\\,1}&{\\;\\,0}&{0}\\end{array}\\right]}{\\left[\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]\\!\\!\\right]}+{\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\\\ {0}\\end{array}\\right]}u}\\\\ {z={\\left[\\!\\!\\left[\\begin{array}{l l l}{\\;\\,0}&{0}&{1}\\\\ {0}&{0}&{2}\\end{array}\\right]}{\\left[\\!\\!\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nwhere $u$ appearing in Equation (5–47) is the unit-step function.These equations can be written as  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{A}\\mathbf{x}+\\mathbf{B}\\mathbf{B}u}\\\\ {z=\\mathbf{C}\\mathbf{C}\\mathbf{x}+D D u}\\end{array}\n$$  \n\nwhere  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{AA}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {-1}&{-1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l}{\\mathbf{A}}\\\\ {\\mathbf{\\Delta}}\\\\ {\\cdots}\\\\ {\\mathbf{C}}\\end{array}\\right]}}\\\\ &{\\mathbf{BB}={\\left[\\begin{array}{l}{0}\\\\ {1}\\\\ {0}\\end{array}\\right]}={\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]},\\qquad\\mathbf{CC}=[0}&{0}&{1],\\qquad D D=[0]}\\end{array}}\n$$  \n\nNote that $x_{3}$ is the third element of x. A plot of the unit-ramp response curve $z(t)$ can be obtained by entering MATLAB Program 5–11 into the computer.A plot of the unitramp response curve obtained from this MATLAB program is shown in Figure 5–27.  \n\n$\\%$ --------------- Unit-ramp response ---------------   \n$\\%$ \\*\\*\\*\\*\\* The unit-ramp response is obtained by adding a new   \n$\\%$ state variable $\\times3$ . The dimension of the state equation   \n$\\%$ is enlarged by one \\*\\*\\*\\*\\*   \n$\\%$ \\*\\*\\*\\*\\* Enter matrices A, B, C, and D of the original state   \n$\\%$ equation and output equation \\*\\*\\*\\*\\*   \n$\\mathsf{A}=\\left[\\boldsymbol{0}\\;\\;\\;1\\;;-1\\;\\;\\;-1\\right];$   \n$\\mathsf{B}=\\left[0;\\;\\;1\\right]$ ;  \n$C=[1\\ \\ 0]$ ;  \n$\\mathsf{D}=[0]$ ;  \n$\\%$ \\*\\*\\*\\*\\* Enter matrices AA, BB, CC, and DD of the new,   \n$\\%$ enlarged state equation and output equation \\*\\*\\*\\*\\*   \n$\\mathsf{A A}=\\left[\\mathsf{A\\ z e r o s}(2,1);\\!\\!\\mathsf{C\\ \\ 0}\\right],$   \n$\\mathtt{B B}=\\mathtt{[B;O]}$ ;  \n$C C=[0\\;\\mathrm{~0~}\\;1]$ ;  \n$\\mathrm{DD}=[0]$ ;  \n$\\%$ \\*\\*\\*\\*\\* Enter step-response command: $\\left[{\\cal Z},\\!{\\cal X},\\!{\\sf t}\\right]=$ step(AA,BB,CC,DD) \\*\\*\\*\\*\\* $[z,\\mathrm{x,t}]=\\mathrm{step}(\\mathrm{AA,BB,CC,DD});$   \n$\\%$ \\*\\*\\*\\*\\* In plotting $\\times3$ add the unit-ramp input t in the plot   \n$\\%$ by entering the following command: $\\mathsf{p l o t}(\\mathrm{t},\\mathsf{x}3,\\mathsf{^1O}^{\\prime},\\mathrm{t},\\mathsf{t},\\mathsf{^1})$ \\*\\*\\*\\*\\*   \n$\\times3=[0\\ \\mathrm{~0~}\\ 1]^{*}\\mathrm{x}^{\\prime};\\mathrm{plot(t,\\mathrm{x}3,^{\\prime}\\mathrm{o}^{\\prime},\\mathrm{t},\\mathrm{t},^{\\prime}-^{\\prime})}$   \ngrid   \ntitle('Unit-Ramp Response')   \nxlabel('t Sec')   \nylabel('Input and Output')  \n\n![](images/db3ba768091c25ab5322a605709399ab42df793ec1d62c15e024be6e6c41e2e0.jpg)  \nFigure 5–27 Unit-ramp response curve.  \n\nObtaining Response to Arbitrary Input. To obtain the response to an arbitrary input, the command lsim may be used.The commands like  \n\n$$\n\\begin{array}{r l}&{\\mathsf{l s i m}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{r},\\mathsf{t})}\\\\ &{\\mathsf{l s i m}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{u},\\mathsf{t})}\\\\ &{\\mathsf{y}=\\mathsf{l s i m}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{r},\\mathsf{t})}\\\\ &{\\mathsf{y}=\\mathsf{l s i m}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{u},\\mathsf{t})}\\end{array}\n$$  \n\nwill generate the response to input time function ror u. See the following two examples. (Also, see Problems A–5–14 through A–5–16 .)  \n\n# EXAMPLE 5–6  \n\nUsing the lsim command, obtain the unit-ramp response of the following system:  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{2s\\,+\\,1}{s^{2}\\,+\\,s\\,+\\,1}}\n$$  \n\nWe may enter MATLAB Program 5–12 into the computer to obtain the unit-ramp response.The resulting plot is shown in Figure 5–28.  \n\n![](images/2b74c48cdf90c3d3db0eee93de170a03968f04b1d27def0573cab95d3ee2bf7c.jpg)  \n\n![](images/f172b9e912bdc25a4065fa1df28dac333a9f7a5c7767a2a584753711fbac5182.jpg)  \nFigure 5–28 Unit-ramp response.  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l}{-\\,1}&{0.5}\\\\ {-\\,1}&{0}\\end{array}\\right]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+{\\left[\\!\\!{\\begin{array}{l}{0}\\\\ {1}\\end{array}}\\right]}\\,u}\\\\ {y={[\\!\\!1\\!\\!}}&{0{]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}}\\end{array}}\n$$  \n\nUsing MATLAB, obtain the response curves $y(t)$ when the input $u$ is given by  \n\n1. $u=$ unit-step input  \n\n2. $u=e^{-t}$  \n\nAssume that the initial state is $\\mathbf{x}(0)=\\mathbf{0}$ .  \n\nA possible MATLAB program to produce the responses of this system to the unit-step input $\\bar{[u=1(t)]}$ and the exponential input $\\bar{[u=e^{-t}]}$ is shown in MATLAB Program 5–13.The resulting response curves are shown in Figures 5–29(a) and (b), respectively.  \n\n![](images/2891dc8486fe1cd15cbe0f97afcca2e25743bc07e1a9e26e20f431f3f643dd2e.jpg)  \n\n![](images/bce225191151ef9491cb129535bdd09155845e4464a56e1d23bcdf1c1560d1ee.jpg)  \nFigure 5–29 (a) Unit-step response; (b) response to input $u=e^{-t}$ .  \n\nResponse to Initial Condition. In what follows we shall present a few methods for obtaining the response to an initial condition. Commands that we may use are “step” or “initial”. We shall first present a method to obtain the response to the initial condition using a simple example.Then we shall discuss the response to the initial condition when the system is given in state-space form. Finally, we shall present a command initial to obtain the response of a system given in a state-space form.  \n\n![](images/b9feca5abb9e19f6f52226038772a935cf9cc1f1dfbac5aa89971137eec8fd81.jpg)  \n\nFigure 5–30 Mechanical system.  \n\nConsider the mechanical system shown in Figure 5–30, where $m\\,{=}\\,1\\;\\mathrm{kg},b\\,{=}\\,3\\;\\mathrm{N{-sec/m}}$ , and $k\\,=\\,2\\;\\mathrm{N/m}$ $\\dot{x}(0)\\,=\\,0.05\\,\\mathrm{m/sec}.$ #. Assume that at .The displacement $t\\,=\\,0$ the mass $x(t)$ is measured from the equilibrium position before the $_m$ is pulled downward such that $x(0)=0.1\\;\\mathrm{m}$ and mass is pulled down. Obtain the motion of the mass subjected to the initial condition. (Assume no external forcing function.)  \n\nThe system equation is  \n\n$$\nm{\\ddot{x}}\\,+\\,b{\\dot{x}}\\,+\\,k x\\,=\\,0\n$$  \n\nwith the initial conditions $x(0)\\,=\\,0.1\\,\\mathrm{{m}}$ and $\\dot{x}(0)\\,=\\,0.05\\;\\mathrm{m/sec}.$ .($x$ is measured from the equilibrium position.) The Laplace transform of the system equation gives  \n\nor  \n\n$$\n\\begin{array}{c}{{m\\big[s^{2}X(s)\\,-\\,s x(0)\\,-\\,\\dot{x}(0)\\big]\\,+\\,b\\big[s X(s)\\,-\\,x(0)\\big]\\,+\\,k X(s)\\,=\\,0}}\\\\ {{}}\\\\ {{\\big(m s^{2}\\,+\\,b s\\,+\\,k\\big)X(s)\\,=\\,m x(0)s\\,+\\,m\\dot{x}(0)\\,+\\,b x(0)}}\\end{array}\n$$  \n\nSolving this last equation for $X(s)$ and substituting the given numerical values, we obtain  \n\n$$\n\\begin{array}{c}{{X(s)=\\displaystyle\\frac{m x(0)s\\,+\\,m\\dot{x}(0)\\,+\\,b x(0)}{m s^{2}\\,+\\,b s\\,+\\,k}}}\\\\ {{=\\displaystyle\\frac{0.1s\\,+\\,0.35}{s^{2}\\,+\\,3s\\,+\\,2}}}\\end{array}\n$$  \n\nThis equation can be written as  \n\n$$\nX(s)={\\frac{0.1s^{2}+0.35s}{s^{2}+3s+2}}{\\frac{1}{s}}\n$$  \n\nHence the motion of the mass $_m$ may be obtained as the unit-step response of the following system:  \n\n$$\nG(s)=\\frac{0.1s^{2}\\,+\\,0.35s}{s^{2}\\,+\\,3s\\,+\\,2}\n$$  \n\nMATLAB Program 5–14 will give a plot of the motion of the mass.The plot is shown in Figure 5–31.  \n\n![](images/681dca06f91631ca2aafd9ce5d729f926779d1025423d199649b6c85a6909e1e.jpg)  \n\n![](images/66af74bc27d4af1cf98e3087772d187dcf90101471eeaf6f11c5fce7ad10a7b5.jpg)  \nFigure 5–31 Response of the mechanical system considered in Example 5–8.  \n\nResponse to Initial Condition (State-Space Approach, Case 1). Consider the system defined by  \n\n$$\n{\\dot{\\mathbf{x}}}\\,=\\,\\mathbf{A}\\mathbf{x},\\qquad\\mathbf{x}(0)\\,=\\,\\mathbf{x}_{0}\n$$  \n\nLet us obtain the response ${\\bf x}(t)$ when the initial condition ${\\bf x}(0)$ is specified.Assume that there is no external input function acting on this system.Assume also that $\\mathbf{X}$ is an $n$ -vector. First, take Laplace transforms of both sides of Equation (5–49).  \n\n$$\ns\\mathbf{X}(s)\\,-\\,\\mathbf{x}(0)\\,=\\,\\mathbf{A}\\mathbf{X}(s)\n$$  \n\nThis equation can be rewritten as  \n\n$$\ns\\mathbf{X}(s)=\\mathbf{A}\\mathbf{X}(\\mathbf{s})\\,+\\,\\mathbf{x}(0)\n$$  \n\nTaking the inverse Laplace transform of Equation (5–50), we obtain  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{x}(0)\\,\\delta(t)\n$$  \n\n(Notice that by taking the Laplace transform of a differential equation and then by taking the inverse Laplace transform of the Laplace-transformed equation we generate a differential equation that involves the initial condition.)  \n\nNow define  \n\n$$\n\\dot{\\textbf{z}}=\\textbf{x}\n$$  \n\nThen Equation (5–51) can be written as  \n\n$$\n\\ddot{\\mathbf{z}}\\,=\\,\\mathbf{A}\\dot{\\mathbf{z}}\\,+\\,\\mathbf{x}(0)\\,\\delta(t)\n$$  \n\nBy integrating Equation (5–53) with respect to $t$ , we obtain  \n\n$$\n\\dot{\\mathbf{z}}=\\mathbf{A}\\mathbf{z}+\\mathbf{x}(0)\\mathbf{1}(t)\\,=\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{B}=\\mathbf{x}(0),\\qquad u=1(t)\n$$  \n\nReferring to Equation (5–52), the state ${\\bf x}(t)$ is given by ${\\dot{\\mathbf{z}}}(t)$ #Thus,  \n\n$$\n\\mathbf{x}={\\dot{\\mathbf{z}}}=\\mathbf{A}\\mathbf{z}+\\mathbf{B}u\n$$  \n\nThe solution of Equations (5–54) and (5–55) gives the response to the initial condition. Summarizing,the response of Equation (5–49) to the initial condition ${\\bf x}(0)$ is obtained by solving the following state-space equations:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}\\,=\\,\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u}\\\\ {\\mathbf{x}\\,=\\,\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{B}=\\mathbf{x}(0),\\qquad u=1(t)\n$$  \n\nMATLAB commands to obtain the response curves, where we do not specify the time vector t(that is, we let the time vector be determined automatically by MATLAB), are given next.  \n\n$$\n[\\mathrm{x,}z,\\mathrm{t}]=\\mathrm{step}(\\mathrm{A,B,A,B});\n$$  \n\n$$\n\\times1\\,=\\,[1\\;\\;\\;0\\;\\;0\\;...\\;0]^{*}\\times\\,^{\\prime};\n$$  \n\n$$\n\\times2=[0\\;\\;1\\;\\;0\\ldots0]^{*}\\times\";\n$$  \n\n$$\n\\begin{array}{l}{\\mathsf{x n}=[0\\;\\;0\\;\\;0\\ldots\\;\\mathsf{1}]^{*}\\mathsf{x}^{1};}\\\\ {\\mathsf{p l o t}(\\mathsf{t},\\mathsf{x}\\mathsf{1}\\,,\\mathsf{t},\\mathsf{x}2,\\ldots\\;\\mathsf{t},\\mathsf{x}\\mathsf{n})}\\end{array}\n$$  \n\nIf we choose the time vector t(for example, let the computation time duration be from $\\mathrm{t}=0$ to $\\mathrm{t}=$ tp with the computing time increment of $\\Delta$ t), then we use the following MATLAB  commands:  \n\n$$\n\\mathrm{{t=0:\\Delta\\!\\Omega\\!:\\,tp;}}\n$$  \n\n$$\n[\\mathrm{x,}\\mathrm{z,t}]=\\mathrm{step}(\\mathrm{A,B,A,B,1,t});\n$$  \n\n$$\n\\times1\\,=\\,[1\\;\\;\\;0\\;\\;0\\;...\\;0]^{*}\\times^{\\prime};\n$$  \n\n$$\n\\times2=[0\\;\\;1\\;\\;0\\ldots0]^{*}\\times\";\n$$  \n\n$$\n\\begin{array}{l}{\\mathsf{x n}=[0\\;\\;0\\;\\;0\\ldots\\;\\mathsf{1}]^{*}\\mathsf{x}^{1};}\\\\ {\\mathsf{p l o t}(\\mathsf{t},\\mathsf{x}\\mathsf{1}\\,,\\mathsf{t},\\mathsf{x}2,\\ldots\\;\\mathsf{t},\\mathsf{x}\\mathsf{n})}\\end{array}\n$$  \n\n(See, for example, Example 5–9.)  \n\nResponse to Initial Condition (State-Space Approach, Case 2). Consider the system defined by  \n\n$$\n\\begin{array}{r l}&{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x},\\qquad\\mathbf{x}(0)\\,=\\,\\mathbf{x}_{0}}\\\\ &{}\\\\ &{\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\n(Assume that $\\mathbf{X}$ is an $n$ -vector and $\\mathbf{y}$ is an $_m$ -vector.) Similar to case 1, by defining  \n\n$$\n\\dot{\\textbf{z}}=\\textbf{x}\n$$  \n\nwe can obtain the following equation:  \n\n$$\n\\dot{\\mathbf{z}}=\\mathbf{A}\\mathbf{z}+\\mathbf{x}(0)\\mathbf{1}(t)\\,=\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{B}=\\mathbf{x}(0),\\qquad u=1(t)\n$$  \n\nNoting that $\\mathbf{x}={\\dot{\\mathbf{z}}}$ #,Equation (5–57) can be written as  \n\n$$\n\\mathbf{y}\\,=\\,\\mathbf{C}\\dot{\\mathbf{z}}\n$$  \n\nBy substituting Equation (5–58) into Equation (5–59), we obtain  \n\n$$\n\\mathbf{y}=\\mathbf{C}(\\mathbf{A}\\mathbf{z}+\\mathbf{B}u)=\\mathbf{C}\\mathbf{A}\\mathbf{z}+\\mathbf{C}\\mathbf{B}u\n$$  \n\nThe solution of Equations (5–58) and (5–60), rewritten here  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}=\\mathbf{A}\\mathbf{z}+\\mathbf{B}u}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{A}\\mathbf{z}+\\mathbf{C}\\mathbf{B}u}\\end{array}\n$$  \n\nwhere $\\mathbf{B}=\\mathbf{x}(0)$ and $u=1(t)$ ,gives the response of the system to a given initial condition. MATLAB commands to obtain the response curves (output curves y1 versus t, y2 versus ${\\mathrm{~t~.~.~}}\\,,$ ym versus t) are shown next for two cases:  \n\nCase A. When the time vector t is not specified (that is, the time vector tis to be determined automatically by MATLAB):  \n\n$$\n[\\mathrm{y,}\\mathrm{z,}\\mathrm{t}]=\\mathrm{step}(\\mathsf{A,B,C^{*}A,C^{*}B});\n$$  \n\n$$\n\\mathsf{y}\\mathsf{1}=[\\mathsf{1}\\;\\;0\\;\\;0\\;\\ldots\\;0]^{*}\\mathsf{y}^{\\prime};\n$$  \n\n$$\n\\forall2=[0\\;\\;1\\;\\;0\\ldots0]^{*}\\forall^{\\prime};\n$$  \n\n$$\n\\begin{array}{r l}&{\\mathsf{y m}=[0\\;\\;0\\;\\;0\\ldots\\;1]^{*}\\mathsf{y}^{1};}\\\\ &{\\mathsf{p l o t}(\\mathsf{t},\\mathsf{y}1,\\mathsf{t},\\mathsf{y}2,\\ldots\\;,\\mathsf{t},\\mathsf{y m})}\\end{array}\n$$  \n\nCase B. When the time vector t is specified:  \n\n$$\n[\\mathsf{y},\\mathsf{z},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{A},\\mathsf{B},\\mathsf{C}^{*}\\mathsf{A},\\mathsf{C}^{*}\\mathsf{B},1\\,,\\mathsf{t})\n$$  \n\n$$\n\\mathsf{y}\\mathsf{1}=[\\mathsf{1}\\;\\;0\\;\\;0\\;\\ldots\\;0]^{*}\\mathsf{y}^{\\prime};\n$$  \n\n$$\n\\forall2=[0\\;\\;1\\;\\;0\\ldots0]^{*}\\forall^{\\prime};\n$$  \n\n$$\n\\begin{array}{r}{\\mathsf{y m}=[0\\;\\;0\\;\\;0\\;\\ldots\\;1]^{*}\\mathsf{y}^{1};}\\\\ {\\mathsf{p l o t}(\\mathsf{t},\\mathsf{y}1,\\mathsf{t},\\mathsf{y}2,\\ldots\\;,\\mathsf{t},\\mathsf{y m})}\\end{array}\n$$  \n\nEXAMPLE 5–9 Obtain the response of the system subjected to the given initial condition.  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\;\\;\\ 0}&{\\;\\;\\;1}\\\\ {-\\;10}&{-\\;5}{\\left]}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]},\\quad{\\left[\\!\\!{\\begin{array}{l}{x_{1}(0)}\\\\ {x_{2}(0)}\\end{array}}\\right]}={\\left[\\!\\!{\\begin{array}{l}{2}\\\\ {1}\\end{array}}\\right]}\n$$  \n\nor  \n\n$$\n\\begin{array}{r}{\\dot{\\bf x}\\,=\\,{\\bf A}{\\bf x},\\qquad{\\bf x}(0)\\,=\\,{\\bf x}_{0}}\\end{array}\n$$  \n\nObtaining the response of the system to the given initial condition resolves to solving the unit-step response of the following system:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}\\,=\\,\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u}\\\\ {\\mathbf{x}\\,=\\,\\mathbf{A}\\mathbf{z}\\,+\\,\\mathbf{B}u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{B}=\\mathbf{x}(0),\\qquad u=1(t)\n$$  \n\nHence a possible MATLAB program for obtaining the response may be given as shown in MATLAB Program 5–15.The resulting response curves are shown in Figure 5–32.  \n\n![](images/3c8296f94e88367e9efbd45d1941a93f266fc5fcc98bbb0b09281df8cb468e47.jpg)  \n\n![](images/3ae55bc64c7f1bfb2ff7513595785b4760329f016b68617c1b8700fcf2668c87.jpg)  \nFigure 5–32 Response of system in Example 5–9 to initial condition.  \n\nFor an illustrative example of how to use Equations (5–58) and (5–60) to find the response to the initial condition, see Problem A–5–16 .  \n\nObtaining Response to Initial Condition by Use of Command Initial .If the system is given in the state-space form, then the following command  \n\nwill produce the response to the initial condition. Suppose that we have the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u,\\qquad\\mathbf{x}(0)\\,=\\,\\mathbf{x}_{0}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,D u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\begin{array}{r l r l r l}&{\\mathbf{A}=\\left[\\begin{array}{c c}{0}&{1}\\\\ {-\\,10}&{-\\,5}\\end{array}\\right],}&&{\\mathbf{B}=\\left[\\begin{array}{c}{0}\\\\ {0}\\end{array}\\right],}&&{\\mathbf{C}=[0}&{0],}&&{D=0}\\\\ &{}\\\\ &{\\mathbf{x}_{0}=\\left[\\begin{array}{c}{2}\\\\ {1}\\end{array}\\right]}&&{\\mathbf{B}=\\left[\\begin{array}{c}{\\mathbf{\\bar{b}}}\\\\ {0}\\end{array}\\right].}\\end{array}\n$$  \n\nThen the command “initial” can be used as shown in MATLAB Program 5–16 to obtain the response to the initial condition. The response curves $x_{1}(t)$ and $x_{2}(t)$ are shown in Figure 5–33.They are the same as those shown in Figure 5–32.  \n\n![](images/235cd33bae66563ed8247d9181c7d1bf53aa2e6dbd58d385954bb64674cda7e3.jpg)  \n\n![](images/d433806c4638ec64be64e5ca8a35c119c58207d89085b43b6edde55bd830db06.jpg)  \n\nFigure 5–33 Response curves to initial condition.  \n\n# EXAMPLE 5–10  \n\nConsider the following system that is subjected to the initial condition. (No external forcing function is present.)  \n\n$$\n\\begin{array}{c}{{\\ddot{y}\\,+\\,8\\dot{y}\\,+\\,17\\dot{y}\\,+\\,10y\\,=\\,0}}\\\\ {{{}}}\\\\ {{y(0)\\,=\\,2,\\qquad\\dot{y}(0)\\,=\\,1,\\qquad\\ddot{y}(0)\\,=\\,0.5}}\\end{array}\n$$  \n\nObtain the response $y(t)$ to the given initial condition.  \n\nBy defining the state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {x_{2}=\\dot{y}}\\\\ {x_{3}=\\ddot{y}}\\end{array}\n$$  \n\nwe obtain the following state-space representation for the system:  \n\n$$\n{\\begin{array}{r l}{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\perp}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\;\\;\\;0}&{\\;\\;1}&{\\;\\;0{\\sqrt{\\mathit{x}_{1}}}}\\\\ {\\;\\;\\;0}&{\\;\\;0}&{\\;\\;1}\\\\ {-\\;10}&{-\\;17}&{-\\;8}\\\\ {\\;\\;\\;}&{\\;\\;0}&{\\;\\;0{\\sqrt{\\mathit{x}_{1}}}}\\\\ {\\;\\;\\;}&{\\;\\;0}&{\\;\\;0{\\Biggl[\\begin{array}{l}{{x_{1}}}\\\\ {{x_{2}}}\\\\ {{x_{3}}}\\end{array}\\Biggr]}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {{x_{2}(0)}}\\\\ {{x_{3}}}\\end{array}\\right]},\\qquad{\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {{x_{2}(0)}}\\\\ {{x_{3}(0)}}\\end{array}\\right]}={\\left[\\begin{array}{l}{2}\\\\ {1}\\\\ {0.5}\\end{array}\\right]}}\\\\ {y=[1}&{0}&{0]{\\left[\\begin{array}{l}{{x_{1}}}\\\\ {{x_{2}}}\\\\ {{x_{3}}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nA possible MATLAB program to obtain the response $y(t)$ is given in MATLAB Program 5–17.   \nThe resulting response curve is shown in Figure 5–34.  \n\n![](images/cd8a6d17a574f27cb2bc34054eb2a46dd7433b96fa4d1383a51064acdf661ab0.jpg)  \n\n![](images/4d6bd953b9f06106bae2ea33500a55c3097e6e37f80b96e8fa730ad8f0d9d19a.jpg)  \nFigure 5–34 Response $y(t)$ to initial condition.  \n\nThe most important problem in linear control systems concerns stability.That is, under what conditions will a system become unstable? If it is unstable, how should we stabilize the system? In Section 5–4 it was stated that a control system is stable if and only if all closed-loop poles lie in the left-half $s$ plane. Most linear closed-loop systems have closed-loop transfer functions of the form  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{b_{0}s^{m}\\,+\\,b_{1}s^{m-1}\\,+\\,\\cdots\\,+\\,b_{m-1}s\\,+\\,b_{m}}{a_{0}s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}={\\frac{B(s)}{A(s)}}\n$$  \n\nwhere the $a$ ’s and $^b$ ’s are constants and $m\\leq n$ . A simple criterion, known as Routh’s stability criterion, enables us to determine the number of closed-loop poles that lie in the right-half $s$ plane without having to factor the denominator polynomial. (The polynomial may include parameters that MATLAB cannot handle.)  \n\nRouth’s Stability Criterion. Routh’s stability criterion tells us whether or not there are unstable roots in a polynomial equation without actually solving for them. This stability criterion applies to polynomials with only a finite number of terms.When the criterion is applied to a control system, information about absolute stability can be obtained directly from the coefficients of the characteristic equation.  \n\nThe procedure in Routh’s stability criterion is as follows:  \n\n1. Write the polynomial in $s$ in the following form:  \n\n$$\na_{0}s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}=0\n$$  \n\nwhere the coefficients are real quantities.We assume that $a_{n}\\neq0$ ; that is, any zero root has been removed.  \n\n2. If any of the coefficients are zero or negative in the presence of at least one positive coefficient, a root or roots exist that are imaginary or that have positive real parts.Therefore, in such a case, the system is not stable. If we are interested in only the absolute stability, there is no need to follow the procedure further. Note that all the coefficients must be positive.This is a necessary condition, as may be seen from the following argument: A polynomial in $s$ having real coefficients can always be factored into linear and quadratic factors, such as $(s+a)$ and $\\left(s^{2}+b s\\,+\\,c\\right)$ the quadratic factors yield complex-conjugate roots of the polynomial.The factor A B$a,b$ $c$ are real. The linear factors yield eal roots and $\\left(s^{2}\\,+\\,b s\\,+\\,c\\right)$ yields roots having negative real parts only i $^b$ dcare both positive. For all roots to have negative real parts, the constants a, b, c , and so on, in all factors must be positive.The product of any number of linear and quadratic factors containing only positive coefficients always yields a polynomial with positive coefficients. It is important to note that the condition that all the coefficients be positive is not sufficient to assure stability. The necessary but not sufficient condition for stability is that the coefficients of Equation (5–61) all be present and all have a positive sign. (If all $a$ ’s are negative, they can be made positive by multiplying both sides of the equation by $-1$ .)  \n\n3. If all coefficients are positive, arrange the coefficients of the polynomial in rows and columns according to the following pattern:  \n\n$$\n\\begin{array}{r l r l}{s^{2}}&{{}\\quad}&{e_{1}}&{e_{2}}\\\\ {s^{1}}&{{}\\quad}&{f_{1}}&{{}}\\\\ {s^{0}}&{{}\\quad}&{g_{1}}&{{}}\\end{array}\n$$  \n\nThe process of forming rows continues until we run out of elements. (The total number of rows is $n\\,+\\,1.$ .) The coefficients $b_{1},b_{2},b_{3}$ , and so on, are evaluated as follows:  \n\n$$\n\\begin{array}{r l r}&{}&{b_{1}=\\frac{a_{1}a_{2}\\,-\\,a_{0}a_{3}}{a_{1}}}\\\\ &{}&\\\\ &{}&{b_{2}=\\frac{a_{1}a_{4}\\,-\\,a_{0}a_{5}}{a_{1}}}\\\\ &{}&\\\\ &{}&{b_{3}=\\frac{a_{1}a_{6}\\,-\\,a_{0}a_{7}}{a_{1}}}\\\\ &{}&\\\\ &{}&{.}\\end{array}\n$$  \n\nThe evaluation of the $^b$ ’s is continued until the remaining ones are all zero. The same pattern of cross-multiplying the coefficients of the two previous rows is followed in evaluating the $c$ ’s, $d$ ’s, $e$ ’s, and so on.That is,  \n\n$$\n\\begin{array}{l}{\\displaystyle c_{1}=\\frac{b_{1}a_{3}-a_{1}b_{2}}{b_{1}}}\\\\ {\\displaystyle c_{2}=\\frac{b_{1}a_{5}-a_{1}b_{3}}{b_{1}}}\\\\ {\\displaystyle c_{3}=\\frac{b_{1}a_{7}-a_{1}b_{4}}{b_{1}}}\\end{array}\n$$  \n\nand  \n\n$$\nd_{1}={\\frac{c_{1}b_{2}-b_{1}c_{2}}{c_{1}}}\n$$  \n\nThis process is continued until the nth row has been completed.The complete array of coefficients is triangular. Note that in developing the array an entire row may be divided or multiplied by a positive number in order to simplify the subsequent numerical calculation without altering the stability conclusion.  \n\nRouth’s stability criterion states that the number of roots of Equation (5–61) with positive real parts is equal to the number of changes in sign of the coefficients of the first column of the array. It should be noted that the exact values of the terms in the first column need not be known; instead, only the signs are needed. The necessary and sufficient condition that all roots of Equation (5–61) lie in the left-half $s$ plane is that all the coefficients of Equation (5–61) be positive and all terms in the first column of the array have positive signs.  \n\n# EXAMPLE 5–11  \n\nLet us apply Routh’s stability criterion to the following third-order polynomial:  \n\n$$\na_{0}s^{3}\\,+\\,a_{1}s^{2}\\,+\\,a_{2}s\\,+\\,a_{3}=\\,0\n$$  \n\nwhere all the coefficients are positive numbers.The array of coefficients becomes  \n\n$$\n\\begin{array}{c c c c}{{s^{3}}}&{{}}&{{a_{0}}}&{{a_{2}}}\\\\ {{s^{2}}}&{{}}&{{a_{1}}}&{{a_{3}}}\\\\ {{}}&{{}}&{{\\frac{a_{1}a_{2}\\,-\\,a_{0}a_{3}}{a_{1}}}}&{{}}\\\\ {{s^{0}}}&{{}}&{{a_{3}}}&{{}}\\end{array}\n$$  \n\nThe condition that all roots have negative real parts is given by  \n\n$$\na_{1}a_{2}>a_{0}a_{3}\n$$  \n\nEXAMPLE 5–12 Consider the following polynomial:  \n\n$$\ns^{4}\\,+\\,2s^{3}\\,+\\,3s^{2}\\,+\\,4s\\,+\\,5\\,=\\,0\n$$  \n\nLet us follow the procedure just presented and construct the array of coefficients. (The first two rows can be obtained directly from the given polynomial. The remaining terms are  \n\nobtained from these. If any coefficients are missing, they may be replaced by zeros in the array.)  \n\n$$\n\\begin{array}{r l}{s^{4}\\qquad}&{1}&{3}&{5}\\\\ {s^{3}\\qquad}&{2}&{4}&{0}\\\\ {s^{2}\\qquad}&{1}&{5}\\\\ {s^{2}\\qquad}&{-6}\\\\ {s^{1}\\qquad}&{5}\\end{array}\\left|\\begin{array}{c c c c c}{s^{4}\\qquad}&{1}&{3}&{5}\\\\ {s^{3}\\qquad}&{2}&{4}&{\\mathcal{B}}&{\\mathrm{The\\;second\\;row\\;is\\;divided}}\\\\ {s}&{1}&{2}&{0}&{\\mathrm{by\\;2.}}\\\\ {s^{2}\\qquad}&{1}&{5}&&\\\\ {s^{1}\\qquad-3}&&&\\\\ {s^{0}\\qquad}&{5}&&&\\end{array}\\right|\n$$  \n\nIn this example, the number of changes in sign of the coefficients in the first column is 2. This means that there are two roots with positive real parts. Note that the result is unchanged when the coefficients of any row are multiplied or divided by a positive number in order to simplify the computation.  \n\nSpecial Cases. If a first-column term in any row is zero, but the remaining terms are not zero or there is no remaining term, then the zero term is replaced by a very small positive number \u000eand the rest of the array is evaluated. For example, consider the following equation:  \n\n$$\ns^{3}\\,+\\,2s^{2}\\,+\\,s\\,+\\,2\\,=\\,0\n$$  \n\nThe array of coefficients is  \n\n$$\n\\begin{array}{c c c}{{s^{3}}}&{{1}}&{{1}}\\\\ {{s^{2}}}&{{2}}&{{2}}\\\\ {{s^{1}}}&{{0\\approx{\\epsilon}}}&{{}}\\\\ {{s^{0}}}&{{2}}&{{}}\\end{array}\n$$  \n\nIf the sign of the coefficient above the zero ( \u000e) is the same as that below it, it indicates that there are a pair of imaginary roots. Actually, Equation (5–62) has two roots at $s=\\pm j$ .  \n\nIf, however, the sign of the coefficient above the zero ( \u000e) is opposite that below it, it indicates that there is one sign change. For example, for the equation  \n\n$$\ns^{3}\\,-\\,3s\\,+\\,2\\,=\\,(s\\,-\\,1)^{2}(s\\,+\\,2)\\,=\\,0\n$$  \n\nthe array of coefficients is  \n\n$$\n\\begin{array}{l}{{\\mathrm{One~sign~change:}\\displaystyle\\left(\\begin{array}{l l l}{{s^{3}}}&{{~~~1}}&{{-3}}\\\\ {{s^{2}}}&{{~~~0\\approx\\epsilon}}&{{2}}\\\\ {{{}}}&{{}}&{{}}\\\\ {{\\diamondsuit^{s1}}}&{{-3-\\displaystyle\\frac{2}{\\epsilon}}}&{{}}\\end{array}\\right)}}\\\\ {{\\mathrm{One~sign~change:}\\displaystyle\\left(\\begin{array}{l l l}{{s^{3}}}&{{~~~1}}&{{-~3}}\\\\ {{s^{2}}}&{{~~~0\\approx\\epsilon}}&{{2}}\\\\ {{{}}}&{{}}&{{}}\\\\ {{\\diamondsuit^{s1}}}&{{-~~3-\\displaystyle\\frac{2}{\\epsilon}}}&{{}}\\end{array}\\right).}}\\end{array}\n$$  \n\nThere are two sign changes of the coefficients in the first column. So there are two roots in the right-half $s$ plane. This agrees with the correct result indicated by the factored form of the polynomial equation.  \n\nIf all the coefficients in any derived row are zero, it indicates that there are roots of equal magnitude lying radially opposite in the $s$ plane—that is, two real roots with equal magnitudes and opposite signs and/or two conjugate imaginary roots. In such a case, the evaluation of the rest of the array can be continued by forming an auxiliary polynomial with the coefficients of the last row and by using the coefficients of the derivative of this polynomial in the next row. Such roots with equal magnitudes and lying radially opposite in the $s$ plane can be found by solving the auxiliary polynomial, which is always even.For a $2n$ -degree auxiliary polynomial,there are $n$ pairs of equal and opposite roots. For example, consider the following equation:  \n\n$$\ns^{5}\\,+\\,2s^{4}\\,+\\,24s^{3}\\,+\\,48s^{2}\\,-\\,25s\\,-\\,50\\,=\\,0\n$$  \n\nThe array of coefficients is  \n\n$$\n{\\begin{array}{r l r l}{s^{5}}&{\\;1}&{24}&{-25}\\\\ {s^{4}}&{\\;2}&{48}&{-50}&{\\leftarrow{\\mathrm{Auxiliary~polynomial~}}P(s)}\\\\ {s^{3}}&{\\;0}&{\\;0}\\end{array}}\n$$  \n\nThe terms in the $s^{3}$ row are all zero. (Note that such a case occurs only in an oddnumbered row.) The auxiliary polynomial is then formed from the coefficients of the $s^{4}$ row.The auxiliary polynomial $P(s)$ is  \n\n$$\nP(s)\\,=\\,2s^{4}\\,+\\,48s^{2}\\,-\\,50\n$$  \n\nwhich indicates that there are two pairs of roots of equal magnitude and opposite sign (that is, two real roots with the same magnitude but opposite signs or two complexconjugate roots on the imaginary axis).These pairs are obtained by solving the auxiliary polynomial equation $P(s)\\,=\\,0.$ The derivative of $P(s)$ with respect to $s$ is  \n\n$$\n\\frac{d P(s)}{d s}=\\,8s^{3}\\,+\\,96s\n$$  \n\nThe terms in the $s^{3}$ row are replaced by the coefficients of the last equation—that is, 8 and 96.The array of coefficients then becomes  \n\n$$\n{\\begin{array}{c c c c c}{s^{5}}&{1}&{24}&{-25}\\\\ {s^{4}}&{2}&{48}&{-50}\\\\ {s^{3}}&{8}&{96}&{\\leftarrow{\\mathrm{Coefficients~of~}}d P\\left(s\\right)/d s}\\\\ {s^{2}}&{24}&{-50}\\\\ {s^{1}}&{112.7}&{0}\\\\ {s^{0}}&{-50}&&\\end{array}}\n$$  \n\nWe see that there is one change in sign in the first column of the new array.Thus,the original equation has one root with a positive real part. By solving for roots of the auxiliary polynomial equation,  \n\nwe obtain  \n\n$$\n\\begin{array}{l c r}{{2s^{4}\\,+\\,48s^{2}\\,-\\,50\\,=\\,0}}\\\\ {{\\,}}\\\\ {{s^{2}\\,=\\,1,}}&{{s^{2}\\,=\\,-25}}\\\\ {{\\,}}\\\\ {{s\\,=\\,\\pm1,}}&{{s\\,=\\,\\pm j5}}\\end{array}\n$$  \n\nor  \n\nThese two pairs of roots of $P(s)$ are a part of the roots of the original equation. As a matter of fact, the original equation can be written in factored form as follows:  \n\n$$\n(s\\,+\\,1)(s\\,-\\,1)(s\\,+\\,j5)(s\\,-\\,j5)(s\\,+\\,2)\\,=\\,0\n$$  \n\nClearly, the original equation has one root with a positive real part.  \n\nRelative Stability Analysis. Routh’s stability criterion provides the answer to the question of absolute stability.This, in many practical cases, is not sufficient.We usually require information about the relative stability of the system. A useful approach for examining relative stability is to shift the $s$ -plane axis and apply Routh’s stability criterion.That is, we substitute  \n\n$$\ns\\,=\\,\\widehat{s}\\,-\\,\\sigma\\qquad\\mathrm{(}\\sigma\\,=\\,\\mathrm{constant)}\n$$  \n\ninto the characteristic equation of the system, write the polynomial in terms of $\\hat{s}$ ;and apply Routh’s stability criterion to the new polynomial in s.The number of changes of sign in the first column of the array developed for the polynomial in is equal to the number of roots that are located to the right of the vertical line $s\\,=\\,-\\sigma.$ .Thus, this test reveals the number of roots that lie to the right of the vertical line $s=-\\sigma$ .  \n\nApplication of Routh’s Stability Criterion to Control-System Analysis. Routh’s stability criterion is of limited usefulness in linear control-system analysis,mainly because it does not suggest how to improve relative stability or how to stabilize an unstable system. It is possible, however, to determine the effects of changing one or two parameters of a system by examining the values that cause instability. In the following, we shall consider the problem of determining the stability range of a parameter value.  \n\nConsider the system shown in Figure 5–35. Let us determine the range of $K$ for stability.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{s(s^{2}\\,+\\,s\\,+\\,1)(s\\,+\\,2)\\,+\\,K}\n$$  \n\nThe characteristic equation is  \n\n$$\ns^{4}\\,+\\,3s^{3}\\,+\\,3s^{2}\\,+\\,2s\\,+\\,K\\,=\\,0\n$$  \n\nThe array of coefficients becomes  \n\n![](images/1af515ae835a923bbe8eda20c5be5b79bf5c5604222e158ac618619e485984b1.jpg)  \nFigure 5–35 Control system.  \n\nFor stability, $K$ must be positive, and all coefficients in the first column must be positive. Therefore,  \n\n$$\n\\frac{14}{9}>K>0\n$$  \n\nWhen $\\begin{array}{r}{K=\\frac{14}{9}}\\end{array}$ ,the system becomes oscillatory and, mathematically, the oscillation is sustained at constant amplitude.  \n\nNote that the ranges of design parameters that lead to stability may be determined by use of Routh’s stability criterion.  \n\n# 5–7 EFFECTS OF INTEGRAL AND DERIVATIVE CONTROL ACTIONS ON SYSTEM PERFORMANCE  \n\nIn this section, we shall investigate the effects of integral and derivative control actions on the system performance. Here we shall consider only simple systems, so that the effects of integral and derivative control actions on system performance can be clearly seen.  \n\nIntegral Control Action. In the proportional control of a plant whose transfer function does not possess an integrator $1/s$ , there is a steady-state error, or offset, in the response to a step input. Such an offset can be eliminated if the integral control action is included in the controller.  \n\nIn the integral control of a plant, the control signal—the output signal from the controller—at any instant is the area under the actuating-error-signal curve up to that instant.The control signal $u(t)$ can have a nonzero value when the actuating error signal $e(t)$ is zero, as shown in Figure 5–36(a).This is impossible in the case of the proportional controller, since a nonzero control signal requires a nonzero actuating error signal. (A nonzero actuating error signal at steady state means that there is an offset.) Figure 5–36(b) shows the curve $e(t)$ versus $t$ and the corresponding curve $u(t)$ versus $t$ when the controller is of the proportional type.  \n\nNote that integral control action,while removing offset or steady-state error,may lead to oscillatory response of slowly decreasing amplitude or even increasing amplitude, both of which are usually undesirable.  \n\n![](images/7b77fa8287629afd077da697fe6a97a01f3d187290d20a20b01ce46fc79dab18.jpg)  \nFigure 5–36 (a) Plots of $e(t)$ and $u(t)$ curves showing nonzero control signal when the actuating error signal is zero (integral control); (b) plots of $e(t)$ and $u(t)$ curves showing zero control signal when the actuating error signal is zero (proportional control).  \n\nFigure 5–37   \nProportional control system.  \n\n![](images/771bde477fc81b20f60f6e90358c7d89f91e29e80099b68a14b6a65734996ab9.jpg)  \n\nProportional Control of Systems. We shall show that the proportional control of a system without an integrator will result in a steady-state error with a step input.We shall then show that such an error can be eliminated if integral control action is included in the controller.  \n\nConsider the system shown in Figure 5–37. Let us obtain the steady-state error in the unit-step response of the system. Define  \n\n$$\nG(s)={\\frac{K}{T s\\,+\\,1}}\n$$  \n\nSince  \n\n$$\n{\\frac{E(s)}{R(s)}}={\\frac{R(s)\\,-\\,C(s)}{R(s)}}=1\\,-\\,{\\frac{C(s)}{R(s)}}={\\frac{1}{1\\,+\\,G(s)}}\n$$  \n\nthe error $E(s)$ is given by  \n\n$$\nE(s)={\\frac{1}{1\\,+\\,G(s)}}\\,R(s)={\\frac{1}{1\\,+\\,{\\frac{K}{T s\\,+\\,1}}}}\\,R(s)\n$$  \n\nFor the unit-step input $R(s)\\,=\\,1/s$ , we have  \n\n$$\nE(s)\\,=\\frac{T s\\,+\\,1}{T s\\,+\\,1\\,+\\,K}\\frac{1}{s}\n$$  \n\nThe steady-state error is  \n\n$$\ne_{\\mathrm{ss}}=\\operatorname*{lim}_{t\\rightarrow\\infty}e(t)\\,=\\operatorname*{lim}_{s\\rightarrow0}s E(s)\\,=\\operatorname*{lim}_{s\\rightarrow0}\\frac{T s\\,+\\,1}{T s\\,+\\,1\\,+\\,K}=\\frac{1}{K\\,+\\,1}\n$$  \n\nSuch a system without an integrator in the feedforward path always has a steady-state error in the step response. Such a steady-state error is called an offset. Figure 5–38 shows the unit-step response and the offset.  \n\n![](images/d760a9a69ffa722f59b2d244d65dc30ebb0b74f2c8661847d4aa62ce07fac43a.jpg)  \nFigure 5–38 Unit-step response and offset.  \n\nFigure 5–39 Integral control system.  \n\n![](images/0d3816930d871ff6efbde60debf725696872b4d9904726ed6628278d92dac5a7.jpg)  \n\nIntegral Control of Systems. Consider the system shown in Figure 5–39. The controller is an integral controller.The closed-loop transfer function of the system is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{s(T s\\,+\\,1)\\,+\\,K}\n$$  \n\nHence  \n\n$$\n{\\frac{E(s)}{R(s)}}={\\frac{R(s)\\,-\\,C(s)}{R(s)}}={\\frac{s(T s\\,+\\,1)}{s(T s\\,+\\,1)\\,+\\,K}}\n$$  \n\nSince the system is stable, the steady-state error for the unit-step response can be obtained by applying the final-value theorem, as follows:  \n\n$$\n\\begin{array}{l}{{e_{\\mathrm{ss}}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s E(s)}}\\\\ {{\\,}}\\\\ {{\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\frac{s^{2}(T s+1)}{T s^{2}+s\\,+\\,K}\\frac{1}{s}}}\\\\ {{\\,}}\\\\ {{=\\,0}}\\end{array}\n$$  \n\nIntegral control of the system thus eliminates the steady-state error in the response to the step input. This is an important improvement over the proportional control alone, which gives offset.  \n\nResponse to Torque Disturbances (Proportional Control). Let us investigate the effect of a torque disturbance occurring at the load element. Consider the system shown in Figure 5–40.The proportional controller delivers torque $T$ to position the load element, which consists of moment of inertia and viscous friction.Torque disturbance is denoted by $D$ .  \n\nAssuming that the reference input is zero or $R(s)\\,=\\,0$ , the transfer function between $C(s)$ and $D(s)$ is given by  \n\n$$\n\\frac{C(s)}{D(s)}=\\frac{1}{J s^{2}\\,+\\,b s\\,+\\,K_{p}}\n$$  \n\n![](images/013e6f51712468c0e55a8dbcee97bac3d05d41872b26750b39abb264bc748a69.jpg)  \nFigure 5–40 Control system with a torque disturbance.   \nChapter 5 /Transient and Steady-State Response Analyses  \n\nHence  \n\n$$\n\\frac{E(s)}{D(s)}=-\\frac{C(s)}{D(s)}=-\\,\\frac{1}{J s^{2}\\,+\\,b s\\,+\\,K_{p}}\n$$  \n\nThe steady-state error due to a step disturbance torque of magnitude $T_{d}$ is given by  \n\n$$\n\\begin{array}{l}{{e_{\\mathrm{ss}}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s E(s)}}\\\\ {{\\displaystyle~~~~=\\operatorname*{lim}_{s\\rightarrow0}\\frac{-s}{J s^{2}+b s+K_{p}}\\frac{T_{d}}{s}}}\\\\ {{\\displaystyle~~~~=-\\,\\frac{T_{d}}{K_{p}}}}\\end{array}\n$$  \n\nAt steady state, the proportional controller provides the torque $-T_{d}$ ,which is equal in magnitude but opposite in sign to the disturbance torque $T_{d}$ .The steady-state output due to the step disturbance torque is  \n\n$$\nc_{\\mathrm{ss}}=-e_{\\mathrm{ss}}={\\frac{T_{d}}{K_{p}}}\n$$  \n\nThe steady-state error can be reduced by increasing the value of the gain $K_{p}$ .Increasing this value, however, will cause the system response to be more oscillatory.  \n\nResponse to Torque Disturbances (Proportional-Plus-Integral Control). To eliminate offset due to torque disturbance, the proportional controller may be replaced by a proportional-plus-integral controller.  \n\nIf integral control action is added to the controller, then, as long as there is an error signal, a torque is developed by the controller to reduce this error, provided the control system is a stable one.  \n\nFigure 5–41 shows the proportional-plus-integral control of the load element, consisting of moment of inertia and viscous friction.  \n\nThe closed-loop transfer function between $C(s)$ and $D(s)$ is  \n\n$$\n\\frac{C(s)}{D(s)}=\\frac{s}{J s^{3}\\,+\\,b s^{2}\\,+\\,K_{p}s\\,+\\,\\frac{K_{p}}{T_{i}}}\n$$  \n\nIn the absence of the reference input, or $r(t)\\,=\\,0$ , the error signal is obtained from  \n\n$$\nE(s)\\,=\\,-\\,\\frac{s}{\\,J s^{3}\\,+\\,b s^{2}\\,+\\,K_{p}s\\,+\\,\\frac{K_{p}}{T_{i}}}\\,D(s)\n$$  \n\n![](images/5e91d3be70c4a592be18d534b0976fd5652130c6e4219e24dcfa13a5be3c6bc7.jpg)  \nFigure 5–41 Proportional-plusintegral control of a load element consisting of moment of inertia and viscous friction.  \n\n![](images/c42f9ad333a8504323e0522ba8b3846565db4c51e479b43385c6e1ac5977454a.jpg)  \nFigure 5–42 Integral control of a load element consisting of moment of inertia and viscous friction.  \n\nIf this control system is stable—that is, if the roots of the characteristic equation  \n\n$$\nJ s^{3}\\,+\\,b s^{2}\\,+\\,K_{p}s\\,+\\,\\frac{K_{p}}{T_{i}}=0\n$$  \n\nhave negative real parts—then the steady-state error in the response to a unit-step disturbance torque can be obtained by applying the final-value theorem as follows:  \n\n$$\n\\begin{array}{r l}&{e_{\\mathrm{ss}}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s E(s)}\\\\ &{\\quad=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\frac{-s^{2}}{J_{s}^{3}\\,+\\,b s^{2}\\,+\\,K_{p}s\\,+\\,\\frac{K_{p}}{T_{i}}^{}}\\frac{1}{s}}\\\\ &{\\quad=0}\\end{array}\n$$  \n\nThus steady-state error to the step disturbance torque can be eliminated if the controller is of the proportional-plus-integral type.  \n\nNote that the integral control action added to the proportional controller has converted the originally second-order system to a third-order one. Hence the control system may become unstable for a large value of $K_{p}$ , since the roots of the characteristic equation may have positive real parts. (The second-order system is always stable if the coefficients in the system differential equation are all positive.)  \n\nIt is important to point out that if the controller were an integral controller, as in Figure 5–42, then the system always becomes unstable, because the characteristic equation  \n\n$$\nJ s^{3}\\,+\\,b s^{2}\\,+\\,K\\,=\\,0\n$$  \n\nwill have roots with positive real parts. Such an unstable system cannot be used in practice.  \n\nNote that in the system of Figure 5–41 the proportional control action tends to stabilize the system,while the integral control action tends to eliminate or reduce steadystate error in response to various inputs.  \n\nDerivative Control Action. Derivative control action, when added to a proportional controller, provides a means of obtaining a controller with high sensitivity. An advantage of using derivative control action is that it responds to the rate of change of the actuating error and can produce a significant correction before the magnitude of the actuating error becomes too large. Derivative control thus anticipates the actuating error, initiates an early corrective action, and tends to increase the stability of the system.  \n\n![](images/6449a3c821427d70537ed678bfc81b36c03eb313bcb7756ef512c19158cbba33.jpg)  \nFigure 5–43 (a) Proportional control of a system with inertia load; (b) response to a unit-step input.  \n\nAlthough derivative control does not affect the steady-state error directly, it adds damping to the system and thus permits the use of a larger value of the gain $K$ , which will result in an improvement in the steady-state accuracy.  \n\nBecause derivative control operates on the rate of change of the actuating error and not the actuating error itself, this mode is never used alone. It is always used in combination with proportional or proportional-plus-integral control action.  \n\nProportional Control of Systems with Inertia Load. Before we discuss further the effect of derivative control action on system performance, we shall consider the proportional control of an inertia load.  \n\nConsider the system shown in Figure 5–43(a). The closed-loop transfer function is obtained as  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K_{p}}{J s^{2}\\,+\\,K_{p}}}\n$$  \n\nSince the roots of the characteristic equation  \n\n$$\nJ s^{2}+K_{p}=0\n$$  \n\nare imaginary, the response to a unit-step input continues to oscillate indefinitely, as shown in Figure 5–43(b).  \n\nControl systems exhibiting such response characteristics are not desirable.We shall see that the addition of derivative control will stabilize the system.  \n\nmodify the proportional controller to a proportional-plus-derivative controller whose Proportional-Plus-Derivative Control of a System with Inertia Load. A BLet us transfer function is $K_{p}(1\\,+\\,T_{d}s).$ .The torque developed by the controller is proportional to $K_{p}(e\\,+\\,T_{d}\\dot{e})$ .Derivative control is essentially anticipatory,measures the instantaneous error velocity, and predicts the large overshoot ahead of time and produces an appropriate counteraction before too large an overshoot occurs.  \n\n![](images/86fb78e0c51a3b1ed2749ccb75ba8c86bc7baba751409076452f26f1fae30aea.jpg)  \nFigure 5–44 (a) Proportional-plus-derivative control of a system with inertia load; (b) response to a unit-step input.  \n\nConsider the system shown in Figure 5–44(a). The closed-loop transfer function is given by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K_{p}\\!\\left(1\\,+\\,T_{d}s\\right)}{J s^{2}\\,+\\,K_{p}T_{d}s\\,+\\,K_{p}}\n$$  \n\nThe characteristic equation  \n\n$$\nJ s^{2}\\,+\\,K_{p}T_{d}s\\,+\\,K_{p}=0\n$$  \n\nnow has two roots with negative real parts for positive values of $J$ ,$K_{p}$ , and $T_{d}$ .Thus derivative control introduces a damping effect.A typical response curve $c(t)$ to a unitstep input is shown in Figure 5–44(b). Clearly, the response curve shows a marked improvement over the original response curve shown in Figure 5–46(b).  \n\nProportional-Plus-Derivative Control of Second-Order Systems. A compromise between acceptable transient-response behavior and acceptable steady-state behavior may be achieved by use of proportional-plus-derivative control action.  \n\nConsider the system shown in Figure 5–45.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K_{p}\\,+\\,K_{d}s}{J s^{2}\\,+\\,(B\\,+\\,K_{d})s\\,+\\,K_{p}}\n$$  \n\nThe steady-state error for a unit-ramp input is  \n\n$$\ne_{\\mathrm{ss}}={\\frac{B}{K_{p}}}\n$$  \n\nThe characteristic equation is  \n\n$$\nJ s^{2}+(B\\mathrm{~+~}K_{d})s\\mathrm{~+~}K_{p}=0\n$$  \n\n![](images/2cb9ee4573c67787fa25e2f99f9a593e6f1b7afeb3f119d2a3c38962d1acaa3f.jpg)  \nFigure 5–45 Control system.  \n\nThe effective damping coefficient of this system is thus $B\\,+\\,K_{d}$ rather than $B$ . Since the damping ratio $\\zeta$ of this system is  \n\n$$\n\\zeta=\\frac{B+K_{d}}{2\\sqrt{K_{p}J}}\n$$  \n\nit is possible to make both the steady-state error $e_{\\mathrm{ss}}$ for a ramp input and the maximum overshoot for a step input small by making $B$ small, $K_{p}$ large,and $K_{d}$ large enough so that $\\zeta$ is between 0.4 and 0.7.  \n\n# 5–8 STEADY-STATE ERRORS IN UNITY-FEEDBACK CONTROL SYSTEMS  \n\nErrors in a control system can be attributed to many factors. Changes in the reference input will cause unavoidable errors during transient periods and may also cause steadystate errors.Imperfections in the system components,such as static friction,backlash,and amplifier drift, as well as aging or deterioration, will cause errors at steady state. In this section, however, we shall not discuss errors due to imperfections in the system components. Rather, we shall investigate a type of steady-state error that is caused by the incapability of a system to follow particular types of inputs.  \n\nAny physical control system inherently suffers steady-state error in response to certain types of inputs.A system may have no steady-state error to a step input, but the same system may exhibit nonzero steady-state error to a ramp input. (The only way we may be able to eliminate this error is to modify the system structure.) Whether a given system will exhibit steady-state error for a given type of input depends on the type of open-loop transfer function of the system, to be discussed in what follows.  \n\nClassification of Control Systems. Control systems may be classified according to their ability to follow step inputs, ramp inputs, parabolic inputs, and so on. This is a reasonable classification scheme, because actual inputs may frequently be considered combinations of such inputs. The magnitudes of the steady-state errors due to these individual inputs are indicative of the goodness of the system.  \n\nConsider the unity-feedback control system with the following open-loop transfer function $G(s)$ :  \n\n$$\nG(s)=\\frac{K\\bigl(T_{a}s\\,+\\,1\\bigr)\\bigl(T_{b}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{m}s\\,+\\,1\\bigr)}{s^{N}\\bigl(T_{1}s\\,+\\,1\\bigr)\\bigl(T_{2}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{p}s\\,+\\,1\\bigr)}\n$$  \n\nIt involves the term $s^{N}$ in the denominator, representing a pole of multiplicity $N$ at the origin.The present classification scheme is based on the number of integrations indicated by the open-loop transfer function.A system is called type 0, type 1, type 2, p, if $N=0$ ,$N\\,=\\,1,N\\,=\\,2,\\dots,$ , respectively. Note that this classification is different from that of the order of a system. As the type number is increased, accuracy is improved; however, increasing the type number aggravates the stability problem. A compromise between steady-state accuracy and relative stability is always necessary.  \n\nWe shall see later that, if $G(s)$ is written so that each term in the numerator and denominator, except the term $s^{N}$ , approaches unity as $s$ approaches zero, then the openloop gain $K$ is directly related to the steady-state error.  \n\nFigure 5–46 Control system.  \n\n![](images/56ff3bfc55588f29c168cbfd514dd542bc756318c425d32a52ca112f057ceb3a.jpg)  \n\nSteady-State Errors. Consider the system shown in Figure 5–46.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1+G(s)}\n$$  \n\nThe transfer function between the error signal $e(t)$ and the input signal $r(t)$ is  \n\n$$\n{\\frac{E(s)}{R(s)}}=1\\,-{\\frac{C(s)}{R(s)}}={\\frac{1}{1\\,+\\,G(s)}}\n$$  \n\nwhere the error $e(t)$ is the difference between the input signal and the output signal.  \n\nThe final-value theorem provides a convenient way to find the steady-state performance of a stable system. Since $E(s)$ is  \n\n$$\nE(s)={\\frac{1}{1+G(s)}}\\,R(s)\n$$  \n\nthe steady-state error is  \n\n$$\ne_{\\mathrm{ss}}=\\operatorname*{lim}_{t\\rightarrow\\infty}e(t)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s E(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}\\frac{s R(s)}{1\\,+\\,G(s)}\n$$  \n\nThe static error constants defined in the following are figures of merit of control systems. The higher the constants, the smaller the steady-state error. In a given system, the output may be the position, velocity, pressure, temperature, or the like. The physical form of the output, however, is immaterial to the present analysis.Therefore, in what follows, we shall call the output “position,” the rate of change of the output “velocity,” and so on. This means that in a temperature control system “position” represents the output temperature,“velocity” represents the rate of change of the output temperature, and so on.  \n\nStatic Position Error Constant $K_{p}$ .The steady-state error of the system for a unit-step input is  \n\n$$\n\\begin{array}{l}{\\displaystyle{e_{\\mathrm{ss}}=\\operatorname*{lim}_{s\\rightarrow0}\\frac{s}{1+G(s)}\\frac{1}{s}}}\\\\ {\\displaystyle{\\qquad=\\frac{1}{1+G(0)}}}\\end{array}\n$$  \n\nThe static position error constant $K_{p}$ is defined by  \n\n$$\nK_{p}=\\operatorname*{lim}_{s\\rightarrow0}G(s)\\,=\\,G(0)\n$$  \n\nThus, the steady-state error in terms of the static position error constant $K_{p}$ is given by  \n\n$$\ne_{\\mathrm{ss}}={\\frac{1}{1\\,+\\,K_{p}}}\n$$  \n\nFor a type 0 system,  \n\n$$\nK_{p}\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}\\frac{K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}=K\n$$  \n\nFor a type 1 or higher system,  \n\n$$\nK_{p}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{K{(T_{a}s\\,+\\,1)}{(T_{b}s\\,+\\,1)}\\cdots}{s^{N}{(T_{1}s\\,+\\,1)}{(T_{2}s\\,+\\,1)}\\cdots}}=\\infty,\\ \\ \\ \\ \\ \\mathrm{for}\\,\\,N\\ge1\n$$  \n\nHence, for a type 0 system, the static position error constant $K_{p}$ is finite, while for a type 1 or higher system, $K_{p}$ is infinite.  \n\nFor a unit-step input, the steady-state error $e_{\\mathrm{ss}}$ may be summarized as follows:  \n\n$$\n\\begin{array}{l l}{{e_{\\mathrm{ss}}=\\displaystyle\\frac{1}{1\\,+\\,K},\\quad}}&{{\\mathrm{for\\,type\\,\\,0\\,systems}}}\\\\ {{e_{\\mathrm{ss}}=0,\\quad}}&{{\\mathrm{for\\,type\\,\\,1\\,or\\,higher\\,systems}}}\\end{array}\n$$  \n\nFrom the foregoing analysis, it is seen that the response of a feedback control system to a step input involves a steady-state error if there is no integration in the feedforward path. (If small errors for step inputs can be tolerated, then a type 0 system may be permissible, provided that the gain $K$ is sufficiently large. If the gain $K$ is too large, however, it is difficult to obtain reasonable relative stability.) If zero steady-state error for a step input is desired, the type of the system must be one or higher.  \n\nStatic Velocity Error Constant $K_{v}$ .The steady-state error of the system with a unit-ramp input is given by  \n\n$$\n\\begin{array}{l}{{e_{\\mathrm{ss}}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\displaystyle\\frac{s}{1\\,+\\,G(s)}\\displaystyle\\frac{1}{s^{2}}}}\\\\ {{\\,\\,\\,\\,\\,\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\displaystyle\\frac{1}{s G(s)}}}\\end{array}\n$$  \n\nThe static velocity error constant $K_{v}$ is defined by  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\to0}s G(s)\n$$  \n\nThus, the steady-state error in terms of the static velocity error constant $K_{v}$ is given by  \n\n$$\ne_{\\mathrm{ss}}=\\frac{1}{K_{v}}\n$$  \n\nThe term velocity error is used here to express the steady-state error for a ramp input.The dimension of the velocity error is the same as the system error.That is,velocity error is not an error in velocity, but it is an error in position due to a ramp input. For a type 0 system,  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}}=0\n$$  \n\n![](images/602984a19fc30722af1c5e008566658e57d4fb1bd3a657d50b9c2e1fa70440f3.jpg)  \nFigure 5–47 Response of a type 1 unity-feedback system to a ramp input.  \n\nFor a type 1 system,  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}\\frac{s K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{s(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}=K\n$$  \n\nFor a type 2 or higher system,  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{s^{N}(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}}=\\infty,\\ \\ \\ \\ \\ \\mathrm{for}\\,{\\cal N}\\ge2\n$$  \n\nThe steady-state error $e_{\\mathrm{ss}}$ for the unit-ramp input can be summarized as follows:  \n\n$$\n\\begin{array}{l l}{{\\displaystyle e_{\\mathrm{ss}}=\\frac{1}{K_{v}}=\\infty,\\ \\ }}&{{\\mathrm{for\\,type\\0\\;systems}}}\\\\ {{\\displaystyle e_{\\mathrm{ss}}=\\frac{1}{K_{v}}=\\frac{1}{K},\\ \\ }}&{{\\mathrm{for\\,type\\1\\;systems}}}\\\\ {{\\displaystyle e_{\\mathrm{ss}}=\\frac{1}{K_{v}}=0,\\ }}&{{\\mathrm{for\\,type\\2\\;or\\,\\,higher\\;systems}}}\\end{array}\n$$  \n\nThe foregoing analysis indicates that a type 0 system is incapable of following a ramp input in the steady state.The type 1 system with unity feedback can follow the ramp input with a finite error.In steady-state operation,the output velocity is exactly the same as the input velocity, but there is a positional error.This error is proportional to the velocity of the input and is inversely proportional to the gain $K$ .Figure 5–47 shows an example of the response of a type 1 system with unity feedback to a ramp input. The type 2 or higher system can follow a ramp input with zero error at steady state.  \n\nStatic Acceleration Error Constant $K_{a}$ .The steady-state error of the system with a unit-parabolic input (acceleration input), which is defined by  \n\n$$\nr(t)=\\frac{t^{2}}{2},\\quad\\mathrm{for}\\,t\\geq0}\\\\ {=\\,0,\\quad\\mathrm{~for~}t<0}\\end{array}\n$$  \n\nis given by  \n\n$$\n\\begin{array}{l}{{e_{\\mathrm{ss}}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\frac{s}{1\\,+\\,G(s)}\\frac{1}{s^{3}}}}\\\\ {{\\,}}\\\\ {{\\,=\\displaystyle\\frac{1}{\\operatorname*{lim}s^{2}G(s)}}}\\end{array}\n$$  \n\nThe static acceleration error constant $K_{a}$ is defined by the equation  \n\n$$\nK_{a}=\\operatorname*{lim}_{s\\to0}s^{2}G(s)\n$$  \n\nThe steady-state error is then  \n\n$$\ne_{\\mathrm{ss}}={\\frac{1}{K_{a}}}\n$$  \n\nNote that the acceleration error, the steady-state error due to a parabolic input, is an error in position.  \n\nThe values of $K_{a}$ are obtained as follows:  \n\nFor a type 0 system,  \n\n$$\nK_{a}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s^{2}K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}}=0\n$$  \n\nFor a type 1 system,  \n\n$$\nK_{a}=\\operatorname*{lim}_{s\\rightarrow0}\\frac{s^{2}K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots}{s(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots}=0\n$$  \n\nFor a type 2 system,  \n\n$$\nK_{a}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s^{2}K{\\left(T_{a}s\\,+\\,1\\right)}{\\left(T_{b}s\\,+\\,1\\right)}\\cdots}{s^{2}{\\left(T_{1}s\\,+\\,1\\right)}{\\left(T_{2}s\\,+\\,1\\right)}\\cdots}}=K\n$$  \n\nFor a type 3 or higher system,  \n\n$$\nK_{a}=\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s^{2}K{\\bigl(}T_{a}s\\,+\\,1{\\bigr)}(T_{b}s\\,+\\,1)\\cdots}{s^{N}{\\bigl(}T_{1}s\\,+\\,1{\\bigr)}(T_{2}s\\,+\\,1)\\cdots}}=\\infty,\\ \\ \\ \\ \\ \\mathrm{for}\\,\\,N\\geq3\n$$  \n\nThus, the steady-state error for the unit parabolic input is  \n\n$$\n\\begin{array}{r l}{{e_{\\mathrm{ss}}=\\infty,}}&{{\\mathrm{for\\;type\\;0\\;and\\;type\\;1\\;systems}}}\\\\ {{}}&{{}}\\\\ {{e_{\\mathrm{ss}}=\\displaystyle\\frac{1}{K},}}&{{\\mathrm{for\\;type\\;2\\;systems}}}\\\\ {{}}&{{}}\\\\ {{e_{\\mathrm{ss}}=0,}}&{{\\mathrm{for\\;type\\;3\\;or\\;higher\\;systems}}}\\end{array}\n$$  \n\n![](images/c3d1dc174f6dfa559198ce6cb9721298d54ce70ccd55aecc246c0ebd6a355c1e.jpg)  \nFigure 5–48 Response of a type 2 unity-feedback system to a parabolic input.  \n\nNote that both type 0 and type 1 systems are incapable of following a parabolic input in the steady state. The type 2 system with unity feedback can follow a parabolic input with a finite error signal. Figure 5–48 shows an example of the response of a type 2 system with unity feedback to a parabolic input. The type 3 or higher system with unity feedback follows a parabolic input with zero error at steady state.  \n\nSummary. Table 5–1 summarizes the steady-state errors for type 0, type 1, and type 2 systems when they are subjected to various inputs. The finite values for steadystate errors appear on the diagonal line.Above the diagonal, the steady-state errors are infinity; below the diagonal, they are zero.  \n\nTable 5–1 Steady-State Error in Terms of Gain $K$   \n\n![](images/77d65381c1bac1c32ac4daa916dcd1fa3e57e29cc31de2fc8b5b714980694012.jpg)  \n\nRemember that the terms position error, velocity error , and acceleration error mean steady-state deviations in the output position. A finite velocity error implies that after transients have died out, the input and output move at the same velocity but have a finite position difference.  \n\nThe error constants $K_{p},K_{v}$ , and $K_{a}$ describe the ability of a unity-feedback system to reduce or eliminate steady-state error.Therefore,they are indicative of the steady-state performance. It is generally desirable to increase the error constants, while maintaining the transient response within an acceptable range. It is noted that to improve the steadystate performance we can increase the type of the system by adding an integrator or integrators to the feedforward path. This, however, introduces an additional stability problem.The design of a satisfactory system with more than two integrators in series in the feedforward path is generally not easy.  \n\nA–5–1. In the system of Figure 5–49, $x(t)$ is the input displacement and $\\theta(t)$ is the output angular displacement. Assume that the masses involved are negligibly small and that all motions are restricted to be small; therefore, the system can be considered linear.The initial conditions for $x$ and $\\theta$ are zeros, or $x(0-)=0$ and $\\theta(0-)\\,=\\,0.$ . Show that this system is a differentiating element. Then obtain the response $\\theta(t)$ when $x(t)$ is a unit-step input.  \n\nSolution. The equation for the system is  \n\n$$\nb(\\dot{x}\\,-\\,L\\dot{\\theta})\\,=\\,k L\\theta\n$$  \n\nor  \n\n$$\nL{\\dot{\\theta}}\\,+\\,{\\frac{k}{b}}\\,L\\theta\\,=\\,{\\dot{x}}\n$$  \n\nThe Laplace transform of this last equation, using zero initial conditions, gives  \n\n$$\n\\left(L s\\,+\\frac k b\\,L\\right)\\!\\theta(s)\\,=\\,s X(s)\n$$  \n\nAnd so  \n\n$$\n\\frac{\\theta(s)}{X(s)}=\\frac{1}{L}\\frac{s}{s\\,+\\,(k/b)}\n$$  \n\nThus the system is a differentiating system.  \n\nFor the unit-step input $X(s)\\,=\\,1/s$ , the output $\\Theta(s)$ becomes  \n\n$$\n\\Theta(s)\\,=\\frac{1}{L}\\frac{1}{s\\,+\\,(k/b)}\n$$  \n\nThe inverse Laplace transform of $\\Theta(s)$ gives  \n\n$$\n\\theta(t)=\\frac{1}{L}e^{-(k/b)t}\n$$  \n\n![](images/cccb8a63296451207427f3f69e0d7052792efa167a57c53f10cc18e2df6945ec.jpg)  \nFigure 5–49 Mechanical system.  \n\n![](images/e3bbc6495b04125e2ba8d97a3055833893558ce2502c527d2b5f77a712658c97.jpg)  \nFigure 5–50 Unit-step input and the response of the mechanical system shown in Figure 5–49.  \n\nFigure 5–51 Gear-train system.  \n\nNote that if the value of $k/b$ is large, the response $\\theta(t)$ approaches a pulse signal, as shown in Figure 5–50.  \n\nConsider the gear-train system shown in Figure 5–51. In this system, a load is driven by a motor through the gear train.Assuming that the stiffness of the shafts of the gear train is infinite (there is neither backlash nor elastic deformation) and that the number of teeth on each gear is proportional to the radius of the gear, obtain the equivalent moment of inertia and equivalent viscous-friction coefficient referred to the motor shaft and referred to the load shaft.  \n\nA–5–2. Gear trains are often used in servo systems to reduce speed, to magnify torque, or to obtain the most efficient power transfer by matching the driving member to the given load.  \n\nIn Figure 5–51 the numbers of teeth on gears 1, 2, 3, and 4 are $N_{1},N_{2},N_{3}$ , and $N_{4}$ , respectively. The angular displacements of shafts, 1, 2, and 3 are $\\theta_{1},\\theta_{2}$ , and $\\theta_{3}$ , respectively.Thus, $\\theta_{2}/\\theta_{1}\\,=\\,N_{1}/N_{2}$ and $\\theta_{3}/\\theta_{2}\\,=\\,N_{3}/N_{4}$ .The moment of inertia and viscous-friction coefficient of each gear-train component are denoted by $J_{1},b_{1};J_{2},b_{2}$ ; and $J_{3},b_{3}$ ; respectively. ( $J_{3}$ and $b_{3}$ include the moment of inertia and friction of the load.)  \n\n![](images/2798b217ff168aecf743022c72fc5984270d1ac624372cf1e6e2e78a4e307e40.jpg)  \n\nSolution. For this gear-train system, we can obtain the following equations: For shaft 1,  \n\n$$\nJ_{1}\\ddot{\\theta}_{1}\\,+\\,b_{1}\\dot{\\theta}_{1}\\,+\\,T_{1}=\\,T_{m}\n$$  \n\nwhere $T_{m}$ is the torque developed by the motor and $T_{1}$ is the load torque on gear 1 due to the rest of the gear train. For shaft 2,  \n\n$$\nJ_{2}\\ddot{\\theta}_{2}\\,+\\,b_{2}\\dot{\\theta}_{2}\\,+\\,T_{3}=\\,T_{2}\n$$  \n\nwhere $T_{2}$ is the torque transmitted to gear 2 and $T_{3}$ is the load torque on gear 3 due to the rest of the gear train. Since the work done by gear 1 is equal to that of gear 2,  \n\n$$\nT_{1}\\theta_{1}=T_{2}\\theta_{2}\\qquad\\mathrm{or}\\qquad T_{2}=T_{1}\\frac{N_{2}}{N_{1}}\n$$  \n\nIf $N_{1}/N_{2}<1$ ,the gear ratio reduces the speed as well as magnifies the torque. For shaft 3,  \n\n$$\n{\\cal J}_{3}\\ddot{\\theta}_{3}\\,+\\,b_{3}\\dot{\\theta}_{3}\\,+\\,T_{L}=\\,T_{4}\n$$  \n\nwhere $T_{L}$ is the load torque and $T_{4}$ is the torque transmitted to gear $4.T_{3}$ and $T_{4}$ are related by  \n\n$$\nT_{4}\\,=\\,T_{3}\\,{\\frac{N_{4}}{N_{3}}}\n$$  \n\nand $\\theta_{3}$ and $\\theta_{1}$ are related by  \n\n$$\n\\theta_{3}\\,=\\,\\theta_{2}\\,{\\frac{N_{3}}{N_{4}}}\\,=\\,\\theta_{1}\\,{\\frac{N_{1}}{N_{2}}}\\,{\\frac{N_{3}}{N_{4}}}\n$$  \n\nEliminating $T_{1},T_{2},T_{3}$ ,and $T_{4}$ from Equations (5–63), (5–64), and (5–65) yields  \n\n$$\nJ_{1}\\ddot{\\theta}_{1}\\,+\\,b_{1}\\dot{\\theta}_{1}\\,+\\,\\frac{N_{1}}{N_{2}}\\bigl(J_{2}\\ddot{\\theta}_{2}\\,+\\,b_{2}\\dot{\\theta}_{2}\\bigr)\\,+\\,\\frac{N_{1}N_{3}}{N_{2}N_{4}}\\bigl(J_{3}\\ddot{\\theta}_{3}\\,+\\,b_{3}\\dot{\\theta}_{3}\\,+\\,T_{L}\\bigr)\\,=\\,T_{m}\n$$  \n\nEliminating $\\theta_{2}$ and $\\theta_{3}$ from this last equation and writing the resulting equation in terms of $\\theta_{1}$ and its time derivatives, we obtain  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\Big[J_{1}+\\Big(\\frac{N_{1}}{N_{2}}\\Big)^{2}J_{2}+\\Big(\\frac{N_{1}}{N_{2}}\\Big)^{2}\\Big(\\frac{N_{3}}{N_{4}}\\Big)^{2}J_{3}\\Big]\\ddot{\\theta}_{1}}\\quad}&{}\\\\ &{+\\;\\left[b_{1}+\\Big(\\frac{N_{1}}{N_{2}}\\Big)^{2}b_{2}+\\Big(\\frac{N_{1}}{N_{2}}\\Big)^{2}\\Big(\\frac{N_{3}}{N_{4}}\\Big)^{2}b_{3}\\Big]\\dot{\\theta}_{1}+\\Big(\\frac{N_{1}}{N_{2}}\\Big)\\Big(\\frac{N_{3}}{N_{4}}\\Big)T_{L}=T_{m}}\\end{array}\n$$  \n\nThus, the equivalent moment of inertia and viscous-friction coefficient of the gear train referred to shaft 1 are given, respectively, by  \n\n$$\n\\begin{array}{l}{\\displaystyle J_{\\mathrm{leq}}\\,=\\,J_{1}\\,+\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}J_{2}\\,+\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}\\bigg(\\frac{N_{3}}{N_{4}}\\bigg)^{2}J_{3}}\\\\ {\\displaystyle b_{\\mathrm{leq}}\\,=\\,b_{1}\\,+\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}b_{2}\\,+\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}\\bigg(\\frac{N_{3}}{N_{4}}\\bigg)^{2}b_{3}}\\end{array}\n$$  \n\nSimilarly,the equivalent moment of inertia and viscous-friction coefficient of the gear train referred to the load shaft (shaft 3) are given, respectively, by  \n\n$$\n\\begin{array}{l}{\\displaystyle J_{3\\mathrm{eq}}\\,=\\,J_{3}\\,+\\,\\bigg(\\frac{N_{4}}{N_{3}}\\bigg)^{2}J_{2}\\,+\\,\\bigg(\\frac{N_{2}}{N_{1}}\\bigg)^{2}\\bigg(\\frac{N_{4}}{N_{3}}\\bigg)^{2}J_{1}}\\\\ {\\displaystyle b_{3\\mathrm{eq}}\\,=\\,b_{3}\\,+\\,\\bigg(\\frac{N_{4}}{N_{3}}\\bigg)^{2}b_{2}\\,+\\,\\bigg(\\frac{N_{2}}{N_{1}}\\bigg)^{2}\\bigg(\\frac{N_{4}}{N_{3}}\\bigg)^{2}b_{1}}\\end{array}\n$$  \n\nThe relationship between $J_{\\mathrm{1eq}}$ and $J_{3\\mathrm{eq}}$ is thus  \n\n$$\nJ_{\\mathrm{1eq}}\\,=\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}\\bigg(\\frac{N_{3}}{N_{4}}\\bigg)^{2}J_{3\\mathrm{eq}}\n$$  \n\nand that between $b_{\\mathrm{1eq}}$ and $b_{3\\mathrm{eq}}$ is  \n\n$$\nb_{\\mathrm{1eq}}\\,=\\,\\bigg(\\frac{N_{1}}{N_{2}}\\bigg)^{2}\\bigg(\\frac{N_{3}}{N_{4}}\\bigg)^{2}b_{\\mathrm{3eq}}\n$$  \n\nThe effect of $J_{2}$ and $J_{3}$ on an equivalent moment of inertia is determined by the gear ratios $N_{1}/N_{2}$ and $N_{3}/N_{4}$ .For speed-reducing gear trains, the ratios, $N_{1}/N_{2}$ and $N_{3}/N_{4}$ are usually less than unity. If $N_{1}/N_{2}\\,\\ll\\,1$ and $N_{3}/N_{4}\\,\\ll\\,1$ ,then the effect of $J_{2}$ and $J_{3}$ on the equivalent moment of inertia $J_{\\mathrm{1eq}}$ is negligible. Similar comments apply to the equivalent viscous-friction coefficient $b_{\\mathrm{1eq}}$ of the gear train. In terms of the equivalent moment of inertia $J_{\\mathrm{1eq}}$ and equivalent viscous-friction coefficient $b_{\\mathrm{1eq}}$ , Equation (5–66) can be simplified to give  \n\n$$\nJ_{\\mathrm{1eq}}\\ddot{\\theta}_{1}\\,+\\,b_{\\mathrm{1eq}}\\dot{\\theta}_{1}\\,+\\,n T_{L}=\\,T_{m}\n$$  \n\nwhere  \n\n$$\nn={\\frac{N_{1}N_{3}}{N_{2}N_{4}}}\n$$  \n\nA–5–3. When the system shown in Figure 5–52(a) is subjected to a unit-step input, the system output responds as shown in Figure 5–52(b). Determine the values of $K$ and $T$ from the response curve.  \n\nSolution. The maximum overshoot of $25.4\\%$ corresponds to $\\zeta=0.4$ . From the response curve we have  \n\n$$\nt_{p}\\,=\\,3\n$$  \n\nConsequently,  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}=\\frac{\\pi}{\\omega_{n}\\sqrt{1-\\zeta^{2}}}=\\frac{\\pi}{\\omega_{n}\\sqrt{1-0.4^{2}}}=3\n$$  \n\n![](images/09aa57b45847d6525a555e965443f90a063a680305a8ae8511e1cae9765a8236.jpg)  \nFigure 5–52 (a) Closed-loop system; (b) unit-step response curve.  \n\nIt follows that  \n\n$$\n\\omega_{n}=1.14\n$$  \n\nFrom the block diagram we have  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K}{T s^{2}\\,+\\,s\\,+\\,K}}\n$$  \n\nfrom which  \n\n$$\n\\omega_{n}={\\sqrt{\\frac{K}{T}}},\\qquad2\\zeta\\omega_{n}={\\frac{1}{T}}\n$$  \n\nTherefore, the values of $T$ and $K$ are determined as  \n\n$$\nT=\\frac{1}{2\\zeta\\omega_{n}}=\\frac{1}{2\\times0.4\\times1.14}=1.09\n$$  \n\n$$\nK\\,=\\,\\omega_{n}^{2}T\\,=\\,1.14^{2}\\times\\,1.09\\,=\\,1.42\n$$  \n\nA–5–4. Determine the values of $K$ and $k$ of the closed-loop system shown in Figure 5–53 so that the maximum overshoot in unit-step response is $25\\%$ and the peak time is 2 sec.Assume that $J=1\\,\\mathrm{kg}–\\mathrm{m}^{2}$ .  \n\nSolution. The closed-loop transfer function is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K}{J s^{2}\\,+\\,K k s\\,+\\,K}}\n$$  \n\nBy substituting $J\\,=\\,1\\;\\mathrm{kg}{-}\\mathrm{m}^{2}$ into this last equation, we have  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K}{s^{2}\\,+\\,K k s\\,+\\,K}}\n$$  \n\nNote that in this problem  \n\n$$\n\\omega_{n}=\\sqrt{K}\\,,\\qquad2\\zeta\\omega_{n}=K k\n$$  \n\nThe maximum overshoot $M_{p}$ is  \n\n$$\nM_{p}\\,=\\,e^{-\\zeta\\pi/\\sqrt{1-\\zeta^{2}}}\n$$  \n\nwhich is specified as $25\\%$ . Hence  \n\n$$\ne^{-\\zeta\\pi/\\sqrt{1-\\zeta^{2}}}=0.25\n$$  \n\nfrom which  \n\n$$\n\\frac{\\zeta\\pi}{\\sqrt{1-\\zeta^{2}}}=1.386\n$$  \n\n![](images/21886a3d2848208626eb6b2eb15f4ab8719e2d4446d20da42ccd254aa6740f56.jpg)  \nFigure 5–53 Closed-loop system.  \n\n$$\n\\zeta\\,=\\,0.404\n$$  \n\nThe peak time $t_{p}$ is specified as 2 sec.And so  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}=2\n$$  \n\nor  \n\n$$\n\\omega_{d}=1.57\n$$  \n\nThen the undamped natural frequency $\\omega_{n}$ is  \n\n$$\n\\omega_{n}={\\frac{\\omega_{d}}{\\sqrt{1\\,-\\,\\zeta^{2}}}}={\\frac{1.57}{\\sqrt{1\\,-\\,0.404^{2}}}}=1.72\\\n$$  \n\nTherefore, we obtain  \n\n$$\nK\\,=\\,\\omega_{n}^{2}\\,=\\,1.72^{2}\\,=\\,2.95\\,\\,\\mathrm{N{\\cdot}m}\n$$  \n\n$$\nk={\\frac{2\\zeta\\omega_{n}}{K}}={\\frac{2\\times0.404\\times1.72}{2.95}}=0.471\\;{\\mathrm{sec}}\n$$  \n\nA–5–5. Figure 5–54(a) shows a mechanical vibratory system.When 2 lb of force (step input) is applied to the system, the mass oscillates, as shown in Figure 5–54(b). Determine $m,b$ , and $k$ of the system from this response curve.The displacement $x$ is measured from the equilibrium position.  \n\nSolution. The transfer function of this system is  \n\n$$\n{\\frac{X(s)}{P(s)}}={\\frac{1}{m s^{2}\\,+\\,b s\\,+\\,k}}\n$$  \n\nSince  \n\n$$\nP(s)={\\frac{2}{s}}\n$$  \n\nwe obtain  \n\n$$\nX(s)=\\frac{2}{s(m s^{2}+b s\\,+\\,k)}\n$$  \n\nIt follows that the steady-state value of $x$ is  \n\n$$\nx(\\infty)\\,=\\,\\operatorname*{lim}_{s\\to0}s X(s)\\,=\\frac{2}{k}=0.1\\,\\mathrm{f}\n$$  \n\n![](images/8e7d129bcaa97bbd1e88ecd325ff8a4e082bd965fccd4c1fbdf992ec86469b2d.jpg)  \nFigure 5–54 (a) Mechanical vibratory system; (b) step-response curve.  \n\nHence  \n\n$$\nk\\,=\\,20\\,\\mathrm{{lb}_{f}/\\mathrm{{ft}}}\n$$  \n\nNote that $M_{p}\\,=\\,9.5\\%$ corresponds to $\\zeta=0.6$ .The peak time $t_{p}$ is given by  \n\n$$\nt_{p}=\\frac{\\pi}{\\omega_{d}}=\\frac{\\pi}{\\omega_{n}\\sqrt{1-\\zeta^{2}}}=\\frac{\\pi}{0.8\\omega_{n}}\n$$  \n\nThe experimental curve shows that $t_{p}\\,=\\,2$ sec.Therefore,  \n\n$$\n\\omega_{n}={\\frac{3.14}{2\\times0.8}}=1.96\\,{\\mathrm{rad/sec}}\n$$  \n\nSince $\\omega_{n}^{2}=k/m=20/m$ , we obtain  \n\n$$\nm={\\frac{20}{\\omega_{n}^{2}}}={\\frac{20}{1.96^{2}}}=5.2\\;{\\mathrm{slugs}}\\,=\\,167\\,{\\mathrm{lb}}\n$$  \n\n(Note that $\\mathrm{1\\slug\\=1\\lb_{f}{\\mathrm{-}s e c}^{2}/f t.}$ .) Then $^b$ is determined from  \n\n$$\n2\\zeta\\omega_{n}=\\frac{b}{m}\n$$  \n\nor  \n\n$$\nb=2\\zeta\\omega_{n}m=2\\times0.6\\times1.96\\times5.2=12.2\\,{\\mathrm{lb}}_{\\mathrm{f}}/{\\mathrm{ft}}/{\\mathrm{sec}}\n$$  \n\nA–5–6. Consider the unit-step response of the second-order system  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nThe amplitude of the exponentially damped sinusoid changes as a geometric series. At time A B$t\\,=\\,t_{p}\\,=\\,\\pi/\\omega_{d}$ ,the amplitude is equal to $e^{-(\\sigma/\\omega_{d})\\pi}$ .After one oscillation, or at $t\\,=\\,t_{p}\\,+\\,2\\pi/\\upomega_{d}\\,=\\,3\\pi/\\omega_{d}$ A B, the amplitude is equal to $e^{-(\\sigma/\\omega_{d})3\\pi}$ ;after another cycle of oscillation, the amplitude is $e^{-(\\sigma/\\omega_{d})\\5\\pi}$ .The logarithm of the ratio of successive amplitudes is called the logarithmic decrement .Determine the logarithmic decrement for this second-order system.Describe a method for experimental determination of the damping ratio from the rate of decay of the oscillation.  \n\nSolution. Let us define the amplitude of the output oscillation at $t\\,=\\,t_{i}$ to be $x_{i}$ , where $\\begin{array}{r}{t_{i}=t_{p}+(i-1)T(T=}\\end{array}$ period of oscillation ). The amplitude ratio per one period of damped 2 oscillation is  \n\n$$\n\\frac{x_{1}}{x_{2}}=\\frac{e^{-(\\sigma/\\omega_{d})\\pi}}{e^{-(\\sigma/\\omega_{d})3\\pi}}=e^{2(\\sigma/\\omega_{d})\\pi}=e^{2\\zeta\\pi/\\sqrt{1-\\zeta^{2}}}\n$$  \n\nThus, the logarithmic decrement $\\delta$ is  \n\n$$\n\\delta=\\ln\\!{\\frac{x_{1}}{x_{2}}}={\\frac{2\\zeta\\pi}{\\sqrt{1-\\zeta^{2}}}}\n$$  \n\nIt is a function only of the damping ratio $\\zeta$ . Thus, the damping ratio $\\zeta$ can be determined by use of the logarithmic. decrement.  \n\nIn the experimental determination of the damping ratio $\\zeta$ from the rate of decay of the oscillation, we measure the amplitude $x_{1}$ at $t\\,=\\,t_{p}$ and amplitude 2 $x_{n}$ at $t\\,=\\,t_{p}\\,+\\,(n\\,-\\,1)T.$ Note that it is necessary to choose $n$ large enough so that the ratio $x_{1}/x_{n}$ is not near unity.Then  \n\n$$\n\\frac{x_{1}}{x_{n}}=e^{(n-1)2\\zeta\\pi/\\sqrt{1-\\zeta^{2}}}\n$$  \n\nor  \n\n$$\n\\ln\\!{\\frac{x_{1}}{x_{n}}}=(n\\,-\\,1)\\,{\\frac{2\\zeta\\pi}{\\sqrt{1-\\zeta^{2}}}}\n$$  \n\nHence  \n\n$$\n\\zeta=\\frac{\\displaystyle\\frac{1}{n-1}\\left(\\ln\\!\\frac{x_{1}}{x_{n}}\\right)}{\\displaystyle\\sqrt{4\\pi^{2}+\\left[\\frac{1}{n-1}\\left(\\ln\\!\\frac{x_{1}}{x_{n}}\\right)\\right]^{2}}}\n$$  \n\n![](images/77ab028f0b99c0c0755d1723b9c2c9eefa1f5ca05f7f3e07ec0f34d931bce981.jpg)  \n\nFigure 5–55   \nSpring-mass-damper system.  \n\nIn the system shown in Figure 5–55, the numerical values of $m,b$ , and $k$ are given as $m=1\\:\\mathrm{kg}$ ,$b\\,=\\,2\\,\\mathrm{N-sec}/\\mathrm{m}$ , and $k\\,=\\,100\\,\\mathrm{N/m}$ . The mass is displaced $0.05\\;\\mathrm{m}$ and released without initial velocity.Find the frequency observed in the vibration.In addition,find the amplitude four cycles later. The displacement $x$ is measured from the equilibrium position.  \n\nSolution. The equation of motion for the system is  \n\n$$\nm{\\ddot{x}}\\,+\\,b{\\dot{x}}\\,+\\,k x\\,=\\,0\n$$  \n\nSubstituting the numerical values for $m,b,$ , and $k$ into this equation gives  \n\n$$\n\\Ddot{x}\\,+\\,2\\Dot{x}\\,+\\,100x\\,=\\,0\n$$  \n\nwhere the initial conditions are $\\mathbf{x}(0)=0.05$ and ${\\dot{x}}(0)=0.$ #From this last equation the undamped natural frequency $\\omega_{n}$ and the damping ratio $\\zeta$ are found to be  \n\n$$\n\\omega_{n}=10,\\qquad\\zeta=0.1\n$$  \n\nThe frequency actually observed in the vibration is the damped natural frequency $\\omega_{d}$ .  \n\n$$\n\\omega_{d}=\\omega_{n}\\sqrt{1-\\zeta^{2}}=10\\sqrt{1-0.01}=9.95\\,\\mathrm{rad/sec}\n$$  \n\nIn the present analysis, ${\\dot{x}}(0)$ is given as zero.Thus, solution $x(t)$ can be written as  \n\n$$\nx(t)\\,=\\,x(0)e^{-\\zeta\\omega_{n}t}\\bigg(\\cos\\omega_{d}t\\,+\\frac{\\zeta}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\omega_{d}t\\bigg)\n$$  \n\nIt follows that at $t\\,=\\,n T$ , where $T\\,=\\,2\\pi/\\omega_{d}$ ,  \n\n$$\nx(n T)\\,=\\,x(0)e^{-\\zeta\\omega_{n}n T}\n$$  \n\nConsequently, the amplitude four cycles later becomes  \n\n$$\n{\\begin{array}{r l}&{x(4T)=x(0)e^{-\\zeta\\omega_{n}4T}=x(0)e^{-(0.1)(10)(4)(0.6315)}}\\\\ &{\\qquad\\qquad=0.05e^{-2.526}=0.05\\times0.07998=0.004\\,\\mathrm{m}}\\end{array}}\n$$  \n\nA–5–8. Obtain both analytically and computationally the unit-step response of tbe following higher-order system:  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{3s^{3}\\,+\\,25s^{2}\\,+\\,72s\\,+\\,80}{s^{4}\\,+\\,8s^{3}\\,+\\,40s^{2}\\,+\\,96s\\,+\\,80}\n$$  \n\n[Obtain the partial-fraction expansion of $C(s)$ with MATLAB when $R(s)$ is a unit-step function.]  \n\nSolution. MATLAB Program 5–18 yields the unit-step response curve shown in Figure 5–56. It also yields the partial-fraction expansion of $C(s)$ as follows:  \n\n$$\n\\begin{array}{r l}{C(s)={\\frac{3s^{3}}{s^{4}+8s^{3}}}+25s^{2}+72s+80\\,}&{{}{\\frac{1}{s}}}\\\\ {={\\frac{-0.2813-j0.1719}{s+2-j4}}+{\\frac{-0.2813+j0.1719}{s+2+j4}}}\\\\ {\\quad}&{{}+{\\frac{-0.4375}{s+2}}+{\\frac{-0.375}{(s+2)^{2}}}+{\\frac{1}{s}}}\\\\ {={\\frac{-0.5626(s+2)}{(s+2)^{2}+4^{2}}}+{\\frac{(0.3438)\\times4}{(s+2)^{2}+4^{2}}}}\\\\ {\\quad}&{{}-{\\frac{0.4375}{s+2}}-{\\frac{0.375}{(s+2)^{2}}}+{\\frac{1}{s}}}\\end{array}\n$$  \n\n# MATLAB Program 5–18  \n\n% ------- Unit-Step Response of C(s)/R(s) and Partial-Fraction Expansion of C(s) -------   \n$\\mathrm{\\sf~num}=[3\\mathrm{~\\bf~25~}\\,72\\mathrm{~\\bf~80}];$ ;  \nde $\\mathsf{n}=\\left[1\\ \\ 8\\ \\ 40\\ \\ 96\\ \\ 80\\right],$ ;  \nstep(num,den);   \n$\\mathsf{v}=[0\\;\\;3\\;\\;0\\;\\;1.2]\\,;\\;\\mathsf{a x i s(v),\\;g}$ rid   \n$\\%$ To obtain the partial-fraction expansion of C(s), enter commands   \n$\\%$ ;  \n$\\%$ $\\mathsf{d e n}\\,1\\,=\\,[1\\ \\ 8\\ \\ 40\\ \\ 96\\ \\ 80\\ \\ 0]\\,;$   \n$\\%$ $[\\mathsf{r},\\mathsf{p},\\mathsf{k}]=\\mathsf{r e s i d u e}(\\mathsf{n u m}\\mathsf{1},\\mathsf{d e n}\\mathsf{1})$   \n$\\mathrm{\\sfnum}1\\,=\\,[25\\;\\;72\\;\\;80]\\,,$ ;  \nd$\\mathsf{e n}\\,1\\,=\\,[1\\ \\ 8\\ \\ 40\\ \\ 96\\ \\ 80\\ \\ 0]\\,;$ ;  \n[r,p,k] $=$ residue(num1,den1)   \n$\\mathsf{r}=$ -0.2813- 0.1719i -0.2813+ 0.1719i -0.4375 -0.3750 -1.0000   \np = -2.0000+ 4.0000i -2.0000- 4.0000i -2.0000 -2.0000 -0   \nk = []  \n\n![](images/ad37c15a84342dd1ecd61f351f7a145c7040e01dac9ff2dc0d821ac568a6fa67.jpg)  \nFigure 5–56 Unit-step response curve.  \n\nHence, the time response $c(t)$ can be given by  \n\n$$\n\\begin{array}{c}{c(t)=-0.5626e^{-2t}\\cos{4t}\\,+\\,0.3438e^{-2t}\\sin{4t}}\\\\ {\\,}\\\\ {-\\,\\,0.4375e^{-2t}\\,-\\,0.375t e^{-2t}\\,+\\,1}\\end{array}\n$$  \n\nThe fact that the response curve is an exponential curve superimposed by damped sinusoidal curves can be seen from Figure 5–56.  \n\nA–5–9. When the closed-loop system involves a numerator dynamics, the unit-step response curve may exhibit a large overshoot. Obtain the unit-step response of the following system with MATLAB:  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{10s\\,+\\,4}{s^{2}\\,+\\,4s\\,+\\,4}\n$$  \n\nObtain also the unit-ramp response with MATLAB.  \n\nSolution. MATLAB Program 5–19 produces the unit-step response as well as the unit-ramp response of the system.The unit-step response curve and unit-ramp response curve, together with the unit-ramp input, are shown in Figures 5–57(a) and (b), respectively.  \n\nNotice that the unit-step response curve exhibits over $215\\%$ of overshoot. The unit-ramp response curve leads the input curve.These phenomena occurred because of the presence of a large derivative term in the numerator.  \n\n![](images/ff98d54b0b302d8eefcd1e51a903a221e7ee35d3527fb85fc5ab418f6394dfd2.jpg)  \n\n![](images/3adac802506529ab8a631562c6acdb891ae16bde7cff6a450e5f38ca795fa628.jpg)  \nFigure 5–57 (a) Unit-step response curve; (b) unit-ramp response curve plotted with unit-ramp input.  \n\nA–5–10. Consider a higher-order system defined by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{6.3223s^{2}+18s\\,+\\,12.811}{s^{4}\\,+\\,6s^{3}+\\,11.3223s^{2}\\,+\\,18s\\,+\\,12.811}\n$$  \n\nUsing MATLAB,plot the unit-step response curve of this system.Using MATLAB,obtain the rise time, peak time, maximum overshoot, and settling time.  \n\nSolution. MATLAB Program 5–20 plots the unit-step response curve as well as giving the rise time, peak time, maximum overshoot, and settling time.The unit-step response curve is shown in Figure 5–58.  \n\nMATLAB Program 5–20   \n$\\%$ ------- This program is to plot the unit-step response curve, as well as to $\\%$ find the rise time, peak time, maximum overshoot, and settling time. $\\%$ In this program the rise time is calculated as the time required for the $\\%$ response to rise from $10\\%$ to $90\\%$ of its final value. -------   \n$\\mathsf{n u m}=[6.3223~~18~~12.811]$ ;  \n$d e\\boldsymbol{\\mathsf{n}}=[1\\ \\ 6\\ \\ 11.3223\\ \\ 18\\ \\ 12.811];$   \n$\\mathrm{t}=0{:}0.02{:}20_{.}$ ;  \n$[\\mathsf{y},\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{t});$ ;  \nplot(t,y)   \ngrid   \ntitle('Unit-Step Response')   \nxlabel('t (sec)')   \nylabel('Output y(t)')   \n$\\boldsymbol{\\mathrm{r}}1=1$ ; while $\\mathsf{y}(\\mathsf{r}\\mathsf{1})<0.1$ ,$\\Gamma1=\\Gamma1+1$ ; end;   \n$\\mathsf{r}2=1\\,;$ while $\\mathsf{y}(\\mathsf{r}2)<0.9$ ,$\\Gamma2=\\mathsf r2\\!+\\!1$ ; end;   \nrise_time $=(\\mathsf{r}2\\!-\\!\\mathsf{r}\\mathsf{\\bar{l}})^{*}\\mathsf{0}.\\;02$   \nrise_time $=$   \n0.5800   \n$[\\mathsf{v m a x,t p}]=\\mathsf{m a x(y)};$ ;  \npeak_time $=(\\mathrm{tp}{-}1)^{*}0.02$   \npeak_time $=$   \n1.6600   \nmax_overshoot $=$ ymax-1   \nmax_overshoot $=$   \n0.6182   \n$\\mathrm{s}=1001$ ; while $\\mathsf{y}(\\mathsf{s})>0.98~\\&~\\mathsf{y}(\\mathsf{s})<1.02;\\mathsf{s}=\\mathsf{s}-1$ ; end;   \nsettling_time $=(\\mathsf{s}_{-}1)^{*}0.02$   \nsettling_time $=$   \n10.0200  \n\n![](images/8931b8e3135835165af0857b30eace8b6ea33e7b4a6a95e6e311579863f547ec.jpg)  \nFigure 5–58 Unit-step response curve.  \n\nA–5–11. Consider the closed-loop system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}+2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nUsing a “for loop,” write a MATLAB program to obtain unit-step response of this system for the following four cases:  \n\n$$\n\\begin{array}{r l r l r l}&{\\mathrm{Case\\1:}}&&{\\zeta=0.3,}&&{\\omega_{n}=1}\\\\ &{}&&\\\\ &{\\mathrm{Case\\2:}}&&{\\zeta=0.5,}&&{\\omega_{n}=2}\\\\ &{}&&\\\\ &{}&&{\\qquad\\quad\\mathrm{Case\\3:}}&&{\\zeta=0.7,}&&{\\omega_{n}=4}\\\\ &{}&&\\\\ &{}&&{\\qquad\\quad\\mathrm{Case\\4:}}&&{\\zeta=0.8,}&&{\\omega_{n}=6}\\end{array}\n$$  \n\nSolution. Define $\\omega_{n}^{2}\\,=\\,a$ and $2\\zeta\\omega_{n}=b$ .Then, $a$ and $^b$ each have four elements as follows:  \n\n$$\n\\begin{array}{l}{{\\mathfrak{a}=[1\\;\\;\\;4\\;\\;\\;16\\;\\;\\;36]}}\\\\ {{}}\\\\ {{\\mathfrak{b}=[0.6\\;\\;\\;2\\;\\;\\;5.6\\;\\;\\;9.6]}}\\end{array}\n$$  \n\nUsing vectors a and b, MATLAB Program 5–21 will produce the unit-step response curves as shown in Figure 5–59.  \n\n![](images/42bcb792c5192da0ebe5ee850b1685d50596abe61cee2cd52b176b85be533c75.jpg)  \n\n![](images/9ea837a96e64ee46188d62a1a7a89ab2fdb4cd862cda3eff493f9c0966211a4d.jpg)  \nFigure 5–59 Unit-step response curves for four cases.  \n\nA–5–12. Using MATLAB, obtain the unit-ramp response of the closed-loop control system whose closedloop transfer function is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{s\\,+\\,10}{s^{3}\\,+\\,6s^{2}\\,+\\,9s\\,+\\,10}}\n$$  \n\nAlso, obtain the response of this system when the input is given by  \n\n$$\nr=e^{-0.5t}\n$$  \n\nSolution. MATLAB Program 5–22 produces the unit-ramp response and the response to the exponential input $r=e^{-0.5t}$ .The resulting response curves are shown in Figures 5–60(a) and (b), respectively.  \n\n![](images/b29a9bc6d5eafb7acdb20843cff8f4e74d604dfa6ec7aa54d3607c010e920db2.jpg)  \n\n![](images/f3b82eb2e6aa68149b628e277d0f18fc0b422deb2d527b0e95d7b9b4f5ed3147.jpg)  \nFigure 5–60 (a) Unit-ramp response curve; (b) response to exponential input $r_{1}\\,=\\,e^{-0.5t}$ .  \n\nA–5–13. Obtain the response of the closed-loop system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{5}{s^{2}\\,+\\,s\\,+\\,5}}\n$$  \n\nwhen the input $r(t)$ is given by  \n\n$$\nr(t)\\,=\\,2\\,+\\,t\n$$  \n\n[The input $r(t)$ is a step input of magnitude 2 plus unit-ramp input.]  \n\nSolution. A possible MATLAB program is shown in MATLAB Program 5–23. The resulting response curve, together with a plot of the input function, is shown in Figure 5–61.  \n\n![](images/8bc59c8a9115d9c76080ec8cbfe06356fb5e8d4b9918676340f5f2a60d6c8455.jpg)  \n\n![](images/5515e926132a419c2bc27e901e8887db99e0131df7604a55e5782a27e1e58db8.jpg)  \nFigure 5–61 Response to input $r(t)\\,=\\,2\\,+\\,t.$ .  \n\nA–5–14. Obtain the response of the system shown in Figure 5–62 when the input $r(t)$ is given by  \n\n$$\nr(t)={\\frac{1}{2}}\\,t^{2}\n$$  \n\n[The input $r(t)$ is the unit-acceleration input.]  \n\nFigure 5–62 Control system.  \n\n![](images/57ef30e2d6d9b5e5256783f88718f6806b409b81b1d36f887c8d1debb4aaf130.jpg)  \n\nSolution. The closed-loop transfer function is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{2}{s^{2}\\,+\\,s\\,+\\,2}}\n$$  \n\nMATLAB Program 5–24 produces the unit-acceleration response.The resulting response,together with the unit-acceleration input, is shown in Figure 5–63.  \n\n![](images/7ec28da86b894342992d15a00ed98f903c5fb49c8df19a4eb3a5e1ad525b3a5e.jpg)  \n\n![](images/f5f053cfd39e2307b1258202b340ea3ddcd912b642770c2d9734b537e4e79fa3.jpg)  \nFigure 5–63 Response to unitacceleration input.  \n\nA–5–15. Consider the system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{1}{s^{2}\\,+\\,2\\zeta s\\,+\\,1}}\n$$  \n\nwhere $\\zeta\\,=\\,0$ ,0.2, 0.4, 0.6, 0.8, and 1.0. Write a MATLAB program using a “for loop” to obtain the two-dimensional and three-dimensional plots of the system output. The input is the unit-step function.  \n\nSolution. MATLAB Program 5–25 is a possible program to obtain two-dimensional and threedimensional plots. Figure 5–64(a) is the two-dimensional plot of the unit-step response curves for various values of $\\zeta.$ . Figure 5–64(b) is the three-dimensional plot obtained by use of the command “mesh(y)” and Figure 5–64(c) is obtained by use of the command “mesh(y ¿)”. (These two three-dimensional plots are basically the same.The only difference is that $x$ axis and $y$ axis are interchanged.)  \n\n# MATLAB Program 5–25  \n\n$\\mathrm{t}=0{:}0.2{:}12,$ ;  \nfor $\\mathsf{n}=1:6$ ;  \n$\\mathsf{n u m}=[1]_{.}$ ;  \n$\\mathsf{d e n}=[1\\;\\;2^{*}(\\mathsf{n}_{-}1)^{*}0.2$ 1];   \n$\\lbrack\\mathsf{y}(1\\!:\\!61,\\!\\mathsf{n}),\\!\\mathrm{x},\\!\\mathrm{t}]=\\mathsf{s t e p}(\\mathsf{n u m},\\!\\mathsf{d e n},\\!\\mathrm{t});$   \nend   \nplot(t,y)   \ngrid   \ntitle('Unit-Step Response Curves')   \nxlabel('t Sec')   \nylabel('Outputs')   \ngtext('\\zeta = 0'),   \ngtext('0.2')   \ngtext('0.4')   \ngtext('0.6')   \n$\\mathrm{gtext}(^{\\prime}0.8^{\\prime})$   \ngtext('1.0')   \n$\\%$ To draw a three-dimensional plot, enter the following command: mesh(y) or mesh(y'). $\\%$ We shall show two three-dimensional plots, one using “mesh(y)” and the other using   \n$\\%$ \"mesh(y')\". These two plots are the same, except that the $\\boldsymbol{\\mathrm{x}}$ axis and y axis are   \n$\\%$ interchanged.   \nmesh(y)   \ntitle('Three-Dimensional Plot of Unit-Step Response Curves using Command \"mesh(y)\"')   \nxlabel ${}^{\\prime}\\mathfrak{n},$ where $\\mathsf{n}=1,2,3,4,5,6^{\\prime})$   \nylabel('Computation Time Points')   \nzlabel('Outputs')   \nmesh(y')   \ntitle('Three-Dimensional Plot of Unit-Step Response Curves using Command \"mesh(y transpose)\"') xlabel('Computation Time Points')   \nylabel ${}^{\\prime}{\\mathfrak{n}},$ where $\\mathsf{n}=1,2,3,4,5,6^{\\prime})$   \nzlabel('Outputs')  \n\n# Figure 5–64  \n\n![](images/2fd711c05b9aaab722134732b1cc49d96716a32880ad365c3b6f2c5e2f9db3d7.jpg)  \n(a) Two-dimensional plot of unit-step response curves; (b) three-dimensional plot of unit-step response curves using command “mesh(y)”; (c) three-dimensional plot of unit-step response curves using command “mesh(y ¿)”.   \nThree-Dimensional Plot of Unit-Step Response Curves using Command “mesh( y)” Three-Dimensional Plot of Unit-Step Response Curves using Command “mesh(y transpose)”  \n\n![](images/795964008d4256301451f60c02316342ecd3ba6fcd51a6e076da4ed563729127.jpg)  \n\nA–5–16. Consider the system subjected to the initial condition as given below.  \n\n![](images/9bb8fddc7bd0d07c0c85fd51686335bc14739301d0b409d87509add52afd67a3.jpg)  \n\n(There is no input or forcing function in this system.) Obtain the response y(t) versus t to the given initial condition by use of Equations (5–58) and (5–60).  \n\nSolution. A possible MATLAB program based on Equations (5–58) and (5–60) is given by MATLAB program 5–26.The response curve obtained here is shown in Figure 5–65. (Notice that this problem was solved by use of the command “initial”in Example 5–16.The response curve obtained here is exactly the same as that shown in Figure 5–34.)  \n\n![](images/46edf4e92370245642f9dbb1a2762ada089b4afe056c86b8ad61e35a89f841db.jpg)  \n\n![](images/b22c27805eec0acac30b335a430995e1bf7ef4029c18f234a10fa9b9d5efeba4.jpg)  \nFigure 5–65 Response y(t) to the given initial condition.  \n\nA–5–17. Consider the following characteristic equation:  \n\n$$\ns^{4}\\,+\\,K s^{3}\\,+\\,s^{2}\\,+\\,s\\,+\\,1\\,=\\,0\n$$  \n\nDetermine the range of $K$ for stability.  \n\nSolution. The Routh array of coefficients is  \n\n$$\n\\begin{array}{c c c c}{{s^{4}}}&{{1}}&{{1}}&{{1}}\\\\ {{s^{3}}}&{{\\;\\;\\;}}&{{K}}&{{1}}&{{0}}\\\\ {{}}&{{}}&{{\\displaystyle{\\frac{K-1}{K}}}}&{{1}}&{{}}\\\\ {{}}&{{}}&{{}}&{{\\displaystyle{s^{1}}}}&{{1-\\frac{K^{2}}{K-1}}}\\\\ {{}}&{{}}&{{}}&{{1}}&{{}}\\end{array}\n$$  \n\nFor stability, we require that  \n\n$$\n\\begin{array}{r}{K>0}\\\\ {\\displaystyle\\frac{K-1}{K}>0}\\\\ {\\displaystyle1-\\frac{K^{2}}{K-1}>0}\\end{array}\n$$  \n\nFrom the first and second conditions, $K$ must be greater than 1. For $K>1$ , notice that the term $1\\,-\\,\\big[K^{2}/(K\\,-\\,1)\\big]$ is always negative, since  \n\n$$\n\\frac{K-1-K^{2}}{K-1}=\\frac{-1+K(1-K)}{K-1}<0\n$$  \n\nThus, the three conditions cannot be fulfilled simultaneously.Therefore, there is no value of $K$ that allows stability of the system.  \n\nA–5–18. Consider the characteristic equation given by  \n\n$$\na_{0}s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,a_{2}s^{n-2}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}=0\n$$  \n\nThe Hurwitz stability criterion, given next, gives conditions for all the roots to have negative real parts in terms of the coefficients of the polynomial.As stated in the discussions of Routh’s stability criterion in Section 5–6, for all the roots to have negative real parts, all the coefficients $a$ ’s must be positive.This is a necessary condition but not a sufficient condition. If this condition is not satisfied, it indicates that some of the roots have positive real parts or are imaginary or zero.A sufficient condition for all the roots to have negative real parts is given in the following Hurwitz stability criterion: If all the coefficients of the polynomial are positive, arrange these coefficients in the following determinant:  \n\n$$\n\\Delta_{n}=\\left|\\begin{array}{c c c c c c c}{{a_{1}}}&{{a_{3}}}&{{a_{5}}}&{{\\cdots}}&{{0}}&{{0}}&{{0}}\\\\ {{a_{0}}}&{{a_{2}}}&{{a_{4}}}&{{\\cdots}}&{{}}&{{\\cdot}}&{{\\cdot}}\\\\ {{0}}&{{a_{1}}}&{{a_{3}}}&{{\\cdots}}&{{a_{n}}}&{{0}}&{{0}}\\\\ {{0}}&{{a_{0}}}&{{a_{2}}}&{{\\cdots}}&{{a_{n-1}}}&{{0}}&{{0}}\\\\ {{.}}&{{.}}&{{.}}&{{}}&{{a_{n-2}}}&{{a_{n}}}&{{0}}\\\\ {{.}}&{{.}}&{{.}}&{{}}&{{a_{n-3}}}&{{a_{n-1}}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{\\cdots}}&{{a_{n-4}}}&{{a_{n-2}}}&{{a_{n}}}\\end{array}\\right|\n$$  \n\nwhere we substituted zero for $a_{s}$ if $s\\,>n$ . For all the roots to have negative real parts, it is necessary and sufficient that successive principal minors of $\\Delta_{n}$ be positive. The successive principal minors are the following determinants:  \n\n$$\n\\Delta_{i}={\\left|\\begin{array}{l l l l}{a_{1}}&{a_{3}}&{\\cdots}&{a_{2i-1}}\\\\ {a_{0}}&{a_{2}}&{\\cdots}&{a_{2i-2}}\\\\ {0}&{a_{1}}&{\\cdots}&{a_{2i-3}}\\\\ {\\cdot}&{\\cdot}&&{\\cdot}\\\\ {0}&{0}&{\\cdots}&{a_{i}}\\end{array}\\right|}\\qquad(i\\,=\\,1,2,\\ldots,n\\,-\\,1)\n$$  \n\nwhere $a_{s}=0$ if $s\\,>n$ . (It is noted that some of the conditions for the lower-order determinants are included in the conditions for the higher-order determinants.) If all these determinants are positive, and $a_{0}>0$ as already assumed, the equilibrium state of the system whose characteristic  \n\nequation is given by Equation (5–67) is asymptotically stable. Note that exact values of determinants are not needed;instead,only signs of these determinants are needed for the stability criterion. Now consider the following characteristic equation:  \n\n$$\na_{0}s^{4}\\,+\\,a_{1}s^{3}\\,+\\,a_{2}s^{2}\\,+\\,a_{3}s\\,+\\,a_{4}=0\n$$  \n\nObtain the conditions for stability using the Hurwitz stability criterion.  \n\nSolution. The conditions for stability are that all the $a$ ’s be positive and that  \n\n$$\n{\\begin{array}{r l}&{\\Delta_{2}={\\left|\\begin{array}{l l}{a_{1}}&{a_{3}}\\\\ {a_{0}}&{a_{2}}\\end{array}\\right|}=a_{1}a_{2}-a_{0}a_{3}>0}\\\\ &{\\Delta_{3}={\\left|\\begin{array}{l l}{a_{1}}&{a_{3}}&{0}\\\\ {a_{0}}&{a_{2}}&{a_{4}}\\\\ {0}&{a_{1}}&{a_{3}}\\end{array}\\right|}}\\\\ &{\\qquad={\\alpha_{1}}(a_{2}a_{3}-a_{1}a_{4})-a_{0}a_{3}^{2}}\\\\ &{=a_{3}(a_{1}a_{2}-a_{0}a_{3})-a_{1}^{2}a_{4}>0}\\end{array}}\n$$  \n\nIt is clear that, if all the $a$ ’s are positive and if the condition $\\Delta_{3}>0$ is satisfied, the condition $\\Delta_{2}>0$ is also satisfied.Therefore,for all the roots of the given characteristic equation to have negative real parts, it is necessary and sufficient that all the coefficients $a$ ’s are positive and $\\Delta_{3}>0$ .  \n\nA–5–19. Show that the first column of the Routh array of  \n\n$$\ns^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,a_{2}s^{n-2}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}=0\n$$  \n\nis given by  \n\n$$\n1,\\quad\\Delta_{1},\\quad{\\frac{\\Delta_{2}}{\\Delta_{1}}},\\quad{\\frac{\\Delta_{3}}{\\Delta_{2}}},\\ldots,\\quad{\\frac{\\Delta_{n}}{\\Delta_{n-1}}}\n$$  \n\nwhere  \n\n$$\n\\Delta_{r}=\\left|\\begin{array}{l l l l l l}{a_{1}}&{1}&{0}&{0}&{\\cdot}&{0}\\\\ {a_{3}}&{a_{2}}&{a_{1}}&{1}&{\\cdot}&{0}\\\\ {a_{5}}&{a_{4}}&{a_{3}}&{a_{2}}&{\\cdot}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {a_{2r-1}}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{a_{r}}\\end{array}\\right|,\\qquad(n\\geq r\\geq1)\\leq n\\leq n\\leq n\\,\\,\\,\n$$  \n\n$$\na_{k}=0\\qquad{\\mathrm{if}}\\;k>n\n$$  \n\nSolution. The Routh array of coefficients has the form  \n\n$$\n\\begin{array}{l l l l l l}{{1}}&{{a_{2}}}&{{a_{4}}}&{{a_{6}}}&{{\\cdots}}&{{a_{n}}}\\\\ {{a_{1}}}&{{a_{3}}}&{{a_{5}}}&{{\\cdots}}&{{}}&{{}}\\\\ {{b_{1}}}&{{b_{2}}}&{{b_{3}}}&{{\\cdots}}&{{}}&{{}}\\\\ {{c_{1}}}&{{c_{2}}}&{{\\cdot}}&{{}}&{{}}&{{}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{}}&{{}}&{{}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{}}&{{}}&{{}}\\end{array}\n$$  \n\nThe first term in the first column of the Routh array is 1.The next term in the first column is $a_{1}$ ,which is equal to $\\Delta_{1}$ .The next term is $b_{1}$ , which is equal to  \n\n$$\n\\frac{a_{1}a_{2}\\,-\\,a_{3}}{a_{1}}=\\frac{\\Delta_{2}}{\\Delta_{1}}\n$$  \n\nThe next term in the first column is $c_{1}$ , which is equal to  \n\n$$\n\\frac{b_{1}a_{3}-a_{1}b_{2}}{b_{1}}=\\frac{\\Bigg[\\frac{a_{1}a_{2}-a_{3}}{a_{1}}\\Bigg]a_{3}-a_{1}\\Bigg[\\frac{a_{1}a_{4}-a_{5}}{a_{1}}\\Bigg]}{\\Bigg[\\frac{a_{1}a_{2}-a_{3}}{a_{1}}\\Bigg]}\n$$  \n\nIn a similar manner the remaining terms in the first column of the Routh array can be found. The Routh array has the property that the last nonzero terms of any columns are the same; that is, if the array is given by  \n\n$$\n\\begin{array}{c c c c}{{a_{0}}}&{{a_{2}}}&{{a_{4}}}&{{a_{6}}}\\\\ {{a_{1}}}&{{a_{3}}}&{{a_{5}}}&{{a_{7}}}\\\\ {{b_{1}}}&{{b_{2}}}&{{b_{3}}}&{{}}\\\\ {{c_{1}}}&{{c_{2}}}&{{c_{3}}}&{{}}\\\\ {{d_{1}}}&{{d_{2}}}&{{}}&{{}}\\\\ {{e_{1}}}&{{e_{2}}}&{{}}&{{}}\\\\ {{f_{1}}}&{{}}&{{}}&{{}}\\\\ {{g_{1}}}&{{}}&{{}}&{{}}\\end{array}\n$$  \n\nthen  \n\n$$\na_{7}=c_{3}=e_{2}=g_{1}\n$$  \n\nand if the array is given by  \n\n$$\n\\begin{array}{r l}&{a_{0}\\quad a_{2}\\quad a_{4}\\quad a_{6}}\\\\ &{a_{1}\\quad a_{3}\\quad a_{5}\\quad0}\\\\ &{b_{1}\\quad b_{2}\\quad b_{3}}\\\\ &{c_{1}\\quad c_{2}\\quad0}\\\\ &{d_{1}\\quad d_{2}}\\\\ &{e_{1}\\quad0}\\\\ &{f_{1}}\\end{array}\n$$  \n\nthen  \n\n$$\na_{6}=b_{3}=d_{2}=f_{1}\n$$  \n\nIn any case, the last term of the first column is equal to $a_{n}$ , or  \n\n$$\na_{n}={\\frac{\\Delta_{n-1}a_{n}}{\\Delta_{n-1}}}={\\frac{\\Delta_{n}}{\\Delta_{n-1}}}\n$$  \n\nFor example, if $n\\,=\\,4$ , then  \n\n$$\n\\Delta_{4}=\\left|\\!\\begin{array}{l l l l}{{a_{1}}}&{{1}}&{{0}}&{{0}}\\\\ {{a_{3}}}&{{a_{2}}}&{{a_{1}}}&{{1}}\\\\ {{a_{5}}}&{{a_{4}}}&{{a_{3}}}&{{a_{2}}}\\\\ {{a_{7}}}&{{a_{6}}}&{{a_{5}}}&{{a_{4}}}\\end{array}\\right|\\!=\\left|\\!\\begin{array}{l l l l}{{a_{1}}}&{{1}}&{{0}}&{{0}}\\\\ {{a_{3}}}&{{a_{2}}}&{{a_{1}}}&{{1}}\\\\ {{0}}&{{a_{4}}}&{{a_{3}}}&{{a_{2}}}\\\\ {{0}}&{{0}}&{{0}}&{{a_{4}}}\\end{array}\\right|\\!=\\Delta_{3}a_{4}\n$$  \n\nThus it has been shown that the first column of the Routh array is given by  \n\n$$\n1,\\quad\\Delta_{1},\\quad\\frac{\\Delta_{2}}{\\Delta_{1}},\\quad\\frac{\\Delta_{3}}{\\Delta_{2}},\\quad\\cdots,\\quad\\frac{\\Delta_{n}}{\\Delta_{n-1}}\n$$  \n\nA–5–20. Show that the Routh’s stability criterion and Hurwitz stability criterion are equivalent.  \n\nSolution. If we write Hurwitz determinants in the triangular form  \n\n$$\n\\Delta_{i}=\\left|\\begin{array}{c c c c c c c}{{a_{11}}}&{{}}&{{}}&{{}}&{{}}&{{\\ast}}\\\\ {{}}&{{a_{22}}}&{{}}&{{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{\\cdot}}&{{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}&{{\\cdot}}&{{}}&{{}}\\\\ {{}}&{{}}&{{}}&{{}}&{{\\cdot}}&{{}}\\\\ {{0}}&{{}}&{{}}&{{}}&{{}}&{{a_{i i}}}\\end{array}\\right|,\\;\\;\\;\\;\\;(i=1,2,\\;...\\;,n)\n$$  \n\nwhere the elements below the diagonal line are all zeros and the elements above the diagonal line any numbers, then the Hurwitz conditions for asymptotic stability become  \n\n$$\n\\Delta_{i}=a_{11}a_{22}\\cdots a_{i i}>0,\\qquad(i=1,2,\\ldots,n)\n$$  \n\nwhich are equivalent to the conditions  \n\n$$\na_{11}>0,\\qquad a_{22}>0,\\qquad\\dots,\\qquad a_{n n}>0\n$$  \n\nWe shall show that these conditions are equivalent to  \n\n$$\na_{1}>0,\\qquad b_{1}>0,\\qquad c_{1}>0,\\qquad\\dots\n$$  \n\nwhere $a_{1},b_{1}$ ,$c_{1},\\ldots,$ , are the elements of the first column in the Routh array. Consider, for example, the following Hurwitz determinant, which corresponds to $i=4$ :  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{1}}&{a_{3}}&{a_{5}}&{a_{7}}\\\\ {a_{0}}&{a_{2}}&{a_{4}}&{a_{6}}\\\\ {0}&{a_{1}}&{a_{3}}&{a_{5}}\\\\ {0}&{a_{0}}&{a_{2}}&{a_{4}}\\end{array}\\right|\n$$  \n\nThe determinant is unchanged if we subtract from the ith row $k$ times the $j$ th row. By subtracting from the second row $a_{0}/a_{1}$ times the first row, we obtain  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{11}}&{a_{3}}&{a_{5}}&{a_{7}}\\\\ {0}&{a_{22}}&{a_{23}}&{a_{24}}\\\\ {0}&{a_{1}}&{a_{3}}&{a_{5}}\\\\ {0}&{a_{0}}&{a_{2}}&{a_{4}}\\end{array}\\right|\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{a_{11}=\\displaystyle a_{1}}}\\\\ {{\\,}}\\\\ {{a_{22}=a_{2}-\\displaystyle\\frac{a_{0}}{a_{1}}a_{3}}}\\\\ {{\\,}}\\\\ {{\\displaystyle a_{23}=a_{4}-\\displaystyle\\frac{a_{0}}{a_{1}}a_{5}}}\\\\ {{\\,}}\\\\ {{a_{24}=a_{6}-\\displaystyle\\frac{a_{0}}{a_{1}}a_{7}}}\\end{array}\n$$  \n\nSimilarly, subtracting from the fourth row $a_{0}/a_{1}$ times the third row yields  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{11}}&{a_{3}}&{a_{5}}&{a_{7}}\\\\ {0}&{a_{22}}&{a_{23}}&{a_{24}}\\\\ {0}&{a_{1}}&{a_{3}}&{a_{5}}\\\\ {0}&{0}&{\\hat{a}_{43}}&{\\hat{a}_{44}}\\end{array}\\right|\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{\\hat{a}_{43}\\,=\\,a_{2}\\,-\\,\\displaystyle\\frac{a_{0}}{a_{1}}\\,a_{3}}}\\\\ {{\\null}}\\\\ {{\\hat{a}_{44}\\,=\\,a_{4}\\,-\\,\\displaystyle\\frac{a_{0}}{a_{1}}\\,a_{5}}}\\end{array}\n$$  \n\nNext, subtracting from the third row $a_{1}/a_{22}$ times the second row yields  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{11}}&{a_{3}}&{a_{5}}&{a_{7}}\\\\ {0}&{a_{22}}&{a_{23}}&{a_{24}}\\\\ {0}&{0}&{a_{33}}&{a_{34}}\\\\ {0}&{0}&{\\hat{a}_{43}}&{\\hat{a}_{44}}\\end{array}\\right|\n$$  \n\nwhere  \n\n$$\n\\begin{array}{l}{{a_{33}\\,=\\,a_{3}\\,-\\,\\frac{a_{1}}{a_{22}}\\,a_{23}}}\\\\ {{\\mathrm{}}}\\\\ {{a_{34}=\\,a_{5}\\,-\\,\\frac{a_{1}}{a_{22}}\\,a_{24}}}\\end{array}\n$$  \n\nFinally, subtracting from the last row $\\hat{a}_{43}/a_{33}$ times the third row yields  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{11}}&{a_{3}}&{a_{5}}&{a_{7}}\\\\ {0}&{a_{22}}&{a_{23}}&{a_{24}}\\\\ {0}&{0}&{a_{33}}&{a_{34}}\\\\ {0}&{0}&{0}&{a_{44}}\\end{array}\\right|\n$$  \n\nwhere  \n\n$$\na_{44}=\\hat{a}_{44}-\\frac{\\hat{a}_{43}}{a_{33}}\\,a_{34}\n$$  \n\nFrom this analysis, we see that  \n\n$$\n\\begin{array}{l}{{\\Delta_{4}=a_{11}a_{22}a_{33}a_{44}}}\\\\ {{\\Delta_{3}=a_{11}a_{22}a_{33}}}\\\\ {{\\Delta_{2}=a_{11}a_{22}}}\\\\ {{\\Delta_{1}=a_{11}}}\\end{array}\n$$  \n\nThe Hurwitz conditions for asymptotic stability  \n\n$$\n\\Delta_{1}>0,\\qquad\\Delta_{2}>0,\\qquad\\Delta_{3}>0,\\qquad\\Delta_{4}>0,\\qquad\\dots.\n$$  \n\nreduce to the conditions  \n\n$$\na_{11}>0,\\qquad a_{22}>0,\\qquad a_{33}>0,\\qquad a_{44}>0,\\qquad\\ldots\n$$  \n\nThe Routh array for the polynomial  \n\n$$\na_{0}s^{4}\\,+\\,a_{1}s^{3}\\,+\\,a_{2}s^{2}\\,+\\,a_{3}s\\,+\\,a_{4}=0\n$$  \n\nwhere $a_{0}>0$ and $n\\,=\\,4$ , is given by  \n\n$$\n\\begin{array}{c c c}{{a_{0}}}&{{a_{2}}}&{{a_{4}}}\\\\ {{a_{1}}}&{{a_{3}}}&{{}}\\\\ {{b_{1}}}&{{b_{2}}}&{{}}\\\\ {{c_{1}}}&{{}}&{{}}\\\\ {{d_{1}}}&{{}}&{{}}\\end{array}\n$$  \n\nFrom this Routh array, we see that  \n\n$$\n\\begin{array}{l}{{a_{11}=\\displaystyle a_{1}}}\\\\ {{\\displaystyle a_{22}=a_{2}-\\frac{a_{0}}{a_{1}}a_{3}=b_{1}}}\\\\ {{\\displaystyle a_{33}=a_{3}-\\frac{a_{1}}{a_{22}}a_{23}=\\frac{a_{3}b_{1}-a_{1}b_{2}}{b_{1}}=c_{1}}}\\\\ {{\\displaystyle a_{44}=\\hat{a}_{44}-\\frac{\\hat{a}_{43}}{a_{33}}a_{34}=a_{4}=d_{1}}}\\end{array}\n$$  \n\n(The last equation is obtained using the fact that $a_{34}=0,\\hat{a}_{44}=a_{4}$ ,and $a_{4}=b_{2}=d_{1}$ .) Hence the Hurwitz conditions for asymptotic stability become  \n\n$$\na_{1}>0,\\qquad b_{1}>0,\\qquad c_{1}>0,\\qquad d_{1}>0\n$$  \n\nThus we have demonstrated that Hurwitz conditions for asymptotic stability can be reduced to Routh’s conditions for asymptotic stability. The same argument can be extended to Hurwitz determinants of any order, and the equivalence of Routh’s stability criterion and Hurwitz stability criterion can be established.  \n\nA–5–21. Consider the characteristic equation  \n\n$$\ns^{4}\\,+\\,2s^{3}\\,+\\,(4\\,+\\,K)s^{2}\\,+\\,9s\\,+\\,25\\,=\\,0\n$$  \n\nUsing the Hurwitz stability criterion, determine the range of $K$ for stability.  \n\nSolution. Comparing the given characteristic equation  \n\n$$\ns^{4}\\,+\\,2s^{3}\\,+\\,(4\\,+\\,K)s^{2}\\,+\\,9s\\,+\\,25\\,=\\,0\n$$  \n\nwith the following standard fourth-order characteristic equation:  \n\n$$\na_{0}s^{4}\\,+\\,a_{1}s^{3}\\,+\\,a_{2}s^{2}\\,+\\,a_{3}s\\,+\\,a_{4}=0\n$$  \n\nwe find  \n\n$$\na_{0}=1,\\ \\ a_{1}=2,\\ \\ a_{2}=4\\,+\\,K,\\ \\ a_{3}=9,\\ \\ a_{4}=25\n$$  \n\nThe Hurwitz stability criterion states that $\\Delta_{4}$ is given by  \n\n$$\n\\Delta_{4}=\\left|\\begin{array}{l l l l}{a_{1}}&{a_{3}}&{0}&{0}\\\\ {a_{0}}&{a_{2}}&{a_{4}}&{0}\\\\ {0}&{a_{1}}&{a_{3}}&{0}\\\\ {0}&{a_{0}}&{a_{2}}&{a_{4}}\\end{array}\\right|\n$$  \n\nFor all the roots to have negative real parts, it is necessary and sufficient that succesive principal minors of $\\Delta_{4}$ be positive.The successive principal minors are  \n\n$$\n{\\begin{array}{r l}&{\\Delta_{1}=\\left|a_{1}\\right|=2}\\\\ &{\\Delta_{2}=\\left|a_{0}\\right|\\,\\,\\,a_{3}\\right|=\\left|{\\begin{array}{r r}{9}&{}\\\\ {1}&{4+K}\\end{array}}\\right|=2K-1}\\\\ &{\\Delta_{3}=\\left|a_{0}\\right|\\,\\,\\,a_{2}\\,\\,\\,a_{4}\\right|=\\left|{\\begin{array}{r r r}{9}&{0}\\\\ {1}&{4+K}&{25}\\end{array}}\\right|=18K-109}\\\\ &{\\Delta_{4}=\\left|0_{0}\\right|\\,\\,\\,a_{1}\\,\\,\\,a_{3}\\right|=\\left|{\\begin{array}{r r r}{0}&{2}&{9}\\\\ {0}&{2}&{9}\\end{array}}\\right|=18K-109}\\end{array}}\n$$  \n\nFor all principal minors to be positive, we require that $\\Delta_{i}(i=1,2,3)$ be positive.Thus, we require  \n\n$$\n\\begin{array}{r}{2K-1>0}\\\\ {18K-109>0}\\end{array}\n$$  \n\nfrom which we obtain the region of $K$ for stability to be  \n\n$$\nK>\\frac{109}{18}\n$$  \n\nA–5–22. Explain why the proportional control of a plant that does not possess an integrating property (which means that the plant transfer function does not include the factor $1/s$ ) suffers offset in response to step inputs.  \n\nSolution. Consider, for example, the system shown in Figure 5–66.At steady state, if $c$ were equal to a nonzero constant $r$ , then $e=0$ and $u\\,=\\,K e\\,=\\,0$ , resulting in $c=0$ , which contradicts the assumption that $c\\,=\\,r\\,=$ nonzero constant.  \n\nFigure 5–66 Control system.  \n\nA nonzero offset must exist for proper operation of such a control system. In other words, at steady state, if $e$ were equal to $r/(1+K)$ , then $u\\,=\\,K r/(1\\,+\\,K)$ and $c\\,=\\,K r/(1\\,+\\,K)$ , which results in the assumed error signal $e\\,=\\,r/(1\\,+\\,K)$ .Thus the offset of $r/(1+K)$ must exist in such a system.  \n\n![](images/a8bffbf8ccafebf5982d47e5f9d428fd329e2c912ce096d8cb7c5b5787280196.jpg)  \n\nA–5–23. The block diagram of Figure 5–67 shows a speed control system in which the output member of the system is subject to a torque disturbance. In the diagram, $\\Omega_{r}(s),\\varOmega(s),T(s)$ and $D(s)$ are the Laplace transforms of the reference speed, output speed, driving torque, and disturbance torque, respectively. In the absence of a disturbance torque, the output speed is equal to the reference speed.  \n\n![](images/406476278385e3d2f8040aff6996fe8cd9561fa3d2ae279da1aaa0bdf02a4ade.jpg)  \nFigure 5–67 Block diagram of a speed control system.  \n\nInvestigate the response of this system to a unit-step disturbance torque. Assume that the reference input is zero, or $\\varOmega_{r}(s)\\,=\\,0$ .  \n\nSolution. Figure 5–68 is a modified block diagram convenient for the present analysis.The closedloop transfer function is  \n\n$$\n{\\frac{\\;\\Omega_{D}(s)\\;}{D(s)}}={\\frac{1}{J s\\;+\\;K}}\n$$  \n\nwhere $\\varOmega_{D}(s)$ is the Laplace transform of the output speed due to the disturbance torque.For a unitstep disturbance torque, the steady-state output velocity is  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\omega_{D}(\\infty)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s.2_{D}(s)}}}\\\\ {{\\,}}\\\\ {{\\displaystyle{=\\,\\operatorname*{lim}_{s\\rightarrow0}\\frac{s}{J_{S}\\,+\\,K}\\frac{1}{s}}}}\\\\ {{\\displaystyle{=\\frac{1}{K}}}}\\end{array}\n$$  \n\nFrom this analysis, we conclude that, if a step disturbance torque is applied to the output member of the system, an error speed will result so that the ensuing motor torque will exactly cancel the disturbance torque.To develop this motor torque, it is necessary that there be an error in speed so that nonzero torque will result. (Discussions continue to Problem A–5–24 .)  \n\n![](images/0e9c4a8743c330d2b221977dc4fc8f0712f37b47ed02d46dc04dd7ed16fdf9fb.jpg)  \nFigure 5–68Block diagram of the speed control system of Figure 5–67 when $\\varOmega_{r}(s)\\,=\\,0$ .  \n\nA–5–24. In the system considered in Problem A–5–23 , it is desired to eliminate as much as possible the speed errors due to torque disturbances.  \n\nIs it possible to cancel the effect of a disturbance torque at steady state so that a constant disturbance torque applied to the output member will cause no speed change at steady state?  \n\nSolution. Suppose that we choose a suitable controller whose transfer function is $G_{c}(s)$ , as shown in Figure 5–69. Then in the absence of the reference input the closed-loop transfer function between the output velocity $\\varOmega_{D}(s)$ and the disturbance torque $D(s)$ is  \n\n$$\n\\begin{array}{r}{\\displaystyle\\frac{\\varOmega_{D}(s)}{D(s)}=\\frac{\\displaystyle\\frac{1}{J s}}{1+\\frac{1}{J s}G_{c}(s)}}\\\\ {\\displaystyle=\\frac{1}{J s+G_{c}(s)}}\\end{array}\n$$  \n\nThe steady-state output speed due to a unit-step disturbance torque is  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\omega_{D}(\\infty)=\\operatorname*{lim}_{s\\rightarrow0}s\\varOmega_{D}(s)}}}\\\\ {{\\displaystyle{=\\operatorname*{lim}_{s\\rightarrow0}\\frac{s}{J s+G_{c}(s)}\\frac{1}{s}}}}\\\\ {{\\displaystyle{=\\frac{1}{G_{c}(0)}}}}\\end{array}\n$$  \n\nTo satisfy the requirement that  \n\n$$\n\\omega_{D}(\\infty)\\,=\\,0\n$$  \n\nwe must choose $G_{c}(0)=\\infty$ .This can be realized if we choose  \n\n$$\nG_{c}(s)={\\frac{K}{s}}\n$$  \n\nIntegral control action will continue to correct until the error is zero. This controller, however, presents a stability problem, because the characteristic equation will have two imaginary roots. One method of stabilizing such a system is to add a proportional mode to the controller or choose  \n\n$$\nG_{c}(s)\\,=\\,K_{p}\\,+\\,{\\frac{K}{s}}\n$$  \n\n![](images/84ac6253d70a2b2f80ae76072009c4f35dcb8219255aaccac4d5b8c60f65acb8.jpg)  \nFigure 5–69 Block diagram of a speed control system.  \n\n![](images/90a8914de4a7b583eba41626953beaad969cbb2c695aeb5a5c85e7f2840967dc.jpg)  \nFigure 5–70 Block diagram of the speed control system of Figure 5–69 when $G_{c}(s)\\,=\\,K_{p}\\,+\\,(K/s)$ and $\\varOmega_{r}(s)\\,=\\,0$ .  \n\nWith this controller, the block diagram of Figure 5–69 in the absence of the reference input can be modified to that of Figure 5–70.The closed-loop transfer function $\\varOmega_{D}(s)/D(s)$ becomes  \n\n$$\n\\frac{\\;\\Omega_{D}(s)}{D(s)}=\\frac{s}{J s^{2}\\,+\\,K_{p}s\\,+\\,K}\n$$  \n\nFor a unit-step disturbance torque, the steady-state output speed is  \n\n$$\n\\omega_{D}(\\infty)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s\\varOmega_{D}(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}\\frac{s^{2}}{J s^{2}\\,+\\,K_{p}s\\,+\\,K}\\frac{1}{s}=\\,0\n$$  \n\nThus, we see that the proportional-plus-integral controller eliminates speed error at steady state. The use of integral control action has increased the order of the system by 1. (This tends to produce an oscillatory response.)  \n\nIn the present system, a step disturbance torque will cause a transient error in the output speed, but the error will become zero at steady state. The integrator provides a nonzero output with zero error. (The nonzero output of the integrator produces a motor torque that exactly cancels the disturbance torque.)  \n\nNote that even if the system may have an integrator in the plant (such as an integrator in the transfer function of the plant), this does not eliminate the steady-state error due to a step disturbance torque.To eliminate this,we must have an integrator before the point where the disturbance torque enters.  \n\nA–5–25. Consider the system shown in Figure 5–71(a). The steady-state error to a unit-ramp input is $e_{\\mathrm{ss}}=2\\zeta/\\omega_{n}$ . Show that the steady-state error for following a ramp input may be eliminated if the input is introduced to the system through a proportional-plus-derivative filter, as shown in Figure 5–71(b), and the value of $k$ is properly set. Note that the error $e(t)$ is given by $r(t)\\,-\\,c(t)$ .  \n\nSolution. The closed-loop transfer function of the system shown in Figure 5–71(b) is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{(1\\,+\\,k s)\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}\n$$  \n\nThen  \n\n$$\nR(s)\\,-\\,C(s)\\,=\\,\\bigg(\\frac{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,-\\,\\omega_{n}^{2}k s}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}\\bigg)R(s)\n$$  \n\n![](images/4aae1b8b84e05eb040de992d414292fd93ece163a025f9e0f3fe4ae951d1998b.jpg)  \nFigure 5–71 (a) Control system; (b) control system with input filter.  \n\nIf the input is a unit ramp, then the steady-state error is  \n\n$$\n{\\begin{array}{r l}&{e(\\infty)=r(\\infty)\\,-\\,c(\\infty)}\\\\ &{\\qquad=\\displaystyle\\operatorname*{lim}_{s\\to0}s\\left({\\frac{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,-\\,\\omega_{n}^{2}k s}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\\right){\\frac{1}{s^{2}}}}\\\\ &{\\qquad={\\frac{2\\zeta\\omega_{n}\\,-\\,\\omega_{n}^{2}k}{\\omega_{n}^{2}}}}\\end{array}}\n$$  \n\nTherefore, if $k$ is chosen as  \n\n$$\nk=\\frac{2\\zeta}{\\omega_{n}}\n$$  \n\nthen the steady-state error for following a ramp input can be made equal to zero.Note that,if there are any variations in the values of $\\zeta$ and/or $\\omega_{n}$ due to environmental changes or aging, then a nonzero steady-state error for a ramp response may result.  \n\nA–5–26. Consider the stable unity-feedback control system with feedforward transfer function $G(s)$ .Suppose that the closed-loop transfer function can be written  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)}=\\frac{\\bigl(T_{a}s\\,+\\,1\\bigr)\\bigl(T_{b}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{m}s\\,+\\,1\\bigr)}{\\bigl(T_{1}s\\,+\\,1\\bigr)\\bigl(T_{2}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{n}s\\,+\\,1\\bigr)}\\qquad(m\\leq n)\n$$  \n\nShow that  \n\n$$\n\\int_{0}^{\\infty}\\!e(t)\\,d t=\\left(T_{1}+T_{2}+\\cdots+T_{n}\\right)-\\left(T_{a}+T_{b}+\\cdots+T_{m}\\right)\n$$  \n\nwhere $e(t)=r(t)\\,-\\,c(t)$ is the error in the unit-step response. Show also that  \n\n$$\n{\\frac{1}{K_{v}}}={\\frac{1}{\\operatorname*{lim}_{s\\rightarrow0}s G(s)}}=\\left(T_{1}+T_{2}+\\cdots+T_{n}\\right)-\\left(T_{a}+T_{b}+\\cdots+T_{m}\\right)\n$$  \n\nSolution. Let us define  \n\n$$\n(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)\\cdots(T_{m}s\\,+\\,1)=P(s)\n$$  \n\nand  \n\n$$\n(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)\\cdots(T_{n}s\\,+\\,1)=Q(s)\n$$  \n\nThen  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{P(s)}{Q(s)}}\n$$  \n\nand  \n\n$$\nE(s)=\\frac{Q(s)\\,-\\,P(s)}{Q(s)}\\,R(s)\n$$  \n\nFor a unit-step input, $R(s)\\,=\\,1/s$ and  \n\n$$\nE(s)=\\frac{Q(s)\\,-\\,P(s)}{s Q(s)}\n$$  \n\nSince the system is stable, $\\textstyle\\int_{0}^{\\infty}\\!e(t)\\,d t$ converges to a constant value. Noting that  \n\n$$\n\\int_{0}^{\\infty}\\!\\!e(t)\\,d t=\\operatorname*{lim}_{s\\to0}s\\,{\\frac{E(s)}{s}}=\\operatorname*{lim}_{s\\to0}E(s)\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{\\infty}\\!e(t)\\,d t=\\operatorname*{lim}_{s\\to0}\\frac{Q(s)\\,-\\,P(s)}{s Q(s)}}}\\\\ &{}&{=\\operatorname*{lim}_{s\\to0}\\frac{Q^{\\prime}(s)\\,-\\,P^{\\prime}(s)}{Q(s)\\,+\\,s Q^{\\prime}(s)}}\\\\ &{}&{=\\operatorname*{lim}_{s\\to0}\\bigl[Q^{\\prime}(s)\\,-\\,P^{\\prime}(s)\\bigr]}\\end{array}\n$$  \n\nSince  \n\n$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}P^{\\prime}(s)=T_{a}+T_{b}+\\cdots+T_{m}}\\\\ {\\quad}\\\\ {\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}Q^{\\prime}(s)=T_{1}+T_{2}+\\cdots+T_{n}}\\end{array}\n$$  \n\nwe have  \n\n$$\n\\int_{0}^{\\infty}\\!e(t)\\,d t=\\left(T_{1}+T_{2}+\\cdots+T_{n}\\right)-\\left(T_{a}+T_{b}+\\cdots+T_{m}\\right)\n$$  \n\nFor a unit-step input $r(t)$ , since  \n\n$$\n\\int_{0}^{\\infty}\\!e(t)\\,d t=\\operatorname*{lim}_{s\\to0}E(s)=\\operatorname*{lim}_{s\\to0}{\\frac{1}{1+G(s)}}\\,R(s)=\\operatorname*{lim}_{s\\to0}{\\frac{1}{1+G(s)}}{\\frac{1}{s}}={\\frac{1}{\\operatorname*{lim}_{s\\to0}s G(s)}}={\\frac{1}{K_{v}}}\n$$  \n\nwe have  \n\n$$\n{\\frac{1}{K_{v}}}={\\frac{1}{\\operatorname*{lim}_{s\\rightarrow0}s G(s)}}=\\left(T_{1}+T_{2}+\\cdots+T_{n}\\right)-\\left(T_{a}+T_{b}+\\cdots+T_{m}\\right)\n$$  \n\nNote that zeros in the left half-plane (that is, positive $T_{a},T_{b},\\dots,T_{m})$ will improve $K_{v}$ . Poles close to the origin cause low velocity-error constants unless there are zeros nearby.  \n\n# PROBLEMS  \n\nB–5–1. A thermometer requires $1\\;\\mathrm{min}$ to indicate $98\\%$ of the response to a step input.Assuming the thermometer to be a first-order system, find the time constant.  \n\nIf the thermometer is placed in a bath, the temperature of which is changing linearly at a rate of $10^{\\circ}/\\mathrm{min}$ , how much error does the thermometer show?  \n\nB–5–2. Consider the unit-step response of a unity-feedback control system whose open-loop transfer function is  \n\n$$\nG(s)={\\frac{1}{s(s+1)}}\n$$  \n\nObtain the rise time, peak time, maximum overshoot, and settling time.  \n\nB–5–3. Consider the closed-loop system given by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nDetermine the values of $\\zeta$ and $\\omega_{n}$ so that the system responds to a step input with approximately $5\\%$ overshoot and with a settling time of 2 sec. (Use the $2\\%$ criterion.)  \n\nB–5–4. Consider the system shown in Figure 5–72.The system is initially at rest. Suppose that the cart is set into motion by an impulsive force whose strength is unity. Can it be stopped by another such impulsive force?  \n\n![](images/b285af6248eebb8fbfe858ec59bf53107224f10c9b9b0814a3f0d3e768a4ffe8.jpg)  \nFigure 5–72 Mechanical system.  \n\nAssume that a record of a damped oscillation is available as shown in Figure 5–73. Determine the damping ratio $\\zeta$ of the system from the graph.  \n\n![](images/aea0035bab7a2e3e090b34ae3620667b560d1c3eefb1802e2ccd5d697ea6f197.jpg)  \nFigure 5–73 Decaying oscillation.  \n\nB–5–5. Obtain the unit-impulse response and the unitstep response of a unity-feedback system whose open-loop transfer function is  \n\n$$\nG(s)=\\frac{2s\\,+\\,1}{s^{2}}\n$$  \n\nB–5–6. An oscillatory system is known to have a transfer function of the following form:  \n\n$$\nG(s)\\,=\\,\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}\n$$  \n\nB–5–7. Consider the system shown in Figure 5–74(a). The damping ratio of this system is 0.158 and the undamped natural frequency is 3.16 rad 'sec. To improve the relative stability,we employ tachometer feedback.Figure 5–74(b) shows such a tachometer-feedback system.  \n\nDetermine the value of $K_{h}$ so that the damping ratio of the system is 0.5.Draw unit-step response curves of both the original and tachometer-feedback systems. Also draw the error-versus-time curves for the unit-ramp response of both systems.  \n\n![](images/c10353ce953103d37f9a04abe90116f44a93d568bcb603fc05b72bb724c9c35d.jpg)  \nFigure 5–74(a) Control system; (b) control system with tachometer feedback.  \n\nB–5–8. Referring to the system shown in Figure 5–75, determine the values of $K$ and $k$ such that the system has a damping ratio $\\zeta$ of 0.7 and an undamped natural frequency $\\omega_{n}$ of 4 rad 'sec.  \n\nB–5–9. Consider the system shown in Figure 5–76. Determine the value of $k$ such that the damping ratio $\\zeta$ is 0.5.Then obtain the rise time $t_{r}$ , peak time $t_{p}$ , maximum overshoot $M_{p}$ , and settling time $t_{s}$ in the unit-step response.  \n\nB–5–10. Using MATLAB, obtain the unit-step response, unit-ramp response, and unit-impulse response of the following system:  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{10}{s^{2}+2s\\,+\\,10}}\n$$  \n\nwhere $R(s)$ and $C(s)$ are Laplace transforms of the input $r(t)$ and output $c(t)$ , respectively.  \n\nB–5–11. Using MATLAB, obtain the unit-step response, unit-ramp response, and unit-impulse response of the following system:  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l}{-1}&{-0.5}\\\\ {1}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\Big[0.5\\!\\!\\!\\Big]u}\\\\ {y=[1}&{0]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nwhere $u$ is the input and $y$ is the output.  \n\nB–5–12. Obtain both analytically and computationally the rise time, peak time, maximum overshoot, and settling time in the unit-step response of a closed-loop system given by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{36}{s^{2}\\,+\\,2s\\,+\\,36}}\n$$  \n\n![](images/fa6ba304694fd6b0fe0520180308b056dff8603c45c910db1c424032556d707f.jpg)  \n\nFigure 5–75 Closed-loop system.  \n\n![](images/a87a273f91417ec8c3094b49ccde48a3352c5b50c3e40f4c46e8d3030b84c441.jpg)  \n\nFigure 5–76 Block diagram of a system.  \n\nB–5–13. Figure 5–77 shows three systems. System I is a positional servo system. System II is a positional servo system with PD control action. System III is a positional servo system with velocity feedback. Compare the unit-step, unitimpulse, and unit-ramp responses of the three systems. Which system is best with respect to the speed of response and maximum overshoot in the step response?  \n\nB–5–14. Consider the position control system shown in Figure 5–78. Write a MATLAB program to obtain a unit-step $x_{1}(t)$ response and a unit-ramp response of the system.Plot curves Cversus $t$ ,$x_{2}(t)$ versus $t$ ,$x_{3}(t)$ versus $t$ , and $e(t)$ versus $t$ where $e(t)\\,=\\,r(t)\\,-\\,x_{1}(t)\\,\\Bigr]$ for both the unit-step response and the unit-ramp response.  \n\n![](images/a52bcd77ce1471eec08b0f4d7b00a4e462da487dbf78c875553c580ec03f7c98.jpg)  \n\n# Figure 5–77  \n\nPositional servo system (System I), positional servo system with PD control action (System II), and positional servo system with velocity feedback (System III).  \n\n![](images/ae96aaeafcece5ab002cb89b305d15223243f11297f69eca511fc94a842a267a.jpg)  \n\nFigure 5–78 Position control system.  \n\nB–5–15. Using MATLAB, obtain the unit-step response curve for the unity-feedback control system whose openloop transfer function is  \n\n$$\nG(s)=\\frac{10}{s(s\\,+\\,2)(s\\,+\\,4)}\n$$  \n\nUsing MATLAB, obtain also the rise time, peak time, maximum overshoot, and settling time in the unit-step response curve.  \n\nB–5–16. Consider the closed-loop system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{2\\zeta s\\,+\\,1}{s^{2}\\,+\\,2\\zeta s\\,+\\,1}}\n$$  \n\nwhere $\\zeta=0.2,0.4,0.6,0.8$ , and 1.0. Using MATLAB, plot a two-dimensional diagram of unit-impulse response curves. Also plot a three-dimensional plot of the response curves.  \n\nB–5–17. Consider the second-order system defined by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{s\\,+\\,1}{s^{2}\\,+\\,2\\zeta s\\,+\\,1}}\n$$  \n\nwhere $\\zeta\\,=\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0.$ . Plot a three-dimensional diagram of the unit-step response curves.  \n\nB–5–18. Obtain the unit-ramp response of the system defined by  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{c c}{0}&{1}\\\\ {-\\,1}&{-\\,1}\\end{array}\\right]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\left[0\\right]\\!\\!u}\\\\ {y=\\left[1\\right.}&{0]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nwhere $u$ is the unit-ramp input. Use the lsim command to obtain the response.  \n\nB–5–19. Consider the differential equation system given by  \n\n$$\n\\ddot{y}\\,+\\,3\\dot{y}\\,+\\,2y\\,=\\,0,\\qquad y(0)\\,=\\,0.1,\\qquad\\dot{y}(0)\\,=\\,0.05\n$$  \n\nUsing MATLAB, obtain the response $y(t)$ , subject to the given initial condition.  \n\nB–5–20. Determine the range of $K$ for stability of a unityfeedback control system whose open-loop transfer function is  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nB–5–21. Consider the following characteristic equation:  \n\n$$\ns^{4}\\,+\\,2s^{3}\\,+\\,(4\\,+\\,K)s^{2}\\,+\\,9s\\,+\\,25\\,=\\,0\n$$  \n\nUsing the Routh stability criterion, determine the range of $K$ for stability.  \n\nB–5–22. Consider the closed-loop system shown in Figure 5–79.   \nDetermine the range of $K$ for stability.Assume that $K>0$ .  \n\n![](images/77a73158f2dcdc6e4690c567b8bc20b7477f245ff3ed203f67ff9bec3159b16f.jpg)  \nFigure 5–79 Closed-loop system.  \n\nB–5–23. Consider the satellite attitude control system shown in Figure 5–80(a).The output of this system exhibits continued oscillations and is not desirable. This system can be stabilized by use of tachometer feedback, as shown in Figure 5–80(b). If $K/J\\,=\\,4$ , what value of $K_{h}$ will yield the damping ratio to be 0.6?  \n\n![](images/c3d79a148e5c5fb3e6a9d0c0e13106ac28b2e04cad154d06021b2d9c7b0c45af.jpg)  \n\n# Figure 5–80  \n\n(a) Unstable satellite attitude control system; (b) stabilized system.  \n\nB–5–24. Consider the servo system with tachometer feedback shown in Figure 5–81. Determine the ranges of stability for $K$ and $K_{h}$ . (Note that $K_{h}$ must be positive.)  \n\nB–5–25. Consider the system  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}\n$$  \n\nwhere matrix $\\mathbf{A}$ is given by  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{\\,\\,0}&{\\,\\,1}&{\\,\\,0}\\\\ {-\\,b_{3}}&{\\,\\,0}&{\\,\\,1}\\\\ {\\,\\,0}&{-\\,b_{2}}&{-\\,b_{1}}\\end{array}\\right]}\n$$  \n\n($\\mathbf{A}$ is called Schwarz matrix.) Show that the first column of the Routh’s array of the characteristic equation $|s\\mathbf{I}-\\mathbf{A}|=0$ consists of $1,b_{1},b_{2}$ , and $b_{1}b_{3}$ .  \n\nB–5–26. Consider a unity-feedback control system with the closed-loop transfer function  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K s\\,+\\,b}{s^{2}\\,+\\,a s\\,+\\,b}\n$$  \n\nDetermine the open-loop transfer function $G(s)$ .  \n\nShow that the steady-state error in the unit-ramp response is given by  \n\n$$\ne_{\\mathrm{ss}}={\\frac{1}{K_{v}}}={\\frac{a\\,-\\,K}{b}}\n$$  \n\nB–5–27. Consider a unity-feedback control system whose open-loop transfer function is  \n\n$$\nG(s)=\\frac{K}{s(J s\\,+\\,B)}\n$$  \n\nDiscuss the effects that varying the values of $K$ and $B$ has on the steady-state error in unit-ramp response. Sketch typical unit-ramp response curves for a small value, medium value, and large value of $K$ , assuming that $B$ is constant.  \n\nB–5–28. If the feedforward path of a control system contains at least one integrating element, then the output continues to change as long as an error is present.The output stops when the error is precisely zero. If an external disturbance enters the system, it is desirable to have an integrating element between the error-measuring element and the point where the disturbance enters, so that the effect of the external disturbance may be made zero at steady state.  \n\nShow that, if the disturbance is a ramp function, then the steady-state error due to this ramp disturbance may be eliminated only if two integrators precede the point where the disturbance enters.  \n\n![](images/499e0f2718b0b34d54626edb3149549bacb780478a60a892f9a51e60f82e65c1.jpg)  \n\nFigure 5–81 Servo system with tachometer feedback.  \n\n# Control Systems Analysis and Design by the Root-Locus Method  \n\nThe basic characteristic of the transient response of a closed-loop system is closely related to the location of the closed-loop poles. If the system has a variable loop gain, then the location of the closed-loop poles depends on the value of the loop gain chosen. It is important, therefore, that the designer know how the closed-loop poles move in the $s$ plane as the loop gain is varied.  \n\nFrom the design viewpoint, in some systems simple gain adjustment may move the closed-loop poles to desired locations.Then the design problem may become the selection of an appropriate gain value. If the gain adjustment alone does not yield a desired result, addition of a compensator to the system will become necessary. (This subject is discussed in detail in Sections 6–6 through 6–9.)  \n\nThe closed-loop poles are the roots of the characteristic equation. Finding the roots of the characteristic equation of degree higher than 3 is laborious and will need computer solution. (MATLAB provides a simple solution to this problem.) However, just finding the roots of the characteristic equation may be of limited value, because as the gain of the open-loop transfer function varies, the characteristic equation changes and the computations must be repeated.  \n\nA simple method for finding the roots of the characteristic equation has been developed by W. R. Evans and used extensively in control engineering. This method, called the root-locus method, is one in which the roots of the characteristic equation are plotted for all values of a system parameter. The roots corresponding to a particular value of this parameter can then be located on the resulting graph. Note that the parameter is usually the gain, but any other variable of the open-loop transfer function may be used. Unless otherwise stated, we shall assume that the gain of the open-loop transfer function is the parameter to be varied through all values, from zero to infinity.  \n\nBy using the root-locus method the designer can predict the effects on the location of the closed-loop poles of varying the gain value or adding open-loop poles and/or open-loop zeros.Therefore, it is desired that the designer have a good understanding of the method for generating the root loci of the closed-loop system, both by hand and by use of a computer software program like MATLAB.  \n\nIn designing a linear control system, we find that the root-locus method proves to be quite useful, since it indicates the manner in which the open-loop poles and zeros should be modified so that the response meets system performance specifications.This method is particularly suited to obtaining approximate results very quickly.  \n\nBecause generating the root loci by use of MATLAB is very simple, one may think sketching the root loci by hand is a waste of time and effort. However, experience in sketching the root loci by hand is invaluable for interpreting computer-generated root loci, as well as for getting a rough idea of the root loci very quickly.  \n\nOutline of the Chapter. The outline of the chapter is as follows: Section 6–1 has presented an introduction to the root-locus method. Section 6–2 details the concepts underlying the root-locus method and presents the general procedure for sketching root loci using illustrative examples. Section 6–3 discusses generating root-locus plots with MATLAB. Section 6–4 treats a special case when the closed-loop system has positive feedback. Section 6–5 presents general aspects of the root-locus approach to the design of closed-loop systems. Section 6–6 discusses the control systems design by lead compensation. Section 6–7 treats the lag compensation technique. Section 6–8 deals with the lag–lead compensation technique. Finally, Section 6–9 discusses the parallel compensation technique.  \n\n# 6–2 ROOT-LOCUS PLOTS  \n\nAngle and Magnitude Conditions. Consider the negative feedback system shown in Figure 6–1.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)H(s)}\n$$  \n\nFigure 6–1 Control system.  \n\n![](images/e4936af7c9638166ca3043ae7ec247cbc9e795feea25a4d0e671be4c96ceabcb.jpg)  \n\nThe characteristic equation for this closed-loop system is obtained by setting the denominator of the right-hand side of Equation (6–1) equal to zero.That is,  \n\n$$\n1\\,+\\,G(s)H(s)\\,=\\,0\n$$  \n\nor  \n\n$$\nG(s)H(s)=-1\n$$  \n\nHere we assume that $G(s)H(s)$ is a ratio of polynomials in $s$ .[It is noted that we can extend the analysis to the case when $G(s)H(s)$ involves the transport lag $e^{-T s}$ .]Since $G(s)H(s)$ is a complex quantity, Equation (6–2) can be split into two equations by equating the angles and magnitudes of both sides, respectively, to obtain the following:  \n\nAngle condition:  \n\n$$\n/G(s)H(s)=\\pm180^{\\circ}(2k\\,+\\,1)\\qquad(k=0,1,2,\\dots)\n$$  \n\nMagnitude condition:  \n\n$$\n|G(s)H(s)|\\,=\\,1\n$$  \n\nThe values of $s$ that fulfill both the angle and magnitude conditions are the roots of the characteristic equation, or the closed-loop poles. A locus of the points in the complex plane satisfying the angle condition alone is the root locus. The roots of the characteristic equation (the closed-loop poles) corresponding to a given value of the gain can be determined from the magnitude condition. The details of applying the angle and magnitude conditions to obtain the closed-loop poles are presented later in this section.  \n\nIn many cases, $G(s)H(s)$ involves a gain parameter $K$ ,and the characteristic equation may be written as  \n\n$$\n1\\,+\\,{\\frac{K(s\\,+\\,z_{1})(s\\,+\\,z_{2})\\cdots(s\\,+\\,z_{m})}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})}}=0\n$$  \n\nThen the root loci for the system are the loci of the closed-loop poles as the gain $K$ is varied from zero to infinity.  \n\nNote that to begin sketching the root loci of a system by the root-locus method we must know the location of the poles and zeros of $G(s)H(s)$ .Remember that the angles of the complex quantities originating from the open-loop poles and open-loop zeros to the test point $s$ are measured in the counterclockwise direction.For example,if $G(s)H(s)$ is given by  \n\n$$\nG(s)H(s)\\,=\\frac{K(s\\,+\\,z_{1})}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})(s\\,+\\,p_{3})(s\\,+\\,p_{4})}\n$$  \n\n![](images/dc611069808f179a4a1324cbd4f157621ec35ac6f9ec03c76f1cc4f918acf19e.jpg)  \nFigure 6–2 (a) and (b) Diagrams showing angle measurements from open-loop poles and open-loop zero to test point $s$ .  \n\nwhere $-p_{2}$ and $-p_{3}$ are complex-conjugate poles, then the angle of $G(s)H(s)$ is  \n\n$$\n\\langle G(s)H(s)=\\,\\phi_{1}\\,-\\,\\theta_{1}\\,-\\,\\theta_{2}\\,-\\,\\theta_{3}\\,-\\,\\theta_{4}\n$$  \n\nwhere $\\phi_{1},\\theta_{1},\\theta_{2},\\theta_{3}$ ,and $\\theta_{4}$ are measured counterclockwise as shown in Figures 6–2(a) and (b).The magnitude of $G(s)H(s)$ for this system is  \n\n$$\n|G(s)H(s)|={\\frac{K B_{1}}{A_{1}A_{2}A_{3}A_{4}}}\n$$  \n\nwhere $A_{1},A_{2},A_{3},A_{4}$ , and $B_{1}$ are the magnitudes of the complex quantities $s\\,+\\,p_{1}$ ,$s\\,+\\,p_{2},s\\,+\\,p_{3},s\\,+\\,p_{4}$ ,and $s\\,+\\,z_{1}$ ,respectively, as shown in Figure 6–2(a).  \n\nNote that, because the open-loop complex-conjugate poles and complex-conjugate zeros,if any,are always located symmetrically about the real axis,the root loci are always symmetrical with respect to this axis.Therefore,we only need to construct the upper half of the root loci and draw the mirror image of the upper half in the lower-half $s$ plane.  \n\nIllustrative Examples. In what follows, two illustrative examples for constructing root-locus plots will be presented. Although computer approaches to the construction of the root loci are easily available, here we shall use graphical computation, combined with inspection, to determine the root loci upon which the roots of the characteristic equation of the closed-loop system must lie. Such a graphical approach will enhance understanding of how the closed-loop poles move in the complex plane as the openloop poles and zeros are moved.Although we employ only simple systems for illustrative purposes, the procedure for finding the root loci is no more complicated for higherorder systems.  \n\nBecause graphical measurements of angles and magnitudes are involved in the analysis, we find it necessary to use the same divisions on the abscissa as on the ordinate axis when sketching the root locus on graph paper.  \n\nConsider the negative feedback system shown in Figure 6–3. (We assume that the value of gain $K$ is nonnegative.) For this system,  \n\n$$\nG(s)=\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)},\\qquad H(s)=1\n$$  \n\nLet us sketch the root-locus plot and then determine the value of $K$ such that the damping ratio $\\zeta$ of a pair of dominant complex-conjugate closed-loop poles is 0.5.  \n\nFor the given system, the angle condition becomes  \n\n$$\n\\begin{array}{l}{\\displaystyle{\\frac{\\slash G(s)}{\\slash G(s)}=\\left\\langle\\frac{K}{s(s+1)(s+2)}\\right.}}\\\\ {\\displaystyle{=-\\big\\langle s-\\big\\langle s+1-\\big\\rangle-\\left.\\frac{\\slash s+2}{\\slash s+1}\\right.}}\\\\ {\\displaystyle{=\\pm180^{\\circ}(2k+1)\\qquad(k=0,1,2,\\dots)}}\\end{array}\n$$  \n\nThe magnitude condition is  \n\n$$\n|G(s)|\\,=\\,\\left|\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}\\right|\\,=\\,1\n$$  \n\nA typical procedure for sketching the root-locus plot is as follows:  \n\n1. Determine the root loci on the real axis. The first step in constructing a root-locus plot is to locate the open-loop poles, $s\\,=\\,0,s\\,=\\,-1$ ,and $s=-2$ ,in the complex plane. (There are no openloop zeros in this system.) The locations of the open-loop poles are indicated by crosses. (The locations of the open-loop zeros in this book will be indicated by small circles.) Note that the starting points of the root loci (the points corresponding to $K=0$ ) are open-loop poles. The number of individual root loci for this system is three, which is the same as the number of open-loop poles.  \n\nTo determine the root loci on the real axis, we select a test point, $s$ .If the test point is on the positive real axis, then  \n\n$$\n\\underline{{{\\slash}}}s\\,=\\,\\left\\langle s\\,+\\,1\\,=\\,\\left\\langle s\\,+\\,2\\,=\\,0^{\\circ}\\right.\n$$  \n\nThis shows that the angle condition cannot be satisfied.Hence,there is no root locus on the positive real axis. Next, select a test point on the negative real axis between 0 and $-1$ .Then  \n\n$$\n\\underline{{{\\slash}s}}\\,=\\,180^{\\circ},\\qquad\\underline{{{\\slash}s}}\\,+\\,1=\\,\\underline{{{\\slash}s}}\\,+\\,2=\\,0^{\\circ}\n$$  \n\nThus  \n\n$$\n-\\,\\Big/s\\,-\\,\\Big/s\\,+\\,1\\,-\\,\\Big/s\\,+\\,2\\,=-180^{\\circ}\n$$  \n\nand the angle condition is satisfied.Therefore, the portion of the negative real axis between 0 and $-1$ forms a portion of the root locus. If a test point is selected between $-1$ and $^{-2}$ ,then  \n\n$$\n\\Big/s\\,=\\,\\big/s\\,+\\,1\\,=\\,180^{\\circ},\\qquad\\big/s\\,+\\,2\\,=\\,0^{\\circ}\n$$  \n\nand  \n\n$$\n-\\,\\Big/s\\,-\\,\\Big/s\\,+\\,1\\,-\\,\\Big/s\\,+\\,2\\,=\\,-360^{\\circ}\n$$  \n\nFigure 6–3 Control system.  \n\n![](images/68229a08154242cf5b54aa6507a68ed131a53c59b0778654e22aead470a9ced0.jpg)  \n\nIt can be seen that the angle condition is not satisfied. Therefore, the negative real axis from $-1$ to $^{-2}$ is not a part of the root locus. Similarly, if a test point is located on the negative real axis from $^{-2}$ to $-\\infty$ , the angle condition is satisfied. Thus, root loci exist on the negative real axis between 0 and $-1$ and between $^{-2}$ and $-\\infty,$ .  \n\n2. Determine the asymptotes of the root loci. The asymptotes of the root loci as $s$ approaches infinity can be determined as follows: If a test point $s$ is selected very far from the origin, then  \n\n$$\n\\operatorname*{lim}_{s\\rightarrow\\infty}G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow\\infty}\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}=\\operatorname*{lim}_{s\\rightarrow\\infty}\\frac{K}{s^{3}}\n$$  \n\nand the angle condition becomes  \n\n$$\n-3\\triangle=\\pm180^{\\circ}(2k\\:+\\:1)\\qquad(k\\:=\\:0,1,2,\\ldots)\n$$  \n\nor  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k\\,+\\,1)}{3}}\\qquad(k\\,=\\,0,1,2,\\dots)\n$$  \n\nSince the angle repeats itself as $k$ is varied, the distinct angles for the asymptotes are determined as $60^{\\circ},-60^{\\circ}$ , and $180^{\\circ}$ . Thus, there are three asymptotes. The one having the angle of $180^{\\circ}$ is the negative real axis.  \n\nBefore we can draw these asymptotes in the complex plane, we must find the point where they intersect the real axis. Since  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nif a test point is located very far from the origin, then $G(s)$ may be written as  \n\n$$\nG(s)\\,=\\,{\\frac{K}{s^{3}\\,+\\,3s^{2}\\,+\\,\\cdots}}\n$$  \n\nFor large values of $s$ , this last equation may be approximated by  \n\n$$\nG(s)\\doteq\\frac{K}{(s+1)^{3}}\n$$  \n\nA root-locus diagram of $G(s)$ given by Equation (6–5) consists of three straight lines.This can be seen as follows:The equation of the root locus is  \n\n$$\n\\left/\\frac{K}{(s\\,+\\,1)^{3}}=\\pm180^{\\circ}(2k\\,+\\,1)\\right.\n$$  \n\nor  \n\n$$\n-3\\/s\\,+\\,1=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nwhich can be written as  \n\n$$\n\\lceil s\\,+\\,1\\,=\\,\\pm60^{\\circ}(2k\\,+\\,1)\n$$  \n\nBy substituting $s\\,=\\,\\sigma\\,+\\,j\\omega$ into this last equation, we obtain  \n\n$$\n\\Big/\\sigma\\,+\\,j\\omega\\,+\\,1\\,=\\,\\pm60^{\\circ}(2k\\,+\\,1)\n$$  \n\nor  \n\n$$\n\\tan^{-1}{\\frac{\\omega}{\\sigma\\,+\\,1}}=\\,60^{\\circ},\\qquad-60^{\\circ},\\qquad0^{\\circ}\n$$  \n\nTaking the tangent of both sides of this last equation,  \n\n$$\n{\\frac{\\omega}{\\sigma\\,+\\,1}}=\\,{\\sqrt{3}}\\,,\\qquad-{\\sqrt{3}}\\,,\\qquad0\n$$  \n\nwhich can be written as  \n\n$$\n\\sigma\\,+\\,1\\,-\\,\\frac{\\omega}{\\sqrt{3}}=0,\\qquad\\sigma\\,+\\,1\\,+\\,\\frac{\\omega}{\\sqrt{3}}=0,\\qquad\\omega\\,=\\,0\n$$  \n\nThese three equations represent three straight lines,as shown in Figure 6–4.The three straight lines shown are the asymptotes. They meet at point $s=-1$ .Thus, the abscissa of the intersection of the asymptotes and the real axis is obtained by setting the denominator of the right-hand side of Equation (6–5) equal to zero and solving for $s$ .The asymptotes are almost parts of the root loci in regions very far from the origin.  \n\n3. Determine the breakaway point. To plot root loci accurately, we must find the breakaway point, where the root-locus branches originating from the poles at 0 and $-1$ break away (as $K$ is increased) from the real axis and move into the complex plane.The breakaway point corresponds to a point in the $s$ plane where multiple roots of the characteristic equation occur.  \n\nA simple method for finding the breakaway point is available. We shall present this method in the following: Let us write the characteristic equation as  \n\n$$\nf(s)\\,=\\,B(s)\\,+\\,K A(s)\\,=\\,0\n$$  \n\n![](images/f647439ff1e023266dcfdcb5cde0c6880ad7e5a19c8489d24c133feff03e8000.jpg)  \n\nwhere $A(s)$ and $B(s)$ do not contain $K$ . Note that $f(s)=0$ has multiple roots at points where  \n\n$$\n\\frac{d f(s)}{d s}=0\n$$  \n\nThis can be seen as follows:Suppose that $f(s)$ has multiple roots of order $r$ ,where $r\\geq2,$ .Then $f(s)$ may be written as  \n\n$$\nf(s)\\,=\\,\\bigl(s\\,-\\,s_{1}\\bigr)^{r}\\bigl(s\\,-\\,s_{2}\\bigr)\\cdots\\bigl(s\\,-\\,s_{n}\\bigr)\n$$  \n\nNow we differentiate this equation with respect to $s$ and evaluate $d f(s)/d s$ at $s\\,=\\,s_{1}$ .Then we get  \n\n$$\n\\frac{d f(s)}{d s}\\left|_{s=s_{1}}\\right.=0\n$$  \n\nThis means that multiple roots of $f(s)$ will satisfy Equation (6–7). From Equation (6–6), we obtain  \n\n$$\n{\\frac{d f(s)}{d s}}=B^{\\prime}(s)\\,+\\,K A^{\\prime}(s)\\,=\\,0\n$$  \n\nwhere  \n\n$$\nA^{\\prime}(s)=\\frac{d A(s)}{d s},\\qquad B^{\\prime}(s)=\\frac{d B(s)}{d s}\n$$  \n\nThe particular value of $K$ that will yield multiple roots of the characteristic equation is obtained from Equation (6–8) as  \n\n$$\nK=-\\,{\\frac{B^{\\prime}(s)}{A^{\\prime}(s)}}\n$$  \n\nIf we substitute this value of $K$ into Equation (6–6), we get  \n\n$$\nf(s)\\,=\\,B(s)\\,-\\,{\\frac{B^{\\prime}(s)}{A^{\\prime}(s)}}\\,A(s)\\,=\\,0\n$$  \n\nor  \n\n$$\nB(s)A^{\\prime}(s)\\,-\\,B^{\\prime}(s)A(s)\\,=\\,0\n$$  \n\nIf Equation (6–9) is solved for $s$ , the points where multiple roots occur can be obtained. On the other hand, from Equation (6–6) we obtain  \n\n$$\nK=-\\,{\\frac{B(s)}{A(s)}}\n$$  \n\nand  \n\n$$\n{\\frac{d K}{d s}}=-\\,{\\frac{B^{\\prime}(s)A(s)\\,-\\,B(s)A^{\\prime}(s)}{A^{2}(s)}}\n$$  \n\nIf $d K/d s$ is set equal to zero, we get the same equation as Equation (6–9). Therefore, the breakaway points can be simply determined from the roots of  \n\n$$\n\\frac{d K}{d s}=0\n$$  \n\nIt should be noted that not all the solutions of Equation (6–9) or of $d K/d s=0$ correspond to actual breakaway points. If a point at which $d K/d s=0$ is on a root locus, it is an actual breakaway or break-in point. Stated differently, if at a point at which $d K/d s\\,=\\,0$ the value of $K$ takes a real positive value, then that point is an actual breakaway or break-in point.  \n\nFor the present example, the characteristic equation $G(s)\\,+\\,1\\,=\\,0$ is given by  \n\n$$\n\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}+1=0\n$$  \n\nor  \n\n$$\nK\\,=\\,-\\big(s^{3}\\,+\\,3s^{2}\\,+\\,2s\\big)\n$$  \n\nBy setting $d K/d s\\,=\\,0$ ,we obtain  \n\n$$\n\\frac{d K}{d s}=-\\big(3s^{2}\\,+\\,6s\\,+\\,2\\big)\\,=\\,0\n$$  \n\nor  \n\n$$\ns\\,=-0.4226,\\qquad s\\,=-1.5774\n$$  \n\nSince the breakaway point must lie on a root locus between 0 and $-1$ ,it is clear that $s\\,=\\,-0.4226$ corresponds to the actual breakaway point. Point $s=-1.5774$ is not on the root locus. Hence, this point is not an actual breakaway or break-in point. In fact, evaluation of the values of $K$ corresponding to $s\\,=\\,-0.4226$ and $s\\,=\\,-1.5774$ yields  \n\n$$\n\\begin{array}{l l}{{K=0.3849,}}&{{\\quad\\mathrm{for}\\;s=-0.4226}}\\\\ {{{}}}&{{{}}}\\\\ {{K=-0.3849,}}&{{\\quad\\mathrm{for}\\;s=-1.5774}}\\end{array}\n$$  \n\n4. Determine the points where the root loci cross the imaginary axis. These points can be found by use of Routh’s stability criterion as follows: Since the characteristic equation for the present system is  \n\n$$\ns^{3}\\,+\\,3s^{2}\\,+\\,2s\\,+\\,K\\,=\\,0\n$$  \n\nthe Routh array becomes  \n\n$$\n\\begin{array}{c c c}{{s^{3}}}&{{\\ }}&{{1}}&{{2}}\\\\ {{s^{2}}}&{{\\ }}&{{3}}&{{K}}\\\\ {{s^{1}}}&{{\\ }}&{{\\displaystyle\\frac{6-K}{3}}}&{{}}\\\\ {{s^{0}}}&{{\\ }}&{{K}}&{{}}\\end{array}\n$$  \n\nThe value of $K$ that makes the $s^{1}$ term in the first column equal zero is $K\\,=\\,6.$ The crossing points on the imaginary axis can then be found by solving the auxiliary equation obtained from the $s^{2}$ row; that is,  \n\n$$\n\\begin{array}{c}{{3s^{2}+K=3s^{2}+6=0}}\\\\ {{}}\\\\ {{s=\\pm j\\sqrt{2}}}\\end{array}\n$$  \n\nwhich yields  \n\nThe frequencies at the crossing points on the imaginary axis are thus $\\omega=\\pm\\sqrt{2}$ .The gain value corresponding to the crossing points is $K=6$ .  \n\nAn alternative approach is to let $s=j\\omega$ in the characteristic equation, equate both the real part and the imaginary part to zero, and then solve for $\\omega$ and $K$ . For the present system, the characteristic equation, with $s=j\\omega,$ , is  \n\n$$\n(j\\omega)^{3}\\,+\\,3(j\\omega)^{2}\\,+\\,2(j\\omega)\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big({\\cal K}\\,-\\,3\\omega^{2}\\big)\\,+\\,j\\big(2\\omega\\,-\\,\\omega^{3}\\big)\\,=\\,0\n$$  \n\nEquating both the real and imaginary parts of this last equation to zero, respectively, we obtain  \n\n$$\nK\\,-\\,3\\omega^{2}=0,\\;\\;\\;\\;\\;\\;2\\omega\\,-\\,\\omega^{3}\\,=\\,0\n$$  \n\n![](images/e3458fa9108f308792bc802779c521ccbe6e94919a6b4628ed52116e05642d8b.jpg)  \n\nFigure 6–5   \nConstruction of root locus.  \n\nfrom which  \n\n$$\n\\omega=\\pm\\sqrt{2}\\,,\\qquad K=6\\qquad\\mathrm{or}\\qquad\\omega=0,\\qquad K=0\n$$  \n\nThus, root loci cross the imaginary axis at $\\omega=\\pm\\sqrt{2}$ ,and the value of $K$ at the crossing points is 6. Also, a root-locus branch on the real axis touches the imaginary axis at $\\omega=0,$ .The value of $K$ is zero at this point.  \n\n5. Choose a test point in the broad neighborhood of the jvaxis and the origin, as shown in Figure 6–5, and apply the angle condition. If a test point is on the root loci, then the sum of the three angles, $\\theta_{1}\\,+\\,\\theta_{2}\\,+\\,\\theta_{3}$ , must be $180^{\\circ}$ . If the test point does not satisfy the angle condition, select another test point until it satisfies the condition. (The sum of the angles at the test point will indicate the direction in which the test point should be moved.) Continue this process and locate a sufficient number of points satisfying the angle condition.  \n\n6. Draw the root loci, based on the information obtained in the foregoing steps, as shown in Figure 6–6.  \n\n![](images/23965e0a069507ec16d5d99c15cd6d7acac535187d58660530f5686490862eca.jpg)  \nFigure 6–6 Root-locus plot.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\n7. Determine a pair of dominant complex-conjugate closed-loop poles such that the damping ratio $\\zeta$ is 0.5. Closed-loop poles with $\\zeta=0.5$ lie on lines passing through the origin and making the angles $\\pm\\cos^{-1}\\zeta=\\pm\\dot{\\cos^{-1}}0.5=\\pm60^{\\circ}$ with the negative real axis.From Figure 6–6,such closedloop poles having $\\zeta=0.5$ are obtained as follows:  \n\n$$\ns_{1}=-0.3337\\,+\\,j0.5780,\\qquad s_{2}=-0.3337\\,-\\,j0.5780\n$$  \n\nThe value of $K$ that yields such poles is found from the magnitude condition as follows:  \n\n$$\n\\begin{array}{l}{{K=|s(s\\,+\\,1)(s\\,+\\,2)|_{s=-0.3337+j0.5780}}}\\\\ {{\\ }}\\\\ {{=1.0383}}\\end{array}\n$$  \n\nUsing this value of $K$ , the third pole is found at $s=-2.3326$ .  \n\nNote that, from step 4, it can be seen that for $K=6$ the dominant closed-loop poles lie on the imaginary axis at $s=\\pm j\\sqrt{2}$ .With this value of $K$ , the system will exhibit sustained oscillations. For $K>6$ ,the dominant closed-loop poles lie in the right-half $s$ plane, resulting in an unstable system.  \n\nFinally, note that, if necessary, the root loci can be easily graduated in terms of $K$ by use of the magnitude condition.We simply pick out a point on a root locus, measure the magnitudes of the three complex quantities $s,s+1$ ,and $s\\,+\\,2$ ,and multiply these magnitudes; the product is equal to the gain value $K$ at that point, or  \n\n$$\n|s|\\,\\cdot\\,|s\\,+\\,1|\\,\\cdot\\,|s\\,+\\,2|\\,=\\,K\n$$  \n\nGraduation of the root loci can be done easily by use of MATLAB. (See Section 6–3.)  \n\n# EXAMPLE 6–2  \n\nIn this example, we shall sketch the root-locus plot of a system with complex-conjugate openloop poles. Consider the negative feedback system shown in Figure 6–7. For this system,  \n\n$$\nG(s)=\\frac{K(s\\,+\\,2)}{s^{2}\\,+\\,2s\\,+\\,3},\\qquad H(s)\\,=\\,1\n$$  \n\nwhere $K\\geq0,$ . It is seen that $G(s)$ has a pair of complex-conjugate poles at  \n\n$$\ns=-1\\,+\\,j\\sqrt{2}\\,,\\qquad s=-1\\,-\\,j\\sqrt{2}\n$$  \n\nA typical procedure for sketching the root-locus plot is as follows:  \n\n1. Determine the root loci on the real axis. For any test point $s$ on the real axis, the sum of the angular contributions of the complex-conjugate poles is $360^{\\circ}$ , as shown in Figure 6–8.Thus the net effect of the complex-conjugate poles is zero on the real axis.The location of the root locus on the real axis is determined from the open-loop zero on the negative real axis.A simple test reveals that a section of the negative real axis, that between $^{-2}$ and $-\\infty$ , is a part of the root locus. It is noted that, since this locus lies between two zeros (at $s=-2$ and $s=-\\infty)$ ), it is actually a part of two root loci, each of which starts from one of the two complex-conjugate poles. In other words, two root loci break in the part of the negative real axis between $^{-2}$ and $-\\infty$ .  \n\n![](images/7470af84d14bac72a9db4ab0390286cf3bd51a5a1be0f0df485c008f1c29aa0c.jpg)  \nFigure 6–7 Control system.  \n\n![](images/16cd5271fa5bf93cc8a383697717f2379022ff8c3746391ca895cc997c888ddc.jpg)  \nFigure 6–8 Determination of the root locus on the real axis.  \n\nSince there are two open-loop poles and one zero,there is one asymptote,which coincides with the negative real axis.  \n\n2. Determine the angle of departure from the complex-conjugate open-loop poles. The presence of a pair of complex-conjugate open-loop poles requires the determination of the angle of departure from these poles. Knowledge of this angle is important, since the root locus near a complex pole yields information as to whether the locus originating from the complex pole migrates toward the real axis or extends toward the asymptote.  \n\nReferring to Figure 6–9, if we choose a test point and move it in the very vicinity of the complex open-loop pole at $s=-p_{1}$ ,we find that the sum of the angular contributions from the pole at $s\\,=\\,p_{2}$ and zero at $s=-z_{1}$ to the test point can be considered remaining the same. If the test point is to be on the root locus, then the sum of $\\phi_{1}^{\\prime},-\\theta_{1}$ , and $-\\theta_{2}^{\\prime}$ must be $\\pm180^{\\circ}(2k\\,+\\,1)$ where $k\\,=\\,0,1,2,\\dots.$ Thus, in the example,  \n\n$$\n\\phi_{1}^{\\prime}\\,-\\,\\left(\\theta_{1}\\,+\\,\\theta_{2}^{\\prime}\\right)\\,=\\,\\pm180^{\\circ}(2k\\,+\\,1)\\\n$$  \n\nor  \n\n$$\n\\theta_{1}\\,=\\,180^{\\circ}\\,-\\,\\theta_{2}^{\\prime}\\,+\\,\\phi_{1}^{\\prime}\\,=\\,180^{\\circ}\\,-\\,\\theta_{2}\\,+\\,\\phi_{1}\n$$  \n\nThe angle of departure is then  \n\n$$\n\\theta_{1}=180^{\\circ}\\mathrm{~-~}\\theta_{2}+\\phi_{1}=180^{\\circ}\\mathrm{~-~}90^{\\circ}\\mathrm{~+~}55^{\\circ}=145^{\\circ}\n$$  \n\n![](images/510c65197836e7434568f44ad8a07aeae6303b05c7476d683e8c609c2b54391a.jpg)  \nFigure 6–9 Determination of the angle of departure.  \n\nSince the root locus is symmetric about the real axis, the angle of departure from the pole at $s=-p_{2}$ is $-145^{\\circ}$ .  \n\n3. Determine the break-in point. A break-in point exists where a pair of root-locus branches coalesces as $K$ is increased. For this problem, the break-in point can be found as follows: Since  \n\n$$\nK=-\\,\\frac{s^{2}\\,+\\,2s\\,+\\,3}{s\\,+\\,2}\n$$  \n\nwe have  \n\n$$\n\\frac{d K}{d s}=-\\,\\frac{(2s\\,+\\,2)(s\\,+\\,2)\\,-\\,\\bigl(s^{2}\\,+\\,2s\\,+\\,3\\bigr)}{(s\\,+\\,2)^{2}}=\\,0\n$$  \n\nwhich gives  \n\n$$\ns^{2}\\,+\\,4s\\,+\\,1\\,=\\,0\n$$  \n\nor  \n\n$$\ns=-3.7320\\qquad\\mathrm{or}\\qquad s=-0.2680\n$$  \n\nNotice that point $s\\,=\\,-3.7320$ is on the root locus. Hence this point is an actual break-in point. (Note that at point $s\\,=\\,-3.7320$ the corresponding gain value is $K\\,=\\,5.4641.$ .) Since point $s=-0.2680$ is not on the root locus, it cannot be a break-in point. (For point $s=-0.2680$ ,the corresponding gain value is $K=-1.4641.$ )  \n\n4. Sketch a root-locus plot, based on the information obtained in the foregoing steps . To determine accurate root loci, several points must be found by trial and error between the breakin point and the complex open-loop poles. (To facilitate sketching the root-locus plot, we should find the direction in which the test point should be moved by mentally summing up the changes on the angles of the poles and zeros.) Figure 6–10 shows a complete root-locus plot for the system considered.  \n\n![](images/f77ac0d779abc5b1d9fdba031cccd6cdfdddc9a5f5a7aba9f88e9d2cf4eb97a0.jpg)  \nFigure 6–10 Root-locus plot.  \n\nThe value of the gain $K$ at any point on root locus can be found by applying the magnitude 1 1 condition or by use of MATLAB (see Section 6–3). For example, the value of $K$ at which the complex-conjugate closed-loop poles have the damping ratio $\\zeta=0.7$ can be found by locating the roots, as shown in Figure 6–10, and computing the value of $K$ as follows:  \n\n$$\nK=\\left|\\frac{(s+1-j\\sqrt{2})(s+1+j\\sqrt{2})}{s+2}\\right|_{s=-1.67+j1.70}=1.34\n$$  \n\nOr use MATLAB to find the value of $K$ . (See Section 6–4.)  \n\nIt is noted that in this system the root locus in the complex plane is a part of a circle. Such a circular root locus will not occur in most systems. Circular root loci may occur in systems that involve two poles and one zero, two poles and two zeros, or one pole and two zeros. Even in such systems, whether circular root loci occur depends on the locations of poles and zeros involved.  \n\nTo show the occurrence of a circular root locus in the present system, we need to derive the equation for the root locus. For the present system, the angle condition is  \n\n$$\n/s\\,+\\,2\\,-\\,\\left/s\\,+\\,1\\,-\\,j\\sqrt{2}\\,-\\,\\left/s\\,+\\,1\\,+\\,j\\sqrt{2}\\,=\\pm180^{\\circ}(2k\\,+\\,1)\\right.\n$$  \n\nIf $s=\\sigma\\,+\\,j\\omega$ is substituted into this last equation, we obtain  \n\n$$\n/\\sigma\\,+\\,2\\,+\\,j\\omega\\,-\\,/\\sigma\\,+\\,1\\,+\\,j\\omega\\,-\\,j\\sqrt{2}\\,-\\,/\\sigma\\,+\\,1\\,+\\,j\\omega\\,+\\,j\\sqrt{2}=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nwhich can be written as  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,2}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,+\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nor  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,+\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,+\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,=\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,2}}\\right)\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\n$$  \n\nTaking tangents of both sides of this last equation using the relationship  \n\n$$\n\\tan(x\\,\\pm\\,y)={\\frac{\\tan x\\,\\pm\\,\\tan y}{1\\,\\mp\\,\\tan x\\tan y}}\n$$  \n\nwe obtain  \n\n$$\n\\tan\\left[\\tan^{-1}\\!\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,+\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,+\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,\\right]\\,=\\,\\tan\\left[\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,2}}\\right)\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\\right]\\,-\\,{\\sqrt{2}}\\,\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,2}}\\right)\\,=\\,-\\,17{\\sqrt{\\,-\\,{\\sqrt{2}}\\,}}\\,\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}\\right)\\,+\\,37{\\sqrt{2}}\\,\\left({\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,2}}\\right)\\,=\\,-\\,17{\\sqrt{\\,-\\,{\\sqrt{2}}}}\\,{\\sqrt{2}}\\,{\\sqrt{2}}\\,{\\sqrt{2}}\n$$  \n\nor  \n\n$$\n{\\frac{{\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}+{\\frac{\\omega\\,+\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}}{1\\,-\\,{\\biggl(}{\\frac{\\omega\\,-\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}{\\biggr)}{\\biggl(}{\\frac{\\omega\\,+\\,{\\sqrt{2}}}{\\sigma\\,+\\,1}}{\\biggr)}}}={\\frac{{\\frac{\\omega}{\\sigma\\,+\\,2}}\\pm\\,0}{1\\,\\mp\\,{\\frac{\\omega}{\\sigma\\,+\\,2}}\\times0}}\n$$  \n\nwhich can be simplified to  \n\n$$\n\\frac{2\\omega(\\sigma\\,+\\,1)}{(\\sigma\\,+\\,1)^{2}\\,-\\,{\\left(\\omega^{2}\\,-\\,2\\right)}}=\\frac{\\omega}{\\sigma\\,+\\,2}\n$$  \n\nor  \n\n$$\n\\omega\\big[(\\sigma\\,+\\,2\\,)^{2}\\,+\\,\\omega^{2}\\,-\\,3\\big]\\,=\\,0\n$$  \n\nThis last equation is equivalent to  \n\n$$\n\\omega\\,=\\,0\\qquad\\mathrm{or}\\qquad(\\sigma\\,+\\,2)^{2}\\,+\\,\\omega^{2}\\,=\\,(\\sqrt{3})^{2}\n$$  \n\nThese two equations are the equations for the root loci for the present system. Notice that the first equation, $\\omega=0$ ,is the equation for the real axis. The real axis from $s=-2$ to $s=-\\infty$ corresponds to a root locus for $K\\ge0$ .The remaining part of the real axis corresponds to a root locus when $K$ is negative. (In the present system, $K$ is nonnegative.) (Note that $K<0$ corresponds to the positive-feedback case.) The second equation for the root locus is an equation of a circle with center at $\\sigma=-2$ ,$\\omega=0$ and the radius equal to $\\sqrt{3}\\,,$ .That part of the circle to the left of the complex-conjugate poles corresponds to a root locus for $K\\geq0$ .The remaining part of the circle corresponds to a root locus when $K$ is negative.  \n\nIt is important to note that easily interpretable equations for the root locus can be derived for simple systems only. For complicated systems having many poles and zeros, any attempt to derive equations for the root loci is discouraged. Such derived equations are very complicated and their configuration in the complex plane is difficult to visualize.  \n\nGeneral Rules for Constructing Root Loci. For a complicated system with many open-loop poles and zeros, constructing a root-locus plot may seem complicated, but actually it is not difficult if the rules for constructing the root loci are applied. By locating particular points and asymptotes and by computing angles of departure from complex poles and angles of arrival at complex zeros, we can construct the general form of the root loci without difficulty.  \n\nWe shall now summarize the general rules and procedure for constructing the root loci of the negative feedback control system shown in Figure 6–11.  \n\nFirst, obtain the characteristic equation  \n\n$$\n1\\,+\\,G(s)H(s)\\,=\\,0\n$$  \n\nThen rearrange this equation so that the parameter of interest appears as the multiplying factor in the form  \n\n$$\n1\\,+\\,{\\frac{K(s\\,+\\,z_{1})(s\\,+\\,z_{2})\\cdots(s\\,+\\,z_{m})}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})}}=0\n$$  \n\nIn the present discussions, we assume that the parameter of interest is the gain $K$ , where $K>0$ .(If $K<0$ ,which corresponds to the positive-feedback case, the angle condition must be modified. See Section 6–4.) Note, however, that the method is still applicable to systems with parameters of interest other than gain. (See Section 6–6.)  \n\n1. Locate the poles and zeros of $G(s)H(s)$ on the splane.The root-locus branches start from open-loop poles and terminate at zeros (finite zeros or zeros at infinity). factored form of the open-loop transfer function, locate the open-loop poles and zeros in the $s$ plane. CNote that the open-loo os are the zeros $G(s)H(s)$ D,From the while the closed-loop zeros consist of the zeros of $G(s)$ and the poles of $H(s)$ .  \n\n![](images/f00de6082f4b33d3a8bf765a0cf9e939d6edc06718c498dda9c3f8137f135811.jpg)  \n\nNote that the root loci are symmetrical about the real axis of the $s$ plane, because the complex poles and complex zeros occur only in conjugate pairs.  \n\nA root-locus plot will have just as many branches as there are roots of the characteristic equation. Since the number of open-loop poles generally exceeds that of zeros, the number of branches equals that of poles. If the number of closed-loop poles is the same as the number of open-loop poles, then the number of individual root-locus branches terminating at finite open-loop zeros is equal to the number $_m$ of the open-loop zeros. The remaining $n\\,-\\,m$ branches terminate at infinity $(n\\mathrm{~-~}m$ implicit zeros at infinity) along asymptotes.  \n\nIf we include poles and zeros at infinity, the number of open-loop poles is equal to that of open-loop zeros. Hence we can always state that the root loci start at the poles of $G(s)H(s)$ and end at the zeros of $G(s)H(s)$ ,as $K$ increases from zero to infinity, where the poles and zeros include both those in the finite $s$ plane and those at infinity.  \n\n2. Determine the root loci on the real axis. Root loci on the real axis are determined by open-loop poles and zeros lying on it. The complex-conjugate poles and complexconjugate zeros of the open-loop transfer function have no effect on the location of the root loci on the real axis because the angle contribution of a pair of complex-conjugate poles or complex-conjugate zeros is $360^{\\circ}$ on the real axis. Each portion of the root locus on the real axis extends over a range from a pole or zero to another pole or zero. In constructing the root loci on the real axis, choose a test point on it. If the total number of real poles and real zeros to the right of this test point is odd, then this point lies on a root locus. If the open-loop poles and open-loop zeros are simple poles and simple zeros, then the root locus and its complement form alternate segments along the real axis.  \n\n3. Determine the asymptotes of root loci. If the test point $s$ is located far from the origin,then the angle of each complex quantity may be considered the same.One open-loop zero and one open-loop pole then cancel the effects of the other. Therefore, the root loci for very large values of $s$ must be asymptotic to straight lines whose angles (slopes) are given by  \n\n$$\n\\mathrm{Angles~of~asymptotes}={\\frac{\\pm180^{\\circ}(2k\\,+\\,1)}{n\\,-\\,m}}\\qquad(k\\,=\\,0,1,2,\\dots)\n$$  \n\nwhere $n=$ number of finite poles of $G(s)H(s)$  \n\nHere, $k\\,=\\,0$ corresponds to the asymptotes with the smallest angle with the real axis.Although $k$ assumes an infinite number of values, as $k$ is increased the angle repeats itself, and the number of distinct asymptotes is $n\\,-\\,m$ .  \n\nAll the asymptotes intersect at a point on the real axis. The point at which they do so is obtained as follows:If both the numerator and denominator of the open-loop transfer function are expanded, the result is  \n\n$$\nG(s)H(s)={\\frac{K[s^{m}+(z_{1}+z_{2}+\\cdots+z_{m})s^{m-1}+\\cdots+z_{1}z_{2}\\cdots z_{m}]}{s^{n}+(p_{1}+p_{2}+\\cdots+p_{n})s^{n-1}+\\cdots+p_{1}p_{2}\\cdots p_{n}}}\n$$  \n\nIf a test point is located very far from the origin, then by dividing the denominator by the numerator, it is possible to write $G(s)H(s)$ as  \n\n$$\nG(s)H(s)=\\frac{K}{s^{n-m}+[(p_{1}+p_{2}+\\cdots+\\,p_{n})-(z_{1}+\\,z_{2}+\\cdots+\\,z_{m})]s^{n-m-1}+\\cdots]}\n$$  \n\nor  \n\n$$\nG(s)H(s)={\\frac{K}{\\left[s+{\\frac{\\left(p_{1}+p_{2}+\\cdots+p_{n}\\right)-\\left(z_{1}+z_{2}+\\cdots+z_{m}\\right)}{n-m}}\\right]^{n-m}}}\n$$  \n\nThe abscissa of the intersection of the asymptotes and the real axis is then obtained by setting the denominator of the right-hand side of Equation (6–12) equal to zero and solving for $s$ ,or  \n\n$$\ns=-{\\frac{\\left(p_{1}+p_{2}+\\cdots+p_{n}\\right)-\\left(z_{1}+z_{2}+\\cdots+z_{m}\\right)}{n-m}}\n$$  \n\n[Example 6–1 shows why Equation (6–13) gives the intersection.] Once this intersection is determined, the asymptotes can be readily drawn in the complex plane.  \n\nIt is important to note that the asymptotes show the behavior of the root loci for $|s|\\gg1$ .A root-locus branch may lie on one side of the corresponding asymptote or may cross the corresponding asymptote from one side to the other side.  \n\n4. Find the breakaway and break-in points. Because of the conjugate symmetry of the root loci, the breakaway points and break-in points either lie on the real axis or occur in complex-conjugate pairs.  \n\nIf a root locus lies between two adjacent open-loop poles on the real axis, then there exists at least one breakaway point between the two poles. Similarly, if the root locus lies between two adjacent zeros (one zero may be located at $-\\infty$ ) on the real axis, then there always exists at least one break-in point between the two zeros. If the root locus lies between an open-loop pole and a zero (finite or infinite) on the real axis, then there may exist no breakaway or break-in points or there may exist both breakaway and break-in points.  \n\nSuppose that the characteristic equation is given by  \n\n$$\nB(s)\\,+\\,K A(s)\\,=\\,0\n$$  \n\nThe breakaway points and break-in points correspond to multiple roots of the characteristic equation.Hence,as discussed in Example 6–1,the breakaway and break-in points can be determined from the roots of  \n\n$$\n{\\frac{d K}{d s}}=-\\,{\\frac{B^{\\prime}(s)A(s)\\,-\\,B(s)A^{\\prime}(s)}{A^{2}(s)}}=0\n$$  \n\nwhere the prime indicates differentiation with respect to $s_{\\scriptscriptstyle\\cdot}$ . It is important to note that the breakaway points and break-in points must be the roots of Equation (6–14), but not all roots of Equation (6–14) are breakaway or break-in points. If a real root of Equation (6–14) lies on the root-locus portion of the real axis, then it is an actual breakaway or break-in point. If a real root of Equation (6–14) is not on the root-locus portion of the real axis, then this root corresponds to neither a breakaway point nor a break-in point.  \n\nIf two roots $s\\implies s_{1}$ and $s=-s_{1}$ of Equation (6–14) are a complex-conjugate pair and if it is not certain whether they are on root loci, then it is necessary to check the corresponding $K$ value. If the value of $K$ corresponding to a root $s\\implies s_{1}$ of $d K/d s=0$ is positive, point $s\\implies s_{1}$ is an actual breakaway or break-in point. (Since $K$ is assumed to be nonnegative, if the value of $K$ thus obtained is negative, or a complex quantity, then point $s\\implies s_{1}$ is neither a breakaway nor a break-in point.)  \n\n5. Determine the angle of departure (angle of arrival) of the root locus from a complex pole (at a complex zero). To sketch the root loci with reasonable accuracy, we must find the directions of the root loci near the complex poles and zeros.If a test point is chosen and moved in the very vicinity of a complex pole (or complex zero), the sum of the angular contributions from all other poles and zeros can be considered to remain the same. Therefore, the angle of departure (or angle of arrival) of the root locus from a complex pole (or at a complex zero) can be found by subtracting from $180^{\\circ}$ the sum of all the angles of vectors from all other poles and zeros to the complex pole (or complex zero) in question, with appropriate signs included.  \n\nAngle of departure from a complex pole $=180^{\\circ}$   \n–(sum of the angles of vectors to a complex pole in question from other poles)   \n$^+$ (sum of the angles of vectors to a complex pole in question from zeros)   \nAngle of arrival at a complex zero $=180^{\\circ}$   \n–(sum of the angles of vectors to a complex zero in question from other zeros)   \n$^+$ (sum of the angles of vectors to a complex zero in question from poles)  \n\nThe angle of departure is shown in Figure 6–12.  \n\n6. Find the points where the root loci may cross the imaginary axis. The points where the root loci intersect the $j\\omega$ axis can be found easily by (a) use of Routh’s stability criterion or (b) letting $s=j\\omega$ in the characteristic equation,equating both the real part and the imaginary part to zero, and solving for $\\omega$ and $K$ .The values of $\\omega$ thus found give the frequencies at which root loci cross the imaginary axis. The $K$ value corresponding to each crossing frequency gives the gain at the crossing point.  \n\n7. Taking a series of test points in the broad neighborhood of the origin of the splane, sketch the root loci. Determine the root loci in the broad neighborhood of the $j\\omega$ axis and the origin. The most important part of the root loci is on neither the real axis nor the asymptotes but is in the broad neighborhood of the $j\\omega$ axis and the origin.The shape of the root loci in this important region in the $s$ plane must be obtained with reasonable accuracy.(If accurate shape of the root loci is needed,MATLAB may be used rather than hand calculations of the exact shape of the root loci.)  \n\n![](images/e27217eeaec33187bffd2087d88c89eed73027706f9962601bfe6a9f9c66471f.jpg)  \nFigure 6–12 Construction of the root locus. [Angle of departure $=180^{\\circ}$ -$(\\theta_{1}\\,+\\,\\theta_{2})\\,+\\,\\phi.]$  \n\n8. Determine closed-loop poles. A particular point on each root-locus branch will be a closed-loop pole if the value of $K$ at that point satisfies the magnitude condition. Conversely, the magnitude condition enables us to determine the value of the gain $K$ at any specific root location on the locus. (If necessary, the root loci may be graduated in terms of $K$ .The root loci are continuous with $K$ .)  \n\nThe value of $K$ corresponding to any point $s$ on a root locus can be obtained using the magnitude condition, or  \n\n$$\nK={\\frac{\\mathrm{product~of~lengths~between~point~}}{\\mathrm{product~of~lengths~between~point~}}}*\\mathrm{to~poles}\n$$  \n\nThis value can be evaluated either graphically or analytically. (MATLAB can be used for graduating the root loci with $K$ . See Section 6–3.)  \n\nIf the gain $K$ of the open-loop transfer function is given in the problem, then by applying the magnitude condition, we can find the correct locations of the closed-loop poles for a given $K$ on each branch of the root loci by a trial-and-error approach or by use of MATLAB, which will be presented in Section 6–3.  \n\nComments on the Root-Locus Plots. It is noted that the characteristic equation of the negative feedback control system whose open-loop transfer function is  \n\n$$\nG(s)H(s)={\\frac{K(s^{m}+b_{1}s^{m-1}+\\cdots+b_{m})}{s^{n}+a_{1}s^{n-1}+\\cdots+a_{n}}}\\qquad(n\\geq m)\n$$  \n\nis an nth-degree algebraic equation in $s.$ . If the order of the numerator of $G(s)H(s)$ is lower than that of the denominator by two or more (which means that there are two or more zeros at infinity), then the coefficient $a_{1}$ is the negative sum of the roots of the equation and is independent of $K$ . In such a case, if some of the roots move on the locus toward the left as $K$ is increased, then the other roots must move toward the right as $K$ is increased.This information is helpful in finding the general shape of the root loci.  \n\nIt is also noted that a slight change in the pole–zero configuration may cause significant changes in the root-locus configurations. Figure 6–13 demonstrates the fact that a slight change in the location of a zero or pole will make the root-locus configuration look quite different.  \n\n![](images/a8e72ba90bc907f43b75813ada8ee6876380be635095d629916556dc9e1cb06b.jpg)  \nFigure 6–13 Root-locus plots.  \n\nCancellation of Poles of $G(s)$ with Zeros of $H(s)$ .It is important to note that if the denominator of $G(s)$ and the numerator of $H(s)$ involve common factors, then the corresponding open-loop poles and zeros will cancel each other, reducing the degree of the characteristic equation by one or more. For example, consider the system shown in Figure 6–14(a). (This system has velocity feedback.) By modifying the block diagram of Figure 6–14(a) to that shown in Figure 6–14(b), it is clearly seen that $G(s)$ and $H(s)$ have a common factor $s\\,+\\,1.$ .The closed-loop transfer function $C(s)/R(s)$ is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)\\,+\\,K(s\\,+\\,1)}\n$$  \n\nThe characteristic equation is  \n\n$$\n\\lceil s(s\\,+\\,2)\\,+\\,K\\rceil(s\\,+\\,1)\\,=\\,0\n$$  \n\nBecause of the cancellation of the terms $(s+1)$ appearing in $G(s)$ and $H(s)$ ,however, we have  \n\n$$\n\\begin{array}{r l}{\\ 1\\ +\\ G(s)H(s)=1\\,-\\frac{K(s\\,+\\,1)}{s(s\\,+\\,1)(s\\,+\\,2)}}&{{}}\\\\ {\\ =\\frac{s(s\\,+\\,2)\\,+\\,K}{s(s\\,+\\,2)}}&{{}}\\end{array}\n$$  \n\nThe reduced characteristic equation is  \n\n$$\ns(s\\,+\\,2)\\,+\\,K\\,=\\,0\n$$  \n\nThe root-locus plot of $G(s)H(s)$ does not show all the roots of the characteristic equation, only the roots of the reduced equation.  \n\nTo obtain the complete set of closed-loop poles, we must add the canceled pole of $G(s)H(s)$ to those closed-loop poles obtained from the root-locus plot of $G(s)H(s)$ .The important thing to remember is that the canceled pole of $G(s)H(s)$ is a closed-loop pole of the system, as seen from Figure 6–14(c).  \n\n![](images/98a08e50d248fc8e5de2351333079071441be5c81c3bab5ef684970495319605.jpg)  \nFigure 6–14 (a) Control system with velocity feedback; (b) and (c) modified block diagrams.  \n\nTypical Pole–Zero Configurations and Corresponding Root Loci. In summarizing, we show several open-loop pole–zero configurations and their corresponding root loci in Table 6–1.The pattern of the root loci depends only on the relative separation of the open-loop poles and zeros. If the number of open-loop poles exceeds the number of finite zeros by three or more, there is a value of the gain $K$ beyond which root loci enter the right-half $s$ plane, and thus the system can become unstable.A stable system must have all its closed-loop poles in the left-half $s$ plane.  \n\n![](images/f0e4ed4f9c8ad5adc949ff3e24b23ddfe68829d8648114921bf2cd619fc8b89e.jpg)  \n\nNote that once we have some experience with the method,we can easily evaluate the changes in the root loci due to the changes in the number and location of the open-loop poles and zeros by visualizing the root-locus plots resulting from various pole–zero configurations.  \n\nSummary. From the preceding discussions, it should be clear that it is possible to sketch a reasonably accurate root-locus diagram for a given system by following simple rules. (The reader should study the various root-locus diagrams shown in the solved problems at the end of the chapter.) At preliminary design stages, we may not need the precise locations of the closed-loop poles. Often their approximate locations are all that is needed to make an estimate of system performance. Thus, it is important that the designer have the capability of quickly sketching the root loci for a given system.  \n\n# 6–3 PLOTTING ROOT LOCI WITH MATLAB  \n\nIn this section we present the MATLAB approach to the generation of root-locus plots and finding relevant information from the root-locus plots.  \n\nPlotting Root Loci with MATLAB. In plotting root loci with MATLAB we deal with the system equation given in the form of Equation (6–11), which may be written as  \n\n$$\n1\\,+\\,K\\,\\frac{\\mathrm{num}}{\\mathrm{den}}=0\n$$  \n\nwhere num is the numerator polynomial and den is the denominator polynomial. That is,  \n\n$$\n{\\begin{array}{r l}&{{\\mathrm{num}}=(s\\,+\\,z_{1})(s\\,+\\,z_{2})\\cdots(s\\,+\\,z_{m})}\\\\ &{\\qquad=s^{m}+\\,{\\bigl(}z_{1}\\,+\\,z_{2}\\,+\\,\\cdots+\\,z_{m}{\\bigr)}s^{m-1}\\,+\\,\\cdots+\\,z_{1}z_{2}\\cdots z_{m}}\\\\ &{{\\mathrm{den}}=(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})}\\\\ &{\\qquad=s^{n}\\,+\\,{\\bigl(}p_{1}\\,+\\,p_{2}+\\cdots+\\,p_{n}{\\bigr)}s^{n-1}\\,+\\,\\cdots+\\,p_{1}p_{2}\\cdots p_{m}}\\end{array}}\n$$  \n\nNote that both vectors num and den must be written in descending powers of $s$ .  \n\nA MATLAB command commonly used for plotting root loci is  \n\nUsing this command, the root-locus plot is drawn on the screen.The gain vector Kis automatically determined. (The vector Kcontains all the gain values for which the closedloop poles are to be computed.)  \n\nFor the systems defined in state space, rlocus(A,B,C,D) plots the root locus of the system with the gain vector automatically determined.  \n\nNote that commands  \n\nuse the user-supplied gain vector K.  \n\nIf it is desired to plot the root loci with marks $^{\\prime}\\mathrm{o}^{\\prime}$ or $\"X\"$ , it is necessary to use the following command:  \n\n$$\n\\begin{array}{r l}&{\\mathsf{r}=\\mathsf{r}|\\mathrm{ocus}(\\mathsf{n u m},\\mathsf{d e n})}\\\\ &{\\mathsf{p l o t}(\\Gamma,\\mathrm{^{\\prime}O^{\\prime}})\\,\\,\\,\\,\\,\\,\\mathrm{or}\\,\\,\\,\\,\\,\\,\\,\\mathsf{p l o t}(\\Gamma,\\mathrm{^{\\prime}x^{\\prime}})}\\end{array}\n$$  \n\nPlotting root loci using marks oor $\\boldsymbol{\\mathrm{x}}$ is instructive, since each calculated closed-loop pole is graphically shown; in some portion of the root loci those marks are densely placed and in another portion of the root loci they are sparsely placed.MATLAB supplies its own set of gain values used to calculate a root-locus plot. It does so by an internal adaptive stepsize routine.Also,MATLAB uses the automatic axis-scaling feature of the plot command.  \n\n# EXAMPLE 6–3  \n\nConsider the system shown in Figure 6–15. Plot root loci with a square aspect ratio so that a line with slope 1 is a true $45^{\\circ}$ line. Choose the region of root-locus plot to be  \n\n$$\n-6\\leq x\\leq6,\\qquad-6\\leq y\\leq6\n$$  \n\nwhere $x$ and $y$ are the real-axis coordinate and imaginary-axis coordinate, respectively. To set the given plot region on the screen to be square, enter the command  \n\n$$\n\\mathrm{\\Deltav=[-6\\,\\,\\,6\\,\\,\\,-6\\,\\,\\,6]\\,;\\,axis\\,\\,(v);\\,axis(^{\\prime}s q u a r e^{\\prime})}\n$$  \n\nWith this command, the region of the plot is as specified and a line with slope 1 is at a true $45^{\\circ}$ ,not skewed by the irregular shape of the screen.  \n\nFor this problem, the denominator is given as a product of first- and second-order terms. So we must multiply these terms to get a polynomial in $s$ .The multiplication of these terms can be done easily by use of the convolution command, as shown next.  \n\nDefine  \n\n$$\n\\begin{array}{l l}{{\\mathrm{a}=\\mathrm{s}\\;(\\mathrm{s}+\\,1);\\;\\;}}&{{\\mathrm{a}=[1\\;\\;\\;1\\;\\;\\;0]}}\\\\ {{\\mathrm{b}=\\mathrm{s}^{2}+\\,4\\mathrm{s}+\\,1\\,6;\\;\\;}}&{{\\mathrm{b}=[1\\;\\;\\;4\\;\\;\\;1\\,6]}}\\end{array}\n$$  \n\nThen we use the following command:  \n\n$$\n{\\mathsf{c}}=\\mathsf{c o n v}({\\mathsf{a}},\\,{\\mathsf{b}})\n$$  \n\nNote that $\\mathsf{c o n v}(\\mathsf{a},\\mathsf{b})$ gives the product of two polynomials a and b.See the following computer output:  \n\n![](images/9c8c6e38d508cba0f39cc1486ba2d58ef2a59193e19354934cef5cb2f4d74a44.jpg)  \n\nThe denominator polynomial is thus found to be  \n\n$$\n\\mathrm{den}=[1\\;\\;5\\;\\;20\\;\\;16\\;\\;0]\n$$  \n\n![](images/c1d0664d26e3b40becc718cebd670b6f52f8e846bb6b19c1b15730bb74e10b44.jpg)  \nFigure 6–15 Control system.  \n\nTo find the complex-conjugate open-loop poles (the roots of $s^{2}\\,+\\,4s\\,+\\,16\\,=\\,0$ ), we may enter the roots command as follows:  \n\nThus, the system has the following open-loop zero and open-loop poles:  \n\nOpen-loop zero: $s=-3$  \n\n$$\ns\\,=\\,0,\\quad s\\,=\\,-1,\\quad s\\,=\\,-2\\,\\pm\\,j3.4641\n$$  \n\nMATLAB Program 6–1 will plot the root-locus diagram for this system. The plot is shown in Figure 6–16.  \n\n![](images/b8fd5865cf865a42ca3776e83bf2bb1a861101ebcf3e4073fd9f63a18640d08b.jpg)  \n\nNote that in MATLAB Program 6–1, instead of  \n\n$$\n\\mathrm{den}=[1\\;\\;5\\;\\;20\\;\\;16\\;\\;0]\n$$  \n\nwe may enter  \n\n$$\n\\mathrm{den}=\\mathsf{c o n v}\\left([1\\;\\;\\;1\\;\\;\\;0],\\;[1\\;\\;\\;4\\;\\;\\;1\\,6]\\right)\n$$  \n\nThe results are the same.  \n\n![](images/49a7cdfa85c98bf4a9663f40fb579bfd1a786435210f78149b3145b145650e59.jpg)  \nFigure 6–16 Root-locus plot.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nEXAMPLE 6–4 Consider the negative feedback system whose open-loop transfer function $G(s)H(s)$ is  \n\n$$\n\\begin{array}{c}{{G(s)H(s)=\\displaystyle\\frac{K}{s(s\\,+\\,0.5)\\left(s^{2}\\,+\\,0.6s\\,+\\,10\\right)}}}\\\\ {{=\\displaystyle\\frac{K}{s^{4}\\,+\\,1.1s^{3}\\,+\\,10.3s^{2}\\,+\\,5s}}}\\end{array}\n$$  \n\nThere are no open-loop zeros. Open-loop poles are located at $s\\,=\\,-0.3\\,+\\,j3.1480$ ,$s\\,=\\,-0.3\\,-\\,j3.1480,s$ $s=-0.5$ ,and $s\\,=\\,0$ .  \n\nEntering MATLAB Program 6–2 into the computer, we obtain the root-locus plot shown in Figure 6–17.  \n\n![](images/3e4a3b988e307a713e4f6d69f4f0984b7770b0f57a4b7be193ae654007d23e2e.jpg)  \n\nNotice that in the regions near $x\\,=\\,-0.3$ ,$y=2.3$ and $x\\,=\\,-0.3$ ,$y\\,=\\,-2.3$ two loci approach each other.We may wonder if these two branches should touch or not.To explore this situation, we may plot the root loci using smaller increments of $K$ in the critical region.  \n\n![](images/5bcbd3be69d5992ccb82c6a715d15c09c883cd23560134952fc1d2153ce4b8c1.jpg)  \nFigure 6–17 Root-locus plot.  \n\nFigure 6–18 Root-locus plot.  \n\n![](images/9633c8b40ab83d4c611dfd1b0be64021d16a7c361f8ad4a4ec87b4a40dd20b91.jpg)  \n\nBy a conventional trial-and-error approach or using the command rlocfind to be presented later in this section, we find the particular region of interest to be $20\\,\\leq\\,K\\,\\leq\\,30.$ . By entering MATLAB Program 6–3, we obtain the root-locus plot shown in Figure 6–18. From this plot, it is clear that the two branches that approach in the upper half-plane (or in the lower half-plane) do not touch.  \n\n![](images/b8b3bd8d6da29d797bac7b290385ee46eda8771bebeab6a1409a46d63e31b299.jpg)  \n\nEXAMPLE 6–5 Consider the system shown in Figure 6–19.The system equations are  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {y=\\mathbf{C}\\mathbf{x}+D u}\\\\ {u=r-y}\\end{array}\n$$  \n\n![](images/cb73453bd606910da8df4c0c33aed62f2e79c014b9a9aefe2cfa11528e598ff4.jpg)  \nFigure 6–19 Closed-loop control system.  \n\nIn this example problem we shall obtain the root-locus diagram of the system defined in state space.As an example let us consider the case where matrices A ,B,C, and $D$ are  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{A}={\\left[\\begin{array}{l l l}{\\phantom{-}0}&{\\phantom{-}1}&{0}\\\\ {\\phantom{-}0}&{0}&{1}\\\\ {-160}&{-56}&{-14}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{\\phantom{-}0}\\\\ {\\phantom{-}1}\\\\ {-14}\\end{array}\\right]}}\\\\ &{\\mathbf{C}=[1\\quad0\\quad0],}&{D=[0]}\\end{array}}\n$$  \n\nThe root-locus plot for this system can be obtained with MATLAB by use of the following command:  \n\nThis command will produce the same root-locus plot as can be obtained by use of the rlocus (num,den) command, where num and den are obtained from  \n\n$$\n[\\mathrm{num},\\mathrm{den}]=\\mathrm{ss}2\\mathrm{tf}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D})\n$$  \n\nas follows:  \n\n$$\n\\begin{array}{c}{{\\mathsf{n u m}=[0\\;\\;0\\;\\;1\\;\\;0]}}\\\\ {{\\mathsf{d e n}=[1\\;\\;14\\;\\;56\\;\\;160]}}\\end{array}\n$$  \n\nMATLAB Program 6–4 is a program that will generate the root-locus plot as shown in Figure 6–20.  \n\n![](images/2f4a972f1e9700c4c6bfb5a95dca568e4d423c59b1c2f61772b46dd7357e7563.jpg)  \n\n![](images/58e7d918e5a6d8462c3ca8ad0a155e0901cc0785d2b96792bb2e73c08efaf147.jpg)  \nFigure 6–20 Root-locus plot of system defined in state space, where A ,B,C, and $D$ are as given by Equation (6–15).  \n\nConstant $\\boldsymbol{\\zeta}$ Loci and Constant $\\pmb{\\omega}_{n}$ Loci. Recall that in the complex plane the damping ratio $\\zeta$ of a pair of complex-conjugate poles can be expressed in terms of the angle $\\phi$ , which is measured from the negative real axis, as shown in Figure 6–21(a) with  \n\n$$\n\\zeta\\,=\\,\\cos\\phi\n$$  \n\nIn other words, lines of constant damping ratio $\\zeta$ are radial lines passing through the origin as shown in Figure 6–21(b). For example, a damping ratio of 0.5 requires that the complex-conjugate poles lie on the lines drawn through the origin making angles of $\\pm60^{\\circ}$ with the negative real axis. (If the real part of a pair of complex-conjugate poles is positive, which means that the system is unstable, the corresponding $\\zeta$ is negative.) The damping ratio determines the angular location of the poles, while the distance of the pole from the origin is determined by the undamped natural frequency $\\omega_{n}$ .The constant $\\omega_{n}$ loci are circles.  \n\n![](images/9405094961609c00f699df09485e7c5c91a2dc681fea3eaaa5266f08bb30d126.jpg)  \nFigure 6–21 (a) Complex poles; (b) lines of constant damping ratio $\\zeta$ .  \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nTo draw constant $\\zeta$ lines and constant $\\omega_{n}$ circles on the root-locus diagram with MATLAB, use the command sgrid .  \n\n# Plotting Polar Grids in the Root-Locus Diagram. The command  \n\nsgrid  \n\noverlays lines of constant damping ratio ( $\\langle\\zeta=0\\sim1$ with 0.1 increment) and circles of constant $\\omega_{n}$ on the root-locus plot.See MATLAB Program 6–5 and the resulting diagram shown in Figure 6–22.  \n\n![](images/3d2add663e39b12e2f4ea6f33b7b65361816573eeae2590de806c0e8c7967dfa.jpg)  \n\nIf only particular constant $\\zeta$ lines (such as the $\\zeta=0.5$ line and $\\zeta\\,=\\,0.707$ line) and particular constant $\\omega_{n}$ circles (such as the $\\omega_{n}\\,=\\,0.5$ circle, $\\omega_{n}=1$ circle, and $\\omega_{n}=2$ circle) are desired, use the following command:  \n\n$$\n\\mathsf{s g r i d}([0.5,\\ 0.707],\\ [0.5,\\ 1,\\ 2])\n$$  \n\nIf we wish to overlay lines of constant $\\zeta$ and circles of constant $\\omega_{n}$ as given above to a root-locus plot of a negative feedback system with  \n\n$$\n\\begin{array}{r}{\\mathsf{n u m}=[0\\;\\;0\\;\\;0\\;\\;1]}\\\\ {\\mathsf{d e n}=[1\\;\\;4\\;\\;5\\;\\;0]}\\end{array}\n$$  \n\n![](images/69f60707a6d7fc2f02907a189befc5ce500b26e313e99bcfac19d606dcd79edd.jpg)  \nConstant $\\zeta$ Lines and Constant $\\omega_{n}$ Circles   \nFigure 6–22 Constant $\\zeta$ lines and constant $\\omega_{n}$ circles.  \n\nthen enter MATLAB Program 6–6 into the computer. The resulting root-locus plot is shown in Figure 6–23.  \n\n![](images/33126d46770d6fced49b8320232b2581b86fdc7c87676a93101a81d0179e4d36.jpg)  \n\nIf we want to omit either the entire constant $\\zeta$ lines or entire constant $\\omega_{n}$ circles, we may use empty brackets [] in the arguments of the sgrid command.For example,if we want to overlay only the constant damping ratio line corresponding to $\\zeta=0.5$ and no constant $\\omega_{n}$ circles on the root-locus plot, then we may use the command  \n\n![](images/89b7d20a4ca26280d6977c60bbb9860daac91bd5b16957a19d6ac4ea4676bbf6.jpg)  \nRoot-Locus Plot with $\\zeta=0.5$ and 0.707 Lines   \nFigure 6–23 Constant $\\zeta$ lines and constant $\\omega_{n}$ circles superimposed on a root-locus plot.  \n\nFigure 6–24Control system.  \n\n![](images/1fd392b88ee559979d5cf40978fcd56295ac734c2e98a2ec0925db30d5f6e084.jpg)  \n\nConditionally Stable Systems. Consider the negative feedback system shown in Figure 6–24.We can plot the root loci for this system by applying the general rules and procedure for constructing root loci, or use MATLAB to get root-locus plots. MATLAB Program 6–7 will plot the root-locus diagram for the system.The plot is shown in Figure 6–25.  \n\n![](images/fa732871cc30caf055e62f7d5b69c4365848c5983af01c67003059fe2f181386.jpg)  \n\nIt can be seen from the root-locus plot of Figure 6–25 that this system is stable only for limited ranges of the value of $K_{\\l}$ —that is, $0<K<12$ and $73\\,<\\,K\\,<\\,154.$ The system becomes unstable for $12<K<73$ and $154<K$ .(If $K$ assumes a value corresponding to unstable operation, the system may break down or may become nonlinear due to a saturation nonlinearity that may exist.) Such a system is called conditionally stable.  \n\n![](images/1b312d91959eeddd8af6b6849fdbc7f4bf1a73e417d284c4eadac6a79e4b1528.jpg)  \n  \nFigure 6–25 Root-locus plot of conditionally stable system.  \n\nIn practice, conditionally stable systems are not desirable. Conditional stability is dangerous but does occur in certain systems—in particular, a system that has an unstable feedforward path. Such an unstable feedforward path may occur if the system has a minor loop. It is advisable to avoid such conditional stability since, if the gain drops beyond the critical value for any reason, the system becomes unstable. Note that the addition of a proper compensating network will eliminate conditional stability.[An addition of a zero will cause the root loci to bend to the left. (See Section 6–5.) Hence conditional stability may be eliminated by adding proper compensation.]  \n\nNonminimum-Phase Systems. If all the poles and zeros of a system lie in the lefthalf $s$ plane, then the system is called minimum phase. If a system has at least one pole or zero in the right-half $s$ plane, then the system is called nonminimum phase. The term nonminimum phase comes from the phase-shift characteristics of such a system when subjected to sinusoidal inputs.  \n\nConsider the system shown in Figure 6–26(a). For this system  \n\n$$\nG(s)=\\frac{K\\!\\left(1\\,-\\,T_{a}s\\right)}{s(T s\\,+\\,1)}\\qquad\\bigl(T_{a}>0\\bigr),\\qquad H(s)=1\n$$  \n\nThis is a nonminimum-phase system, since there is one zero in the right-half $s$ plane. For this system, the angle condition becomes  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\big/G(s)}{\\big/}=\\Bigg\\langle\\!\\frac{K\\big(T_{a}s\\,-\\,1\\big)}{s\\big(T s\\,+\\,1\\big)}\\qquad}}\\\\ &{=\\Bigg\\lceil\\!\\frac{K\\big(T_{a}s\\,-\\,1\\big)}{s\\big(T s\\,+\\,1\\big)}+\\,180^{\\circ}\\qquad}\\\\ &{=\\pm180^{\\circ}(2k\\,+\\,1)\\qquad(k\\,=\\,0,1,2,\\dots)}\\end{array}\n$$  \n\nor  \n\n$$\n\\left\\langle\\frac{K(T_{a}s\\,-\\,1)}{s(T s\\,+\\,1)}=0^{\\circ}\\right.\n$$  \n\n![](images/3f2af6649ea56f7cde610436f1c493bb21d62a80df5b95c2607c5f77f0972918.jpg)  \nFigure 6–26 (a) Nonminimumphase system; (b) root-locus plot.  \n\nThe root loci can be obtained from Equation (6–16). Figure 6–26(b) shows a root-locus plot for this system. From the diagram, we see that the system is stable if the gain $K$ is less than $1/T_{a}$ .  \n\nFigure 6–27 Root-locus plot of $G(s)=\\frac{K(1-0.5s)}{s(s+1)}.$  \n\n![](images/d4ff42ab9299114b7311e26215c4dee611ccf5184b96216ac243db7898d6e6cc.jpg)  \n\nTo obtain a root-locus plot with MATLAB, enter the numerator and denominator as usual. For example, if $T=1$ sec and $T_{a}\\,=\\,0.5$ sec ,enter the following num and den in the program:  \n\n$$\n\\begin{array}{r}{\\mathsf{n u m}=[-0.5\\ \\ 1]}\\\\ {\\mathsf{d e n}=[1\\ \\ 1\\ \\ 0]}\\end{array}\n$$  \n\nMATLAB Program 6–8 gives the plot of the root loci shown in Figure 6–27.  \n\n![](images/d2cb4ec43077a695b37ac648c2209887160431568baa043f7132413cff5ae3f8.jpg)  \n\nOrthogonality of Root Loci and Constant-Gain Loci. Consider the negative feedback system whose open-loop transfer function is $G(s)H(s)$ .In the $G(s)H(s)$ plane, the loci of $|G(s)H(s)|\\,=$ constant are circles centered at the origin, and the loci corresponding to $\\ L G(s)H(s)=\\pm180^{\\circ}(2k\\mathrm{~+~}1)$ $(k\\,=\\,0,1,2,.\\,.\\,.)$ lie on the negative real axis of the $G(s)H(s)$ plane, as shown in Figure 6–28. [Note that the complex plane employed here is not the $s$ plane, but the $G(s)H(s)$ plane.]  \n\n![](images/46d8dc8c99b3340031d7f64484c46805f9867200e4397b52adf23c66fe70ab7a.jpg)  \nFigure 6–28 Plots of constantgain and constantphase loci in the $G(s)H(s)$ plane.  \n\nThe root loci and constant-gain loci in the $s$ plane are conformal mappings of the loci of $/G(s)H(s)=\\pm180^{\\circ}(2k\\mathrm{~+~}1)$ and of $|G(s)H(s)|\\,=$ constant in the $G(s)H(s)$ plane.  \n\nSince the constant-phase and constant-gain loci in the $G(s)H(s)$ plane are orthogonal, the root loci and constant-gain loci in the $s$ plane are orthogonal. Figure 6–29(a) shows the root loci and constant-gain loci for the following system:  \n\n$$\nG(s)\\,=\\frac{K(s\\,+\\,2)}{s^{2}\\,+\\,2s\\,+\\,3},\\qquad H(s)\\,=\\,1\n$$  \n\n![](images/fcfe759c3047d1c6507a9441cf5a98f810c2810cf86c523c9fd0f0e006b73ebb.jpg)  \n\n# Figure 6–29  \n\not loci and const $G(s)\\,=\\,K(s\\,+\\,2)/(s^{2}\\,+\\,2s\\,+\\,3)$ ,$H(s)=1$ (b) system with $G(s)\\,=\\,K/[s(s^{'}+1)(s\\,+\\,2)],\\dot{H(s)}\\,=\\,1$  \n\nNotice that since the pole–zero configuration is symmetrical about the real axis, the constant-gain loci are also symmetrical about the real axis.  \n\nFigure 6–29(b) shows the root loci and constant-gain loci for the system:  \n\n$$\nG(s)=\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)},\\qquad H(s)=1\n$$  \n\nNotice that since the configuration of the poles in the $s$ plane is symmetrical about the real axis and the line parallel to the imaginary axis passing through point ($\\sigma=-1$ ,$\\omega=0$ ), the constant-gain loci are symmetrical about the $\\omega\\,=\\,0$ line (real axis) and the $\\sigma=-1$ line.  \n\nFrom Figures 6–29(a) and (b), notice that every point in the $s$ plane has the corresponding $K$ value. If we use a command rlocfind (presented next), MATLAB will give the $K$ value of the specified point as well as the nearest closed-loop poles corresponding to this $K$ value.  \n\nFinding the Gain Value $K$ at an Arbitrary Point on the Root Loci. In MATLAB analysis of closed-loop systems, it is frequently desired to find the gain value $K$ at an arbitrary point on the root locus. This can be accomplished by using the following rlocfind command:  \n\n$$\n[\\mathsf{K},\\,\\mathsf{r}]=\\mathsf{r}\\mathsf{l o c f i n d}(\\mathsf{n u m},\\,\\mathsf{d e n})\n$$  \n\nThe rlocfind command, which must follow an rlocus command, overlays movable $x$ -ycoordinates on the screen. Using the mouse, we position the origin of the $x{-}y$ coordinates over the desired point on the root locus and press the mouse button. Then MATLAB displays on the screen the coordinates of that point, the gain value at that point, and the closed-loop poles corresponding to this gain value.  \n\nIf the selected point is not on the root locus, such as point A in Figure 6–29(a), the rlocfind command gives the coordinates of this selected point, the gain value of this point, such as $K=2$ , and the locations of the closed-loop poles, such as points B and C corresponding to this $K$ value.[Note that every point on the $s$ plane has a gain value.See, for example, Figures 6–29 (a) and (b).]  \n\n# 6–4 ROOT-LOCUS PLOTS OF POSITIVE FEEDBACK SYSTEMS  \n\nRoot Loci for Positive-Feedback Systems.\\* In a complex control system, there may be a positive-feedback inner loop as shown in Figure 6–30. Such a loop is usually stabilized by the outer loop.In what follows,we shall be concerned only with the positivefeedback inner loop.The closed-loop transfer function of the inner loop is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,-\\,G(s)H(s)}\n$$  \n\nThe characteristic equation is  \n\n$$\n1\\,-\\,G(s)H(s)\\,=\\,0\n$$  \n\n\\* Reference W-4  \n\n![](images/3ea4bbafe48827c96f6be12dab6cf75ac9628b9eacc9f0f243691e4bb4174fbe.jpg)  \n\nFigure 6–30 Control system.  \n\nThis equation can be solved in a manner similar to the development of the root-locus method for negative-feedback systems presented in Section 6–2. The angle condition, however, must be altered.  \n\nEquation (6–17) can be rewritten as  \n\n$$\nG(s)H(s)\\,=\\,1\n$$  \n\nwhich is equivalent to the following two equations:  \n\n$$\n\\begin{array}{l l}{{\\displaystyle\\underline{{{/G(s)}}}{H(s)}=0^{\\circ}\\pm k360^{\\circ}\\qquad(k=0,1,2,\\dots)}}\\\\ {{\\displaystyle\\underline{{{|G(s)}}}{H(s)|}=1}}\\end{array}\n$$  \n\nFor the positive-feedback case, the total sum of all angles from the open-loop poles and zeros must be equal to $0^{\\circ}\\pm\\,k360^{\\circ}$ .Thus the root locus follows a $0^{\\circ}$ locus in contrast to the $180^{\\circ}$ locus considered previously.The magnitude condition remains unaltered.  \n\nTo illustrate the root-locus plot for the positive-feedback system, we shall use the following transfer functions $G(s)$ and $H(s)$ as an example.  \n\n$$\nG(s)=\\frac{K(s+2)}{(s+3)(s^{2}+2s+2)},\\qquad H(s)=1\n$$  \n\nThe gain $K$ is assumed to be positive.  \n\nThe general rules for constructing root loci for negative-feedback systems given in Section 6–2 must be modified in the following way:  \n\nRule 2 is Modified as Follows: If the total number of real poles and real zeros to the right of a test point on the real axis is even, then this test point lies on the root locus.  \n\nRule 3 is Modified as Follows:  \n\n$$\n\\mathrm{Angles\\;of\\;asymptotes}={\\frac{\\pm k360^{\\circ}}{n\\,-\\,m}}\\qquad(k\\,=\\,0,1,2,\\dots)\n$$  \n\nwhere $n=$ number of finite poles of $G(s)H(s)$  \n\nRule 5 is Modified as Follows: When calculating the angle of departure (or angle of arrival) from a complex open-loop pole (or at a complex zero), subtract from $0^{\\circ}$ the sum of all angles of the vectors from all the other poles and zeros to the complex pole (or complex zero) in question, with appropriate signs included.  \n\nOther rules for constructing the root-locus plot remain the same.We shall now apply the modified rules to construct the root-locus plot.  \n\n1. Plot the open-loop poles $(s=-1\\,+\\,j,s\\,=-1\\,-\\,j,s\\,=-3)$ and zero ($[s=-2]$ )in the complex plane.As $K$ is increased from 0 to $\\infty$ , the closed-loop poles start at the open-loop poles and terminate at the open-loop zeros (finite or infinite), just as in the case of negative-feedback systems.  \n\n2. Determine the root loci on the real axis. Root loci exist on the real axis between $^{-2}$ and $+\\infty$ and between $^{-3}$ and $-\\infty$ .  \n\n3. Determine the asymptotes of the root loci. For the present system,  \n\n$$\n\\mathrm{Angles\\;of\\;asymptote}={\\frac{\\pm k360^{\\circ}}{3\\,-\\,1}}=\\pm180^{\\circ}\n$$  \n\nThis simply means that asymptotes are on the real axis.  \n\n4. Determine the breakaway and break-in points. Since the characteristic equation is  \n\n$$\n(s\\,+\\,3)\\bigl(s^{2}\\,+\\,2s\\,+\\,2\\bigr)\\,-\\,K(s\\,+\\,2)\\,=\\,0\n$$  \n\nwe obtain  \n\n$$\nK=\\frac{(s\\,+\\,3)(s^{2}\\,+\\,2s\\,+\\,2)}{s\\,+\\,2}\n$$  \n\nBy differentiating $K$ with respect to $s$ ,we obtain  \n\n$$\n\\frac{d K}{d s}=\\frac{2s^{3}\\,+\\,11s^{2}\\,+\\,20s\\,+\\,10}{(s\\,+\\,2)^{2}}\n$$  \n\nNote that  \n\n$$\n\\begin{array}{c}{2s^{3}\\,+\\,11s^{2}\\,+\\,20s\\,+\\,10\\,=\\,2(s\\,+\\,0.8)\\bigl(s^{2}\\,+\\,4.7s\\,+\\,6.24\\bigr)}\\\\ {\\,=\\,2(s\\,+\\,0.8)(s\\,+\\,2.35\\,+\\,j0.77)(s\\,+\\,2.35\\,-\\,j0.77)}\\end{array}\n$$  \n\nPoint $s\\,=\\,-0.8$ is on the root locus. Since this point lies between two zeros (a finite zero and an infinite zero), it is an actual break-in point. Points $s\\,=\\,-2.35\\,\\pm\\,j0.77$ do not satisfy the angle condition and, therefore, they are neither breakaway nor break-in points.  \n\n5. Find the angle of departure of the root locus from a complex pole. For the complex pole at $s=-1+j$ ,the angle of departure $\\theta$ is  \n\n$$\n\\theta\\,=\\,0^{\\circ}\\,-\\,27^{\\circ}\\,-\\,90^{\\circ}\\,+\\,45^{\\circ}\n$$  \n\nor  \n\n$$\n\\theta=-72^{\\circ}\n$$  \n\n(The angle of departure from the complex pole at $s=-1\\,-\\,j$ is $72^{\\circ}$ .)  \n\n6. Choose a test point in the broad neighborhood of the $j\\omega$ axis and the origin and apply the angle condition. Locate a sufficient number of points that satisfy the angle condition.  \n\nFigure 6–31 shows the root loci for the given positive-feedback system.The root loci are shown with dashed lines and a curve.  \n\nNote that if  \n\n$$\nK>\\frac{(s\\,+\\,3)(s^{2}\\,+\\,2s\\,+\\,2)}{s\\,+\\,2}\\,\\bigg|_{s=0}=3\n$$  \n\n![](images/ce675d54ef759a372e38efb26d8411e6fa8d4264e7341fb5fd34185fc0ba2d8d.jpg)  \nFigure 6–31 Root-locus plot for the positive-feedback system with $G(s)\\,=\\,K(s\\,+\\,2)/$ $\\big[(s\\,+\\,3)\\big(s^{2}\\,+\\,2s\\,+\\,2\\big)\\big],$ $H(s)=1$  \n\none real root enters the right-half $s$ plane. Hence, for values of $K$ greater than 3, the system becomes unstable. (For $K>3$ ,the system must be stabilized with an outer loop.) Note that the closed-loop transfer function for the positive-feedback system is given by  \n\n$$\n\\begin{array}{l}{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,-\\,G(s)H(s)}}}\\\\ {\\displaystyle{\\qquad=\\frac{K(s\\,+\\,2)}{(s\\,+\\,3)\\big(s^{2}\\,+\\,2s\\,+\\,2\\big)\\,-\\,K(s\\,+\\,2)}}}\\end{array}\n$$  \n\nTo compare this root-locus plot with that of the corresponding negative-feedback system,we show in Figure 6–32 the root loci for the negative-feedback system whose closedloop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K(s\\,+\\,2)}{(s\\,+\\,3)\\bigl(s^{2}\\,+\\,2s\\,+\\,2\\bigr)\\,+\\,K(s\\,+\\,2)}\n$$  \n\nTable 6–2 shows various root-locus plots of negative-feedback and positive-feedback systems.The closed-loop transfer functions are given by  \n\n$$\n\\begin{array}{l l}{{\\displaystyle{\\frac{C}{R}}=\\frac{G}{1\\,+\\,G H},\\quad}}&{{\\mathrm{for\\,negative-feedback\\;systems}}}\\\\ {{\\displaystyle{\\frac{C}{R}}=\\frac{G}{1\\,-\\,G H},\\quad}}&{{\\mathrm{for\\,positive-feedback\\;systems}}}\\end{array}\n$$  \n\n![](images/0bbd5a168ed5d23ac9b7d960b3a79c34c1c6a6fa36e67c2606546ba5a73061f7.jpg)  \nFigure 6–32 Root-locus plot for the negative-feedback system with $G(s)\\,=\\,K(s\\,+\\,2)/$ $\\big[(s\\,+\\,3)\\big(s^{2}\\,+\\,2s\\,+\\,2\\big)\\big],$ $H(s)=1$  \n\nwhere $G H$ is the open-loop transfer function. In Table 6–2, the root loci for negativefeedback systems are drawn with heavy lines and curves,and those for positive-feedback systems are drawn with dashed lines and curves.  \n\n![](images/33db78540924ee4d6e9dd14a1ab5846eaee2663f45e1bba1d1b1686eaf79690e.jpg)  \nTable 6–2 Root-Locus Plots of Negative-Feedback and PositiveFeedback Systems   \nHeavy lines and curves correspond to negative-feedback systems; dashed lines and curves correspond to positive-feedback systems.  \n\nPreliminary Design Consideration. In building a control system, we know that proper modification of the plant dynamics may be a simple way to meet the performance specifications.This,however,may not be possible in many practical situations because the plant may be fixed and not modifiable.Then we must adjust parameters other than those in the fixed plant. In this book, we assume that the plant is given and unalterable.  \n\nIn practice,the root-locus plot of a system may indicate that the desired performance cannot be achieved just by the adjustment of gain (or some other adjustable parameter). In fact, in some cases, the system may not be stable for all values of gain (or other adjustable parameter). Then it is necessary to reshape the root loci to meet the performance specifications.  \n\nThe design problems, therefore, become those of improving system performance by insertion of a compensator. Compensation of a control system is reduced to the design of a filter whose characteristics tend to compensate for the undesirable and unalterable characteristics of the plant.  \n\nDesign by Root-Locus Method. The design by the root-locus method is based on reshaping the root locus of the system by adding poles and zeros to the system’s open-loop transfer function and forcing the root loci to pass through desired closed-loop poles in the $s$ plane.The characteristic of the root-locus design is its being based on the assumption that the closed-loop system has a pair of dominant closed-loop poles.This means that the effects of zeros and additional poles do not affect the response characteristics very much.  \n\nIn designing a control system, if other than a gain adjustment (or other parameter adjustment) is required,we must modify the original root loci by inserting a suitable compensator.Once the effects on the root locus of the addition of poles and/or zeros are fully understood,we can readily determine the locations of the pole(s) and zero(s) of the compensator that will reshape the root locus as desired. In essence, in the design by the rootlocus method, the root loci of the system are reshaped through the use of a compensator so that a pair of dominant closed-loop poles can be placed at the desired location.  \n\nSeries Compensation and Parallel (or Feedback) Compensation. Figures 6–33(a) and (b) show compensation schemes commonly used for feedback control systems. Figure 6–33(a) shows the configuration where the compensator $G_{c}(s)$ is placed in series with the plant.This scheme is called series compensation.  \n\nAn alternative to series compensation is to feed back the signal(s) from some element(s) and place a compensator in the resulting inner feedback path,as shown in Figure 6–33(b). Such compensation is called parallel compensation or feedback compensation .  \n\nIn compensating control systems, we see that the problem usually boils down to a suitable design of a series or parallel compensator.The choice between series compensation and parallel compensation depends on the nature of the signals in the system, the power levels at various points, available components, the designer’s experience, economic considerations, and so on.  \n\nIn general,series compensation may be simpler than parallel compensation;however, series compensation frequently requires additional amplifiers to increase the gain and/or to provide isolation.(To avoid power dissipation,the series compensator is inserted at the lowest energy point in the feedforward path.) Note that, in general, the number of components required in parallel compensation will be less than the number of components  \n\nFigure 6–33   \n(a) Series   \ncompensation;   \n(b) parallel or feedback compensation.  \n\n![](images/208cba8a73e68eff767e856e7556c2def2f0e8563583fa957a6325c3c678d948.jpg)  \n\nin series compensation, provided a suitable signal is available, because the energy transfer is from a higher power level to a lower level. (This means that additional amplifiers may not be necessary.)  \n\nIn Sections 6–6 through 6–9 we first discuss series compensation techniques and then present a parallel compensation technique using a design of a velocity-feedback control system.  \n\nCommonly Used Compensators. If a compensator is needed to meet the performance specifications, the designer must realize a physical device that has the prescribed transfer function of the compensator.  \n\nNumerous physical devices have been used for such purposes.In fact,many noble and useful ideas for physically constructing compensators may be found in the literature.  \n\nIf a sinusoidal input is applied to the input of a network, and the steady-state output (which is also sinusoidal) has a phase lead, then the network is called a lead network. (The amount of phase lead angle is a function of the input frequency.) If the steady-state output has a phase lag, then the network is called a lag network. In a lag–lead network, both phase lag and phase lead occur in the output but in different frequency regions; phase lag occurs in the low-frequency region and phase lead occurs in the high-frequency region.A compensator having a characteristic of a lead network,lag network,or lag–lead network is called a lead compensator, lag compensator, or lag–lead compensator.  \n\nAmong the many kinds of compensators, widely employed compensators are the lead compensators, lag compensators, lag–lead compensators, and velocity-feedback (tachometer) compensators. In this chapter we shall limit our discussions mostly to these types. Lead, lag, and lag–lead compensators may be electronic devices (such as circuits using operational amplifiers) or $R C$ networks (electrical, mechanical, pneumatic, hydraulic, or combinations thereof) and amplifiers.  \n\nFrequently used series compensators in control systems are lead, lag, and lag–lead compensators. PID controllers which are frequently used in industrial control systems are discussed in Chapter 8.  \n\n![](images/9da7ce74453b80b2c7d5dadffa12f48b4733ef92f961a15a81b798a0d0a99101.jpg)  \nFigure 6–34 (a) Root-locus plot of a single-pole system; (b) root-locus plot of a two-pole system; (c) root-locus plot of a three-pole system.  \n\nIt is noted that in designing control systems by the root-locus or frequency-response methods the final result is not unique,because the best or optimal solution may not be precisely defined if the time-domain specifications or frequency-domain specifications are given.  \n\nEffects of the Addition of Poles. The addition of a pole to the open-loop transfer function has the effect of pulling the root locus to the right, tending to lower the system’s relative stability and to slow down the settling of the response. (Remember that the addition of integral control adds a pole at the origin, thus making the system less stable.) Figure 6–34 shows examples of root loci illustrating the effects of the addition of a pole to a single-pole system and the addition of two poles to a single-pole system.  \n\nEffects of the Addition of Zeros. The addition of a zero to the open-loop transfer function has the effect of pulling the root locus to the left,tending to make the system more stable and to speed up the settling of the response. (Physically, the addition of a zero in the feedforward transfer function means the addition of derivative control to the system.The effect of such control is to introduce a degree of anticipation into the system and speed up the transient response.) Figure 6–35(a) shows the root loci for a system that is stable for small gain but unstable for large gain. Figures 6–35(b), (c), and (d) show root-locus plots for the system when a zero is added to the open-loop transfer function. Notice that when a zero is added to the system of Figure 6–35(a), it becomes stable for all values of gain.  \n\n![](images/801e07fee28491316f29f297db7bd9208799d045562e70c99e66b36137334ee2.jpg)  \nFigure 6–35 (a) Root-locus plot of a three-pole system; (b), (c), and (d) root-locus plots showing effects of addition of a zero to the three-pole system.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\n# 6–6 LEAD COMPENSATION  \n\nIn Section 6–5 we presented an introduction to compensation of control systems and discussed preliminary materials for the root-locus approach to control-systems design and compensation. In this section we shall present control-systems design by use of the lead compensation technique. In carrying out a control-system design, we place a compensator in series with the unalterable transfer function $G(s)$ to obtain desirable behavior. The main problem then involves the judicious choice of the pole(s) and zero(s) of the compensator $G_{c}(s)$ to have the dominant closed-loop poles at the desired location in the $s$ plane so that the performance specifications will be met.  \n\n![](images/667fba03830d45ddfc8427af471502bc5854bbfaddbd80ea550f365b4fac8672.jpg)  \nFigure 6–36 Electronic circuit that is a lead network if $R_{1}C_{1}>R_{2}C_{2}$ and a lag network if $R_{1}C_{1}<R_{2}C_{2}$ .  \n\nLead Compensators and Lag Compensators. There are many ways to realize lead compensators and lag compensators, such as electronic networks using operational amplifiers, electrical $R C$ networks, and mechanical spring-dashpot systems.  \n\nFigure 6–36 shows an electronic circuit using operational amplifiers. The transfer function for this circuit was obtained in Chapter 3 as follows [see Equation (3–36)]:  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{R_{2}R_{4}}{R_{1}R_{3}}\\frac{R_{1}C_{1}s\\,+\\,1}{R_{2}C_{2}s\\,+\\,1}=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\\frac{s\\,+\\,\\frac{1}{R_{1}C_{1}}}{s\\,+\\,\\frac{1}{R_{2}C_{2}}}\n$$  \n\n$$\n=K_{c}\\alpha\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}=K_{c}{\\frac{s\\,+\\,{\\frac{1}{T}}}{s\\,+\\,{\\frac{1}{\\alpha T}}}}\n$$  \n\nwhere  \n\n$$\nT\\,=\\,R_{1}C_{1},\\qquad\\alpha T\\,=\\,R_{2}C_{2},\\qquad K_{c}=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\n$$  \n\n![](images/69ea121f48da0a36af35520a5b9b279e0f700c836b8f1666b55b1407a972efb1.jpg)  \nFigure 6–37Pole-zero configurations: (a) lead network; (b) lag network.  \n\nNotice that  \n\n$$\nK_{c}\\alpha=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\\frac{R_{2}C_{2}}{R_{1}C_{1}}=\\frac{R_{2}R_{4}}{R_{1}R_{3}},\\qquad\\alpha=\\frac{R_{2}C_{2}}{R_{1}C_{1}}\n$$  \n\nThis network has a dc gain of $K_{c}\\alpha\\,=\\,R_{2}R_{4}/\\bigl(R_{1}R_{3}\\bigr)$ .  \n\nFrom Equation (6–18), we see that this network is a lead network if $R_{1}C_{1}>R_{2}C_{2}$ ,or $\\alpha<1.$ .It is a lag network if $R_{1}C_{1}<R_{2}C_{2}$ .The pole-zero configurations of this network when $R_{1}C_{1}>R_{2}C_{2}$ and $R_{1}C_{1}<R_{2}C_{2}$ are shown in Figure 6–37(a) and (b), respectively.  \n\nLead Compensation Techniques Based on the Root-Locus Approach. The root-locus approach to design is very powerful when the specifications are given in terms of time-domain quantities, such as the damping ratio and undamped natural frequency of the desired dominant closed-loop poles, maximum overshoot, rise time, and settling time.  \n\nConsider a design problem in which the original system either is unstable for all values of gain or is stable but has undesirable transient-response characteristics. In such a case, the reshaping of the root locus is necessary in the broad neighborhood of the $j\\omega$ axis and the origin in order that the dominant closed-loop poles be at desired locations in the complex plane.This problem may be solved by inserting an appropriate lead compensator in cascade with the feedforward transfer function.  \n\nThe procedures for designing a lead compensator for the system shown in Figure 6–38 by the root-locus method may be stated as follows:  \n\n1. From the performance specifications, determine the desired location for the dominant closed-loop poles.  \n\n![](images/ea2c2e9bf9d25caaabe61200f159338f42f35b4dffcff4dc5ce6392b5237d8ff.jpg)  \nFigure 6–38 Control system.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\n2. By drawing the root-locus plot of the uncompensated system (original system), ascertain whether or not the gain adjustment alone can yield the desired closedloop poles. If not, calculate the angle deficiency $\\phi$ .This angle must be contributed by the lead compensator if the new root locus is to pass through the desired locations for the dominant closed-loop poles.  \n\n3. Assume the lead compensator $G_{c}(s)$ to be  \n\n$$\nG_{c}(s)=K_{c}\\alpha\\frac{T s+1}{\\alpha T s+1}=K_{c}\\frac{s+\\displaystyle\\frac{1}{T}}{s+\\displaystyle\\frac{1}{\\alpha T}},~~~~~(0<\\alpha<1)\n$$  \n\nwhere $\\alpha$ and $T$ are determined from the angle deficiency. $K_{c}$ is determined from the requirement of the open-loop gain.  \n\n4. If static error constants are not specified, determine the location of the pole and zero of the lead compensator so that the lead compensator will contribute the necessary angle $\\phi$ . If no other requirements are imposed on the system, try to make the value of $\\alpha$ as large as possible.A larger value of $\\alpha$ generally results in a larger value of $K_{v}$ ,which is desirable. Note that  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)=K_{c}\\alpha\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)\n$$  \n\n5. Determine the value of $K_{c}$ of the lead compensator from the magnitude condition.  \n\nOnce a compensator has been designed, check to see whether all performance specifications have been met. If the compensated system does not meet the performance specifications, then repeat the design procedure by adjusting the compensator pole and zero until all such specifications are met. If a large static error constant is required, cascade a lag network or alter the lead compensator to a lag–lead compensator.  \n\nNote that if the selected dominant closed-loop poles are not really dominant, or if the selected dominant closed-loop poles do not yield the desired result, it will be necessary to modify the location of the pair of such selected dominant closed-loop poles. (The closed-loop poles other than dominant ones modify the response obtained from the dominant closed-loop poles alone.The amount of modification depends on the location of these remaining closed-loop poles.) Also, the closed-loop zeros affect the response if they are located near the origin.  \n\n# EXAMPLE 6–6  \n\nConsider the position control system shown in Figure 6–39(a). The feedforward transfer function is  \n\n$$\nG(s)={\\frac{10}{s(s+1)}}\n$$  \n\nThe root-locus plot for this system is shown in Figure 6–39(b).The closed-loop transfer function for the system is  \n\n$$\n\\begin{array}{l}{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{10}{s^{2}\\,+\\,s\\,+\\,10}}}\\\\ {\\displaystyle{\\qquad=\\frac{10}{(s\\,+\\,0.5\\,+\\,j3.1225)(s\\,+\\,0.5\\,-\\,j3.1225)}}}\\end{array}\n$$  \n\n![](images/b6922ee3d1831da7f7e8a5c33040606d36e1f1b3eb3eb1ca9567eaa3571b9432.jpg)  \n\nFigure 6–39 (a) Control system; (b) root-locus plot.  \n\nThe closed-loop poles are located at  \n\n$$\ns\\,=\\,-0.5\\,\\pm\\,j3.1225\n$$  \n\nThe damping ratio of the closed-loop poles is $\\zeta=(1/2)/\\sqrt{10}=0.1581.$ .The undamped natural frequency of the closed-loop poles is $\\dot{\\omega_{n}}\\dot{=}\\sqrt{10}=3.1623\\,\\mathrm{r}a$ 'sec.Because the damping ratio is small, this system will have a large overshoot in the step response and is not desirable.  \n\nIt is desired to design a lead compensator $G_{c}(s)$ as shown in Figure 6–40(a) so that the dominant closed-loop poles have the damping ratio $\\zeta=0.5$ and the undamped natural frequency $\\omega_{n}=3$ rad 'sec. The desired location of the dominant closed-loop poles can be determined from  \n\n$$\n\\begin{array}{r l}{s^{2}\\,+\\,2\\zeta\\,\\omega_{n}s\\,+\\,\\omega_{n}^{2}\\,=\\,s^{2}\\,+\\,3s\\,+\\,9}&{{}}\\\\ {\\,=\\,(s\\,+\\,1.5\\,+\\,j\\,2.5981)(s\\,+\\,1.5\\,-\\,j\\,2.5981)}\\end{array}\n$$  \n\nas follows:  \n\n$$\ns\\,=\\,-1.5\\,\\pm\\,j\\,2.5981\n$$  \n\n![](images/a1d0526535bce448afc2bb2746671c9e06d15380200e3efc86afe8c1e509b1b7.jpg)  \n\n[See Figure 6–40 (b).] In some cases, after the root loci of the original system have been obtained, the dominant closed-loop poles may be moved to the desired location by simple gain adjustment. This is, however, not the case for the present system.Therefore, we shall insert a lead compensator in the feedforward path.  \n\nA general procedure for determining the lead compensator is as follows: First, find the sum of the angles at the desired location of one of the dominant closed-loop poles with the open-loop poles and zeros of the original system, and determine the necessary angle $\\phi$ to be added so that the total sum of the angles is equal to $\\pm180^{\\circ}(2k\\,+\\,1)$ The lead compensator must contribute this angle $\\phi$ . (If the angle $\\phi$ is quite large, then two or more lead networks may be needed rather than a single one.)  \n\nAssume that the lead compensator $G_{c}(s)$ has the transfer function as follows:  \n\n$$\nG_{c}(s)=K_{c}\\alpha\\frac{T s+1}{\\alpha T s+1}=K_{c}\\frac{s+\\displaystyle\\frac{1}{T}}{s+\\displaystyle\\frac{1}{\\alpha T}},~~~~~(0<\\alpha<1)\n$$  \n\nThe angle from the pole at the origin to the desired dominant closed-loop pole at $s\\,{=}\\,{-}1.5~+~{}\\,{j}2.5981$ is $120^{\\circ}$ .The angle from the pole at $s=-1$ to the desired closed-loop pole is $100.894^{\\circ}$ . Hence, the angle deficiency is  \n\n$$\n{\\mathrm{Angle~deficiency}}\\,=\\,180^{\\circ}-120^{\\circ}-100.894^{\\circ}\\,=\\,-40.894^{\\circ}\n$$  \n\nDeficit angle $40.894^{\\circ}$ must be contributed by a lead compensator.  \n\nNote that the solution to such a problem is not unique. There are infinitely many solutions. We shall present two solutions to the problem in what follows.  \n\nMethod 1. There are many ways to determine the locations of the zero and pole of the lead compensator. In what follows we shall introduce a procedure to obtain a largest possible value for $\\alpha$ . (Note that a larger value of $\\alpha$ will produce a larger value of $K_{v}$ .In most cases, the larger the $K_{v}$ is, the better the system performance.) First,draw a horizontal line passing through point $P$ ,the desired location for one of the dominant closed-loop poles.This is shown as line $P A$ in Figure 6–41. Draw also a line connecting point $P$ and the origin. Bisect the angle between the lines $P A$ and $P O$ , as shown in Figure 6–41.Draw two lines $P C$ and $P D$ that make angles $\\pm\\phi/2$ with the bisector $P B$ .The intersections of $P C$ and $P D$ with the negative real axis give the necessary locations for the pole and zero of the lead network.The compensator thus designed will make point $P$ a point on the root locus of the compensated system.The open-loop gain is determined by use of the magnitude condition.  \n\nIn the present system, the angle of $G(s)$ at the desired closed-loop pole is  \n\n$$\n\\left/{\\frac{10}{s(s\\,+\\,1)}}\\right|_{s=-1.5+j2.5981}=-220.894^{\\circ}\n$$  \n\n![](images/c55c2e1795b79cb62ab161ae8a318786f1ca2cd65267316bc8b9a8dffcbdd0be.jpg)  \nFigure 6–41Determination of the pole and zero of a lead network.  \n\nThus, if we need to force the root locus to go through the desired closed-loop pole, the lead compensator must contribute $\\phi\\,=\\,40.894^{\\circ}$ at this point. By following the foregoing design procedure, we can determine the zero and pole of the lead compensator.  \n\nReferring to Figure 6–42, if we bisect angle APO and take $40.894^{\\circ}/2$ each side, then the locations of the zero and pole are found as follows:  \n\n$$\n_{\\mathrm{zero\\;at}\\;s}=-1.9432\n$$  \n\nThus, $G_{c}(s)$ can be given as  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\ {\\frac{s+{\\frac{1}{T}}}{s+{\\frac{1}{\\alpha T}}}}=K_{c}\\ {\\frac{s+1.9432}{s+4.6458}}\n$$  \n\n(For this compensator the value of $\\alpha$ is $\\alpha=1.9432/4.6458=0.418.)$  \n\nThe value of $K_{c}$ can be determined by use of the magnitude condition.  \n\n$$\n\\left|K_{c}\\,\\frac{s\\,+\\,1.9432}{s\\,+\\,4.6458}\\frac{10}{s(s\\,+\\,1)}\\right|_{s=-1.5+j2.5981}=1\n$$  \n\nor  \n\n$$\nK_{c}=\\left.\\left|\\frac{(s\\,+\\,4.6458)s(s\\,+\\,1)}{10(s\\,+\\,1.9432)}\\right|_{s=-1.5+j2.5981}=1.2287\n$$  \n\nHence, the lead compensator $G_{c}(s)$ just designed is given by  \n\n$$\nG_{c}(s)\\,=\\,1.2287\\,\\frac{s\\,+\\,1.9432}{s\\,+\\,4.6458}\n$$  \n\nThen, the open-loop transfer function of the designed system becomes  \n\n$$\nG_{c}(s)G(s)=1.2287\\left({\\frac{s+1.9432}{s+4.6458}}\\right){\\frac{10}{s(s\\mathrm{~+~}1)}}\n$$  \n\nand the closed-loop transfer function becomes  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{12.287(s+1.9432)}{s(s+1)(s+4.6458)+12.287(s+1.9432)}}}\\\\ {\\displaystyle{=\\frac{12.287s+23.876}{s^{3}+5.646s^{2}+16.933s+23.876}}}\\end{array}\n$$  \n\n![](images/64373e1e0930147fd51415965b7ebe9ae372e06eb1f3ead59d0ad9b9e8b16659.jpg)  \nFigure 6–42 Determination of the pole and zero of the lead compensator.  \n\n![](images/26972c62ce01f5ac94d4d22e5789d7bab527337c70742241961ae93b3de620c0.jpg)  \nFigure 6–43Root-locus plot of the designed system.  \n\nFigure 6–43 shows the root-locus plot for the designed system. It is worthwhile to check the static velocity error constant $K_{v}$ for the system just designed.  \n\n$$\n\\begin{array}{r l}{\\lefteqn{K_{v}=\\operatorname*{lim}_{s\\to0}s G_{c}(s)G(s)}}\\\\ &{=\\operatorname*{lim}_{s\\to0}\\;s\\Bigg[1.2287\\,\\frac{s+1.9432}{s+4.6458}\\frac{10}{s(s\\;+\\;1)}\\Bigg]}\\\\ &{=5.139}\\end{array}\n$$  \n\nNote that the third closed-loop pole of the designed system is found by dividing the characteristic equation by the known factors as follows:  \n\n$$\ns^{3}+5.646s^{2}+16.933s+23.875=(s+1.5+j2.5981)(s+1.5-j2.5981)(s+2.659)\n$$  \n\nThe foregoing compensation method enables us to place the dominant closed-loop poles at the desired points in the complex plane. The third pole at $s=-2.65$ is fairly close to the added zero at $-1.9432$ . Therefore, the effect of this pole on the transient response is relatively small. Since no restriction has been imposed on the nondominant pole and no specification has been given concerning the value of the static velocity error coefficient, we conclude that the present design is satisfactory.  \n\nMethod 2. If we choose the zero of the lead compensator at $s=-1$ so that it will cancel the plant pole at $s=-1$ , then the compensator pole must be located at $s=-3$ . (See Figure 6–44.) Hence the lead compensator becomes  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\frac{s\\,+\\,1}{s\\,+\\,3}\n$$  \n\nThe value of $K_{c}$ can be determined by use of the magnitude condition.  \n\n$$\n\\left|K_{c}\\,\\frac{s\\,+\\,1}{s\\,+\\,3}\\frac{10}{s(s\\,+\\,1)}\\right|_{s=-1.5+j2.5981}=\\,1\n$$  \n\nFigure 6–44   \nCompensator pole and zero.  \n\nor  \n\n![](images/5741f22765add42dacb8178d3135c360d1660f68866bdb243d4eaf5da727c94e.jpg)  \n\n$$\nK_{c}=\\left|\\left.\\frac{s(s\\,+\\,3)}{10}\\right|_{s=-1.5+j2.5981}=0.9\n$$  \n\nHence  \n\n$$\nG_{c}(s)\\,=\\,0.9\\,\\frac{s\\,+\\,1}{s\\,+\\,3}\n$$  \n\nThe open-loop transfer function of the designed system then becomes  \n\n$$\nG_{c}(s)G(s)\\,=\\,0.9\\,\\frac{s\\,+\\,1}{s\\,+\\,3}\\frac{10}{s(s\\,+\\,1)}=\\frac{9}{s(s\\,+\\,3)}\n$$  \n\nThe closed-loop transfer function of the compensated system becomes  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{9}{s^{2}+3s\\,+\\,9}}\n$$  \n\nNote that in the present case the zero of the lead compensator will cancel a pole of the plant, resulting in the second-order system,rather than the third-order system as we designed using Method 1. The static velocity error constant for the present case is obtained as follows:  \n\n$$\n\\begin{array}{l}{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)}\\\\ {\\qquad=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\ s\\Bigg[\\frac{9}{s(s+3)}\\Bigg]=3}\\end{array}\n$$  \n\nNotice that the system designed by Method 1 gives a larger value of the static velocity error constant.This means that the system designed by Method 1 will give smaller steady-state errors in following ramp inputs than the system designed by Method 2.  \n\nFor different combinations of a zero and pole of the compensator that contributes $40.894^{\\circ}$ , the value of $K_{v}$ will be different.Although a certain change in the value of $K_{v}$ can be made by altering the pole-zero location of the lead compensator, if a large increase in the value of $K_{v}$ is desired, then we must alter the lead compensator to a lag–lead compensator.  \n\nComparison of step and ramp responses of the compensated and uncompensated systems. In what follows we shall compare the unit-step and unit-ramp responses of the three systems: the original uncompensated system, the system designed by Method 1, and the system designed by Method 2. The MATLAB program used to obtain unit-step response curves is given in MATLAB Program 6–9, where num1 and den1 denote the numerator and denominator of the system designed by Method 1 and num2 and den2 denote that designed by Method 2.Also, num and den are used for the original uncompensated system.The resulting unit-step response curves are shown in Figure 6–45.The MATLAB program to obtain the unit-ramp response curves of the  \n\n# MATLAB Program 6–9  \n\n% \\*\\*\\*\\*\\* Unit-Step Response of Compensated and Uncompensated Systems \\*\\*\\*\\*\\* $\\mathsf{n u m1}=[12.287~\\,23.876]$ ;  \nden1 $=$ [1  5.646  16.933  23.876];   \nn$.1\\mathsf{m}2=[9]$ ;  \nden $2=[1\\;\\;3$ 9];   \nnu $\\mathsf{m}=[10]$ ;  \n$\\mathsf{d e n}=[1\\;\\;1\\;\\;\\;1\\;\\;]$ ;  \n$\\mathrm{t}=0{:}0.05{:}5$ ;  \nc1 = step(num1,den1,t);   \n${\\mathsf{c}}2={\\mathsf{s t e p}}({\\mathsf{n u m}}2,{\\mathsf{d e n}}2,{\\mathsf{t}});$   \n${\\mathsf{C}}=$ step(num,den,t);   \n$\\mathrm{plot}(\\mathrm{t},\\mathrm{c}\\,^{1},\\mathrm{}^{\\prime}\\mathrm{-}^{\\prime},\\mathrm{t},\\mathrm{c}\\,^{2},\\mathrm{}^{\\prime}\\mathrm{.}^{\\prime},\\mathrm{t},\\mathrm{c},\\mathrm{}^{\\prime}\\mathrm{x}^{\\prime})$   \ngrid   \ntitle('Unit-Step Responses of Compensated Systems and Uncompensated System') xlabel('t Sec')   \nylabel('Outputs c1, c2, and c')   \ntext(1.51,1.48,'Compensated System (Method 1)')   \ntext(0.9,0.48,'Compensated System (Method 2)')   \ntext(2.51,0.67,'Uncompensated System')  \n\n![](images/c312a405f497761fafdd3fc8a706f14e7c2eaba8417d470e0149c2b6ba8b01f9.jpg)  \nFigure 6–45 Unit-step response curves of designed systems and original uncompensated system.  \n\ndesigned systems is given in MATLAB Program 6–10, where we used the step command to obtain unit-ramp responses by using the numerators and denominators for the systems designed by Method 1 and Method 2 as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathsf{n u m}1=[1\\,2.287\\;\\;23.876]}\\\\ &{\\mathsf{d e n}1=[1\\;\\;5.646\\;\\;16.933\\;\\;23.876\\;\\;0]}\\\\ &{\\mathsf{n u m}2=[9]}\\\\ &{\\mathsf{d e n}2=[1\\;\\;3\\;9\\;\\;0]}\\end{array}\n$$  \n\n![](images/9c4d2a327e292429e377aa4d0487730a2f7093eb50e7863027bc0276bfd9eb42.jpg)  \nFigure 6–46 Unit-ramp response curves of designed systems.  \n\nThe resulting unit-ramp response curves are shown in Figure 6–46.  \n\n![](images/8f82dc46cfebd460823ace429a786e728d9a8d00d12e4735d94b3b68a4255a00.jpg)  \n\nIn examining these response curves notice that the compensated system designed by Method 1 exhibits a little bit larger overshoot in the step response than the compensated system designed by Method 2. However, the former has better response characteristics for the ramp input than the latter. So it is difficult to say which one is better.The decision on which one to choose should be made by the response requirements (such as smaller overshoots for step type inputs or smaller steady-state errors in following ramp or changing inputs) expected in the designed system. If both smaller overshoots in step inputs and smaller steady-state errors in following changing inputs are required, then we might use a lag–lead compensator. (See Section 6–8 for the lag–lead compensation techniques.)  \n\n# 6–7 LAG COMPENSATION  \n\nElectronic Lag Compensator Using Operational Amplifiers. The configuration of the electronic lag compensator using operational amplifiers is the same as that for the lead compensator shown in Figure 6–36. If we choose $R_{2}C_{2}>R_{1}C_{1}$ in the circuit shown in Figure 6–36, it becomes a lag compensator. Referring to Figure 6–36, the transfer function of the lag compensator is given by  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=\\hat{K}_{c}\\beta\\frac{T s+1}{\\beta T s+1}=\\hat{K}_{c}\\frac{s+\\frac{1}{T}}{s+\\frac{1}{\\beta T}}\n$$  \n\nwhere  \n\n$$\nT\\,=\\,R_{1}C_{1},\\qquad\\beta T\\,=\\,R_{2}C_{2},\\qquad\\beta\\,=\\frac{R_{2}C_{2}}{R_{1}C_{1}}>1,\\qquad\\hat{K}_{c}=\\frac{R_{4}C_{1}}{R_{3}C_{2}}\n$$  \n\nNote that we use $\\beta$ instead of $\\alpha$ in the above expressions. [In the lead compensator we A Bused $\\alpha$ to indicate the ratio $R_{2}C_{2}/\\big(R_{1}C_{1}\\big)$ ,which was less than 1, or $0<\\alpha<1.]$ In this book we always assume that $0<\\alpha<1$ and $\\beta>1$ .  \n\nLag Compensation Techniques Based on the Root-Locus Approach. Consider the problem of finding a suitable compensation network for the case where the system exhibits satisfactory transient-response characteristics but unsatisfactory steady-state characteristics. Compensation in this case essentially consists of increasing the openloop gain without appreciably changing the transient-response characteristics.This means that the root locus in the neighborhood of the dominant closed-loop poles should not be changed appreciably, but the open-loop gain should be increased as much as needed. This can be accomplished if a lag compensator is put in cascade with the given feedforward transfer function.  \n\nTo avoid an appreciable change in the root loci, the angle contribution of the lag network should be limited to a small amount, say less than $5^{\\circ}$ . To assure this, we place the pole and zero of the lag network relatively close together and near the origin of the $s$ plane.Then the closed-loop poles of the compensated system will be shifted only slightly from their original locations. Hence, the transient-response characteristics will be changed only slightly.  \n\nConsider a lag compensator $G_{c}(s)$ ,where  \n\n$$\nG_{c}(s)\\,=\\,\\hat{K}_{c}\\beta\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}=\\hat{K}_{c}\\frac{s\\,+\\,\\frac{1}{T}}{s\\,+\\,\\frac{1}{\\beta T}}\n$$  \n\nIf we place the zero and pole of the lag compensator very close to each other, then at $s\\implies s_{1}$ ,where $s_{1}$ is one of the dominant closed-loop poles,the magnitudes $s_{1}+\\,(1/T)$ and $s_{1}+\\left[1/(\\beta T)\\right]$ are almost equal, or  \n\n$$\n|G_{c}(s_{1})|\\,=\\,\\left|\\hat{K}_{c}\\,\\frac{s_{1}+\\displaystyle\\frac{1}{T}}{s_{1}+\\displaystyle\\frac{1}{\\beta T}}\\right|\\div\\hat{K}_{c}\n$$  \n\nTo make the angle contribution of the lag portion of the compensator small, we require  \n\n$$\n-5^{\\circ}<\\left\\langle\\frac{s_{1}+\\frac{1}{T}}{s_{1}+\\frac{1}{\\beta T}}<0^{\\circ}\\right.\n$$  \n\nThis implies that if gain $\\hat{K}_{c}$ of the lag compensator is set equal to 1, the alteration in the transient-response characteristics will be very small,despite the fact that the overall gain of the open-loop transfer function is increased by a factor of $\\beta$ , where $\\beta>1$ .If the pole and zero are placed very close to the origin, then the value of $\\beta$ can be made large. (A large value of $\\beta$ may be used,provided physical realization of the lag compensator is possible.) It is noted that the value of $T$ must be large, but its exact value is not critical. However, it should not be too large in order to avoid difficulties in realizing the phase-lag compensator by physical components.  \n\nAn increase in the gain means an increase in the static error constants. If the openloop transfer function of the uncompensated system is $G(s)$ ,then the static velocity error constant $K_{v}$ of the uncompensated system is  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\to0}s G(s)\n$$  \n\nIf the compensator is chosen as given by Equation (6–19), then for the compensated system with the open-loop transfer function $G_{c}(s)G(s)$ the static velocity error constant $\\hat{K}_{v}$ becomes  \n\n$$\n\\hat{K}_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}G_{c}(s)K_{v}\\,=\\,\\hat{K}_{c}\\beta K_{v}\n$$  \n\nwhere $K_{v}$ is the static velocity error constant of the uncompensated system.  \n\nThus if the compensator is given by Equation (6–19), then the static velocity error constant is increased by a factor of $\\hat{K}_{c}\\,\\bar{\\beta}$ ,where $\\hat{K}_{c}$ is approximately unity.  \n\nThe main negative effect of the lag compensation is that the compensator zero that will be generated near the origin creates a closed-loop pole near the origin.This closedloop pole and compensator zero will generate a long tail of small amplitude in the step response, thus increasing the settling time.  \n\nDesign Procedures for Lag Compensation by the Root-Locus Method. The procedure for designing lag compensators for the system shown in Figure 6–47 by the root-locus method may be stated as follows (we assume that the uncompensated system meets the transient-response specifications by simple gain adjustment; if this is not the case, refer to Section 6–8):  \n\n1. Draw the root-locus plot for the uncompensated system whose open-loop transfer function is $G(s)$ .Based on the transient-response specifications, locate the dominant closed-loop poles on the root locus. 2. Assume the transfer function of the lag compensator to be given by Equation (6–19):  \n\n$$\nG_{c}(s)\\,=\\,\\hat{K}_{c}\\beta\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}=\\hat{K}_{c}\\frac{s\\,+\\,\\frac{1}{T}}{s\\,+\\,\\frac{1}{\\beta T}}\n$$  \n\nThen the open-loop transfer function of the compensated system becomes $G_{c}(s)G(s)$ .  \n\n3. Evaluate the particular static error constant specified in the problem.   \n4. Determine the amount of increase in the static error constant necessary to satisfy the specifications.   \n5. Determine the pole and zero of the lag compensator that produce the necessary increase in the particular static error constant without appreciably altering the original root loci. (Note that the ratio of the value of gain required in the specifications and the gain found in the uncompensated system is the required ratio between the distance of the zero from the origin and that of the pole from the origin.)   \n6. Draw a new root-locus plot for the compensated system. Locate the desired dominant closed-loop poles on the root locus. (If the angle contribution of the lag network is very small—that is, a few degrees—then the original and new root loci are almost identical. Otherwise, there will be a slight discrepancy between them.Then locate, on the new root locus, the desired dominant closed-loop poles based on   \n7. Adjust gain the transient-response specifications.) $\\hat{K}_{c}$ of the compensator from the magnitude condition so that the domA Binant closed-loop poles lie at the desired location. $\\hat{K}_{c}$ will be approximately 1.  \n\n![](images/2e887fd7adf9c06542b5cba12c4d83107fde30d2fd19e64bbc9819b1ab1c7c5d.jpg)  \nFigure 6–47 Control system.  \n\nConsider the system shown in Figure 6–48(a).The feedforward transfer function is  \n\n$$\nG(s)=\\frac{1.06}{s(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nThe root-locus plot for the system is shown in Figure 6–48(b). The closed-loop transfer function becomes  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{1.06}{s(s+1)(s+2)+1.06}}}}\\\\ {{\\displaystyle{\\qquad=\\frac{1.06}{(s+0.3307-j0.5864)(s+0.3307+j0.5864)(s+2.3386)}}}}\\end{array}\n$$  \n\nThe dominant closed-loop poles are  \n\n$$\ns\\,=\\,-0.3307\\,\\pm\\,j0.5864\n$$  \n\nThe damping ratio of the dominant closed-loop poles is $\\zeta=0.491$ .The undamped natural frequency of the dominant closed-loop poles is 0.673 rad 'sec.The static velocity error constant is $0.53\\;\\mathrm{sec}^{-\\mathrm{i}}$ .  \n\nIt is desired to increase the static velocity error constant $K_{v}$ to about $5\\,\\mathrm{sec}^{-1}$ without appreciably changing the location of the dominant closed-loop poles.  \n\nTo meet this specification, let us insert a lag compensator as given by Equation (6–19) in cascade with the given feedforward transfer function.To increase the static velocity error constant by a factor of about 10, let us choose $\\beta=10$ and place the zero and pole of the lag compensator at $s\\,=\\,-0.05$ and $s=-0.005$ , respectively.The transfer function of the lag compensator becomes  \n\n$$\nG_{c}(s)\\,=\\,\\hat{K}_{c}\\,\\frac{s\\,+\\,0.05}{s\\,+\\,0.005}\n$$  \n\n![](images/7ac6d609cbbd220096285bd19c4331b51335efdeccc55813cf1abb17a91906ec.jpg)  \nFigure 6–48 (a) Control system; (b) root-locus plot.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\n![](images/2317d9513fb61d6e65c17b7802d52004258c01dd1ac30aee43558175ea49b6fa.jpg)  \nFigure 6–49 Compensated system.  \n\nThe angle contribution of this lag network near a dominant closed-loop pole is about $4^{\\circ}$ . Because this angle contribution is not very small, there is a small change in the new root locus near the desired dominant closed-loop poles.  \n\nThe open-loop transfer function of the compensated system then becomes  \n\n$$\n\\begin{array}{r}{G_{c}(s)G(s)\\,=\\,\\hat{K}_{c}\\frac{s\\,+\\,0.05}{s\\,+\\,0.005}\\frac{1.06}{s\\,(s\\,+\\,1)(s\\,+\\,2)}}\\\\ {=\\frac{K(s\\,+\\,0.05)}{s\\left(s\\,+\\,0.005\\right)\\left(s\\,+\\,1\\right)\\left(s\\,+\\,2\\right)}}\\end{array}\n$$  \n\nwhere  \n\n$$\nK=1.06\\hat{K}_{c}\n$$  \n\nThe block diagram of the compensated system is shown in Figure 6–49.The root-locus plot for the compensated system near the dominant closed-loop poles is shown in Figure 6–50(a),together with the original root-locus plot. Figure 6–50(b) shows the root-locus plot of the compensated system  \n\n![](images/b2f97b0130b9259dd03f4aa7654cbc741070b114f6483b865a04c8734bdb35d0.jpg)  \nRoot-Locus Plots of Compensated and Uncompensated Systems  \n\n![](images/5118581ca2fe56cf9a6dd45db8ca53e1f3ad46f781dc438545d619a02a7fc711.jpg)  \nRoot-Locus Plot of Compensated System near the Origin   \nFigure 6–50 (a) Root-locus plots of the compensated system and uncompensated system; (b) root-locus plot of compensated system near the origin.  \n\nnear the origin.The MATLAB program to generate the root-locus plots shown in Figures 6–50(a) and (b) is given in MATLAB Program 6–11.  \n\n![](images/8ebf8e522019649f800975ee51cd28e4744f40e13612c5d20e9c1f512dff7971.jpg)  \n\nIf the damping ratio of the new dominant closed-loop poles is kept the same, then these poles are obtained from the new root-locus plot as follows:  \n\n$$\ns_{1}=-0.31\\,+\\,j0.55,\\qquad s_{2}=-0.31\\,-\\,j0.55\n$$  \n\nThe open-loop gain $K$ is determined from the magnitude condition as follows:  \n\n$$\nK=\\left|\\frac{s(s\\mathrm{~+~}0.005)(s\\mathrm{~+~}1)(s\\mathrm{~+~}2)}{s\\mathrm{~+~}0.05}\\right|_{s=-0.31+j0.55}\n$$  \n\nThen the lag compensator gain $\\hat{K}_{c}$ is determined as  \n\n$$\n\\hat{K}_{c}=\\frac{K}{1.06}=\\frac{1.0235}{1.06}=0.9656\n$$  \n\nThus the transfer function of the lag compensator designed is  \n\n$$\nG_{c}(s)\\,=\\,0.9656\\,\\frac{s\\,+\\,0.05}{s\\,+\\,0.005}=\\,9.656\\,\\frac{20s\\,+\\,1}{200s\\,+\\,1}\n$$  \n\nThen the compensated system has the following open-loop transfer function:  \n\n$$\n\\begin{array}{c}{{G_{1}(s)=\\displaystyle\\frac{1.0235(s~+~0.05)}{s(s~+~0.005)(s~+~1)(s~+~2)}}}\\\\ {{=\\displaystyle\\frac{5.12(20s~+~1)}{s(200s~+~1)(s~+~1)(0.5s~+~1)}}}\\end{array}\n$$  \n\nThe static velocity error constant $K_{v}$ is  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\to0}s G_{1}(s)\\,=\\,5.12\\sec^{-1}\n$$  \n\nIn the compensated system, the static velocity error constant has increased to $5.12\\ s e c^{-1}$ ,or $5.12/0.53\\,=\\,9.66$ times the original value. (The steady-state error with ramp inputs has decreased to about $10\\%$ of that of the original system.) We have essentially accomplished the design objective of increasing the static velocity error constant to $5\\;\\mathrm{sec}^{-1}$ .  \n\nNote that, since the pole and zero of the lag compensator are placed close together and are located very near the origin, their effect on the shape of the original root loci has been small. Except for the presence of a small closed root locus near the origin,the root loci of the compensated and the uncompensated systems are very similar to each other.However,the value of the static velocity error constant of the compensated system is 9.66 times greater than that of the uncompensated system.  \n\nThe two other closed-loop poles for the compensated system are found as follows:  \n\n$$\ns_{3}=-2.326,\\qquad s_{4}=-0.0549\n$$  \n\nThe addition of the lag compensator increases the order of the system from 3 to 4, adding one additional closed-loop pole close to the zero of the lag compensator. (The added closed-loop pole at $s\\,=\\,-0.0549$ is close to the zero at $s\\,=\\,-0.05.$ .) Such a pair of a zero and pole creates a long tail of small amplitude in the transient response, as we will see later in the unit-step response. Since the pole at $s=-2.326$ is very far from the $j\\omega$ axis compared with the dominant closed-loop poles, the effect of this pole on the transient response is also small. Therefore, we may consider the closed-loop poles at $s\\,=\\,-0.31\\,\\pm\\,j0.55$ to be the dominant closed-loop poles.  \n\nThe undamped natural frequency of the dominant closed-loop poles of the compensated system is 0.631 rad 'sec.This value is about $6\\%$ less than the original value, 0.673 rad 'sec.This implies that the transient response of the compensated system is slower than that of the original system. The response will take a longer time to settle down.The maximum overshoot in the step response will increase in the compensated system. If such adverse effects can be tolerated, the lag compensation as discussed here presents a satisfactory solution to the given design problem.  \n\nNext, we shall compare the unit-ramp responses of the compensated system against the uncompensated system and verify that the steady-state performance is much better in the compensated system than the uncompensated system.  \n\nTo obtain the unit-ramp response with MATLAB, we use the step command for the system $C(s)/[s R(s)]$ .Since $C(s)/[s R(\\bar{s})]$ for the compensated system is  \n\n$$\n\\begin{array}{c}{{\\displaystyle\\frac{C(s)}{s R(s)}=\\frac{1.0235(s+0.05)}{s\\big[s(s+0.005)(s+1)(s+2)+1.0235(s+0.05)\\big]}}}\\\\ {{\\displaystyle=\\frac{1.0235s+0.0512}{s^{5}+3.005s^{4}+2.015s^{3}+1.0335s^{2}+0.0512s}}}\\end{array}\n$$  \n\nwe have  \n\nAlso, $C(s)/[s R(s)]$ for the uncompensated system is  \n\n$$\n\\begin{array}{c}{{\\frac{C(s)}{s R(s)}=\\frac{1.06}{s\\left[s(s+1)(s+2)\\right.+\\left.1.06\\right]}}}\\\\ {{=\\frac{1.06}{s^{4}\\,+\\,3s^{3}\\,+\\,2s^{2}\\,+\\,1.06s}}}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\begin{array}{r l}{\\textsf{n u m}=\\left[1.06\\right]}&{{}}\\\\ {\\textsf{d e n}=\\left[1\\ \\ 3\\ \\ 2\\ \\ 1.06\\ \\ 0\\right]}&{{}}\\end{array}\n$$  \n\nMATLAB Program 6–12 produces the plot of the unit-ramp response curves. Figure 6–51 shows the result. Clearly, the compensated system shows much smaller steady-state error (one-tenth of the original steady-state error) in following the unit-ramp input.  \n\n# MATLAB Program 6–12  \n\n![](images/ba5da1c1b7df5222398a4babe12cabf59cd7b6b9fcea20bf365f1a401d0ddbdd.jpg)  \n\n![](images/aba564919a3ce27e7504e1c1c014979e7efb56d81271739ebcdf207df80ae94a.jpg)  \nFigure 6–51 Unit-ramp responses of compensated and uncompensated systems. [The compensator is given by Equation (6–20).]  \n\nMATLAB Program 6–13 gives the unit-step response curves of the compensated and uncompensated systems. The unit-step response curves are shown in Figure 6–52. Notice that the lag-compensated system exhibits a larger maximum overshoot and slower response than the original uncompensated system. Notice that a pair of the pole at $s\\,=\\,-0.0549$ and zero at  \n\n# MATLAB Program 6–13  \n\n$\\%$ \\*\\*\\*\\*\\* Unit-step responses of compensated system and   \n$\\%$ uncompensated system \\*\\*\\*\\*\\*   \n$\\%$ \\*\\*\\*\\*\\* Enter the numerators and denominators of the   \n$\\%$ compensated and uncompensated systems \\*\\*\\*\\*\\*   \nn$\\mathsf{u m c}=[1.0235\\ \\ 0.0512]\\,,$ ;  \ndenc $=$ [1  3.005  2.015  1.0335  0.0512];   \n$\\sf{n u m}=[1.06]$ ;  \n$\\mathsf{d e n}=[1\\;\\;3\\;\\;2\\;\\;1.06]\\,,$ ;  \n$\\%$ \\*\\*\\*\\*\\* Specify the time range (such as $\\mathrm{t}=0{:}0.1:40)$ and enter   \n$\\%$ step command and plot command. \\*\\*\\*\\*\\*   \n$\\mathrm{t}=0{:}0.1:40;$ ;  \n${\\boldsymbol{C}}^{\\parallel}=$ step(numc,denc,t);   \n$\\mathsf{c}2=\\mathsf{s t e p}(\\mathsf{n u m,d e n,t})$ c2 = step(num,den,t);   \n$\\mathsf{p l o t}(\\mathrm{t},\\mathrm{c}\\,{1}\\,,^{\\prime}\\,\\mathrm{-}\\,^{\\prime},\\mathrm{t},\\mathrm{c}\\,{2}\\,,^{\\prime}\\,\\mathrm{.}^{\\prime})$   \ngrid   \ntext(13,1.12,'Compensated system')   \ntext(13.6,0.88,'Uncompensated system')   \ntitle('Unit-Step Responses of Compensated and Uncompensated Systems') xlabel('t Sec')   \nylabel('Outputs c1 and c2')  \n\n![](images/b087a8149710d284766a781291e63457aafa7a0f31a3f8ec5d160a9ed1e07634.jpg)  \nFigure 6–52 Unit-step responses of compensated and uncompensated systems. [The compensator is given by Equation (6–20).]  \n\n$s\\,=\\,-0.05$ generates a long tail of small amplitude in the transient response. If a larger maximum overshoot and a slower response are not desired, we need to use a lag–lead compensator as presented in Section 6–8.  \n\nComments. It is noted that under certain circumstances, however, both lead compensator and lag compensator may satisfy the given specifications (both transientresponse specifications and steady-state specifications.) Then either compensation may be used.  \n\n# 6–8 LAG–LEAD COMPENSATION  \n\nLead compensation basically speeds up the response and increases the stability of the system. Lag compensation improves the steady-state accuracy of the system, but reduces the speed of the response.  \n\nIf improvements in both transient response and steady-state response are desired, then both a lead compensator and a lag compensator may be used simultaneously.Rather than introducing both a lead compensator and a lag compensator as separate units, however, it is economical to use a single lag–lead compensator.  \n\nLag–lead compensation combines the advantages of lag and lead compensations. Since the lag–lead compensator possesses two poles and two zeros, such a compensation increases the order of the system by 2, unless cancellation of pole(s) and zero(s) occurs in the compensated system.  \n\nElectronic Lag–Lead Compensator Using Operational Amplifiers. Figure 6–53 shows an electronic lag–lead compensator using operational amplifiers. The transfer  \n\nFigure 6–53 Lag–lead compensator.  \n\n![](images/70be88814ab8b77667f07beb39ca517a1d49077f9ccf3a702deb3a01a924391f.jpg)  \n\nfunction for this compensator may be obtained as follows: The complex impedance $Z_{1}$ is given by  \n\n$$\n{\\frac{1}{Z_{1}}}={\\frac{1}{R_{1}+{\\frac{1}{C_{1}s}}}}+{\\frac{1}{R_{3}}}\n$$  \n\nor  \n\n$$\nZ_{1}=\\frac{\\left(R_{1}C_{1}s\\,+\\,1\\right)R_{3}}{\\left(R_{1}\\,+\\,R_{3}\\right)C_{1}s\\,+\\,1}\n$$  \n\nSimilarly, complex impedance $Z_{2}$ is given by  \n\n$$\nZ_{2}=\\frac{\\left(R_{2}C_{2}s\\,+\\,1\\right)R_{4}}{\\left(R_{2}\\,+\\,R_{4}\\right)C_{2}s\\,+\\,1}\n$$  \n\nHence, we have  \n\n$$\n\\frac{E(s)}{E_{i}(s)}=-\\frac{Z_{2}}{Z_{1}}=-\\,\\frac{R_{4}}{R_{3}}\\frac{\\big(R_{1}\\,+\\,R_{3}\\big)C_{1}s\\,+\\,1}{R_{1}C_{1}s\\,+\\,1}\\cdot\\frac{R_{2}C_{2}s\\,+\\,1}{\\big(R_{2}\\,+\\,R_{4}\\big)C_{2}s\\,+\\,1}\n$$  \n\nThe sign inverter has the transfer function  \n\n$$\n{\\frac{E_{o}(s)}{E(s)}}=-\\,{\\frac{R_{6}}{R_{5}}}\n$$  \n\nThus the transfer function of the compensator shown in Figure 6–53 is  \n\n$$\n{\\frac{E_{o}(s)}{E_{i}(s)}}={\\frac{E_{o}(s)}{E(s)}}{\\frac{E(s)}{E_{i}(s)}}={\\frac{R_{4}R_{6}}{R_{3}R_{5}}}\\left[{\\frac{(R_{1}+R_{3})C_{1}s\\,+\\,1}{R_{1}C_{1}s\\,+\\,1}}\\right]\\left[{\\frac{R_{2}C_{2}s\\,+\\,1}{(R_{2}+R_{4})C_{2}s\\,+\\,1}}\\right]\n$$  \n\nLet us define  \n\n$$\nT_{1}=\\big(R_{1}+\\,R_{3}\\big)C_{1},\\qquad\\frac{T_{1}}{\\gamma}=R_{1}C_{1},\\qquad T_{2}=R_{2}C_{2},\\qquad\\beta T_{2}=\\big(R_{2}+\\,R_{4}\\big)C_{2}\n$$  \n\nThen Equation (6–21) becomes  \n\n$$\n\\frac{E_{o}(s)}{E_{i}(s)}=K_{c}\\frac{\\beta}{\\gamma}\\left(\\frac{T_{1}s\\,+\\,1}{\\frac{T_{1}}{\\gamma}\\,s\\,+\\,1}\\right)\\bigg(\\frac{T_{2}s\\,+\\,1}{\\beta T_{2}s\\,+\\,1}\\bigg)\\,=\\,K_{c}\\frac{\\bigg(s\\,+\\,\\frac{1}{T_{1}}\\bigg)\\bigg(s\\,+\\,\\frac{1}{T_{2}}\\bigg)}{\\bigg(s\\,+\\,\\frac{\\gamma}{T_{1}}\\bigg)\\bigg(s\\,+\\,\\frac{1}{\\beta T_{2}}\\bigg)}\n$$  \n\nwhere  \n\n$$\n\\gamma=\\frac{R_{1}+R_{3}}{R_{1}}>1,\\qquad\\beta=\\frac{R_{2}+R_{4}}{R_{2}}>1,\\qquad K_{c}=\\frac{R_{2}R_{4}R_{6}}{R_{1}R_{3}R_{5}}\\frac{R_{1}+R_{3}}{R_{2}+R_{4}}\n$$  \n\nNote that $\\gamma$ is often chosen to be equal to $\\beta$ .  \n\nLag–lead Compensation Techniques Based on the Root-Locus Approach. Consider the system shown in Figure 6–54.Assume that we use the lag–lead compensator:  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\frac{\\beta}{\\gamma}\\frac{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}{\\Big(\\frac{T_{1}}{\\gamma}s\\,+\\,1\\Big)(\\beta T_{2}s\\,+\\,1)}\\,=\\,K_{c}\\left(\\frac{s\\,+\\,\\frac{1}{T_{1}}}{s\\,+\\,\\frac{\\gamma}{T_{1}}}\\right)\\left(\\frac{s\\,+\\,\\frac{1}{T_{2}}}{s\\,+\\,\\frac{1}{\\beta T_{2}}}\\right)\n$$  \n\nwhere $\\beta>1$ and $\\gamma>1$ .(Consider $K_{c}$ to belong to the lead portion of the lag–lead compensator.)  \n\nIn designing lag–lead compensators, we consider two cases where $\\gamma\\neq\\beta$ and $\\gamma=\\beta$ .  \n\nCase 1. $\\gamma\\neq\\beta$ .In this case, the design process is a combination of the design of the lead compensator and that of the lag compensator.The design procedure for the lag–lead compensator follows:  \n\n1. From the given performance specifications, determine the desired location for the dominant closed-loop poles.   \n2. Using the uncompensated open-loop transfer function $G(s)$ ,determine the angle deficiency $\\phi$ if the dominant closed-loop poles are to be at the desired location.The phase-lead portion of the lag–lead compensator must contribute this angle $\\phi$ .  \n3. Assuming that we later choose $T_{2}$ sufficiently large so that the magnitude of the lag portion  \n\n![](images/990f34ef8dc0a81c532dd47ab9fd2d376a672c6d303b26c2703d45037aa2cab4.jpg)  \nFigure 6–54 Control system.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nis approximately unity, where $s\\implies s_{1}$ is one of the dominant closed-loop poles, choose the values of $T_{1}$ and $\\gamma$ from the requirement that  \n\n$$\n\\left\\langle{\\frac{s_{1}+{\\frac{1}{T_{1}}}}{s_{1}+{\\frac{\\gamma}{T_{1}}}}}=\\phi\n$$  \n\nThe choice of $T_{1}$ and $\\gamma$ is not unique. (Infinitely many sets of $T_{1}$ and $\\gamma$ are possible.) Then determine the value of $K_{c}$ from the magnitude condition:  \n\n$$\nK_{c}\\frac{s_{1}+\\frac{1}{T_{1}}}{s_{1}+\\frac{\\gamma}{T_{1}}}G(s_{1})\\Bigg|\\,=\\,1\n$$  \n\n4. If the static velocity error constant $K_{v}$ is specified, determine the value of $\\beta$ to satisfy the requirement for $K_{v}$ .The static velocity error constant $K_{v}$ is given by  \n\n$$\n\\begin{array}{l}{{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)}}\\\\ {{\\displaystyle~~~=\\operatorname*{lim}_{s\\rightarrow0}s K_{c}\\left(\\frac{s+\\frac{1}{T_{1}}}{s+\\frac{\\gamma}{T_{1}}}\\right)\\left(\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{\\beta T_{2}}}\\right)G(s)}}\\\\ {{\\displaystyle~~~=\\operatorname*{lim}_{s\\rightarrow0}s K_{c}\\frac{\\beta}{\\gamma}G(s)}}\\end{array}\n$$  \n\nwhere $K_{c}$ and $\\gamma$ are already determined in step 3. Hence, given the value of $K_{v}$ , the value of $\\beta$ can be determined from this last equation. Then, using the value of $\\beta$ thus determined, choose the value of $T_{2}$ such that  \n\n$$\n\\left|\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}\\right|\\doteq1\n$$  \n\n$$\n-5^{\\circ}<\\left\\langle\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}<0^{\\circ}\\right.\n$$  \n\n(The preceding design procedure is illustrated in Example 6–8.)  \n\nCase 2. $\\gamma=\\beta$ .If $\\gamma=\\beta$ is required in Equation (6–23), then the preceeding design procedure for the lag–lead compensator may be modified as follows:  \n\n1. From the given performance specifications, determine the desired location for the dominant closed-loop poles.  \n\n2. The lag–lead compensator given by Equation (6–23) is modified to  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\,{\\frac{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}{\\displaystyle{\\left({\\frac{T_{1}}{\\beta}}\\,s\\,+\\,1\\right)}(\\beta T_{2}s\\,+\\,1)}}=\\,K_{c}\\,{\\frac{\\displaystyle{\\left(s\\,+\\,{\\frac{1}{T_{1}}}\\right)}\\left(s\\,+\\,{\\frac{1}{T_{2}}}\\right)}{\\displaystyle{\\left(s\\,+\\,{\\frac{\\beta}{T_{1}}}\\right)}\\left(s\\,+\\,{\\frac{1}{\\beta T_{2}}}\\right)}}\n$$  \n\nwhere $\\beta>1$ .The open-loop transfer function of the compensated system is $G_{c}(s)G(s)$ .If the static velocity error constant $K_{v}$ is specified, determine the value of constant $K_{c}$ from the following equation:  \n\n$$\n\\begin{array}{l}{{K_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)}}\\\\ {{\\ =\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s K_{c}G(s)}}\\end{array}\n$$  \n\n3. To have the dominant closed-loop poles at the desired location, calculate the angle contribution $\\phi$ needed from the phase-lead portion of the lag–lead compensator.  \n\n4. For the lag–lead compensator, we later choose $T_{2}$ sufficiently large so that  \n\n$$\n\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}\n$$  \n\nis approximately unity, where $s\\,=\\,s_{1}$ is one of the dominant closed-loop poles. Determine the values of $T_{1}$ and $\\beta$ from the magnitude and angle conditions:  \n\n$$\nK_{c}\\Bigg(\\frac{s_{1}+\\frac{1}{T_{1}}}{s_{1}+\\frac{\\beta}{T_{1}}}\\Bigg)G(s_{1})\\Bigg|\\,=\\,1\n$$  \n\n$$\n\\left\\langle{\\frac{s_{1}+{\\frac{1}{T_{1}}}}{s_{1}+{\\frac{\\beta}{T_{1}}}}}=\\phi\n$$  \n\n5. Using the value of $\\beta$ just determined, choose $T_{2}$ so that  \n\n$$\n\\left.{\\frac{s_{1}+{\\frac{1}{T_{2}}}}{s_{1}+{\\frac{1}{\\beta T_{2}}}}}\\right|_{\\div}+1\n$$  \n\n$$\n-5^{\\circ}<\\left\\langle\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}<0^{\\circ}\\right.\n$$  \n\nThe value of $\\beta T_{2}$ ,the largest time constant of the lag–lead compensator, should not be too large to be physically realized. (An example of the design of the lag–lead compensator when $\\gamma=\\beta$ is given in Example 6–9.)  \n\nConsider the control system shown in Figure 6–55.The feedforward transfer function is  \n\n$$\nG(s)={\\frac{4}{s(s+0.5)}}\n$$  \n\nThis system has closed-loop poles at  \n\n$$\ns\\,=\\,-0.2500\\,\\pm\\,j1.9843\n$$  \n\nThe damping ratio is 0.125, the undamped natural frequency is 2 rad/sec, and the static velocity error constant is $8\\;\\mathrm{sec}^{-1}$ .  \n\nIt is desired to make the damping ratio of the dominant closed-loop poles equal to 0.5 and to increase the undamped natural frequency to 5 rad 'sec and the static velocity error constant to $80\\;\\mathrm{sec}^{-1}$ . Design an appropriate compensator to meet all the performance specifications.  \n\nLet us assume that we use a lag–lead compensator having the transfer function  \n\n$$\nG_{c}(s)=K_{c}\\left(\\frac{s+\\frac{1}{T_{1}}}{s+\\frac{\\gamma}{T_{1}}}\\right)\\left(\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{\\beta T_{2}}}\\right)\\qquad(\\gamma>1,\\beta>1)\n$$  \n\nwhere $\\gamma$ is not equal to $\\beta$ .Then the compensated system will have the open-loop transfer function  \n\n$$\nG_{c}(s)G(s)=K_{c}\\left({\\frac{s+{\\frac{1}{T_{1}}}}{s+{\\frac{\\gamma}{T_{1}}}}}\\right)\\left({\\frac{s+{\\frac{1}{T_{2}}}}{s+{\\frac{1}{\\beta T_{2}}}}}\\right)G(s)\n$$  \n\nFrom the performance specifications, the dominant closed-loop poles must be at  \n\n$$\ns\\,=\\,-2.50\\,\\pm\\,j4.33\n$$  \n\nSince  \n\n$$\n\\left/{\\frac{4}{s(s\\,+\\,0.5)}}\\,\\right|_{s=-2.50+j4.33}=-235^{\\circ}\n$$  \n\nthe phase-lead portion of the lag–lead compensator must contribute $55^{\\circ}$ so that the root locus passes through the desired location of the dominant closed-loop poles.  \n\nTo design the phase-lead portion of the compensator, we first determine the location of the zero and pole that will give $55^{\\circ}$ contribution. There are many possible choices, but we shall here choose the zero at $s\\,=\\,-0.5$ so that this zero will cancel the pole at $s=-0.5$ of the plant. Once the zero is chosen, the pole can be located such that the angle contribution is $55^{\\circ}$ . By simple calculation or graphical analysis, the pole must be located at $s\\,=\\,-5.02$ .Thus, the phase-lead portion of the lag–lead compensator becomes  \n\n![](images/0a3108dea49c6a6fcfe5b1c39ad57ccf38bf694ee6634a036c22ef6ee160d67a.jpg)  \nFigure 6–55 Control system.  \n\nThus  \n\n$$\nT_{1}=2,~~~~~~\\gamma=\\frac{5.02}{0.5}=10.04\n$$  \n\nNext we determine the value of $K_{c}$ from the magnitude condition:  \n\n$$\n\\left|K_{c}\\frac{s\\,+\\,0.5}{s\\,+\\,5.02}\\frac{4}{s(s\\,+\\,0.5)}\\right|_{s=-2.5+j4.33}\\,=\\,1\n$$  \n\nHence,  \n\n$$\nK_{c}=\\left|\\frac{(s\\,+\\,5.02)s}{4}\\right|_{s=-2.5+j4.33}=\\,6.26\n$$  \n\nThe phase-lag portion of the compensator can be designed as follows: First the value of $\\beta$ is determined to satisfy the requirement on the static velocity error constant:  \n\n$$\n\\begin{array}{l}{{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s K_{c}\\frac{\\beta}{\\gamma}\\,G(s)}}\\\\ {{\\,\\,\\,}}\\\\ {{=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s(6.26)\\,\\displaystyle\\frac{\\beta}{10.04}\\frac{4}{s(s+0.5)}=4.988\\beta=80}}\\end{array}\n$$  \n\nHence, $\\beta$ is determined as  \n\n$$\n\\beta=16.04\n$$  \n\nFinally, we choose the value $T_{2}$ such that the following two conditions are satisfied:  \n\n$$\n\\left|\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{16.04T_{2}}}\\right|_{s=-2.5+j4.33}\\ \\doteq1,\\qquad-5^{\\circ}<\\left|\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{16.04T_{2}}\\right|_{s=-2.5+j4.33}}<0^{\\circ}\n$$  \n\nWe may choose several values for $T_{2}$ and check if the magnitude and angle conditions are satisfied.After simple calculations we find for $T_{2}=5$  \n\n$$\n1>\\mathrm{magnitude}>0.98,~~~~~~~~~-2.10^{\\circ}<\\mathrm{angle}<0^{\\circ}\n$$  \n\nSince $T_{2}=5$ satisfies the two conditions, we may choose  \n\n$$\nT_{2}=5\n$$  \n\nNow the transfer function of the designed lag–lead compensator is given by  \n\n$$\n\\begin{array}{l l}{\\displaystyle G_{c}(s)\\,=\\,(6.26)\\left(\\frac{s\\,+\\,\\frac{1}{2}}{s\\,+\\,\\frac{10.04}{2}}\\right)\\left(\\frac{s\\,+\\,\\frac{1}{5}}{s\\,+\\,\\frac{1}{16.04\\times5}}\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\,=\\,6.26\\bigg(\\frac{s\\,+\\,0.5}{s\\,+\\,5.02}\\bigg)\\bigg(\\frac{s\\,+\\,0.2}{s\\,+\\,0.01247}\\bigg)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\n$$  \n\nThe compensated system will have the open-loop transfer function  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{25.04(s\\,+\\,0.2)}{s(s\\,+\\,5.02)(s\\,+\\,0.01247)}\n$$  \n\nBecause of the cancellation of the $(s+0.5)$ terms,the compensated system is a third-order system. (Mathematically, this cancellation is exact, but practically such cancellation will not be exact because some approximations are usually involved in deriving the mathematical model of the system and, as a result, the time constants are not precise.) The root-locus plot of the compensated system is shown in Figure 6–56(a).An enlarged view of the root-locus plot near the origin is shown in Figure 6–56(b). Because the angle contribution of the phase lag portion of the lag–lead compensator is quite small, there is only a small change in the location of the dominant closedloop poles from the desired location, $s=-2.5\\,\\pm\\,j4.33$ .The characteristic equation for the compensated system is  \n\n$$\ns(s\\,+\\,5.02)(s\\,+\\,0.01247)\\,+\\,25.04(s\\,+\\,0.2)\\,=\\,0\n$$  \n\nor  \n\n$$\ns^{3}\\,+\\,5.0325s^{2}\\,+\\,25.1026s\\,+\\,5.008\n$$  \n\n$$\n=(s\\,+\\,2.4123\\,+\\,j4.2756)(s\\,+\\,2.4123\\,-\\,j4.2756)(s\\,+\\,0.2078)\\,=\\,0\n$$  \n\nHence the new closed-loop poles are located at  \n\n$$\ns\\,=\\,-2.4123\\,\\pm\\,j4.2756\n$$  \n\nThe new damping ratio is $\\zeta=0.491$ .Therefore the compensated system meets all the required performance specifications. The third closed-loop pole of the compensated system is located at $s=-0.2078$ .Since this closed-loop pole is very close to the zero at $s=-0.2$ ,the effect of this pole on the response is small. (Note that, in general, if a pole and a zero lie close to each other on the negative real axis near the origin, then such a pole-zero combination will yield a long tail of small amplitude in the transient response.)  \n\n![](images/d5bac44b53f7cd8e1a6215d5e32e7c5f5865033d61bd4bd24d10b91dd8de7ccb.jpg)  \n  \nFigure 6–56 (a) Root-locus plot of the compensated system; (b) root-locus plot near the origin.  \n\n![](images/3e8b58bd08c603ba770279b748cdbdfc0dd19bb565a9887c616028cd24868036.jpg)  \n\n![](images/fe4384abb9db5d01f26da1131c8fd0908ff23390a0ece2105c0bae1a49a3cd0a.jpg)  \nUnit-Ramp Responses of Compensated and Uncompensated Systems  \n\n![](images/50285326b6975d6a0fe480590e5317a5400f74d18a2aeeaab684c422e1d5bd1c.jpg)  \nFigure 6–57 Transient-response curves for the compensated system and uncompensated system. (a) Unit-step response curves; (b) unit-ramp response curves.  \n\n# EXAMPLE 6–9  \n\nThe unit-step response curves and unit-ramp response curves before and after compensation are shown in Figure 6–57. (Notice a long tail of a small amplitude in the unit-step response of the compensated system.)  \n\nConsider the control system of Example 6–8 again. Suppose that we use a lag–lead compensator of the form given by Equation (6–24), or  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\frac{\\biggl(s\\,+\\,\\frac{1}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{T_{2}}\\biggr)}{\\biggl(s\\,+\\,\\frac{\\beta}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{\\beta T_{2}}\\biggr)}\\qquad(\\beta>1)\n$$  \n\nAssuming the specifications are the same as those given in Example 6–8, design a compensator $G_{c}(s)$ .  \n\nThe desired locations for the dominant closed-loop poles are at  \n\n$$\ns\\,=\\,-2.50\\,\\pm\\,j4.33\n$$  \n\nThe open-loop transfer function of the compensated system is  \n\n$$\nG_{c}(s)G(s)\\,=\\,K_{c}\\frac{\\biggl(s\\,+\\,\\frac{1}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{T_{2}}\\biggr)}{\\biggl(s\\,+\\,\\frac{\\beta}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{\\beta T_{2}}\\biggr)}\\,{\\cdot\\,\\frac{4}{s(s\\,+\\,0.5)}}\n$$  \n\nSince the requirement on the static velocity error constant $K_{v}$ is $80\\;\\mathrm{sec}^{-1}$ ,we have  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}K_{c}\\,{\\frac{4}{0.5}}=8K_{c}\\,=\\,80\n$$  \n\nThus  \n\n$$\nK_{c}=10\n$$  \n\nThe time constant $T_{1}$ and the value of $\\beta$ are determined from  \n\n$$\n\\left|{\\frac{s+{\\frac{1}{T_{1}}}}{s+{\\frac{\\beta}{T_{1}}}}}\\right|\\left|{\\frac{40}{s(s+0.5)}}\\right|_{s=-2.5+j4.33}=\\left|{\\frac{s+{\\frac{1}{T_{1}}}}{s+{\\frac{\\beta}{T_{1}}}}}\\right|{\\frac{8}{4.77}}=1\n$$  \n\n$$\n\\left\\vert\\frac{s\\,+\\frac{1}{T_{1}}}{s\\,+\\frac{\\beta}{T_{1}}}\\right\\vert_{s=-2.5+j4.33}\\,=\\,55^{\\circ}\n$$  \n\n(The angle deficiency of $55^{\\circ}$ was obtained in Example 6–8.) Referring to Figure 6–58, we can easily locate points $A$ and $B$ such that  \n\n$$\n\\angle A P B=55^{\\circ},\\qquad{\\frac{{\\overline{{P A}}}}{{\\overline{{P B}}}}}={\\frac{4.77}{8}}\n$$  \n\n(Use a graphical approach or a trigonometric approach.) The result is  \n\n$$\n\\overline{{A O}}\\,=\\,2.38,\\qquad\\overline{{B O}}\\,=\\,8.34\n$$  \n\nor  \n\n$$\nT_{1}=\\frac{1}{2.38}=0.420,\\;\\;\\;\\;\\;\\;\\beta=8.34T_{1}=3.503\n$$  \n\nThe phase-lead portion of the lag–lead network thus becomes  \n\n$$\n10\\biggl(\\frac{s\\,+\\,2.38}{s\\,+\\,8.34}\\biggr)\n$$  \n\nFor the phase-lag portion, we choose $T_{2}$ such that it satisfies the conditions  \n\n$$\n\\left|\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{3.503T_{2}}}\\right|_{s=-2.50+j4.33}\\doteq1,\\qquad-5^{\\circ}<\\left|\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{3.503T_{2}}\\right|_{s=-2.50+j4.33}}<0^{\\circ}\n$$  \n\n![](images/7d2ead6a4b273e361160fd0174d9b24dc7cc1b689de8c1806ed5e240fbd3fcac.jpg)  \nFigure 6–58 Determination of the desired pole-zero location.  \n\nBy simple calculations, we find that if we choose $T_{2}=5$ , then  \n\n$$\n1>\\mathrm{magnitude}>0.98,\\;\\;\\;\\;\\;\\;\\;\\;\\;-1.5^{\\circ}<\\mathrm{{\\;angle}<0^{\\circ}}\n$$  \n\nand if we choose $T_{2}=10$ , then  \n\n$$\n1>\\mathrm{magnitude}>0.99,\\;\\;\\;\\;\\;\\;\\;\\;\\;-1^{\\circ}<\\mathrm{angle}<0^{\\circ}\n$$  \n\nSince $T_{2}$ is one of the time constants of the lag–lead compensator, it should not be too large. If $T_{2}=10$ can be acceptable from practical viewpoint, then we may choose $T_{2}=10.$ . Then  \n\n$$\n\\frac{1}{\\beta T_{2}}=\\frac{1}{3.503\\times10}=0.0285\n$$  \n\nThus, the lag–lead compensator becomes  \n\n$$\nG_{c}(s)\\,=\\,(10)\\bigg(\\frac{s\\,+\\,2.38}{s\\,+\\,8.34}\\bigg)\\bigg(\\frac{s\\,+\\,0.1}{s\\,+\\,0.0285}\\bigg)\n$$  \n\nThe compensated system will have the open-loop transfer function  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{40(s\\,+\\,2.38)(s\\,+\\,0.1)}{(s\\,+\\,8.34)(s\\,+\\,0.0285)s(s\\,+\\,0.5)}\n$$  \n\nNo cancellation occurs in this case, and the compensated system is of fourth order. Because the angle contribution of the phase lag portion of the lag–lead network is quite small, the dominant closed-loop poles are located very near the desired location. In fact, the location of the dominant closed-loop poles can be found from the characteristic equation as follows: The characteristic equation of the compensated system is  \n\n$$\n(s\\,+\\,8.34)(s\\,+\\,0.0285)s(s\\,+\\,0.5)\\,+\\,40(s\\,+\\,2.38)(s\\,+\\,0.1)\\,=\\,0\n$$  \n\nwhich can be simplified to  \n\n$$\n{\\begin{array}{r l}&{s^{4}+8.8685s^{3}+44.4219s^{2}+99.3188s+9.52}\\\\ &{=(s+2.4539+j4.3099)(s+2.4539-j4.3099)(s+0.1003)(s+3.8604)=0}\\end{array}}\n$$  \n\nThe dominant closed-loop poles are located at  \n\n$$\ns=-2.4539\\,\\pm\\,j4.3099\n$$  \n\nThe other closed-loop poles are located at  \n\n$$\ns\\,=\\,-0.1003;\\qquad s\\,=\\,-3.8604\n$$  \n\nSince the closed-loop pole at $s\\,=\\,-0.1003$ is very close to a zero at $s=-0.1$ ,they almost cancel each other.Thus, the effect of this closed-loop pole is very small.The remaining closed-loop pole ($s=-3.8604\\$ )does not quite cancel the zero at $s=-2.4.$ .The effect of this zero is to cause a larger overshoot in the step response than a similar system without such a zero. The unit-step response curves of the compensated and uncompensated systems are shown in Figure 6–59(a).The unit-ramp response curves for both systems are depicted in Figure 6–59(b).  \n\n![](images/6cc41230f269db679964b2a7cd016dcc93f89ba1101d1da64a4a466f3229c4f0.jpg)  \nUnit-Ramp Responses of Compensated and Uncompensated Systems  \n\n![](images/b4e9b135e0918921a34fbda1e2eb8b2af32d595d9f11cc3aff217ab3a4735952.jpg)  \nFigure 6–59 (a) Unit-step response curves for the compensated and uncompensated systems; (b) unit-ramp response curves for both systems.  \n\nThe maximum overshoot in the step response of the compensated system is approximately $38\\%$ . (This is much larger than the maximum overshoot of $21\\%$ in the design presented in Example 6–8.) It is possible to decrease the maximum overshoot by a small amount from $38\\%$ , but not to $20\\%$ if $\\gamma=\\beta$ is required, as in this example. Note that by not requiring $\\gamma=\\beta$ , we have an additional parameter to play with and thus can reduce the maximum overshoot.  \n\n# 6–9 PARALLEL COMPENSATION  \n\nThus far we have presented series compensation techniques using lead, lag, or lag–lead compensators.In this section we discuss parallel compensation technique.Because in the parallel compensation design the controller (or compensator) is in a minor loop, the design may seem to be more complicated than in the series compensation case. It is, however, not complicated if we rewrite the characteristic equation to be of the same form as the characteristic equation for the series compensated system. In this section we present a simple design problem involving parallel compensation.  \n\n$$\n\\frac{C}{R}=\\frac{G_{c}G}{1+G_{c}G H}\n$$  \n\nBasic Principle for Designing Parallel Compensated System. Referring to Figure 6–60(a), the closed-loop transfer function for the system with series compensation is  \n\nThe characteristic equation is  \n\n$$\n1\\,+\\,G_{c}G H\\,=\\,0\n$$  \n\nGiven $G$ and $H$ , the design problem becomes that of determining the compensator $G_{c}$ that satisfies the given specification.  \n\n![](images/e61f2975a3edfe2f7c6c46134a9ca9d3d269b00665cf1fb0f42bf6474d02d94f.jpg)  \nFigure 6–60 (a) Series compensation; (b) parallel or feedback compensation.  \n\nThe closed-loop transfer function for the system with parallel compensation [Figure 6–60(b)] is  \n\n$$\n\\frac{C}{R}=\\frac{G_{1}G_{2}}{1\\,+\\,G_{2}G_{c}\\,+\\,G_{1}G_{2}H}\n$$  \n\nThe characteristic equation is  \n\n$$\n1\\,+\\,G_{1}G_{2}H\\,+\\,G_{2}G_{c}\\,=\\,0\n$$  \n\nBy dividing this characteristic equation by the sum of the terms that do not involve $G_{c}$ ,we obtain  \n\n$$\n1\\,+\\,\\frac{G_{c}G_{2}}{1\\,+\\,G_{1}G_{2}H}\\,=\\,0\n$$  \n\nIf we define  \n\n$$\nG_{f}=\\frac{G_{2}}{1+G_{1}G_{2}H}\n$$  \n\nthen Equation (6–25) becomes  \n\n$$\n1\\,+\\,G_{c}G_{f}\\,=\\,0\n$$  \n\nSince $G_{f}$ is a fixed transfer function, the design of $G_{c}$ becomes the same as the case of series compensation. Hence the same design approach applies to the parallel compensated system.  \n\nVelocity Feedback Systems. A velocity feedback system (tachometer feedback system) is an example of parallel compensated systems.The controller (or compensator) in such a system is a gain element.The gain of the feedback element in a minor loop must be determined properly so that the entire system satisfies the given design specifications.The characteristic of such a velocity feedback system is that the variable parameter does not appear as a multiplying factor in the open-loop transfer function, so that direct application of the root-locus design technique is not possible. However, by rewriting the characteristic equation such that the variable parameter appears as a multiplying factor, then the root-locus approach to the design is possible.  \n\nAn example of control system design using parallel compensation technique is presented in Example 6–10.  \n\n# EXAMPLE 6–10  \n\nConsider the system shown in Figure 6–61. Draw a root-locus diagram.Then determine the value of $k$ such that the damping ratio of the dominant closed-loop poles is 0.4.  \n\nHere the system involves velocity feedback.The open-loop transfer function is  \n\n![](images/883739ca275502f1160a61d41ce1ae3a3e050e3e34641f70c08204798cc424e9.jpg)  \nFigure 6–61 Control system.  \n\nNotice that the adjustable variable $k$ does not appear as a multiplying factor. The characteristic equation for the system is  \n\n$$\ns^{3}\\,+\\,5s^{2}\\,+\\,4s\\,+\\,20k s\\,+\\,20\\,=\\,0\n$$  \n\nDefine  \n\n$$\n20k\\,=\\,K\n$$  \n\nThen Equation (6–26) becomes  \n\n$$\ns^{3}\\,+\\,5s^{2}\\,+\\,4s\\,+\\,K s\\,+\\,20\\,=\\,0\n$$  \n\nDividing both sides of Equation (6–27) by the sum of the terms that do not contain $K$ , we get  \n\n$$\n1\\,+\\,\\frac{K s}{s^{3}\\,+\\,5s^{2}\\,+\\,4s\\,+\\,20}=0\n$$  \n\nor  \n\n$$\n1\\,+\\,{\\frac{K s}{(s\\,+\\,j2)(s\\,-\\,j2)(s\\,+\\,5)}}=0\n$$  \n\nEquation (6–28) is of the form of Equation (6–11).  \n\nWe shall now sketch the root loci of the system given by Equation (6–28). Notice that the open-loop poles are located at $s\\,=\\,j2,s\\,=\\,-j2,s\\,=\\,-5$ ,and the open-loop zero is located at $s\\,=\\,0$ .The root locus exists on the real axis between 0 and $^{-5}$ .Since  \n\n$$\n\\operatorname*{lim}_{s\\to\\infty}{\\frac{K s}{(s\\,+\\,j2)(s\\,-\\,j2)(s\\,+\\,5)}}=\\operatorname*{lim}_{s\\to\\infty}{\\frac{K}{s^{2}}}\n$$  \n\nwe have  \n\n$$\n\\mathrm{Angles\\;of\\;asymptote}\\;=\\frac{\\pm180^{\\circ}(2k\\:+\\:1)}{2}=\\;\\pm\\:90^{\\circ}\n$$  \n\nThe intersection of the asymptotes with the real axis can be found from  \n\n$$\n\\operatorname*{lim}_{s\\rightarrow\\infty}{\\frac{K s}{s^{3}\\,+\\,5s^{2}\\,+\\,4s\\,+\\,20}}=\\operatorname*{lim}_{s\\rightarrow\\infty}{\\frac{K}{s^{2}\\,+\\,5s\\,+\\,\\cdots\\,}}=\\operatorname*{lim}_{s\\rightarrow\\infty}{\\frac{K}{(s\\,+\\,2.5)^{2}}}\n$$  \n\nas  \n\n$$\ns=-2.5\n$$  \n\nThe angle of departure (angle $\\theta$ ) from the pole at $s\\,=\\,j2$ is obtained as follows:  \n\n$$\n\\theta\\,=\\,180^{\\circ}\\,-\\,90^{\\circ}\\,-\\,21.8^{\\circ}\\,+\\,90^{\\circ}\\,=\\,158.2^{\\circ}\n$$  \n\nThus, the angle of departure from the pole $s\\,=\\,j2$ is $158.2^{\\circ}$ . Figure 6–62 shows a root-locus plot for the system. Notice that two branches of the root locus originate from the poles at $s=\\pm j2$ and terminate on the zeros at infinity. The remaining one branch originates from the pole at $s=-5$ and terminates on the zero at $s\\,=\\,0$ .  \n\nNote that the closed-loop poles with $\\zeta=0.4$ must lie on straight lines passing through the origin and making the angles $\\pm66.42^{\\circ}$ with the negative real axis. In the present case, there are two intersections of the root-locus branch in the upper half $s$ plane and the straight line of angle $66.42^{\\circ}$ .Thus,two values of $K$ will give the damping ratio $\\zeta$ of the closed-loop poles equal to 0.4.At point $P$ ,the value of $K$ is  \n\n$$\nK=\\left.\\left|\\frac{(s\\,+\\,j2)(s\\,-\\,j2)(s\\,+\\,5)}{s}\\right|_{s=-1.0490+j2.4065}=8.9801\n$$  \n\nHence  \n\n$$\nk={\\frac{K}{20}}=0.4490\\qquad{\\mathrm{at~point~}}P\n$$  \n\n![](images/74a876317acceafb3e638c766b24db3dda0735aad0d964fe3f5811465fd3dc65.jpg)  \nFigure 6–62 Root-locus plot for the system shown in Figure 6–61.  \n\nAt point $Q$ , the value of $K$ is  \n\n$$\nK=\\left.\\left|\\frac{(s\\,+\\,j2)(s\\,-\\,j2)(s\\,+\\,5)}{s}\\right|_{s=-2.1589+j4.9652}=28.260\n$$  \n\nHence  \n\n$$\nk={\\frac{K}{20}}=1.4130\\qquad{\\mathrm{at~point}}\\,Q\n$$  \n\nThus, we have two solutions for this problem. For $k\\,=\\,0.4490$ ,the three closed-loop poles are located at  \n\n$$\ns=-1.0490\\,+\\,j2.4065,\\qquad s=-1.0490\\,-\\,j2.4065,\\qquad s=-2.9021\n$$  \n\nFor $k=1.4130$ ,the three closed-loop poles are located at  \n\n$$\ns=-2.1589\\,+\\,j4.9652,\\qquad s=-2.1589\\,-\\,j4.9652,\\qquad s=-0.6823\n$$  \n\nIt is important to point out that the zero at the origin is the open-loop zero, but not the closed-loop zero. This is evident, because the original system shown in Figure 6–61 does not have a closed-loop zero, since  \n\n$$\n\\frac{G(s)}{R(s)}=\\frac{20}{s(s\\,+\\,1)(s\\,+\\,4)\\,+\\,20(1\\,+\\,k s)}\n$$  \n\nThe open-loop zero at $s\\,=\\,0$ was introduced in the process of modifying the characteristic equation such that the adjustable variable $K=20k$ was to appear as a multiplying factor.  \n\nWe have obtained two different values of $k$ to satisfy the requirement that the damping ratio of the dominant closed-loop poles be equal to 0.4. The closed-loop transfer function with $k\\,=\\,0.4490$ is given by  \n\n$$\n\\begin{array}{c c c}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{20}{s^{3}\\,+\\,5s^{2}\\,+\\,12.98s\\,+\\,20}}}}\\\\ {{\\displaystyle{}}}\\\\ {{\\displaystyle{=\\,\\frac{20}{(s\\,+\\,1.0490\\,+\\,j2.4065)(s\\,+\\,1.0490\\,-\\,j2.4065)(s\\,+\\,2.9021)}}}}\\end{array}\n$$  \n\nThe closed-loop transfer function with $k=1.4130$ is given by  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{20}{s^{3}+5s^{2}+32.26s+20}}}}\\\\ {{\\displaystyle{\\qquad=\\frac{20}{(s+2.1589+j4.9652)(s+2.1589-j4.9652)(s+0.6823)}}}}\\end{array}\n$$  \n\nNotice that the system with $k\\,=\\,0.4490$ has a pair of dominant complex-conjugate closed-loop poles, while in the system with $k=1.4130$ the real closed-loop pole at $s\\,=\\,-0.6823$ is dominant, and the complex-conjugate closed-loop poles are not dominant. In this case, the response characteristic is primarily determined by the real closed-loop pole.  \n\nLet us compare the unit-step responses of both systems. MATLAB Program 6–14 may be used for plotting the unit-step response curves in one diagram. The resulting unit-step response curves $\\bar{[c_{1}(t)}$ for $k\\,=\\,0.4490$ and $c_{2}(t)$ for $k=1.4130^{^{\\circ}}$ are shown in Figure 6–63.  \n\n![](images/fad863b85c16065a8b62f5ef1c9d0484446ff13166dae440d2f16b84c2731801.jpg)  \n\n![](images/bb647be545f78d3ad6f4af17230c31a8132a14ccb2f8458ebf37b9d48d3ea2a9.jpg)  \nFigure 6–63 Unit-step response curves for the system shown in Figure 6–61 when the damping ratio $\\zeta$ of the dominant closedloop poles is set equal to 0.4. (Two possible values of $k$ give the damping ratio $\\zeta$ equal to 0.4.)  \n\nFrom Figure 6–63 we notice that the response of the system with $k\\,=\\,0.4490$ is oscillatory. (The effect of the closed-loop pole at $s\\,=\\,-2.9021$ on the unit-step response is small.) For the system with $k=1.4130$ ,the oscillations due to the closed-loop poles at $s\\,=\\,-2.1589\\,\\pm\\,j4.9652$ damp out much faster than purely exponential response due to the closed-loop pole at $s=-0.6823$ .  \n\nThe system with $k\\,=\\,0.4490$ (which exhibits a faster response with relatively small overshoot) has a much better response characteristic than the system with $k=1.4130$ (which exhibits a slow overdamped response).Therefore, we should choose $k\\,=\\,0.4490$ for the present system.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–6–1. Sketch the root loci for the system shown in Figure 6–64(a). (The gain $K$ is assumed to be positive.) Observe that for small or large values of $K$ the system is overdamped and for medium values of $K$ it is underdamped.  \n\nSolution. The procedure for plotting the root loci is as follows:  \n\n1. Locate the open-loop poles and zeros on the complex plane. Root loci exist on the negative real axis between 0 and $-1$ and between $^{-2}$ and $^{-3}$ .  \n2. The number of open-loop poles and that of finite zeros are the same.This means that there are no asymptotes in the complex region of the $s$ plane.   \n3. Determine the breakaway and break-in points.The characteristic equation for the system is  \n\n$$\n1\\,+\\,{\\frac{K(s\\,+\\,2)(s\\,+\\,3)}{s(s\\,+\\,1)}}=0\n$$  \n\nor  \n\n$$\nK=-\\,\\frac{s(s\\,+\\,1)}{(s\\,+\\,2)(s\\,+\\,3)}\n$$  \n\n![](images/6b884db1acf23823d91091181427f8b863ff17c30808dc9c1fa4e155c449b79a.jpg)  \nFigure 6–64 (a) Control system; (b) root-locus plot.  \n\nThe breakaway and break-in points are determined from  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{d K}{d s}=-\\,\\frac{(2s\\,+\\,1)(s\\,+\\,2)(s\\,+\\,3)\\,-\\,s(s\\,+\\,1)(2s\\,+\\,5)}{\\big[(s\\,+\\,2)(s\\,+\\,3)\\big]^{2}}}\\\\ {\\displaystyle\\qquad=-\\,\\frac{4(s\\,+\\,0.634)(s\\,+\\,2.366)}{\\big[(s\\,+\\,2)(s\\,+\\,3)\\big]^{2}}}\\\\ {\\displaystyle\\qquad=\\,0}\\end{array}\n$$  \n\nas follows:  \n\n$$\ns\\,=-0.634,\\qquad s\\,=-2.366\n$$  \n\nNotice that both points are on root loci. Therefore, they are actual breakaway or break-in points.At point $s=-0.634$ ,the value of $K$ is  \n\n$$\nK=-\\,{\\frac{(-0.634)(0.366)}{(1.366)(2.366)}}=0.0718\n$$  \n\nSimilarly, at $s=-2.366$ ,  \n\n$$\nK=-\\,{\\frac{(-2.366)(-1.366)}{(-0.366)(0.634)}}=14\n$$  \n\n(Because point $s=-0.634$ lies between two poles, it is a breakaway point, and because point $s=-2.366$ lies between two zeros, it is a break-in point.)  \n\n4. Determine a sufficient number of points that satisfy the angle condition. (It can be found that the root loci involve a circle with center at $-1.5$ that passes through the breakaway and break-in points.) The root-locus plot for this system is shown in Figure 6–64(b).  \n\nNote that this system is stable for any positive value of $K$ since all the root loci lie in the lefthalf $s$ plane.  \n\nSmall values of $K$ ($0<K<0.0718)$ )correspond to an overdamped system. Medium values of $K$ $(0.0718<K<14)$ )correspond to an underdamped system. Finally, large values of $K$ $(14<K)$ )correspond to an overdamped system.With a large value of $K$ , the steady state can be reached in much shorter time than with a small value of $K$ .  \n\nThe value of $K$ should be adjusted so that system performance is optimum according to a given performance index.  \n\nA–6–2. Sketch the root loci for the system shown in Figure 6–65(a).  \n\nSolution. A root locus exists on the real axis between points $s=-1$ and $s=-3.6$ .The asymptotes can be determined as follows:  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{3-1}}=90^{\\circ},-90^{\\circ}\n$$  \n\nThe intersection of the asymptotes and the real axis is found from  \n\n$$\ns\\,=-\\,{\\frac{0\\,+\\,0\\,+\\,3.6\\,-\\,1}{3\\,-\\,1}}=-1.3\n$$  \n\n![](images/f918283266242bdaf098c8273f2a419b0917334a4612fa660375a641710c1d10.jpg)  \n\n# Figure 6–65  \n\n(a) Control system; (b) root-locus plot.  \n\nSince the characteristic equation is  \n\n$$\ns^{3}\\,+\\,3.6s^{2}\\,+\\,K(s\\,+\\,1)\\,=\\,0\n$$  \n\nwe have  \n\n$$\nK=-\\,\\frac{s^{3}\\,+\\,3.6s^{2}}{s\\,+\\,1}\n$$  \n\nThe breakaway and break-in points are found from  \n\n$$\n{\\frac{d K}{d s}}=-\\,{\\frac{\\left(3s^{2}\\,+\\,7.2s\\right)\\!(s\\,+\\,1)\\,-\\,\\left(s^{3}\\,+\\,3.6s^{2}\\right)}{(s\\,+\\,1)^{2}}}=0\n$$  \n\nor  \n\n$$\ns^{3}\\,+\\,3.3s^{2}\\,+\\,3.6s\\,=\\,0\n$$  \n\nfrom which we get  \n\n$$\ns\\,=\\,0,\\qquad s\\,=\\,-1.65\\,+\\,j0.9367,\\qquad s\\,=\\,-1.65\\,-\\,j0.9367\n$$  \n\nPoint $s=0$ corresponds to the actual breakaway point. But points $s\\,=\\,1.65\\,\\pm\\,j0.9367$ are neither breakaway nor break-in points, because the corresponding gain values $K$ become complex quantities.  \n\nTo check the points where root-locus branches may cross the imaginary axis, substitute $s=j\\omega$ into the characteristic equation, yielding.  \n\n$$\n(j\\omega)^{3}\\,+\\,3.6(j\\omega)^{2}\\,+\\,K j\\omega\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big(K\\,-\\,3.6\\omega^{2}\\big)\\,+\\,j\\omega\\big(K\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nNotice that this equation can be satisfied only if $\\omega=0,K=0.$ .Because of the presence of a double pole at the origin, the root locus is tangent to the $j\\omega$ axis at $\\omega\\,=\\,0,$ The root-locus branches do not cross the $j\\omega$ axis. Figure 6–65(b) is a sketch of the root loci for this system.  \n\nA–6–3. Sketch the root loci for the system shown in Figure 6–66(a).  \n\nSolution. A root locus exists on the real axis between point $s=-0.4$ and $s=-3.6.$ The angles of asymptotes can be found as follows:  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{3-1}}=90^{\\circ},-90^{\\circ}\n$$  \n\nThe intersection of the asymptotes and the real axis is obtained from  \n\n$$\ns\\,=-\\,\\frac{0\\,+\\,0\\,+\\,3.6\\,-\\,0.4}{3\\,-\\,1}=-1.6\n$$  \n\nNext we shall find the breakaway points. Since the characteristic equation is  \n\n$$\ns^{3}\\,+\\,3.6s^{2}\\,+\\,K s\\,+\\,0.4K\\,=\\,0\n$$  \n\nwe have  \n\n$$\nK=-\\,\\frac{s^{3}\\,+\\,3.6s^{2}}{s\\,+\\,0.4}\n$$  \n\n![](images/33444ba060323932d6defa3cd917997508efe1c417f556267a38a5163cb3e088.jpg)  \nFigure 6–66 (a) Control system; (b) root-locus plot.  \n\nThe breakaway and break-in points are found from  \n\n$$\n{\\frac{d K}{d s}}=-\\,{\\frac{\\left(3s^{2}\\,+\\,7.2s\\right)\\!(s\\,+\\,0.4)\\,-\\,\\left(s^{3}\\,+\\,3.6s^{2}\\right)}{(s\\,+\\,0.4)^{2}}}=0\n$$  \n\nfrom which we get  \n\n$$\ns^{3}\\,+\\,2.4s^{2}\\,+\\,1.44s\\,=\\,0\n$$  \n\nor  \n\n$$\ns(s\\,+\\,1.2)^{2}=0\n$$  \n\nThus, the breakaway or break-in points are at $s=0$ and $s=-1.2$ .Note that A B$s=-1.2$ is a double root.When a double root occurs in ${d K}/{d s}=0$ at point $s\\,=-1.2,d^{2}K/(d s^{2})\\,=\\,0$ at this point.The value of gain $K$ at point $s=-1.2$ is  \n\n$$\nK=-\\left.\\frac{s^{3}\\,+\\,3.6s^{2}}{s\\,+\\,4}\\,\\right|_{s=-1.2}=4.32\n$$  \n\nThis means that with $K=4.32$ the characteristic equation has a triple root at point $s=-1.2.$ .This can be easily verified as follows:  \n\n$$\ns^{3}+3.6s^{2}+4.32s+1.728=(s\\,+\\,1.2)^{3}=0\n$$  \n\nHence, three root-locus branches meet at point $s=-1.2$ .The angles of departures at point $s=-1.2$ of the root locus branches that approach the asymptotes are $\\pm180^{\\circ}/3$ that is, $60^{\\circ}$ and $-60^{\\circ}$ . (See Problem A–6–4 .)  \n\nFinally,we shall examine if root-locus branches cross the imaginary axis.By substituting $s=j\\omega$ into the characteristic equation, we have  \n\n$$\n(j\\omega)^{3}\\,+\\,3.6(j\\omega)^{2}\\,+\\,K(j\\omega)\\,+\\,0.4K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big(0.4K\\,-\\,3.6\\omega^{2}\\big)\\,+\\,j\\omega\\big(K\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nThis equation can be satisfied only if $\\omega\\,=\\,0,K\\,=\\,0.$ At point $\\omega=0$ ,the root locus is tangent to the jvaxis because of the presence of a double pole at the origin.There are no points where rootlocus branches cross the imaginary axis.  \n\nA sketch of the root loci for this system is shown in Figure 6–66(b).  \n\nA–6–4. Referring to Problem A–6–3 , obtain the equations for the root-locus branches for the system shown in Figure 6–66(a). Show that the root-locus branches cross the real axis at the breakaway point at angles $\\pm60^{\\circ}$ .  \n\nSolution. The equations for the root-locus branches can be obtained from the angle condition  \n\n$$\n\\left/\\frac{K(s\\,+\\,0.4)}{s^{2}(s\\,+\\,3.6)}=\\pm180^{\\circ}(2k\\,+\\,1)\\right.\n$$  \n\nwhich can be rewritten as  \n\n$$\n/s\\,+\\,0.4\\,-\\,2\\,/s\\,-\\,\\,/s\\,+\\,3.6=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nBy substituting $s\\,=\\,\\sigma\\,+\\,j\\omega$ ,we obtain  \n\n$$\n\\underline{{{\\left(\\sigma\\,+\\,j\\omega\\,+\\,0.4\\,-\\,2\\,\\underline{{{\\left(\\sigma\\,+\\,j\\omega\\,-\\,\\left/\\sigma\\right.}\\right.}}\\right)}}}+\\underline{{{j\\omega}}}+\\mathrm{{j}}\\omega+3.6=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nor  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,0.4}}\\right)\\,-\\,2\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,3.6}}\\right)\\,=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nBy rearranging, we have  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,0.4}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma}}\\right)\\,=\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma}}\\right)\\,+\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,+\\,3.6}}\\right)\\,{\\pm}180^{\\circ}(2k\\,+\\,1)\n$$  \n\nTaking tangents of both sides of this last equation, and noting that  \n\n$$\n\\tan\\!\\left[\\tan^{-1}\\!\\left(\\frac{\\omega}{\\sigma\\,+\\,3.6}\\right)\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\\right]\\,=\\frac{\\omega}{\\sigma\\,+\\,3.6}\n$$  \n\nwe obtain  \n\n$$\n{\\frac{{\\cfrac{\\omega}{\\sigma+0.4}}-{\\cfrac{\\omega}{\\sigma}}}{1+{\\cfrac{\\omega}{\\sigma+0.4}}{\\cfrac{\\omega}{\\sigma}}}}={\\frac{{\\cfrac{\\omega}{\\sigma}}+{\\cfrac{\\omega}{\\sigma+3.6}}}{1-{\\cfrac{\\omega}{\\sigma}}{\\cfrac{\\omega}{\\sigma+3.6}}}}\n$$  \n\nwhich can be simplified to  \n\n$$\n\\frac{\\omega\\sigma\\mathrm{~-~}\\omega(\\sigma\\mathrm{~+~}0.4)}{(\\sigma\\mathrm{~+~}0.4)\\sigma\\mathrm{~+~}\\omega^{2}}=\\frac{\\omega(\\sigma\\mathrm{~+~}3.6)\\mathrm{~+~}\\omega\\sigma}{\\sigma(\\sigma\\mathrm{~+~}3.6)\\mathrm{~-~}\\omega^{2}}\n$$  \n\nor  \n\n$$\n\\omega\\!\\left(\\sigma^{3}\\,+\\,2.4\\sigma^{2}\\,+\\,1.44\\sigma\\,+\\,1.6\\omega^{2}\\,+\\,\\sigma\\omega^{2}\\right)\\,=\\,0\n$$  \n\nwhich can be further simplified to  \n\n$$\n\\omega[\\sigma(\\sigma\\,+\\,1.2)^{2}\\,+\\,(\\sigma\\,+\\,1.6)\\omega^{2}]\\,=\\,0\n$$  \n\nFor $\\sigma\\neq-1.6$ ,we may write this last equation as  \n\n$$\n\\omega\\Bigg[\\omega\\mathrm{~-~}(\\sigma\\mathrm{~+~}1.2)\\sqrt{\\frac{-\\sigma}{\\sigma\\mathrm{~+~}1.6}}\\Bigg]\\Bigg[\\omega\\mathrm{~+~}(\\sigma\\mathrm{~+~}1.2)\\sqrt{\\frac{-\\sigma}{\\sigma\\mathrm{~+~}1.6}}\\Bigg]\\mathrm{~=~}0\n$$  \n\nwhich gives the equations for the root locus as follows:  \n\n$$\n\\begin{array}{l}{\\omega=0}\\\\ {\\omega=(\\sigma\\mathrm{~+~}1.2)\\sqrt{\\frac{\\mathrm{~-~}\\sigma}{\\sigma\\mathrm{~+~}1.6}}}\\\\ {\\omega=-(\\sigma\\mathrm{~+~}1.2)\\sqrt{\\frac{\\mathrm{~-~}\\sigma}{\\sigma\\mathrm{~+~}1.6}}}\\end{array}\n$$  \n\nThe equation $\\omega=0$ represents the real axis. The root locus for $0\\le K\\le\\infty$ is between points $s=-0.4$ and $s=-3.6$ .(The real axis other than this line segment and the origin $s\\,=\\,0$ corresponds to the root locus for $-\\infty\\leq K<0.$ )  \n\nThe equations  \n\n$$\n\\omega=\\pm(\\sigma\\,+\\,1.2)\\sqrt{\\frac{-\\sigma}{\\sigma\\,+\\,1.6}}\n$$  \n\nrepresent the complex branches for $0\\le K\\le\\infty.$ .These two branches lie between 1 $\\sigma=-1.6$ and $\\sigma\\,=\\,0$ . [See Figure 6–66(b).] The slopes of the complex root-locus branches at the breakaway point $(\\sigma=-1.2)$ )can be found by evaluating $d\\omega/d\\sigma$ of Equation (6–29) at point $\\sigma=-1.2$ .  \n\n$$\n\\left.{\\frac{d\\omega}{d\\sigma}}\\right|_{\\sigma=-1.2}=\\pm{\\sqrt{\\frac{-\\sigma}{\\sigma\\,+\\,1.6}}}\\,\\right|_{\\sigma=-1.2}=\\pm{\\sqrt{\\frac{1.2}{0.4}}}=\\pm{\\sqrt{3}}\n$$  \n\nSince $\\tan^{-1}{\\sqrt{3}}=60^{\\circ}$ ,the root-locus branches intersect the real axis with angles $\\pm60^{\\circ}$ .  \n\nA–6–5. Consider the system shown in Figure 6–67(a). Sketch the root loci for the system. Observe that for small or large values of $K$ the system is underdamped and for medium values of $K$ it is overdamped.  \n\nSolution. A root locus exists on the real axis between the origin and $-\\infty,$ The angles of asymptotes of the root-locus branches are obtained as  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{3}}=60^{\\circ},-60^{\\circ},-180^{\\circ}\n$$  \n\nThe intersection of the asymptotes and the real axis is located on the real axis at  \n\n$$\ns=-\\,{\\frac{0\\,+\\,2\\,+\\,2}{3}}=-1.3333\n$$  \n\nThe breakaway and break-in points are found from $d K/d s=0.$ .Since the characteristic equation is  \n\n$$\ns^{3}\\,+\\,4s^{2}\\,+\\,5s\\,+\\,K\\,=\\,0\n$$  \n\nFigure 6–67 (a) Control system; (b) root-locus plot.  \n\n![](images/5382cc98c645eb18d45829c8ce7d349f9977c9b259cb283bb7fca53adddb9a59.jpg)  \n(b)  \n\nwe have  \n\n$$\nK\\,=\\,-(s^{3}\\,+\\,4s^{2}\\,+\\,5s)\n$$  \n\nNow we set  \n\n$$\n{\\frac{d K}{d s}}=-(3s^{2}\\,+\\,8s\\,+\\,5)\\,=\\,0\n$$  \n\nwhich yields  \n\n$$\ns=-1,\\qquad s=-1.6667\n$$  \n\nSince these points are on root loci,they are actual breakaway or break-in points. (At point $s=-1$ ,the value of $K$ is 2, and at point $s=-1.6667$ ,the value of $K$ is 1.852.)  \n\nThe angle of departure from a complex pole in the upper-half $s$ plane is obtained from  \n\n$$\n\\theta\\,=\\,180^{\\circ}\\,-\\,153.43^{\\circ}\\,-\\,90^{\\circ}\n$$  \n\nor  \n\n$$\n\\theta=-63.43^{\\circ}\n$$  \n\nThe root-locus branch from the complex pole in the upper-half $s$ plane breaks into the real axis at $s=-1.6667$ .  \n\nNext we determine the points where root-locus branches cross the imaginary axis. By substituting $s=j\\omega$ into the characteristic equation, we have  \n\n$$\n(j\\omega)^{3}\\,+\\,4(j\\omega)^{2}\\,+\\,5(j\\omega)\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big(K\\,-\\,4\\omega^{2}\\big)\\,+\\,j\\omega\\big(5\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nfrom which we obtain  \n\n$$\n\\omega\\,=\\,\\pm\\,\\sqrt{5}\\,,\\qquad K\\,=\\,20\\qquad\\mathrm{or}\\qquad\\omega\\,=\\,0,\\qquad K\\,=\\,0\n$$  \n\nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nRoot-locus branches cross the imaginary axis at $\\omega={\\sqrt{5}}$ and $\\omega=-\\sqrt{5}$ .The root-locus branch on the real axis touches the $j\\omega$ axis at $\\omega=0,$ .A sketch of the root loci for the system is shown in Figure 6–67(b).  \n\nNote that since this system is of third order, there are three closed-loop poles. The nature of the system response to a given input depends on the locations of the closed-loop poles.  \n\nFor $0<K<1.852$ ,there are a set of complex-conjugate closed-loop poles and a real closedloop pole. For $1.852\\leq K\\leq2$ ,there are three real closed-loop poles. For example, the closedloop poles are located at  \n\n$$\n\\begin{array}{l l l l}{{s=-1.667,\\quad}}&{{s=-1.667,\\quad}}&{{s=-0.667,\\quad}}&{{\\mathrm{for}\\:K=1.852}}\\\\ {{s=-1,\\quad}}&{{s=-1,\\quad}}&{{s=-2,\\quad}}&{{\\mathrm{for}\\:K=2}}\\end{array}\n$$  \n\nFor $2<K$ ,there are a set of complex-conjugate closed-loop poles and a real closed-loop pole. Thus, small values of $K$ ($0<K<1.852)$ correspond to an underdamped system. (Since the real closed-loop pole dominates, only a small ripple may show up in the transient response.) Medium values of $K$ $(1.852\\leq K\\leq2)$ )correspond to an overdamped system. Large values of $K$ ($2<K$ )correspond to an underdamped system.With a large value of $K$ , the system responds much faster than with a smaller value of $K$ .  \n\nA–6–6. Sketch the root loci for the system shown in Figure 6–68(a).  \n\nSolution. The open-loop poles are located at $s=0,s=-1,s=-2\\,+\\,j3$ ,and $s=-2\\,-\\,j3.$ A root locus exists on the real axis between points $s\\,=\\,0$ and $s=-1$ .The angles of the asymptotes are found as follows:  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{4}}=45^{\\circ},-45^{\\circ},135^{\\circ},-135^{\\circ}\n$$  \n\n![](images/761827aa0a5145d10b30c3017646321cf5b6594dfc258407dcc6f73dc737408a.jpg)  \n\nFigure 6–68 (a) Control system; (b) root-locus plot.  \n\nThe intersection of the asymptotes and the real axis is found from  \n\n$$\ns=-\\,\\frac{0\\,+\\,1\\,+\\,2\\,+\\,2}{4}=-1.25\n$$  \n\nThe breakaway and break-in points are found from $d K/d s=0.$ .Noting that  \n\n$$\nK=-s(s\\,+\\,1)\\bigl(s^{2}\\,+\\,4s\\,+\\,13\\bigr)=-\\bigl(s^{4}\\,+\\,5s^{3}\\,+\\,17s^{2}\\,+\\,13s\\bigr)\n$$  \n\nwe have  \n\n$$\n\\frac{d K}{d s}=-\\big(4s^{3}\\,+\\,15s^{2}\\,+\\,34s\\,+\\,13\\big)\\,=\\,0\n$$  \n\nfrom which we get  \n\n$$\ns=-0.467,\\;\\;\\;\\;\\;\\;s=-1.642\\,+\\,j2.067,\\;\\;\\;\\;\\;\\;s=-1.642\\,-\\,j2.067\n$$  \n\nPoint $s=-0.467$ is on a root locus.Therefore, it is an actual breakaway point.The gain values $K$ corresponding to points $s\\,=-1.642\\,\\pm\\,j2.067$ are complex quantities. Since the gain values are not real positive, these points are neither breakaway nor break-in points.  \n\nThe angle of departure from the complex pole in the upper-half $s$ plane is  \n\n$$\n\\theta\\,=\\,180^{\\circ}\\,-\\,123.69^{\\circ}\\,-\\,108.44^{\\circ}\\,-\\,90^{\\circ}\n$$  \n\nor  \n\n$$\n\\theta=-142.13^{\\circ}\n$$  \n\nNext we shall find the points where root loci may cross the $j\\omega$ axis. Since the characteristic equation is  \n\n$$\ns^{4}\\,+\\,5s^{3}\\,+\\,17s^{2}\\,+\\,13s\\,+\\,K\\,=\\,0\n$$  \n\nby substituting $s=j\\omega$ into it we obtain  \n\n$$\n(j\\omega)^{4}\\,+\\,5(j\\omega)^{3}\\,+\\,17(j\\omega)^{2}\\,+\\,13(j\\omega)\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big(K\\,+\\,\\omega^{4}\\,-\\,17\\omega^{2}\\big)\\,+\\,j\\omega\\big(13\\,-\\,5\\omega^{2}\\big)\\,=\\,0\n$$  \n\nfrom which we obtain  \n\n$$\n\\omega\\,=\\,\\pm\\,1.6125,\\qquad K\\,=\\,37.44\\qquad\\mathrm{or}\\qquad\\omega\\,=\\,0,\\qquad K\\,=\\,0\n$$  \n\nThe root-locus branches that extend to the right-half $s$ plane cross the imaginary axis at $\\omega=\\pm1.6125$ .Also, the root-locus branch on the real axis touches the imaginary axis at $\\omega=0$ .Figure 6–68(b) shows a sketch of the root loci for the system. Notice that each root-locus branch that extends to the right-half $s$ plane crosses its own asymptote.  \n\nA–6–7. Sketch the root loci of the control system shown in Figure 6–69(a). Determine the range of gain $K$ for stability.  \n\nSolution. Open-loop poles are located at $s=1,s=-2\\,+\\,j\\sqrt{3}$ and $s\\,=\\,-2\\,-\\,j\\sqrt{3}\\,.$ A root locus exists on the real axis between points $s\\,=\\,1$ and $s=-\\infty.$ . The asymptotes of the root-locus branches are found as follows:  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{3}}=60^{\\circ},-60^{\\circ},180^{\\circ}\n$$  \n\nThe intersection of the asymptotes and the real axis is obtained as  \n\n$$\ns\\,=-\\,{\\frac{-1\\,+\\,2\\,+\\,2}{3}}=-1\n$$  \n\nThe breakaway and break-in points can be located from ${d K}/{d s}\\,=\\,0$ .Since  \n\n$$\nK=-(s\\,-\\,1)\\big(s^{2}\\,+\\,4s\\,+\\,7\\big)\\,=-\\big(s^{3}\\,+\\,3s^{2}\\,+\\,3s\\,-\\,7\\big)\n$$  \n\nwe have  \n\n$$\n\\frac{d K}{d s}=-\\big(3s^{2}\\,+\\,6s\\,+\\,3\\big)\\,=\\,0\n$$  \n\nwhich yields  \n\n$$\n(s\\,+\\,1)^{2}\\,=\\,0\n$$  \n\n![](images/55cd20219b01c1d6f3f7216c719a4ce3e0d5592c0087df88905dd9ae75b39262.jpg)  \n\nFigure 6–69 (a) Control system; (b) root-locus plot.  \n\nThus the equation $d K/d s=0$ has a double root at $s=-1$ .(This means that the characteristic equation has a triple root at $s=-1.$ ) The breakaway point is located at $s=-1,$ .Three root-locus branches meet at this breakaway point.The angles of departure of the branches at the breakaway point are $\\pm180^{\\circ}/3$ —that is, $60^{\\circ}$ and $-60^{\\circ}$ .  \n\nWe shall next determine the points where root-locus branches may cross the imaginary axis. Noting that the characteristic equation is  \n\n$$\n(s\\,-\\,1)\\bigl(s^{2}\\,+\\,4s\\,+\\,7\\bigr)\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\ns^{3}\\,+\\,3s^{2}\\,+\\,3s\\,-\\,7\\,+\\,K\\,=\\,0\n$$  \n\nwe substitute $s=j\\omega$ into it and obtain  \n\n$$\n(j\\omega)^{3}\\,+\\,3(j\\omega)^{2}\\,+\\,3(j\\omega)\\,-\\,7\\,+\\,K\\,=\\,0\n$$  \n\nBy rewriting this last equation, we have  \n\n$$\n\\big({\\cal K}\\,-\\,7\\,-\\,3\\omega^{2}\\big)\\,+\\,j\\omega\\big(3\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nThis equation is satisfied when  \n\n$$\n\\omega=\\pm\\sqrt{3}\\,,\\qquad K=7\\,+\\,3\\omega^{2}=16\\qquad\\mathrm{or}\\qquad\\omega=0,\\qquad K=7\n$$  \n\nThe root-locus branches cross the imaginary axis at $\\omega=\\pm\\sqrt{3}$ (where $K=16$ ) and $\\omega=0$ (where $K=7$ ). Since the value of gain $K$ at the origin is 7, the range of gain value $K$ for stability is  \n\n$$\n7<K<16\n$$  \n\nFigure 6–69(b) shows a sketch of the root loci for the system. Notice that all branches consist of parts of straight lines.  \n\nThe fact that the root-locus branches consist of straight lines can be verified as follows: Since the angle condition is  \n\n$$\n\\left/{\\frac{K}{(s\\,-\\,1)(s\\,+\\,2\\,+\\,j{\\sqrt{3}})(s\\,+\\,2\\,-\\,j{\\sqrt{3}})}}=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nwe have  \n\n$$\n-/s\\,-\\,1\\,-\\,/s\\,+\\,2\\,+\\,j\\sqrt{3}\\,-\\,/s\\,+\\,2\\,-\\,j\\sqrt{3}=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nBy substituting $s\\,=\\,\\sigma\\,+\\,j\\omega$ into this last equation,  \n\n$$\n/\\sigma\\,-\\,1\\,+\\,j\\omega\\,+\\,/\\sigma\\,+\\,2\\,+\\,j\\omega\\,+\\,j\\sqrt{3}\\,+\\,/\\sigma\\,+\\,2\\,+\\,j\\omega\\,-\\,j\\sqrt{3}\\,=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nor  \n\n$$\n/\\sigma\\,+\\,2\\,+\\,j(\\omega\\,+\\,\\sqrt{3})\\,+\\,\\,\\left/\\sigma\\,+\\,2\\,+\\,j(\\omega\\,-\\,\\sqrt{3}\\right)\\,=-\\,/\\sigma\\,-\\,1\\,+\\,j\\omega\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\n$$  \n\nwhich can be rewritten as  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega\\,+\\,{\\sqrt{3}}}{\\sigma\\,+\\,2}}\\right)\\,+\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,-\\,{\\sqrt{3}}}{\\sigma\\,+\\,2}}\\right)\\,=\\,-\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma\\,-\\,1}}\\right)\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\n$$  \n\nTaking tangents of both sides of this last equation, we obtain  \n\n$$\n{\\frac{{\\frac{\\omega+{\\sqrt{3}}}{\\sigma+2}}+{\\frac{\\omega-{\\sqrt{3}}}{\\sigma+2}}}{1-\\left({\\frac{\\omega+{\\sqrt{3}}}{\\sigma+2}}\\right)\\left({\\frac{\\omega-{\\sqrt{3}}}{\\sigma+2}}\\right)}}=-{\\frac{\\omega}{\\sigma-1}}\n$$  \n\nor  \n\n$$\n\\frac{2\\omega(\\sigma\\,+\\,2)}{\\sigma^{2}\\,+\\,4\\sigma\\,+\\,4\\,-\\,\\omega^{2}\\,+\\,3}=-\\,\\frac{\\omega}{\\sigma\\,-\\,1}\n$$  \n\nwhich can be simplified to  \n\n$$\n2\\omega(\\sigma\\,+\\,2)(\\sigma\\,-\\,1)\\,=-\\omega\\bigl(\\sigma^{2}\\,+\\,4\\sigma\\,+\\,7\\,-\\,\\omega^{2}\\bigr)\n$$  \n\nor  \n\n$$\n\\omega\\big(3\\sigma^{2}\\,+\\,6\\sigma\\,+\\,3\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nFurther simplification of this last equation yields  \n\n$$\n\\omega\\bigg(\\sigma\\,+\\,1\\,+\\,\\frac{1}{\\sqrt{3}}\\,\\omega\\bigg)\\bigg(\\sigma\\,+\\,1\\,-\\,\\frac{1}{\\sqrt{3}}\\,\\omega\\bigg)\\,=\\,0\n$$  \n\nwhich defines three lines:  \n\n$$\n\\omega=0,\\qquad\\sigma+1+\\frac{1}{\\sqrt{3}}\\,\\omega=0,\\qquad\\sigma+1-\\frac{1}{\\sqrt{3}}\\,\\omega=0\n$$  \n\nThus the root-locus branches consist of three lines. Note that the root loci for $K>0$ consist of portions of the straight lines as shown in Figure 6–69(b). (Note that each straight line starts from an open-loop pole and extends to infinity in the direction of $180^{\\circ},60^{\\circ}$ , or $-60^{\\circ}$ measured from the real axis.) The remaining portion of each straight line corresponds to $K<0$ .  \n\nA–6–8. Consider a unity-feedback control system with the following feedforward transfer function:  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nUsing MATLAB, plot the root loci and their asymptotes.  \n\nSolution. We shall plot the root loci and asymptotes on one diagram. Since the feedforward transfer function is given by  \n\n$$\n\\begin{array}{c}{{G(s)=\\displaystyle\\frac{K}{s(s+1)(s+2)}}}\\\\ {{=\\displaystyle\\frac{K}{s^{3}+3s^{2}+2s}}}\\end{array}\n$$  \n\nthe equation for the asymptotes may be obtained as follows: Noting that  \n\n$$\n\\operatorname*{lim}_{s\\rightarrow\\infty}{\\frac{K}{s^{3}\\,+\\,3s^{2}\\,+\\,2s}}\\div\\operatorname*{lim}_{s\\rightarrow\\infty}{\\frac{K}{s^{3}\\,+\\,3s^{2}\\,+\\,3s\\,+\\,1}}={\\frac{K}{(s\\,+\\,1)^{3}}}\n$$  \n\nthe equation for the asymptotes may be given by  \n\n$$\nG_{a}(s)\\,=\\frac{K}{(s\\,+\\,1)^{3}}\n$$  \n\nHence, for the system we have  \n\nand for the asymptotes,  \n\nIn using the following root-locus and plot commands  \n\n$$\n\\begin{array}{l}{\\mathsf{r}=\\mathsf{r}|\\mathsf{o c u s}(\\mathsf{n u m},\\mathsf{d e n})}\\\\ {\\mathsf{a}=\\mathsf{r}|\\mathsf{o c u s}(\\mathsf{n u m a},\\mathsf{d e n a})}\\\\ {\\mathsf{p l o t}([\\mathsf{r~\\ a l})}\\end{array}\n$$  \n\nthe number of rows of rand that of a must be the same. To ensure this, we include the gain constant Kin the commands. For example,  \n\nMATLAB Program 6–15 will generate a plot of root loci and their asymptotes as shown in Figure 6–70.  \n\n![](images/7ff0a27fb33218e01235f223bfb496d561adfd1f983819c7a35f3747f83e071b.jpg)  \n\n![](images/61930b9f75b0724a1ee2462f481b602e6640f8e2ef91e125c05e294babe6371b.jpg)  \nFigure 6–70 Root-locus plot.  \n\nDrawing two or more plots in one diagram can also be accomplished by using the hold command. MATLAB Program 6–16 uses the hold command. The resulting root-locus plot is shown in Figure 6–71.  \n\n![](images/3e1fe88215538ddb3094695e003b697412731072b04cc6b942f20ca686645956.jpg)  \n\n![](images/9462dda80d26c86bb128e1e30e6b31257865618c54b28b8273e161b3cff257aa.jpg)  \nFigure 6–71 Root-locus plot.  \n\nA–6–9. Plot the root loci and asymptotes for a unity-feedback system with the following feedforward transfer function:  \n\n$$\nG(s)\\,={\\frac{K}{(s^{2}\\,+\\,2s\\,+\\,2)(s^{2}\\,+\\,2s\\,+\\,5)}}\n$$  \n\nDetermine the exact points where the root loci cross the $j\\omega$ axis  \n\nSolution. The feedforward transfer function $G(s)$ can be written as  \n\n$$\nG(s)\\,=\\,\\frac{K}{s^{4}\\,+\\,4s^{3}\\,+\\,11s^{2}\\,+\\,14s\\,+\\,10}\n$$  \n\nNote that as $s$ approaches infinity, $\\operatorname*{lim}_{s\\to\\infty}G(s)$ can be written as  \n\n$$\n{\\begin{array}{c}{\\displaystyle\\operatorname*{lim}_{s\\to\\infty}G(s)\\,=\\,\\operatorname*{lim}_{s\\to\\infty}\\,{\\frac{K}{s^{4}\\,+\\,4s^{3}\\,+\\,11s^{2}\\,+\\,14s\\,+\\,10}}}\\\\ {\\displaystyle\\qquad\\qquad\\div\\operatorname*{lim}_{s\\to\\infty}\\,{\\frac{K}{s^{4}\\,+\\,4s^{3}\\,+\\,6s^{2}\\,+\\,4s\\,+\\,1}}}\\\\ {\\displaystyle\\qquad=\\operatorname*{lim}_{s\\to\\infty}\\,{\\frac{K}{(s\\,+\\,1)^{4}}}}\\end{array}}\n$$  \n\nwhere we used the following formula:  \n\n$$\n(s\\,+\\,a)^{4}=s^{4}\\,+\\,4a s^{3}\\,+\\,6a^{2}s^{2}\\,+\\,4a^{3}s\\,+\\,a^{4}\n$$  \n\nThe expression  \n\n$$\n\\operatorname*{lim}_{s\\rightarrow\\infty}G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow\\infty}\\,{\\frac{K}{(s\\,+\\,1)^{4}}}\n$$  \n\ngives the equation for the asymptotes.  \n\nThe MATLAB program to plot the root loci of $G(s)$ and the asymptotes is given in MATLAB Program 6–17. Note that the numerator and denominator for $G(s)$ are  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=[1]}\\\\ {\\mathsf{d e n}=[1\\ \\ 4\\ \\ 11\\ \\ 14\\ \\ 10]}\\end{array}\n$$  \n\nFor the numerator and denominator of the asymptotes $\\operatorname*{lim}_{s\\to\\infty}G(s)$ we used  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m a}=[1]}\\\\ {\\mathsf{d e n a}=[1\\ \\ 4\\ \\ 6\\ \\ 4\\ \\ 1]}\\end{array}\n$$  \n\nFigure 6–72 shows the plot of the root loci and asymptotes. Since the characteristic equation for the system is  \n\n$$\n(s^{2}\\,+\\,2s\\,+\\,2)(s^{2}\\,+\\,2s\\,+\\,5)\\,+\\,K\\,=\\,0\n$$  \n\n![](images/f20b46d6bb273a796a9ff45d8ab6883b3e9a5c5c3033016487417a323fdfd926.jpg)  \n\n![](images/27f2109c0dbf6692238c6b8aaba4517e7a093fd4daa6ac0fc3a6e90071f2ad1f.jpg)  \nFigure 6–72 Plot of root loci and asymptotes.  \n\nthe points where the root loci cross the imaginary axis can be found by substituting $s=j\\omega$ with the characteristic equation as follows:  \n\n$$\n\\begin{array}{l}{{[(j\\omega)^{2}+2j\\omega+2][(j\\omega)^{2}+2j\\omega+5]+K}}\\\\ {{{}}}\\\\ {{=(\\omega^{4}-11\\omega^{2}+10+K)+j(-4\\omega^{3}+14\\omega)=0}}\\end{array}\n$$  \n\nand equating the imaginary part to zero.The result is  \n\n$$\n\\omega=\\pm1.8708\n$$  \n\nThus the exact points where the root loci cross the $j\\omega$ axis are $\\omega=\\pm1.8708$ .By equating the real part to zero, we get the gain value $K$ at the crossing points to be 16.25.  \n\nA–6–10. Consider a unity-feedback control system with the feed-forward transfer function $G(s)$ given by  \n\n$$\nG(s)=\\frac{K(s\\nonumber+1)}{(s^{2}+2s+2)(s^{2}+2s+5)}\n$$  \n\nPlot a root-locus diagram with MATLAB.  \n\nSolution. The feedforward transfer function $G(s)$ can be written as  \n\n$$\nG(s)\\,=\\frac{K(s\\,+\\,1)}{s^{4}\\,+\\,4s^{3}\\,+\\,11s^{2}\\,+\\,14s\\,+\\,10}\n$$  \n\nA possible MATLAB program to plot a root-locus diagram is shown in MATLAB Program 6–18.   \nThe resulting root-locus plot is shown in Figure 6–73.  \n\n![](images/ed4b1907f3ac242fbf1326c22ab14d64c2f949651efc5eb0d79b2a524fc1f937.jpg)  \n\n![](images/4eb2bb2981e240e0f8905ad32501bb901239eeb2d93586de7c09494bcadf118e.jpg)  \nReal Axis  \n\nA–6–11. Obtain the transfer function of the mechanical system shown in Figure 6–74. Assume that the displacement $x_{i}$ is the input and displacement $x_{o}$ is the output of the system.  \n\nSolution. From the diagram we obtain the following equations of motion:  \n\n$$\n\\begin{array}{l}{{b_{2}\\big(\\dot{x}_{i}\\,-\\,\\dot{x}_{o}\\big)\\,=\\,b_{1}\\big(\\dot{x}_{o}\\,-\\,\\dot{y}\\big)\\,}}\\\\ {{\\,}}\\\\ {{b_{1}\\big(\\dot{x}_{o}\\,-\\,\\dot{y}\\big)\\,=\\,k y}}\\end{array}\n$$  \n\n![](images/7cd6e5213e2a51fad80e5a2e35e4c576f07345c8a6ed35ba94d2571f4edba113.jpg)  \nFigure 6–73 Plot of root loci.   \nFigure 6–74 Mechanical system.  \n\nTaking the Laplace transforms of these two equations, assuming zero initial conditions, and then eliminating $Y(s)$ ,we obtain  \n\n$$\n{\\frac{X_{o}(s)}{X_{i}(s)}}={\\frac{b_{2}}{b_{1}+\\,b_{2}}}{\\frac{{\\frac{b_{1}}{k}}s\\,+\\,1}{{\\frac{b_{2}}{b_{1}\\,+\\,b_{2}}}{\\frac{b_{1}}{k}}s\\,+\\,1}}\n$$  \n\nThis is the transfer function between $X_{o}(s)$ and $X_{i}(s)$ By defining  \n\n$$\n{\\frac{b_{1}}{k}}=T,\\qquad{\\frac{b_{2}}{b_{1}+b_{2}}}=\\alpha<1\n$$  \n\nwe obtain  \n\n$$\n{\\frac{X_{o}(s)}{X_{i}(s)}}=\\alpha\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}={\\frac{s\\,+\\,{\\frac{1}{T}}}{s\\,+\\,{\\frac{1}{\\alpha T}}}}\n$$  \n\nThis mechanical system is a mechanical lead network.  \n\n# A–6–12.  \n\nObtain the transfer function of the mechanical system shown in Figure 6–75.Assume that the displacement $x_{i}$ is the input and displacement $x_{o}$ is the output.  \n\nSolution. The equations of motion for this system are  \n\n$$\n\\begin{array}{c}{b_{2}(\\dot{x}_{i}-\\,\\dot{x}_{o})\\,+\\,k_{2}(x_{i}\\,-\\,x_{o})\\,=\\,b_{1}(\\dot{x}_{o}\\,-\\,\\dot{y})}\\\\ {b_{1}(\\dot{x}_{o}\\,-\\,\\dot{y})\\,=\\,k_{1}y}\\end{array}\n$$  \n\n![](images/673d78aeeb068bcd489936fd0b47c327e2d4f4bdfc280a40cfbd24896088a7fa.jpg)  \nFigure 6–75 Mechanical system.  \n\nBy taking the Laplace transforms of these two equations, assuming zero initial conditions, we obtain  \n\n$$\n\\begin{array}{r}{b_{2}[s X_{i}(s)\\,-\\,s X_{o}(s)]\\,+\\,k_{2}\\big[X_{i}(s)\\,-\\,X_{o}(s)\\big]\\,=\\,b_{1}\\big[s X_{o}(s)\\,-\\,s Y(s)\\big]\\,}\\\\ {b_{1}\\big[s X_{o}(s)\\,-\\,s Y(s)\\big]\\,=\\,k_{1}Y(s)\\qquad\\qquad\\qquad}\\end{array}\n$$  \n\nIf we eliminate $Y(s)$ from the last two equations, the transfer function $X_{o}(s)/X_{i}(s)$ can be obtained as  \n\n$$\n\\frac{X_{o}(s)}{X_{i}(s)}=\\frac{\\bigg(\\displaystyle\\frac{b_{1}}{k_{1}}\\,s\\,+\\,1\\bigg)\\bigg(\\displaystyle\\frac{b_{2}}{k_{2}}\\,s\\,+\\,1\\bigg)}{\\bigg(\\displaystyle\\frac{b_{1}}{k_{1}}\\,s\\,+\\,1\\bigg)\\bigg(\\displaystyle\\frac{b_{2}}{k_{2}}\\,s\\,+\\,1\\bigg)\\,+\\,\\displaystyle\\frac{b_{1}}{k_{2}}\\,s}\n$$  \n\nDefine  \n\n$$\nT_{1}=\\frac{b_{1}}{k_{1}},~~~~T_{2}=~\\frac{b_{2}}{k_{2}},\n$$  \n\nIf $k_{1},k_{2},b_{1}$ ,and $b_{2}$ are chosen such that there exists a $\\beta$ that satisfies the following equation:  \n\n$$\n\\frac{b_{1}}{k_{1}}+\\frac{b_{2}}{k_{2}}+\\frac{b_{1}}{k_{2}}=\\frac{T_{1}}{\\beta}+\\beta T_{2}\\qquad(\\beta>1)\n$$  \n\nthen $X_{o}(s)/X_{i}(s)$ can be obtained as  \n\n$$\n\\frac{X_{o}(s)}{X_{i}(s)}=\\frac{\\bigl(T_{1}s\\,+\\,1\\bigr)\\bigl(T_{2}s\\,+\\,1\\bigr)}{\\biggl(\\frac{T_{1}}{\\beta}\\,s\\,+\\,1\\biggr)\\bigl(\\beta T_{2}s\\,+\\,1\\bigr)}=\\frac{\\biggl(s\\,+\\,\\frac{1}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{T_{2}}\\biggr)}{\\biggl(s\\,+\\,\\frac{\\beta}{T_{1}}\\biggr)\\biggl(s\\,+\\,\\frac{1}{\\beta T_{2}}\\biggr)}\n$$  \n\n[Note that depending on the choice of $k_{1},k_{2},$ $b_{1}$ , and $b_{2}$ ,there does not exist a $\\beta$ that satisfies Equation (6–30).]  \n\nIf such a $\\beta$ exists and if for a given $s_{1}$ (where $s=s_{1}$ is one of the dominant closed-loop poles of the control system to which we wish to use this mechanical device) the following conditions are satisfied:  \n\n$$\n\\left|\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}\\right|\\overset{\\div}{\\div}1,\\qquad\\qquad-5^{\\circ}<\\left\\lceil\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}<0^{\\circ}\\right\\rceil\n$$  \n\nthen the mechanical system shown in Figure 6–75 acts as a lag–lead compensator.  \n\n![](images/63b3921a247ff85b622edd487e8f54d4175b92c3f2b8c318cf37507171eceadc.jpg)  \nFigure 6–76 Space-vehicle control system.   \nSensor  \n\nA–6–13. Consider the model for a space-vehicle control system shown in Figure 6–76. Design a lead compensator $G_{c}(s)$ such that the damping ratio $\\zeta$ and the undamped natural frequency $\\omega_{n}$ of the dominant closed-loop poles are 0.5 and 2 rad 'sec, respectively.  \n\n# Solution.  \n\nFirst Attempt: Assume the lead compensator $G_{c}(s)$ to be  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\Bigg(\\frac{s\\,+\\frac{1}{T}}{s+\\frac{1}{\\alpha T}}\\Bigg)\\qquad(0<\\alpha<1)\n$$  \n\nFrom the given specifications, $\\zeta=0.5$ and $\\omega_{n}=2$ rad 'sec, the dominant closed-loop poles must be located at  \n\n$$\ns=-1\\,\\pm\\,j{\\sqrt{3}}\n$$  \n\nWe first calculate the angle deficiency at this closed-loop pole.  \n\n$$\n{\\begin{array}{r l}&{{\\mathrm{Angle~deficiency}}=-120^{\\circ}\\mathrm{~-~}120^{\\circ}\\mathrm{~-~}10.8934^{\\circ}\\mathrm{~+~}180^{\\circ}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\mathrm{-70.8934^{\\circ}}}\\end{array}}\n$$  \n\nThis angle deficiency must be compensated by the lead compensator. There are many ways to determine the locations of the pole and zero of the lead network. Let us choose the zero of the compensator at $s=-1,$ .Then, referring to Figure 6–77, we have the following equation:  \n\n![](images/e083a82da3d5a598ab94eac9fa328644901059a75b2787c0d253c10083cee032.jpg)  \nFigure 6–77 Determination of the pole of the lead network.  \n\n$$\n{\\frac{1.73205}{x\\,-\\,1}}=\\tan(90^{\\circ}\\,-\\,70.8934^{\\circ})\\,=\\,0.34641\n$$  \n\nor  \n\n$$\nx\\,=\\,1\\,+\\,{\\frac{1.73205}{0.34641}}\\,=\\,6\n$$  \n\nHence,  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\,\\frac{s\\,+\\,1}{s\\,+\\,6}\n$$  \n\nThe value of $K_{c}$ can be determined from the magnitude condition  \n\n$$\nK_{c}\\left|\\frac{s\\,+\\,1}{s\\,+\\,6}\\frac{1}{s^{2}}\\frac{1}{0.1s\\,+\\,1}\\right|_{s=-1+j\\sqrt{3}}=1\n$$  \n\nas follows:  \n\n$$\nK_{c}=\\left.\\left|\\frac{(s\\,+\\,6)s^{2}(0.1s\\,+\\,1)}{s\\,+\\,1}\\right|_{s=-1+j\\sqrt{3}}=11.2000\n$$  \n\nThus  \n\n$$\nG_{c}(s)\\,=\\,11.2\\,\\frac{s\\,+\\,1}{s\\,+\\,6}\n$$  \n\nSince the open-loop transfer function becomes  \n\n$$\n\\begin{array}{r l}&{G_{c}(s)G(s)H(s)=11.2\\,\\displaystyle\\frac{s\\,+\\,1}{(s\\,+\\,6)s^{2}(0.1s\\,+\\,1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{11.2(s\\,+\\,1)}{0.1s^{4}\\,+\\,1.6s^{3}\\,+\\,6s^{2}}}\\end{array}\n$$  \n\na root-locus plot of the compensated system can be obtained easily with MATLAB by entering num and den and using rlocus command.The result is shown in Figure 6–78.  \n\n![](images/e96c897dc33c0dfd2029e7a36d56a58e5ce8070f1922c94cbe08e7ccfcae0654.jpg)  \nRoot-Locus Plot of Compensated System   \nFigure 6–78 Root-locus plot of the compensated system.  \n\n![](images/aed048b60bc79b458eccd7e6210e0e014353dafd63e9d5bb377b07e4426b6992.jpg)  \nFigure 6–79 Unit-step response of the compensated system.  \n\nThe closed-loop transfer function for the compensated system becomes  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{11.2(s\\,+\\,1)(0.1s\\,+\\,1)}{(s\\,+\\,6)s^{2}(0.1s\\,+\\,1)\\,+\\,11.2(s\\,+\\,1)}\n$$  \n\nFigure 6–79 shows the unit-step response curve. Even though the damping ratio of the dominant closed-loop poles is 0.5, the amount of overshoot is very much higher than expected.A closer look at the root-locus plot reveals that the presence of the zero at $s=-1$ is increasing the amount of the maximum overshoot. [In general, if a closed-loop zero or zeros (compensator zero or zeros) lie to the right of the dominant pair of the complex poles, then the dominant poles are no longer dominant.] If large maximum overshoot cannot be tolerated, the compensator zero(s) should be shifted sufficiently to the left.  \n\nIn the current design, it is desirable to modify the compensator and make the maximum overshoot smaller. This can be done by modifying the lead compensator, as presented in the following second attempt.  \n\nSecond Attempt: To modify the shape of the root loci, we may use two lead networks, each contributing half the necessary lead angle, which is $70.8934^{\\circ}/2=35.4467^{\\circ}$ .Let us choose the location of the zeros at $s=-3$ .(This is an arbitrary choice. Other choices such as $s=-2.5$ and $s=-4$ may be made.)  \n\nOnce we choose two zeros at $s=-3$ ,the necessary location of the poles can be determined as shown in Figure 6–80, or  \n\n$$\n\\begin{array}{r}{\\frac{1.73205}{y\\mathrm{~-~}1}=\\tan\\left(40.89334^{\\circ}\\mathrm{~-~}35.4467^{\\circ}\\right)}\\\\ {\\mathrm{~}=\\tan5.4466^{\\circ}=0.09535}\\end{array}\n$$  \n\nwhich yields  \n\n$$\ny\\,=\\,1\\,+\\,{\\frac{1.73205}{0.09535}}\\,=\\,19.1652\n$$  \n\n![](images/daf5f123d1fac3659231a07d6d74701578894d6d5e9fe0c53ba5c47ff45b8e78.jpg)  \nFigure 6–80 Determination of the pole of the lead network.  \n\nHence, the lead compensator will have the following transfer function:  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\bigg({\\frac{s\\,+\\,3}{s\\,+\\,19.1652}}\\bigg)^{2}\\,\n$$  \n\nThe value of $K_{c}$ can be determined from the magnitude condition as follows:  \n\n$$\n\\left|K_{c}\\!\\left(\\frac{s\\,+\\,3}{s\\,+\\,19.1652}\\right)^{2}\\frac1{s^{2}}\\frac1{0.1s\\,+\\,1}\\right|_{s=-1+j\\sqrt{3}}=1\n$$  \n\nor  \n\n$$\nK_{c}=174.3864\n$$  \n\nThen the lead compensator just designed is  \n\n$$\nG_{c}(s)\\,=\\,174.3864\\bigg({\\frac{s\\,+\\,3}{s\\,+\\,19.1652}}\\bigg)^{2}\n$$  \n\nThen the open-loop transfer function becomes  \n\n$$\nG_{c}(s)G(s)H(s)=174.3864\\bigg(\\frac{s+3}{s+19.1652}\\bigg)^{2}\\frac{1}{s^{2}}\\frac{1}{0.1s+1}\n$$  \n\nA root-locus plot for the compensated system is shown in Figure 6–81(a). Notice that there is no closed-loop zero near the origin.An expanded view of the root-locus plot near the origin is shown in Figure 6–81(b).  \n\nThe closed-loop transfer function becomes  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{174.3864(s\\,+\\,3)^{2}(0.1s\\,+\\,1)}{(s\\,+\\,19.1652)^{2}s^{2}(0.1s\\,+\\,1)\\,+\\,174.3864(s\\,+\\,3)^{2}}}\n$$  \n\nThe closed-loop poles are found as follows:  \n\n$$\n\\begin{array}{l}{{s=-1\\,\\pm\\,j1.73205}}\\\\ {{\\,}}\\\\ {{s=-9.1847\\,\\pm\\,j7.4814}}\\\\ {{\\,}}\\\\ {{s=-27.9606}}\\end{array}\n$$  \n\n![](images/0a65df8511644eafc23a9e6c2d90269b51975ab25a1b6daf7e8666cd9700518a.jpg)  \nFigure 6–81 (a) Root-locus plot of compensated system; (b) root-locus plot near the origin.  \n\nFigures 6–82(a) and (b) show the unit-step response and unit-ramp response of the compensated system.The unit-step response curve is reasonable and the unit-ramp response looks acceptable. Notice that in the unit-ramp response the output leads the input by a small amount.This is because the system has a feedback transfer function $1/(0.1s\\,+\\,1)$ .If the feedback signal versus $t$ is plotted, together with the unit-ramp input, the former will not lead the input ramp at steady state. See Figure 6–82(c).  \n\n![](images/47be089da569a535a5256abbafe1ce2c6611157d33f7451ef808630dbe359f75.jpg)  \n\n![](images/7e48d75994d18d56700e7993f1ee4294cc3ac0932b2ac1171c77aca143b4309e.jpg)  \nFigure 6–82 (a) unit-step response of the compensated system; (b) unit-ramp response of the compensated system; (c) a plot of feedback signal versus $t$ in the unit-ramp response.   \nA–6–14. Consider a system with an unstable plant as shown in Figure 6–83(a). Using the root-locus approach, design a proportional-plus-derivative controller that is, determine the values of BA $K_{p}$ and $T_{d}$ such that the damping ratio $\\zeta$ of the closed-loop system is 0.7 and the undamped natural frequency $\\omega_{n}$ is 0.5 rad 'sec.  \n\nSolution. Note that the open-loop transfer function involves two poles at $s=1.085$ and $s=-1.085$ and one zero at $s=-1/T_{d}$ ,which is unknown at this point.  \n\nSince the desired closed-loop poles must have $\\omega_{n}\\,=\\,0.5$ rad 'sec and $\\zeta=0.7$ ,they must be located at  \n\n$$\ns\\,=\\,0.5\\,/180^{\\circ}\\,\\pm\\,45.573^{\\circ}\n$$  \n\n![](images/911b6169c2e5cedf90547da1313f7f858bb5cd029bdbb2b19758b86d3fdfb2ba.jpg)  \nFigure 6–83 (a) PD control of an unstable plant; (b) root-locus diagram for the system.  \n\n($\\zeta=0.7$ corresponds to a line having an angle of $45.573^{\\circ}$ with the negative real axis.) Hence, the desired closed-loop poles are at  \n\n$$\ns\\,=\\,-0.35\\,\\pm\\,j0.357\n$$  \n\nThe open-loop poles and the desired closed-loop pole in the upper half-plane are located in the diagram shown in Figure 6–83(b).The angle deficiency at point $s\\,=\\,-0.35\\,+\\,j0.357$ is  \n\n$$\n-166.026^{\\circ}\\mathrm{~-~}25.913^{\\circ}\\mathrm{~+~}180^{\\circ}\\mathrm{~=~}-11.939^{\\circ}\n$$  \n\nThis means that the zero at $s=-1/T_{d}$ must contribute $11.939^{\\circ}$ , which, in turn, determines the location of the zero as follows:  \n\n$$\ns\\,=-\\,\\frac{1}{T_{d}}=-2.039\n$$  \n\nHence, we have  \n\n$$\nK_{p}(1\\,+\\,T_{d}s)\\,=\\,K_{p}T_{d}\\Bigl(\\frac{1}{T_{d}}\\,+\\,s\\Bigr)\\,=\\,K_{p}T_{d}(s\\,+\\,2.039)\n$$  \n\nThe value of $T_{d}$ is  \n\n$$\nT_{d}=\\frac{1}{2.039}=0.4904\n$$  \n\nThe value of gain $K_{p}$ can be determined from the magnitude condition as follows:  \n\n$$\n\\left|K_{p}T_{d}\\frac{s\\,+\\,2.039}{10000(s^{2}-1.1772)}\\right|_{s=-0.35+j0.357}=1\n$$  \n\nor  \n\n$$\nK_{p}T_{d}\\,=\\,6999.5\\\n$$  \n\nHence,  \n\n$$\nK_{p}=\\frac{6999.5}{0.4904}=14{,}273\n$$  \n\nBy substituting the numerical values of $T_{d}$ and $K_{p}$ into Equation (6–31), we obtain  \n\n$$\nK_{p}(1\\,+\\,T_{d}s)=14{,}273(1\\,+\\,0.4904s)\\,=\\,6999.5(s\\,+\\,2.039)\n$$  \n\nwhich gives the transfer function of the desired proportional-plus-derivative controller.  \n\nA–6–15. Consider the control system shown in Figure 6–84. Design a lag compensator 1 $G_{c}(s)$ such that the static velocity error constant $K_{v}$ is $50\\;\\mathrm{sec}^{-1}$ without appreciably changing the location of the original closed-loop poles, which are at $s=-2\\,\\pm\\,j\\sqrt{6}$  \n\nSolution. Assume that the transfer function of the lag compensator is  \n\n$$\nG_{c}(s)=\\hat{K}_{c}{\\frac{s+{\\frac{1}{T}}}{s+{\\frac{1}{\\beta T}}}}\\qquad(\\beta>1)\n$$  \n\nFigure 6–84 Control system.  \n\n![](images/186bfedf667d5bcf04978a318dd37766546fed2fdb4d1f9456dd191a5607d01c.jpg)  \n\nSince $K_{v}$ is specified as $50\\;\\mathrm{sec}^{-1}$ , we have  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)\\,\\frac{10}{s(s\\,+\\,4)}=\\hat{K}_{c}\\beta2.5\\,=\\,50\\\n$$  \n\nThus  \n\n$$\n\\hat{K}_{c}\\beta=20\\\n$$  \n\nNow choose ${\\hat{K}}_{c}=1.$ Then  \n\n$$\n\\beta=20\n$$  \n\nChoose $T=10,$ .Then the lag compensator can be given by  \n\n$$\nG_{c}(s)\\,=\\,\\frac{s\\,+\\,0.1}{s\\,+\\,0.005}\n$$  \n\nThe angle contribution of the lag compensator at the closed-loop pole $s=-2\\,+\\,j{\\sqrt{6}}$ is  \n\n$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert G_{c}(s)\\right\\rvert_{s=-2+j\\sqrt{6}}\\!=\\tan^{-1}\\!\\frac{\\sqrt{6}}{-1.9}-\\tan^{-1}\\!\\frac{\\sqrt{6}}{-1.995}}&{{}}\\\\ {\\displaystyle=-1.3616^{\\circ}}&{{}}\\end{array}\n$$  \n\nwhich is small.The magnitude of $G_{c}(s)$ at $s=-2\\,+\\,j6$ is 0.981. Hence the change in the location of the dominant closed-loop poles is very small.  \n\nThe open-loop transfer function of the system becomes  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{s\\,+\\,0.1}{s\\,+\\,0.005}\\frac{10}{s(s\\,+\\,4)}\n$$  \n\nThe closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{10s\\,+\\,1}{s^{3}\\,+\\,4.005s^{2}\\,+\\,10.02s\\,+\\,1}\n$$  \n\nTo compare the transient-response characteristics before and after the compensation,the unit-step and unit-ramp responses of the compensated and uncompensated systems are shown in Figures 6–85(a) and (b), respectively.The steady-state error in the unit-ramp response is shown in Figure 6–85(c).The designed lag compensator is acceptable.  \n\n![](images/b085be0a10851ebe836d0e3c1634bd1a5bf3a4ba6850214df930a93885a53b87.jpg)  \nFigure 6–85 (a) Unit-step responses of the compensated and uncompensated systems; (b) unitramp responses of both systems; (c) unit-ramp responses showing steady-state errors.  \n\nA–6–16. Consider a unity-feedback control system whose feedforward transfer function is given by  \n\n$$\nG(s)=\\frac{10}{s(s\\,+\\,2)(s\\,+\\,8)}\n$$  \n\nDesign a compensator such that the dominant closed-loop poles are located at $s=-2\\,\\pm\\,j2{\\sqrt{3}}$ and the static velocity error constant $K_{v}$ is equal to $80\\;\\mathrm{sec}^{-1}$ .  \n\nSolution. The static velocity error constant of the uncompensated system is 1 $\\begin{array}{r}{K_{v}=\\frac{10}{16}=0.625}\\end{array}$ Since $K_{v}\\,=\\,80$ is required, we need to increase the open-loop gain by 128. (This implies that we need a lag compensator.) The root-locus plot of the uncompensated system reveals that it is not possible to bring the dominant closed-loop poles to $-2\\,\\pm\\,\\bar{j}2\\sqrt{3}$ by just a gain adjustment alone. See Figure 6–86. (This means that we also need a lead compensator.) Therefore, we shall employ a lag–lead compensator.  \n\nLet us assume the transfer function of the lag–lead compensator to be  \n\n$$\nG_{c}(s)=K_{c}\\left({\\frac{s+{\\frac{1}{T_{1}}}}{s+{\\frac{\\beta}{T_{1}}}}}\\right)\\left({\\frac{s+{\\frac{1}{T_{2}}}}{s+{\\frac{1}{\\beta T_{2}}}}}\\right)\n$$  \n\nwhere $K_{c}=128$ .This is because  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s K_{c}G(s)\\,=\\,K_{c}\\,{\\frac{10}{16}}=80\n$$  \n\nand we obtain $K_{c}=128$ .The angle deficiency at the desired closed-loop pole $s=-2\\,+\\,j2{\\sqrt{3}}$ is  \n\n$$\n\\mathrm{\\bf\\dot{y}=\\Omega-120^{\\circ}\\,-\\,90^{\\circ}\\,-\\,30^{\\circ}\\,+\\,180^{\\circ}=\\Omega-60^{\\circ}}\n$$  \n\nThe lead portion of the lag–lead compensator must contribute $60^{\\circ}.$ . To choose $T_{1}$ we may use the graphical method presented in Section 6–8.  \n\n![](images/1fb37d5e75b52aeae5c48059b1b5c1f9265c232e92f5b743a65dc24b176e86ca.jpg)  \nFigure 6–86 Root-locus plot of $G(s)\\,=\\,10/$ $\\left[s(s\\,+\\,2)(s\\,+\\,8)\\right]$ .  \n\nThe lead portion must satisfy the following conditions:  \n\n$$\n\\left|128\\left(\\frac{s_{1}+\\frac{1}{T_{1}}}{s_{1}+\\frac{\\beta}{T_{1}}}\\right)G(s_{1})\\right|_{s_{1}=-2+j2\\sqrt{3}}=1\n$$  \n\nand  \n\n$$\n\\left\\langle\\frac{s_{1}+\\displaystyle\\frac{1}{T_{1}}}{s_{1}+\\displaystyle\\frac{\\beta}{T_{1}}}\\right\\rangle_{s}\\!=\\!60^{\\circ}\n$$  \n\nThe first condition can be simplified as  \n\n$$\n{\\Bigg|}{\\frac{s_{1}+{\\frac{1}{T_{1}}}}{s_{1}+{\\frac{\\beta}{T_{1}}}}}{\\Bigg|}_{s_{1}=-2+j2{\\sqrt{3}}}={\\frac{1}{13.3333}}\n$$  \n\nBy using the same approach as used in Section 6–8, the zero $\\left(s\\,=1/T_{1}\\right)$ and pole $\\left(s=\\beta/T_{1}\\right)$ can be determined as follows:  \n\n$$\n\\frac{1}{T_{1}}=3.70,\\qquad\\frac{\\beta}{T_{1}}=53.35\n$$  \n\nSee Figure 6–87.The value of $\\beta$ is thus determined as  \n\n$$\n\\beta=14.419\n$$  \n\nFor the lag portion of the compensator, we may choose  \n\n$$\n\\frac{1}{\\beta T_{2}}=0.01\n$$  \n\n![](images/3921371d86a02f35bc92da7e46ceba92a31617f5c2c9e5ee4971698450d3eeec.jpg)  \nFigure 6–87 Graphical determination of the zero and pole of the lead portion of the compensator.  \n\nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nThen  \n\n$$\n{\\frac{1}{T_{2}}}=0.1442\n$$  \n\nNoting that  \n\n$$\n\\begin{array}{r}{\\left|\\frac{s_{1}+\\,0.1442}{s_{1}+\\,0.01}\\right|_{s_{1}=-2+j2\\sqrt{3}}=0.9837}\\\\ {\\left|\\frac{s_{1}+\\,0.1442}{s_{1}+\\,0.01}\\right|_{s_{1}=-2+j2\\sqrt{3}}=-1.697^{\\circ}}\\end{array}\n$$  \n\nthe angle contribution of the lag portion is $-1.697^{\\circ}$ and the magnitude contribution is 0.9837.This means that the dominant closed-loop poles lie close to the desired location $s\\,=\\,-2\\,\\pm\\,j2{\\sqrt{3}}$ .Thus the compensator designed,  \n\n$$\nG_{c}(s)\\,=\\,128\\bigg({\\frac{s\\,+\\,3.70}{s\\,+\\,53.35}}\\bigg)\\bigg({\\frac{s\\,+\\,0.1442}{s\\,+\\,0.01}}\\bigg)\n$$  \n\nis acceptable.The feedforward transfer function of the compensated system becomes  \n\n$$\nG_{c}(s)G(s)={\\frac{1280(s+3.7)(s+0.1442)}{s(s+53.35)(s+0.01)(s+2)(s+8)}}\n$$  \n\nA root-locus plot of the compensated system is shown in Figure 6–88(a).An enlarged root-locus plot near the origin is shown in Figure 6–88(b).  \n\n![](images/e7afc101f58103098d7e681892c87bcc5a7cf37849dd26cf08e166355f77fa4c.jpg)  \nFigure 6–88 (a) Root-locus plot of compensated system; (b) root-locus plot near the origin.  \n\n![](images/aac4c191f6a20a393181af3b24c3bf7ea644838abf70560bb27122db92583ee9.jpg)  \n\n![](images/7d705a714e7de737ff8ffa03a1515ee1a8e824ae36c89bfbf04eb22a424d3462.jpg)  \n\n![](images/d9ef40c768fa186d483be67df5ef296b1ae48c985b042b847ff2fc53c511832a.jpg)  \nFigure 6–89 (a) Unit-step responses of compensated and uncompensated systems; (b) unitramp responses of both systems.  \n\nTo verify the improved system performance of the compensated system, see the unit-step responses and unit-ramp responses of the compensated and uncompensated systems shown in Figures 6–89 (a) and (b), respectively.  \n\nA–6–17. Consider the system shown in Figure 6–90. Design a lag–lead compensator such that the static velocity error constant $K_{v}$ is $50~\\mathrm{sec}^{-1}$ and the damping ratio $\\zeta$ of the dominant closedloop poles is 0.5. (Choose the zero of the lead portion of the lag–lead compensator to cancel the pole at $s=-1$ of the plant.) Determine all closed-loop poles of the compensated system.  \n\nFigure 6–90 Control system.  \n\n![](images/4e9c318c598a7abace847f71c05c1e981ac83f9554e85817fb8fa8797c157851.jpg)  \n\nSolution. Let us employ the lag–lead compensator given by  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\left(\\frac{s+\\frac{1}{T_{1}}}{s+\\frac{\\beta}{T_{1}}}\\right)\\left(\\frac{s+\\frac{1}{T_{2}}}{s+\\frac{1}{\\beta T_{2}}}\\right)\\,=\\,K_{c}\\frac{\\bigl(T_{1}s\\,+\\,1\\bigr)\\bigl(T_{2}s\\,+\\,1\\bigr)}{\\Bigl(\\frac{T_{1}}{\\beta}\\,s\\,+\\,1\\Bigr)\\bigl(\\beta T_{2}s\\,+\\,1\\bigr)}\n$$  \n\nwhere $\\beta>1.$ .Then  \n\n$$\n\\begin{array}{r l}{\\lefteqn{K_{v}=\\operatorname*{lim}_{s\\to0}s G_{c}(s)G(s)}}\\\\ &{=\\operatorname*{lim}_{s\\to0}s\\frac{K_{c}\\left(T_{1}s\\;+\\;1\\right)\\left(T_{2}s\\;+\\;1\\right)}{\\left(\\frac{T_{1}}{\\beta}s\\;+\\;1\\right)\\left(\\beta T_{2}s\\;+\\;1\\right)}\\frac{1}{s(s\\;+\\;1)(s\\;+\\;5)}}\\\\ &{=\\frac{K_{c}}{5}}\\end{array}\n$$  \n\nThe specification that $K_{v}\\,=\\,50\\;\\mathrm{sec}^{-1}$ determines the value of $K_{c}$ , or  \n\n$$\nK_{c}=250\n$$  \n\nWe now choose $T_{1}=1$ so that $s^{\\mathrm{~+~}}(1/T_{1})$ will cancel the $(s+1)$ term of the plant. The lead portion then becomes  \n\n$$\n\\frac{s\\,+\\,1}{s\\,+\\,\\beta}\n$$  \n\nFor the lag portion of the lag–lead compensator we require  \n\n$$\n\\left|\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}\\right|\\doteq1,\\qquad-5^{\\circ}<\\sqrt{\\frac{s_{1}+\\frac{1}{T_{2}}}{s_{1}+\\frac{1}{\\beta T_{2}}}}<0^{\\circ}\n$$  \n\nwhere $s\\,=\\,s_{1}$ is one of the dominant closed-loop poles. Noting these requirements for the lag portion of the compensator, at $s=s_{1}$ , the open-loop transfer function becomes  \n\n$$\nG_{c}(s_{1})G(s_{1})\\doteq K_{c}\\Bigl(\\frac{s_{1}+1}{s_{1}+\\beta}\\Bigr)\\,\\frac{1}{s_{1}(s_{1}+1)(s_{1}+5)}=K_{c}\\,\\frac{1}{s_{1}(s_{1}+\\beta)(s_{1}+5)}\n$$  \n\nThen at $s=s_{1}$ , the following magnitude and angle conditions must be satisfied:  \n\n$$\n\\begin{array}{l}{\\displaystyle\\left|K_{c}\\frac{1}{s_{1}\\big(s_{1}+\\beta\\big)\\big(s_{1}+5\\big)}\\right|=1}\\\\ {\\displaystyle\\left/K_{c}\\frac{1}{s_{1}\\big(s_{1}+\\beta\\big)\\big(s_{1}+5\\big)}\\right.=\\pm180^{\\circ}(2k\\,+\\,1)}\\end{array}\n$$  \n\nwhere $k\\,=\\,0,1,2,\\dots.$ In Equations (6–32) and (6–33), 1 $\\beta$ and $s_{1}$ are unknowns. Since the damping ratio $\\zeta$ of the dominant closed-loop poles is specified as 0.5,the closed-loop pole $s=s_{1}$ can be written as  \n\n$$\ns_{1}=-x\\,+\\,j\\sqrt{3}x\n$$  \n\nwhere $x$ is as yet undetermined.  \n\nNotice that the magnitude condition, Equation (6–32), can be rewritten as  \n\n$$\n\\left|\\frac{K_{c}}{(-x\\,+\\,j\\sqrt{3}x)(-x\\,+\\,\\beta\\,+\\,j\\sqrt{3}x)(-x\\,+\\,5\\,+\\,j\\sqrt{3}x)}\\right|\\,=\\,1\n$$  \n\nNoting that $K_{c}=250$ ,we have  \n\n$$\nx\\sqrt{(\\beta-x)^{2}+3x^{2}}\\sqrt{(5-x)^{2}+3x^{2}}=125\n$$  \n\nThe angle condition, Equation (6–33), can be rewritten as  \n\n$$\n\\begin{array}{r l}&{\\;\\;\\;\\;\\left\\langle K_{c}\\,\\frac{1}{\\left(-x\\,+\\,j\\sqrt{3}x\\right)\\left(-x\\,+\\,\\beta\\,+\\,j\\sqrt{3}x\\right)\\left(-x\\,+\\,5\\,+\\,j\\sqrt{3}x\\right)}\\right.}\\\\ &{=-120^{\\circ}\\,-\\,\\tan^{-1}\\!\\left(\\frac{\\sqrt{3}x}{-x\\,+\\,\\beta}\\right)\\,-\\,\\tan^{-1}\\!\\left(\\frac{\\sqrt{3}x}{\\,-\\,-\\,x\\,+\\,5}\\right)\\,=-180^{\\circ}}\\end{array}\n$$  \n\nor  \n\n$$\n\\tan^{-1}\\!\\left(\\frac{\\sqrt{3}x}{-x\\,+\\,\\beta}\\right)\\,+\\,\\tan^{-1}\\!\\left(\\frac{\\sqrt{3}x}{-x\\,+\\,5}\\right)\\,=\\,60^{\\circ}\n$$  \n\nWe need to solve Equations (6–34) and (6–35) for $\\beta$ and $x$ . By several trial-and-error calculations, it can be found that  \n\n$$\n\\beta=16.025,\\qquad x=1.9054\n$$  \n\nThus  \n\n$$\ns_{1}=-1.9054\\,+\\,j\\sqrt{3}\\left(1.9054\\right)\\,=-1.9054\\,+\\,j3.3002\n$$  \n\nThe lag portion of the lag–lead compensator can be determined as follows:Noting that the pole and zero of the lag portion of the compensator must be located near the origin, we may choose  \n\n$$\n\\frac{1}{\\beta T_{2}}=0.01\n$$  \n\nThat is,  \n\n$$\n\\frac{1}{T_{2}}=0.16025\\qquad\\mathrm{or}\\qquad T_{2}=6.25\n$$  \n\nWith the choice of $T_{2}=6.25$ we find  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\left|{\\frac{s_{1}+{\\frac{1}{T_{2}}}}{s_{1}+{\\frac{1}{\\beta T_{2}}}}}\\right|=\\left|{\\frac{-1.9054\\,+\\,j3.3002\\,+\\,0.16025}{-1.9054\\,+\\,j3.3002\\,+\\,0.01}}\\right|}}\\\\ {\\displaystyle{\\left|{\\vphantom{\\frac{s_{1}+{\\frac{1}{\\beta T_{2}}}}{s_{1}+{\\frac{1}{\\beta\\eta_{2}}}}}}\\right|}=\\left|{\\frac{-1.74515\\,+\\,j3.3002}{-1.89054\\,+\\,j3.3002}}\\right|=0.98\\div1}}\\end{array}\n$$  \n\nand  \n\n$$\n\\underbrace{\\left\\langle{\\frac{s_{1}+{\\frac{1}{T_{2}}}}{s_{1}+{\\frac{1}{\\beta T_{2}}}}}=\\overbrace{-1.9054+j3.3002\\ +0.16025}^{-1.9054\\ +\\ j3.3002\\ +\\ 0.16025}\\right.}_{\\mathrm{=}}\n$$  \n\nSince  \n\n$$\n-5^{\\circ}<-1.937^{\\circ}<0^{\\circ}\n$$  \n\nour choice of $T_{2}=6.25$ is acceptable. Then the lag–lead compensator just designed can be written as  \n\n$$\nG_{c}(s)\\,=\\,250\\bigg(\\frac{s\\,+\\,1}{s\\,+\\,16.025}\\bigg)\\bigg(\\frac{s\\,+\\,0.16025}{s\\,+\\,0.01}\\bigg)\n$$  \n\nTherefore, the compensated system has the following open-loop transfer function:  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{250(s\\,+\\,0.16025)}{s(s\\,+\\,0.01)(s\\,+\\,5)(s\\,+\\,16.025)}\n$$  \n\nA root-locus plot of the compensated system is shown in Figure 6–91(a).An enlarged root-locus plot near the origin is shown in Figure 6–91(b).  \n\nThe closed loop transfer function becomes  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{250(s+0.16025)}{s(s+0.01)(s+5)(s+16.025)\\,+250(s+0.16025)}\n$$  \n\nThe closed-loop poles are located at  \n\n$$\n\\begin{array}{l}{{s=-1.8308\\,\\pm\\,j3.2359}}\\\\ {{s=-0.1684}}\\\\ {{s=-17.205}}\\end{array}\n$$  \n\nNotice that the dominant closed-loop poles $s\\,=-1.8308\\,\\pm\\,j3.2359$ differ from the dominant closed-loop poles $s=\\pm s_{1}$ assumed in the computation of $\\beta$ and $T_{2}$ .Small deviations of the dominant closed-loop poles $s\\,=-1.8308\\,\\pm\\,j3.2359$ from $s\\,=\\pm s_{1}=-1.9054\\,\\pm\\,j3.3002$ are due to the approximations involved in determining the lag portion of the compensator.[See Equations (6–36) and (6–37).]  \n\n![](images/bef8ac4b2743ff3f8b13e678c48db5a7a6242766f769c934054cca27317f36fe.jpg)  \nRoot-Locus Plot of Compensated System near the Origin  \n\n![](images/12c58117b669984a3e861b9f4f9c723b38058570f7d236ca31ae5cd8197bccc0.jpg)  \nFigure 6–91 (a) Root-locus plot of compensated system; (b) rootlocus plot near the origin.  \n\nFigures 6–92(a) and (b) show the unit-step response and unit-ramp response of the designed system, respectively. Note that the closed-loop pole at $s\\,=\\,-0.1684$ almost cancels the zero at $s\\,=\\,-0.16025$ .However, this pair of closed-loop pole and zero located near the origin produces a long tail of small amplitude. Since the closed-loop pole at $s\\,=\\,-17.205$ is located very much farther to the left compared to the closed-loop poles at $s\\,=-1.8308\\,\\pm\\,j3.2359$ the effect of this real pole on the system response is very small. Therefore, the closed-loop poles at $s\\,=-1.8308\\,\\pm\\,j3.2359$ are indeed dominant closed-loop poles that determine the response characteristics of the closed-loop system. In the unit-ramp response, the steady-state error in following the unit-ramp input eventually becomes $1/K_{v}\\stackrel{\\star}{=}\\frac{1}{50}\\stackrel{\\star}{=}0.02$  \n\n![](images/702e3812e444d23d3f88e89bc5132fab5b26d69283f8db264d469beb519612a0.jpg)  \nUnit-Ramp Response of Compensated System  \n\n![](images/6c7849de50d75c2fc5240fb94b9705ad18ce35df61bfeea41fdf958ab5578dcb.jpg)  \nFigure 6–92 (a) Unit-step response of the compensated system; (b) unit-ramp response of the compensated system.  \n\nA–6–18. Figure 6–93(a) is a block diagram of a model for an attitude-rate control system.The closed-loop transfer function for this system is  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{2s\\,+\\,0.1}{s^{3}\\,+\\,0.1s^{2}\\,+\\,6s\\,+\\,0.1}}}}\\\\ {{\\displaystyle{\\phantom{\\frac{C(s)}{R(s)}}=\\frac{2(s\\,+\\,0.05)}{(s\\,+\\,0.0417\\,+\\,j2.4489)(s\\,+\\,0.0417\\,-\\,j2.4489)(s\\,+\\,0.0167)}}}}\\end{array}\n$$  \n\nThe unit-step response of this system is shown in Figure 6–93(b). The response shows highfrequency oscillations at the beginning of the response due to the poles at $s\\,=\\,-0.0417\\,\\pm\\,j2.4489$ The response is dominated by the pole at $s\\,=\\,-0.0167.$ .The settling time is approximately 240 sec.  \n\n![](images/ae090884807e4cc8a13347210bedc4d3ed286c1480de6517171d3522148c00e5.jpg)  \n\n![](images/2a2917f8f75f672dd8c0a991898db3cd72da29366698d0a35808f97d3f9c68f5.jpg)  \nFigure 6–93 (a) Attitude-rate control system; (b) unit-step response.  \n\nIt is desired to speed up the response and also eliminate the oscillatory mode at the beginning of the response. Design a suitable compensator such that the dominant closed-loop poles are at $s=-2\\,\\pm\\,j2{\\sqrt{3}}$ .  \n\nSolution. Figure 6–94 shows a block diagram for the compensated system.Note that the open-loop zero at $s\\,=\\,-0.05$ and the open-loop pole at $s\\,=\\,0$ generate a closed-loop pole between $s=0$ and $s=-0.05$ .Such a closed-loop pole becomes a dominant closed-loop pole and makes the response quite slow. Hence, it is necessary to replace this zero by a zero that is located far away from the $j\\omega$ axis—for example, a zero at $s=-4$ .  \n\n![](images/148873f317a39f5dec480d77cfa0f9dafd4d69268a7e0cd37792f2ccd63432f6.jpg)  \nFigure 6–94 Compensated attitude-rate control system.   \nChapter 6 /Control Systems Analysis and Design by the Root-Locus Method  \n\nWe now choose the compensator in the following form:  \n\n$$\nG_{c}(s)\\,=\\,\\hat{G}_{c}(s)\\,\\frac{s\\,+\\,4}{2s\\,+\\,0.1}\n$$  \n\nThen the open-loop transfer function of the compensated system becomes  \n\n$$\n\\begin{array}{c}{{G_{c}(s)G(s)=\\hat{G}_{c}(s)\\,{\\displaystyle\\frac{s\\,+\\,4}{2s\\,+\\,0.1}}\\displaystyle\\frac{1}{s}\\displaystyle\\frac{2s\\,+\\,0.1}{s^{2}\\,+\\,0.1s\\,+\\,4}}}\\\\ {{=\\hat{G}_{c}(s)\\,{\\displaystyle\\frac{s\\,+\\,4}{s\\bigl(s^{2}\\,+\\,0.1s\\,+\\,4\\bigr)}}}}\\end{array}\n$$  \n\nTo determine $\\hat{G}_{c}(s)$ by the root-locus method, we need to find the angle deficiency at the desired closed-loop pole $s=-2\\,+\\,j2{\\sqrt{3}}$ The angle deficiency can be found as follows:  \n\n$$\n{\\begin{array}{r l}{\\mathrm{Angle~deficiency}\\,=-143.088^{\\circ}\\,-\\,120^{\\circ}\\,-\\,109.642^{\\circ}\\,+\\,60^{\\circ}\\,+\\,180^{\\circ}}&{}\\\\ {\\,=-132.73^{\\circ}}\\end{array}}\n$$  \n\nHence, the lead compensator $\\hat{G}_{c}(s)$ must provide $132.73^{\\circ}$ . Since the angle deficiency is $-132.73^{\\circ}$ ,we need two lead compensators, each providing $66.365^{\\circ}$ .Thus $\\hat{G}_{c}(s)$ will have the following form:  \n\n$$\n\\hat{G}_{c}(s)\\,=\\,K_{c}\\bigg(\\frac{s\\,+\\,s_{z}}{s\\,+\\,s_{p}}\\bigg)^{2}\n$$  \n\nSuppose that we choose two zeros at $s\\,=\\,-2$ .Then the two poles of the lead compensators can be obtained from  \n\n$$\n{\\frac{3.4641}{s_{p}\\,-\\,2}}=\\tan(90^{\\circ}\\,-\\,66.365^{\\circ})\\,=\\,0.4376169\n$$  \n\nor  \n\n$$\n\\begin{array}{c}{{s_{p}=2\\,+\\frac{3.4641}{0.4376169}}}\\\\ {{=\\,9.9158}}\\end{array}\n$$  \n\n(See Figure 6–95.) Hence,  \n\n$$\n\\hat{G}_{c}(s)\\,=\\,K_{c}\\bigg(\\frac{s\\,+\\,2}{s\\,+\\,9.9158}\\bigg)^{2}\n$$  \n\n![](images/216cd4169ef19bfd789148dd738b0bc33be8a2ebc34736f04d64328bbe3d8998.jpg)  \nFigure 6–95 Pole and zero of $\\hat{G}_{c}(s)$ .  \n\nThe entire compensator $G_{c}(s)$ for the system becomes  \n\n$$\nG_{c}(s)\\,=\\,{\\hat{G}}_{c}(s)\\,{\\frac{s\\,+\\,4}{2s\\,+\\,0.1}}=\\,K_{c}\\,{\\frac{(s\\,+\\,2)^{2}}{(s\\,+\\,9.9158)^{2}}}{\\frac{s\\,+\\,4}{2s\\,+\\,0.1}}\n$$  \n\nThe value of $K_{c}$ can be determined from the magnitude condition. Since the open-loop transfer function is  \n\n$$\nG_{c}(s)G(s)\\,=\\,K_{c}\\,\\frac{(s\\,+\\,2)^{2}(s\\,+\\,4)}{(s\\,+\\,9.9158)^{2}s(s^{2}\\,+\\,0.1s\\,+\\,4)}\n$$  \n\nthe magnitude condition becomes  \n\n$$\n\\left|K_{c}\\frac{(s\\,+\\,2)^{2}(s\\,+\\,4)}{(s\\,+\\,9.9158)^{2}s(s^{2}\\,+\\,0.1s\\,+\\,4)}\\right|_{s=-2+j2\\sqrt3}=1\n$$  \n\nHence,  \n\n$$\n\\begin{array}{r l}{\\lefteqn{K_{c}=\\left|\\frac{(s\\mathrm{~+~}9.9158)^{2}s\\left(s^{2}\\mathrm{~+~}0.1s\\mathrm{~+~}4\\right)}{(s\\mathrm{~+~}2)^{2}(s\\mathrm{~+~}4)}\\right|_{s=-2+j2\\sqrt{3}}}}\\\\ &{=88.0227}\\end{array}\n$$  \n\nThus the compensator $G_{c}(s)$ becomes  \n\n$$\nG_{c}(s)\\,=\\,88.0227\\,\\frac{(s\\,+\\,2)^{2}(s\\,+\\,4)}{(s\\,+\\,9.9158)^{2}(2s\\,+\\,0.1)}\n$$  \n\nThe open-loop transfer function is given by  \n\n$$\nG_{c}(s)G(s)=\\frac{88.0227(s\\,+\\,2)^{2}(s\\,+\\,4)}{(s\\,+\\,9.9158)^{2}s(s^{2}\\,+\\,0.1s\\,+\\,4)}\n$$  \n\nA root-locus plot for the compensated system is shown in Figure 6–96.The closed-loop poles for the compensated system are indicated in the plot.The closed-loop poles, the roots of the characteristic equation  \n\n$$\n(s\\,+\\,9.9158)^{2}s(s^{2}\\,+\\,0.1s\\,+\\,4)\\,+\\,88.0227(s\\,+\\,2)^{2}(s\\,+\\,4)\\,=\\,0\n$$  \n\n![](images/a8e387886b05a372584df9a544a6245b40d0d191542a260a6bf4b2e9a5fb5eea.jpg)  \nFigure 6–96 Root-locus plot of the compensated system.  \n\nare as follows:  \n\n$$\n\\begin{array}{l}{{s=-2.0000\\,\\pm\\,j3.4641}}\\\\ {{s=-7.5224\\,\\pm\\,j6.5326}}\\\\ {{s=-0.8868}}\\end{array}\n$$  \n\nNow that the compensator has been designed, we shall examine the transient-response characteristics with MATLAB.The closed-loop transfer function is given by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{88.0227(s+2)^{2}(s+4)}{(s+9.9158)^{2}s(s^{2}+0.1s+4)+88.0227(s+2)^{2}(s+4)}\n$$  \n\nFigures 6–97(a) and (b) show the plots of the unit-step response and unit-ramp response of the compensated system.These response curves show that the designed system is acceptable.  \n\n![](images/79c8843d3eacc1c4a6ee71f792d512a7c0ea32eab09f5c9d9736883975e035cb.jpg)  \n\n![](images/5b617e20a0775567aa725dc400407b0a5b3f696727240a4a1191ba0ab4abce5f.jpg)  \nFigure 6–97 (a) Unit-step response of the compensated system; (b) unit-ramp response of the compensated system.  \n\nA–6–19. Consider the system shown in Figure 6–98(a).Determine the value of $a$ such that the damping ratio $\\zeta$ of the dominant closed poles is 0.5.  \n\nSolution. The characteristic equation is  \n\n$$\n1+{\\frac{10(s\\,+\\,a)}{s(s\\,+\\,1)(s\\,+\\,8)}}=0\n$$  \n\nThe variable $a$ is not a multiplying factor. Hence, we need to modify the characteristic equation. Since the characteristic equation can be written as  \n\n$$\ns^{3}\\,+\\,9s^{2}\\,+\\,18s\\,+\\,10a\\,=\\,0\n$$  \n\nwe rewrite this equation such that $a$ appears as a multiplying factor as follows:  \n\n$$\n1\\,+\\,\\frac{10a}{s\\bigl(s^{2}\\,+\\,9s\\,+\\,18\\bigr)}=0\n$$  \n\nDefine  \n\n$$\n10a\\,=\\,K\n$$  \n\nThen the characteristic equation becomes  \n\n$$\n1\\,+\\,{\\frac{K}{s\\big(s^{2}\\,+\\,9s\\,+\\,18\\big)}}=0\n$$  \n\nNotice that the characteristic equation is in a suitable form for the construction of the root loci.  \n\n![](images/348a77eda9c3108f5dd94e37b1dac5b8ab8615db5893daa7cc2df01d620d7bb9.jpg)  \n\n# Figure 6–98  \n\n(a) Control system; (b) root-locus plot, where $K=10a$ .  \n\nThis system involves three poles and no zero.The three poles are at $s=0,s=-3$ ,and $s=-6$ .A root-locus branch exists on the real axis between points $s\\,=\\,0$ and $s\\,=\\,-3.$ Also,another branch exists between points $s=-6$ and $s=-\\infty$ .  \n\nThe asymptotes for the root loci are found as follows:  \n\n$$\n{\\mathrm{Angles~of~asymptotes}}={\\frac{\\pm180^{\\circ}(2k+1)}{3}}=60^{\\circ},-60^{\\circ},180^{\\circ}\n$$  \n\nThe intersection of the asymptotes and the real axis is obtained from  \n\n$$\ns=-\\,\\frac{0\\,+\\,3\\,+\\,6}{3}=-3\n$$  \n\nThe breakaway and break-in points can be determined from $d K/d s=0$ ,where  \n\n$$\nK\\,=\\,-(s^{3}\\,+\\,9s^{2}\\,+\\,18s)\n$$  \n\nNow we set  \n\n$$\n\\frac{d K}{d s}=-\\big(3s^{2}\\,+\\,18s\\,+\\,18\\big)\\,=\\,0\n$$  \n\nwhich yields  \n\n$$\ns^{2}\\,+\\,6s\\,+\\,6\\,=\\,0\n$$  \n\nor  \n\n$$\ns\\,=-1.268,\\qquad s\\,=-4.732\n$$  \n\nPoint $s=-1.268$ is on a root-locus branch.Hence,point $s=-1.268$ is an actual breakaway point.But point $s=-4.732$ is not on the root locus and therefore is neither a breakaway nor break-in point.  \n\nNext we shall find points where root-locus branches cross the imaginary axis. We substitute $s=j\\omega$ in the characteristic equation, which is  \n\n$$\ns^{3}\\,+\\,9s^{2}\\,+\\,18s\\,+\\,K\\,=\\,0\n$$  \n\nas follows:  \n\n$$\n(j\\omega)^{3}\\,+\\,9(j\\omega)^{2}\\,+\\,18(j\\omega)\\,+\\,K\\,=\\,0\n$$  \n\nor  \n\n$$\n\\big(K\\,-\\,9\\omega^{2}\\big)\\,+\\,j\\omega\\big(18\\,-\\,\\omega^{2}\\big)\\,=\\,0\n$$  \n\nfrom which we get  \n\n$$\n\\omega=\\pm3\\sqrt{2}\\,,\\qquad K=9\\omega^{2}=162\\qquad\\mathrm{or}\\qquad\\omega=0,\\qquad K=0\n$$  \n\nThe crossing points are at $\\omega=\\pm3\\sqrt{2}$ and the corresponding value of gain $K$ is 162.Also, a rootlocus branch touches the imaginary axis at $\\omega\\,=\\,0,$ . Figure 6–98(b) shows a sketch of the root loci for the system.  \n\nSince the damping ratio of the dominant closed-loop poles is specified as 0.5, the desired closed-loop pole in the upper-half $s$ plane is located at the intersection of the root-locus branch in the upper-half $s$ plane and a straight line having an angle of $60^{\\circ}$ with the negative real axis.The desired dominant closed-loop poles are located at  \n\n$$\ns=-1\\,+\\,j1.732,\\qquad s=-1\\,-\\,j1.732\n$$  \n\nAt these points, the value of gain $K$ is 28. Hence,  \n\n$$\na=\\frac{K}{10}=2.8\n$$  \n\nSince the system involves two or more poles than zeros (in fact, three poles and no zero), the third pole can be located on the negative real axis from the fact that the sum of the three closedloop poles is $-9.$ .Hence, the third pole is found to be at  \n\nor  \n\n$$\n\\begin{array}{c}{{s=-9\\,-\\,(-1\\,+\\,j1.732)\\,-\\,(-1\\,-\\,j1.732)}}\\\\ {{{}}}\\\\ {{{}}}\\\\ {{{s=-7}}}\\end{array}\n$$  \n\nA–6–20. Consider the system shown in Figure 6–99(a). Sketch the root loci of the system as the velocity feedback gain $k$ varies from zero to infinity. Determine the value of $k$ such that the closed-loop poles have the damping ratio $\\zeta$ of 0.7.  \n\nSolution. The open-loop transfer function is  \n\n$$\n{\\mathrm{Open-loop~transfer~function}}={\\frac{10}{(s\\,+\\,1\\,+\\,10k)s}}\n$$  \n\nSince $k$ is not a multilying factor, we modify the equation such that $k$ appears as a multiplying factor. Since the characteristic equation is  \n\n$$\ns^{2}\\,+\\,s\\,+\\,10k s\\,+\\,10\\,=\\,0\n$$  \n\nwe rewrite this equation as follows:  \n\n$$\n1\\,+\\,\\frac{10k s}{s^{2}\\,+\\,s\\,+\\,10}\\,=\\,0\n$$  \n\nDefine  \n\n$$\n10k\\,=\\,K\n$$  \n\nThen Equation (6–38) becomes  \n\n$$\n1\\,+\\,\\frac{K s}{s^{2}\\,+\\,s\\,+\\,10}\\,=\\,0\n$$  \n\n![](images/3f3421f186352ce22d2da058796489b8ae3cc0e8387c82b09980e6ade45d0c88.jpg)  \nFigure 6–99 (a) Control system; (b) root-locus plot, where $K=10k$ .  \n\nNotice that the system has a zero at $s\\,=\\,0$ and two poles at $s\\,=\\,-0.5\\,\\pm\\,j3.1225.$ .Since this system involves two poles and one zero, there is a possibility that a circular root locus exists. In fact, this system has a circular root locus, as will be shown. Since the angle condition is  \n\n$$\n\\left/\\frac{K s}{s^{2}\\,+\\,s\\,+\\,10}=\\pm180^{\\circ}(2k\\,+\\,1)\\right.\n$$  \n\nwe have  \n\n$$\n/s\\,-\\,\\,/s\\,+\\,0.5\\,+\\,j3.1225\\,-\\,\\,/s\\,+\\,0.5\\,-\\,j3.1225\\,=\\pm180^{\\circ}(2k\\,+\\,1)\n$$  \n\nBy substituting $s\\,=\\,\\sigma\\,+\\,j\\omega$ into this last equation and rearranging, we obtain  \n\n$$\n\\left\\langle\\sigma\\,+\\,0.5\\,+\\,j(\\omega\\,+\\,3.1225)\\,+\\,\\left\\langle\\sigma\\,+\\,0.5\\,+\\,j(\\omega\\,-\\,3.1225)\\,=\\,\\left\\langle\\sigma\\,+\\,j\\omega\\,\\pm\\,180^{\\circ}(2k\\,+\\,180^{\\circ})\\right\\rangle\\right.\n$$  \n\nwhich can be rewritten as  \n\n$$\n\\tan^{-1}\\!\\left({\\frac{\\omega\\,+\\,3.1225}{\\sigma\\,+\\,0.5}}\\right)\\,+\\,\\tan^{-1}\\!\\left({\\frac{\\omega\\,-\\,3.1225}{\\sigma\\,+\\,0.5}}\\right)\\,=\\,\\tan^{-1}\\!\\left({\\frac{\\omega}{\\sigma}}\\right)\\,\\pm\\,180^{\\circ}(2k\\,+\\,1)\n$$  \n\nTaking tangents of both sides of this last equation, we obtain  \n\n$$\n{\\frac{{\\frac{\\omega\\,+\\,3.1225}{\\sigma\\,+\\,0.5}}\\,+\\,{\\frac{\\omega\\,-\\,3.1225}{\\sigma\\,+\\,0.5}}}{1\\,-\\,{\\biggl(}{\\frac{\\omega\\,+\\,3.1225}{\\sigma\\,+\\,0.5}}{\\biggr)}{\\biggl(}{\\frac{\\omega\\,-\\,3.1225}{\\sigma\\,+\\,0.5}}{\\biggr)}}}={\\frac{\\omega}{\\sigma}}\n$$  \n\nwhich can be simplified to  \n\n$$\n\\frac{2\\omega(\\sigma\\,+\\,0.5)}{(\\sigma\\,+\\,0.5)^{2}-\\left(\\omega^{2}\\,-\\,3.1225^{2}\\right)}=\\frac{\\omega}{\\sigma}\n$$  \n\nor  \n\n$$\n\\omega(\\sigma^{2}\\,-\\,10\\,+\\,\\omega^{2})\\,=\\,0\n$$  \n\nwhich yields  \n\n$$\n\\omega\\,=\\,0\\qquad\\mathrm{or}\\qquad\\sigma^{2}\\,+\\,\\omega^{2}\\,=\\,10\n$$  \n\nNotice that $\\omega=0$ corresponds to the real axis.The negative real axis (between $s=0$ and $s=-\\infty$ )corresponds to $K\\geq0$ , and the positive real axis corresponds to $K<0$ .The equation  \n\n$$\n\\sigma^{2}\\,+\\,\\omega^{2}\\,=\\,10\n$$  \n\nis an equation of a circle with center at $\\sigma\\,=\\,0,\\omega\\,=\\,0$ with the radius equal to $\\sqrt{10}$ .A portion of this circle that lies to the left of the complex poles corresponds to the root locus for $K>0$ . (The portion of the circle which lies to the right of the complex poles corresponds to the root locus for $K<0,$ .) Figure 6–99(b) shows a sketch of the root loci for $K>0$ .  \n\nSince we require $\\zeta=0.7$ for the closed-loop poles, we find the intersection of the circular root locus and a line having an angle of $45.57^{\\circ}$ (note that cos $45.57^{\\circ}=0.7^{\\circ}$ ) with the negative real axis. The intersection is at $s\\,=\\,-2.214\\,+\\,j2.258.$ .The gain $K$ corresponding to this point is 3.427. Hence, the desired value of the velocity feedback gain $k$ is  \n\n$$\nk=\\frac{K}{10}=0.3427\n$$  \n\n# PROBLEMS  \n\nB–6–1. Plot the root loci for the closed-loop control system with  \n\n$$\nG(s)\\,=\\,\\frac{K(s\\,+\\,1)}{s^{2}},\\qquad H(s)\\,=\\,1\n$$  \n\nB–6–2. Plot the root loci for the closed-loop control system with  \n\n$$\nG(s)=\\frac{K}{s(s+1)(s^{2}+4s\\,+\\,5)},\\qquad H(s)=1\n$$  \n\nB–6–3. Plot the root loci for the system with  \n\n$$\nG(s)=\\frac{K}{s(s+0.5)(s^{2}+0.6s\\,+\\,10)},\\qquad H(s)=1\n$$  \n\nB–6–4. Show that the root loci for a control system with  \n\n$$\nG(s)\\,=\\frac{K\\bigl(s^{2}\\,+\\,6s\\,+\\,10\\bigr)}{s^{2}\\,+\\,2s\\,+\\,10},\\qquad H(s)\\,=\\,1\n$$  \n\nare arcs of the circle centered at the origin with radius equal to $\\sqrt{10}$ .  \n\nB–6–5. Plot the root loci for a closed-loop control system with  \n\n$$\nG(s)=\\frac{K(s\\,+\\,0.2)}{s^{2}(s\\,+\\,3.6)},\\qquad H(s)=1\n$$  \n\nB–6–6. Plot the root loci for a closed-loop control system with  \n\n$$\nG(s)\\,=\\frac{K(s\\,+\\,9)}{s(s^{2}\\,+\\,4s\\,+\\,11)},\\qquad H(s)\\,=\\,1\n$$  \n\nLocate the closed-loop poles on the root loci such that the dominant closed-loop poles have a damping ratio equal to 0.5. Determine the corresponding value of gain $K$ .  \n\nB–6–7. Plot the root loci for the system shown in Figure 6–100. Determine the range of gain $K$ for stability.  \n\n![](images/c8a7800fb4dccf2885e938ab58bb405d91dfceae529cddaf7410205ac5222b21.jpg)  \nFigure 6–100 Control system.  \n\nB–6–8. Consider a unity-feedback control system with the following feedforward transfer function:  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s^{2}\\,+\\,4s\\,+\\,8)}\n$$  \n\nPlot the root loci for the system. If the value of gain $K$ is set equal to 2, where are the closed-loop poles located?  \n\nB–6–9. Consider the system whose open-loop transfer function is given by  \n\n$$\nG(s)H(s)\\,=\\frac{K(s\\,-\\,0.6667)}{s^{4}\\,+\\,3.3401s^{3}\\,+\\,7.0325s^{2}}\n$$  \n\nShow that the equation for the asymptotes is given by  \n\n$$\nG_{a}(s)H_{a}(s)\\,=\\frac{K}{s^{3}\\,+\\,4.0068s^{2}\\,+\\,5.3515s\\,+\\,2.3825}\n$$  \n\nUsing MATLAB, plot the root loci and asymptotes for the system.  \n\nB–6–10. Consider the unity-feedback system whose feedforward transfer function is  \n\n$$\nG(s)={\\frac{K}{s(s+1)}}\n$$  \n\nThe constant-gain locus for the system for a given value of $K$ is defined by the following equation:  \n\n$$\n\\left|\\frac{K}{s(s\\,+\\,1)}\\right|\\,=\\,1\n$$  \n\nShow that the constant-gain loci for $0\\le K\\le\\infty$ may be given by  \n\n$$\n\\left[\\sigma(\\sigma\\,+\\,1)\\,+\\,\\omega^{2}\\right]^{2}\\,+\\,\\omega^{2}\\,=\\,K^{2}\n$$  \n\nSketch the constant-gain loci for $K=1,2,5,10.$ , and 20 on the $s$ plane.  \n\nB–6–11. Consider the system shown in Figure 6–101. Plot the root loci with MATLAB. Locate the closed-loop poles when the gain $K$ is set equal to 2.  \n\n![](images/5ffe720c21d9f8b6b0382a62b7195ce95a9e915f84251eeec3a306ba566bcfd7.jpg)  \n\nFigure 6–101 Control system.  \n\nB–6–12. Plot root-locus diagrams for the nonminimum-phase systems shown in Figures 6–102(a) and (b), respectively.  \n\n![](images/3a70e2af423ce46942955f9dc151c3385ddd987980cd284d36bff081765cf6a3.jpg)  \nFigure 6–102 (a) and (b) Nonminimum-phase systems.  \n\nB–6–13. Consider the mechanical system shown in Figure 6–103. It consists of a spring and two dashpots. Obtain the transfer function of the system. The displacement $x_{i}$ is the input and displacement $x_{o}$ is the output. Is this system a mechanical lead network or lag network?  \n\n![](images/7704ca62a435c458d64d055c9c4400208dc86315070fa6c9666800ecdeb69bf2.jpg)  \n\nFigure 6–103 Mechanical system.  \n\nB–6–14. Consider the system shown in Figure 6–104. Plot the root loci for the system. Determine the value of $K$ such that the damping ratio $\\zeta$ of the dominant closed-loop poles is 0.5. Then determine all closed-loop poles. Plot the unitstep response curve with MATLAB.  \n\n![](images/f5c25e04c30fe8911220a59ae49bd9392562f67f6f89b6f45f987238e420d960.jpg)  \nFigure 6–104 Control system.  \n\nB–6–15. Determine the values of $K,T_{1}$ ,and $T_{2}$ of the system shown in Figure 6–105 so that the dominant closed-loop poles have the damping ratio $\\zeta=0.5$ and the undamped natural frequency $\\omega_{n}=3$ rad 'sec.  \n\n![](images/1b1a9e5a153c9a13c6b5ce19f6bfc9e2fff1e6ed10794d763f3b994ad6b17ed4.jpg)  \nFigure 6–105 Control system.  \n\nB–6–16. Consider the control system shown in Figure 6–106. Determine the gain $K$ and time constant $T$ of the controller $G_{c}(s)$ such that the closed-loop poles are located at $s=-2\\,\\pm\\,j2$ .  \n\n![](images/94f08e5e5bd136601bed82aad85a20d72642b530c70a814061cc90746379cd81.jpg)  \nFigure 6–106 Control system.  \n\nB–6–17. Consider the system shown in Figure 6–107. De1 sign a lead compensator such that the dominant closed-loop poles are located at $s=-2\\,\\pm\\,j2{\\sqrt{3}}$ Plot the unit-step response curve of the designed system with MATLAB.  \n\n![](images/6302aa1aa00080fe4274df48b41ec8a5c55deeb0f5aed687db9cc6caa2707573.jpg)  \n\n# Figure 6–107 Control system.  \n\nB–6–18. Consider the system shown in Figure 6–108. Design a compensator such that the dominant closed-loop poles are located at $s=-1\\,\\pm\\,j1$ .  \n\n![](images/444de1ab846647046d644aab4cc89fd082f803a41f175d1e502a4c6889e50e27.jpg)  \n\n# Figure 6–108 Control system.  \n\nB–6–19. Referring to the system shown in Figure 6–109, design a compensator such that the static velocity error constant $K_{v}$ is $20\\,\\mathrm{sec}^{-1}$ without appreciably changing the original location $(s=-2\\,\\pm\\,j2\\sqrt{3})$ of a pair of the complex-conjugate closed-loop poles.  \n\nB–6–22. Consider the control system shown in Figure 6–112. Design a compensator such that the unit-step response curve will exhibit maximum overshoot of $30\\%$ or less and settling time of 3 sec or less.  \n\n![](images/e9b00a0b4c0e5953e73df1617e1f8a0607ff0ddf4c1e0a6595370dab20670ea4.jpg)  \n\nFigure 6–109 Control system.  \n\nB–6–20. Consider the angular-positional system shown in Figure 6–110. The dominant closed-loop poles are located at $s\\,=\\,-3.60\\,\\pm\\,j4.80.$ The damping ratio $\\zeta$ of the dominant closed-loop poles is 0.6.The static velocity error constant $K_{v}$ is $4.1\\;\\mathrm{sec}^{-1}$ , which means that for a ramp input of $360^{\\circ}/\\mathrm{sec}$ the steady-state error in following the ramp input is  \n\n![](images/5a18c59bb5d6d3cda8f21b025b274c368254104b863f6badc63db6a9f19e7d57.jpg)  \n\nFigure 6–112 Control system.  \n\nB–6–23. Consider the control system shown in Figure 6–113. Design a compensator such that the unit-step response curve will exhibit maximum overshoot of $25\\%$ or less and settling time of 5 sec or less.  \n\n$$\ne_{v}={\\frac{\\theta_{i}}{K_{v}}}={\\frac{360^{\\circ}/{\\mathrm{sec}}}{4.1\\ {\\mathrm{sec}}^{-1}}}=87.8^{\\circ}\n$$  \n\nIt is desired to decrease $e_{v}$ to one-tenth of the present value, or to increase the value of the static velocity error constant $K_{v}$ to $41\\,\\mathrm{sec}^{-1}$ .It is also desired to keep the damping ratio $\\zeta$ of the dominant closed-loop poles at 0.6.A small change in the undamped natural frequency $\\omega_{n}$ of the dominant closedloop poles is permissible.Design a suitable lag compensator to increase the static velocity error constant as desired.  \n\n![](images/3e9bfb8835984a85d10e446ac06154f2b72868b17da782fd1775ae198f9e6b52.jpg)  \n\nFigure 6–110 Angular-positional system.  \n\nB–6–21. Consider the control system shown in Figure 6–111. Design a compensator such that the dominant closed-loop poles are located at $s=-2\\,\\pm\\,j2{\\sqrt{3}}$ and the static velocity error constant $K_{v}$ is $50\\;\\mathrm{sec}^{-1}$ .  \n\n![](images/c92f7ed1bc0e34394fe1d79ea59d65897edab3254bf230694473448bcf1a402e.jpg)  \n\nFigure 6–111 Control system.  \n\n![](images/77de4c488a43bd8df4c70f65c5da7e5b86aa52866723333a38a3c17e6d40c256.jpg)  \nFigure 6–113 Control system.  \n\nB–6–24. Consider the system shown in Figure 6–114, which involves velocity feedback. Determine the values of the amplifier gain $K$ and the velocity feedback gain $K_{h}$ so that the following specifications are satisfied:  \n\n1. Damping ratio of the closed-loop poles is 0.5   \n2. Settling time $\\leq2$ sec   \n3. Static velocity error constant $K_{v}\\ge50\\;\\mathrm{sec}^{-1}$   \n4. $0<K_{h}<1$  \n\n![](images/88c5c26f3d254a1a8e4d3c959b5c0d3a7452ecc66ce5ef594e5fa059653a2a54.jpg)  \n\nFigure 6–114 Control system.  \n\nB–6–25. Consider the system shown in Figure 6–115. The system involves velocity feedback. Determine the value of gain $K$ such that the dominant closed-loop poles have a damping ratio of 0.5. Using the gain $K$ thus determined, obtain the unit-step response of the system.  \n\n![](images/229172042a5c3f97c4101353d55befc5d56fd9f4bf609cb4eb60e2f5627fd7fc.jpg)  \n\nFigure 6–115 Control system.  \n\nB–6–26. Consider the system shown in Figure 6–116. Plot the root loci as $a$ varies from 0 to $\\infty.$ Determine the value of a such that the damping ratio of the dominant closed-loop poles is 0.5.  \n\n![](images/5e97c4c2dd54b80d9e0fa5afbbcd8c686338635f1b4e55e694be9de641283f62.jpg)  \nFigure 6–116 Control system.  \n\nB–6–27. Consider the system shown in Figure 6–117. Plot the root loci as the value of $k$ varies from 0 to $\\infty,$ .What value of $k$ will give a damping ratio of the dominant closed-loop poles equal to 0.5? Find the static velocity error constant of the system with this value of $k$ .  \n\nFigure 6–117 Control system.  \n\n![](images/a01b2cb146f4bc96245c4558af50ae59789495994a21c167b81f84ad70c17a56.jpg)  \n\nB–6–28. Consider the system shown in Figure 6–118. Assuming that the value of gain $K$ varies from 0 to $\\infty$ , plot the root loci when $K_{h}\\,=\\,0.1$ , 0.3, and 0.5.  \n\nCompare unit-step responses of the system for the following three cases:  \n\n$$\n\\begin{array}{l l}{{K=10,}}&{{K_{h}=0.1}}\\\\ {{}}&{{}}\\\\ {{K=10,}}&{{K_{h}=0.3}}\\\\ {{}}&{{}}\\\\ {{K=10,}}&{{K_{h}=0.5}}\\end{array}\n$$  \n\n![](images/52fa3d17ec4b9d2a871dd83cd384ace45eec9f206fcb233ee9a9a4e1c2313271.jpg)  \n\n# Control Systems Analysis and Design by the Frequency-Response Method  \n\nBy the term frequency response, we mean the steady-state response of a system to a sinusoidal input. In frequency-response methods, we vary the frequency of the input signal over a certain range and study the resulting response.  \n\nIn this chapter we present frequency-response approaches to the analysis and design of control systems.The information we get from such analysis is different from what we get from root-locus analysis. In fact, the frequency response and root-locus approaches complement each other. One advantage of the frequency-response approach is that we can use the data obtained from measurements on the physical system without deriving its mathematical model. In many practical designs of control systems both approaches are employed. Control engineers must be familiar with both.  \n\nFrequency-response methods were developed in 1930s and 1940s by Nyquist, Bode, Nichols, and many others. The frequency-response methods are most powerful in conventional control theory.They are also indispensable to robust control theory.  \n\nThe Nyquist stability criterion enables us to investigate both the absolute and relative stabilities of linear closed-loop systems from a knowledge of their open-loop frequencyresponse characteristics. An advantage of the frequency-response approach is that frequency-response tests are, in general, simple and can be made accurately by use of readily available sinusoidal signal generators and precise measurement equipment.Often the transfer functions of complicated components can be determined experimentally by frequency-response tests. In addition, the frequency-response approach has the advantages that a system may be designed so that the effects of undesirable noise are negligible and that such analysis and design can be extended to certain nonlinear control systems.  \n\nAlthough the frequency response of a control system presents a qualitative picture of the transient response,the correlation between frequency and transient responses is indirect,except for the case of second-order systems. In designing a closed-loop system, we adjust the frequency-response characteristic of the open-loop transfer function by using several design criteria in order to obtain acceptable transient-response characteristics for the system.  \n\nObtaining Steady-State Outputs to Sinusoidal Inputs. We shall show that the steady-state output of a transfer function system can be obtained directly from the sinusoidal transfer function—that is, the transfer function in which $s$ is replaced by $j\\omega$ ,where $\\omega$ is frequency.  \n\nConsider the stable,linear,time-invariant system shown in Figure 7–1.The input and output of the system,whose transfer function is $G(s)$ ,are denoted by $x(t)$ and $y(t)$ ,respectively. If the input $x(t)$ is a sinusoidal signal, the steady-state output will also be a sinusoidal signal of the same frequency, but with possibly different magnitude and phase angle.  \n\nLet us assume that the input signal to the system is given by  \n\n$$\nx(t)\\,=\\,X\\sin\\omega t\n$$  \n\n[In this book “ $\\mathbf{\\omega}^{\\bullet}\\omega^{\\bullet}$ is always measured in rad/sec. When the frequency is measured in cycle/sec, we use notation $\\mathbf{\\nabla}^{\\ast}\\!f^{\\ast}$ ”.That is, $\\omega=2\\pi f$ .]  \n\nSuppose that the transfer function $G(s)$ of the system can be written as a ratio of two polynomials in $s$ ; that is,  \n\n$$\nG(s)\\,={\\frac{p(s)}{q(s)}}={\\frac{p(s)}{(s\\,+\\,s_{1})(s\\,+\\,s_{2})\\cdots(s\\,+\\,s_{n})}}\n$$  \n\nThe Laplace-transformed output $Y(s)$ of the system is then  \n\n$$\nY(s)=G(s)X(s)={\\frac{p(s)}{q(s)}}\\,X(s)\n$$  \n\nwhere $X(s)$ is the Laplace transform of the input $x(t)$ .  \n\nIt will be shown that, after waiting until steady-state conditions are reached, the frequency response can be calculated by replacing $s$ in the transfer function by $j\\omega$ . It will also be shown that the steady-state response can be given by  \n\n$$\nG(j\\omega)\\,=\\,M e^{j\\phi}\\,=\\,M\\,\\bigl/\\phi\n$$  \n\nwhere $M$ is the amplitude ratio of the output and input sinusoids and $\\phi$ is the phase shift between the input sinusoid and the output sinusoid. In the frequency-response test, the input frequency $\\omega$ is varied until the entire frequency range of interest is covered.  \n\nThe steady-state response of a stable, linear, time-invariant system to a sinusoidal input does not depend on the initial conditions. (Thus, we can assume the zero initial condition.) If $Y(s)$ has only distinct poles, then the partial fraction expansion of Equation (7–1) when $x(t)=X$ sin vtyields  \n\n$$\n{\\begin{array}{r l}&{Y(s)=G(s)X(s)\\,=\\,G(s)\\,{\\frac{\\omega X}{s^{2}\\,+\\,\\omega^{2}}}}\\\\ &{\\qquad={\\cfrac{a}{s\\,+\\,j\\omega}}+{\\cfrac{a}{s\\,-\\,j\\omega}}+{\\cfrac{b_{1}}{s\\,+\\,s_{1}}}+{\\cfrac{b_{2}}{s\\,+\\,s_{2}}}+\\cdots+{\\cfrac{b_{n}}{s\\,+\\,s_{n}}}}\\end{array}}\n$$  \n\nFigure 7–1 Stable, linear, timeinvariant system.  \n\nwhere $a$ and the $b_{i}$ (where $i=1,2,\\dots,n)$ )are constants and $\\bar{a}$ –is the complex conjugate of $a$ .The inverse Laplace transform of Equation (7–2) gives  \n\n$$\ny(t)=a e^{-j\\omega t}+\\bar{a}e^{j\\omega t}+b_{1}e^{-s_{1}t}+b_{2}e^{-s_{2}t}+\\cdots+b_{n}e^{-s_{n}t}\\qquad(t\\geq0)\n$$  \n\nFor a stable system, $-s_{1},-s_{2},\\ldots,-s_{n}$ have negative real parts.Therefore, as $t$ approaches infinity, the terms $e^{-s_{1}t}$ ,$e^{-s_{2}t},\\ldots$ ,and $e^{-s_{n}t}$ approach zero.Thus, all the terms on the righthand side of Equation (7–3), except the first two, drop out at steady state.  \n\nIf $Y(s)$ involves multiple poles $s_{j}$ of multiplicity $m_{j}$ ,then $y(t)$ will involve terms such as $t^{h_{j}}e^{-s_{j}t}\\bigl(h_{j}=0,1,2,\\ldots,m_{j}-1\\bigr)$ .For a stable system, the terms $t^{h_{j}}e^{-s_{j}t}$ approach zero as $t$ approaches infinity.  \n\nThus, regardless of whether the system is of the distinct-pole type or multiple-pole type, the steady-state response becomes  \n\n$$\ny_{\\mathrm{ss}}(t)\\,=\\,a e^{-j\\omega t}\\,+\\,\\bar{a}e^{j\\omega t}\n$$  \n\nwhere the constant $a$ can be evaluated from Equation (7–2) as follows:  \n\n$$\na\\,=\\,G(s)\\,\\frac{\\omega X}{s^{2}\\,+\\,\\omega^{2}}\\left(s\\,+\\,j\\omega\\right)\\bigg|_{s=-j\\omega}=-\\,\\frac{X G(-j\\omega)}{2j}\n$$  \n\nNote that  \n\n$$\n{\\bar{a}}=G(s)\\,{\\frac{\\omega X}{s^{2}\\,+\\,\\omega^{2}}}\\left(s\\,-\\,j\\omega\\right)\\left|_{s=j\\omega}={\\frac{X G(j\\omega)}{2j}}\n$$  \n\nSince $G(j\\omega)$ is a complex quantity, it can be written in the following form:  \n\n$$\nG(j\\omega)\\,=\\,\\left|G(j\\omega)\\right|e^{j\\phi}\n$$  \n\nwhere $\\left|G(j\\omega)\\right|$ represents the magnitude and $\\phi$ represents the angle of $G(j\\omega)$ ; that is,  \n\n$$\n\\phi\\,=\\,\\underbrace{\\big/G(j\\omega)}_{\\mathrm{Teal}\\;\\mathrm{part}\\;\\mathrm{of}\\;G(j\\omega)}\\_\n$$  \n\nThe angle $\\phi$ may be negative, positive, or zero. Similarly, we obtain the following expression for $G(-j\\omega)$ :  \n\n$$\nG(-j\\omega)\\,=\\,\\left|G(-j\\omega)\\right|e^{-j\\phi}\\,=\\,\\left|G(j\\omega)\\right|e^{-j\\phi}\n$$  \n\nThen, noting that  \n\n$$\na={\\it\\Delta}-\\frac{X|G(j\\omega)|e^{-j\\phi}}{2j},~~~~~\\bar{a}=\\frac{X|G(j\\omega)|e^{j\\phi}}{2j}\n$$  \n\nEquation (7–4) can be written  \n\n$$\n\\begin{array}{r l}{\\lefteqn{y_{\\mathrm{ss}}(t)\\,=\\,X\\bigl|G(j\\omega)\\bigr|\\,\\frac{e^{j(\\omega t+\\phi)}\\,-\\,e^{-j(\\omega t+\\phi)}}{2j}}}\\\\ &{=\\,X\\bigl|G(j\\omega)\\bigr|\\,\\sin{(\\omega t\\,+\\,\\phi)}}\\\\ &{=\\,Y\\sin{(\\omega t\\,+\\,\\phi)}}\\end{array}\n$$  \n\n![](images/ac704118206d1109e6b415a5010d794711a40273e8a06739ebc479dbf4ccacfb.jpg)  \nFigure 7–2 Input and output sinusoidal signals.   \nOutput y(t)=Ysin ( vt+f)  \n\nwhere $Y\\,=\\,{\\cal X}\\big|G(j\\omega)\\big|$ .We see that a stable, linear, time-invariant system subjected to a   \nsinusoidal input will, at steady state, have a sinusoidal output of the same frequency as   \nthe input. But the amplitude and phase of the output will, in general, be different from   \nthose of the input. In fact, the amplitude of the output is given by the product of that of $\\left|\\bar{G}(j\\omega)\\right|$ , while the phase angle differs from that of the input by the amount   \n$\\phi\\,=\\,\\bigl/G\\bigl(j\\omega\\bigr).$ ). An example of input and output sinusoidal signals is shown in Figure 7–2. On the basis of this, we obtain this important result: For sinusoidal inputs,  \n\n$$\n\\begin{array}{l l}{\\displaystyle{\\left|G(j\\omega)\\right|=\\left|\\frac{Y(j\\omega)}{X(j\\omega)}\\right|=\\operatorname*{amplitude}_{\\mathrm{input}~s i n u s o i d}\\mathrm{of~the~output~sinuisoid~to~the~}}}\\\\ {\\displaystyle{\\left|G(j\\omega)=\\left|\\frac{Y(j\\omega)}{X(j\\omega)}=\\operatorname*{phase}_{\\mathrm{the~input}~s i n u s o i d}\\right|\\mathrm{ne~output~sinusoid~with~respect~}}}\\end{array}\n$$  \n\nHence, the steady-state response characteristics of a system to a sinusoidal input can be obtained directly from  \n\n$$\n\\frac{Y(j\\omega)}{X(j\\omega)}=G(j\\omega)\n$$  \n\nThe function $G(j\\omega)$ is called the sinusoidal transfer function. It is the ratio of $Y(j\\omega)$ to $X(j\\omega)$ ,is a complex quantity, and can be represented by the magnitude and phase angle with frequency as a parameter.The sinusoidal transfer function of any linear system is obtained by substituting $j\\omega$ for $s$ in the transfer function of the system.  \n\nAs already mentioned in Chapter 6,a positive phase angle is called phase lead,and a negative phase angle is called phase lag.A network that has phase-lead characteristics is called a lead network, while a network that has phase-lag characteristics is called a lag network.  \n\n# EXAMPLE 7–1  \n\nConsider the system shown in Figure 7–3.The transfer function $G(s)$ is  \n\n$$\nG(s)={\\frac{K}{T s\\,+\\,1}}\n$$  \n\nFor the sinusoidal input $x(t)\\,=\\,X\\sin\\omega t$ , the steady-state output $y_{\\mathrm{ss}}(t)$ can be found as follows: Substituting $j\\omega$ for $s$ in $G(s)$ yields  \n\n$$\nG(j\\omega)=\\frac{K}{j T\\omega\\,+\\,1}\n$$  \n\nFigure 7–3 First-order system.  \n\n$$\n\\xrightarrow{x}\\cdots\\Biggl[\\frac{X}{T s+1}\\xrightarrow{y}\n$$  \n\nThe amplitude ratio of the output to the input is  \n\n$$\n\\left|G(j\\omega)\\right|=\\frac{K}{\\sqrt{1+T^{2}\\omega^{2}}}\n$$  \n\nwhile the phase angle $\\phi$ is  \n\n$$\n\\phi\\,=\\,\\left/G(j\\omega)\\right.=-\\tan^{-1}T\\omega\n$$  \n\nThus, for the input $x(t)\\,=\\,X\\sin\\omega t$ , the steady-state output $y_{\\mathrm{ss}}(t)$ can be obtained from Equation (7–5) as follows:  \n\n$$\ny_{\\mathrm{ss}}(t)=\\frac{X K}{\\sqrt{1\\,+\\,T^{2}\\omega^{2}}}\\sin\\!\\left(\\omega t\\,-\\,\\tan^{-1}T\\omega\\right)\n$$  \n\nFrom Equation (7–6), it can be seen that for small $\\omega$ , the amplitude of the steady-state output $y_{\\mathrm{ss}}(t)$ is almost equal to $K$ times the amplitude of the input.The phase shift of the output is small for small $\\omega$ . For large $\\omega$ , the amplitude of the output is small and almost inversely proportional to $\\omega$ .The phase shift approaches $-90^{\\circ}$ as $\\omega$ approaches infinity.This is a phase-lag network.  \n\nEXAMPLE 7–2 Consider the network given by  \n\n$$\nG(s)=\\frac{s+\\displaystyle\\frac{1}{T_{1}}}{s+\\displaystyle\\frac{1}{T_{2}}}\n$$  \n\nDetermine whether this network is a lead network or lag network.  \n\nFor the sinusoidal input $x(t)\\,=\\,X\\sin\\omega t$ , the steady-state output $y_{\\mathrm{ss}}(t)$ can be found as follows: Since  \n\n$$\nG(j\\omega)=\\frac{j\\omega+\\displaystyle\\frac{1}{T_{1}}}{j\\omega+\\displaystyle\\frac{1}{T_{2}}}=\\frac{T_{2}(1+T_{1}j\\omega)}{T_{1}\\big(1+T_{2}j\\omega\\big)}\n$$  \n\nwe have  \n\n$$\n\\bigl|G(j\\omega)\\bigr|\\,=\\frac{T_{2}\\sqrt{1\\,+\\,T_{1}^{2}\\omega^{2}}}{T_{1}\\sqrt{1\\,+\\,T_{2}^{2}\\omega^{2}}}\n$$  \n\nand  \n\n$$\n\\phi\\,=\\,\\left/G(j\\omega)\\right.=\\,\\tan^{-1}T_{1}\\omega\\,-\\,\\tan^{-1}T_{2}\\omega\n$$  \n\nThus the steady-state output is  \n\n$$\ny_{\\mathrm{ss}}(t)\\,=\\frac{X T_{2}\\sqrt{1\\,+\\,T_{1}^{2}\\omega^{2}}}{T_{1}\\sqrt{1\\,+\\,T_{2}^{2}\\omega^{2}}}\\,\\mathrm{sin}(\\omega t\\,+\\,\\tan^{-1}T_{1}\\omega\\,-\\,\\tan^{-1}T_{2}\\omega)\n$$  \n\nFrom this expression, we find that if $T_{1}>T_{2}$ ,then $\\tan^{-1}T_{1}\\omega\\,-\\,\\tan^{-1}T_{2}\\omega>0.$ .Thus, if $T_{1}>T_{2}$ ,then the network is a lead network. If $T_{1}<T_{2}$ ,then the network is a lag network.  \n\nPresenting Frequency-Response Characteristics in Graphical Forms. The sinusoidal transfer function, a complex function of the frequency $\\omega$ , is characterized by its magnitude and phase angle, with frequency as the parameter. There are three commonly used representations of sinusoidal transfer functions:  \n\n1. Bode diagram or logarithmic plot   \n2. Nyquist plot or polar plot   \n3. Log-magnitude-versus-phase plot (Nichols plots)  \n\nWe shall discuss these representations in detail in this chapter. We shall include the MATLAB approach to obtain Bode diagrams, Nyquist plots, and Nichols plots.  \n\nOutline of the Chapter. Section 7–1 has presented introductory material on the frequency response. Section 7–2 presents Bode diagrams of various transfer-function systems. Section 7–3 treats polar plots of transfer functions. Section 7–4 discusses log-magnitude-versus-phase plots. Section 7–5 gives a detailed account of the Nyquist stability criterion. Section 7–6 discusses the stability analysis based on the Nyquist stability criterion. Section 7–7 introduces measures of relative stability analysis. Section 7–8 presents a method for obtaining the closed-loop frequency response from the open-loop frequency response by use of the M and N circles. The Nichols chart is introduced here. Section 7–9 treats experimental determination of transfer functions. Section 7–10 presents introductory aspects of control systems design by the frequency-response approach. Sections 7–11, 7–12, and 7–13 give detailed accounts of lead compensation, lag compensation, and lag–lead compensation techniques, respectively.  \n\n# 7–2 BODE DIAGRAMS  \n\nBode Diagrams or Logarithmic Plots. A Bode diagram consists of two graphs: One is a plot of the logarithm of the magnitude of a sinusoidal transfer function; the other is a plot of the phase angle; both are plotted against the frequency on a logarithmic scale.  \n\nThe standard representation of the logarithmic magnitude of $G(j\\omega)$ is $20\\log\\left|G(j\\omega)\\right|$ ,where the base of the logarithm is 10.The unit used in this representation of the magnitude is the decibel, usually abbreviated dB. In the logarithmic representation, the curves are drawn on semilog paper, using the log scale for frequency and the linear scale for either magnitude (but in decibels) or phase angle (in degrees). (The frequency range of interest determines the number of logarithmic cycles required on the abscissa.)  \n\nThe main advantage of using the Bode diagram is that multiplication of magnitudes can be converted into addition. Furthermore, a simple method for sketching an approximate log-magnitude curve is available. It is based on asymptotic approximations. Such approximation by straight-line asymptotes is sufficient if only rough information on the frequency-response characteristics is needed. Should the exact curve be desired, corrections can be made easily to these basic asymptotic plots. Expanding the low-frequency range by use of a logarithmic scale for the frequency is highly advantageous, since characteristics at low frequencies are most important in practical systems. Although it is not possible to plot the curves right down to zero frequency because of the logarithmic frequency $\\operatorname{log}0\\,=\\,-\\infty)$ ), this does not create a serious problem.  \n\nNote that the experimental determination of a transfer function can be made simple if frequency-response data are presented in the form of a Bode diagram.  \n\nBasic Factors of $G(j\\omega)H(j\\omega)$ .As stated earlier, the main advantage in using the logarithmic plot is the relative ease of plotting frequency-response curves. The basic factors that very frequently occur in an arbitrary transfer function $G(j\\omega)H(j\\omega)$ are  \n\n1. Gain $K$  \n\n2. Integral and derivative factors $\\left(j\\omega\\right)^{\\mp1}$   \n3. First-order factors $(1\\,+\\,j\\omega T)^{\\mp1}$ D  \n4. Quadratic factors $\\left[1\\,+\\,2\\zeta\\bigl(j\\omega/\\omega_{n}\\bigr)\\,+\\,\\bigl(j\\omega/\\omega_{n}\\bigr)^{2}\\right]^{\\mp1}$  \n\nOnce we become familiar with the logarithmic plots of these basic factors, it is possible to utilize them in constructing a composite logarithmic plot for any general form of $G(j\\omega)H(j\\omega)$ by sketching the curves for each factor and adding individual curves graphically, because adding the logarithms of the gains corresponds to multiplying them together.  \n\nThe Gain K. A number greater than unity has a positive value in decibels, while a number smaller than unity has a negative value.The log-magnitude curve for a constant gain $K$ is a horizontal straight line at the magnitude of $20\\log K$ decibels.The phase angle of the gain $K$ is zero. The effect of varying the gain $K$ in the transfer function is that it raises or lowers the log-magnitude curve of the transfer function by the corresponding constant amount, but it has no effect on the phase curve.  \n\nA number–decibel conversion line is given in Figure 7–4. The decibel value of any number can be obtained from this line. As a number increases by a factor of 10, the corresponding decibel value increases by a factor of 20. This may be seen from the following:  \n\n$$\n20\\log(K\\times10)=20\\log K\\,+\\,20\n$$  \n\nSimilarly,  \n\n$$\n20\\log\\bigl(K\\,\\times\\,10^{n}\\bigr)\\,=\\,20\\log K\\,+\\,20n\n$$  \n\n![](images/b02394fdf8e72c36620038778578a045dc51b6c9fe8ed6a12ff4cf80a4624a3c.jpg)  \nFigure 7–4 Number–decibel conversion line.  \n\nNote that, when expressed in decibels, the reciprocal of a number differs from its value only in sign; that is, for the number $K$ ,  \n\n$$\n20\\log K=-20\\log{\\frac{1}{K}}\n$$  \n\nIntegral and Derivative Factors $(j\\omega)^{\\mp1}$ .The logarithmic magnitude of $1/j\\omega$ in decibels is  \n\n$$\n20\\log\\left|\\frac{1}{j\\omega}\\right|\\,=-20\\log\\omega\\,\\mathrm{dB}\n$$  \n\nThe phase angle of $1/j\\omega$ is constant and equal to $-90^{\\circ}$ .  \n\nIn Bode diagrams, frequency ratios are expressed in terms of octaves or decades.An octave is a frequency band from $\\omega_{1}$ to $2\\omega_{1}$ , where $\\omega_{1}$ is any frequency value.A decade is a frequency band from $\\omega_{1}$ to $10\\omega_{1}$ , where again $\\omega_{1}$ is any frequency. (On the logarithmic scale of semilog paper, any given frequency ratio can be represented by the same horizontal distance. For example, the horizontal distance from $\\omega=1$ to $\\omega\\,=\\,10$ is equal to that from $\\omega=3$ to $\\omega\\,=\\,30.$ .)  \n\nIf the log magnitude $-20\\log\\omega$ dB is plotted against $\\omega$ on a logarithmic scale, it is a straight line.To draw this straight line,we need to locate one point $(0\\,\\mathrm{dB},\\omega=1)$ )on it.Since  \n\n$$\n(-20\\log10\\omega)\\;\\mathrm{dB}\\,=\\,(-20\\log\\omega\\,-\\,20)\\;\\mathrm{dB}\n$$  \n\nthe slope of the line is $-20$ dB 'decade (or $^{-6}$ dB 'octave). Similarly, the log magnitude of $j\\omega$ in decibels is  \n\n$$\n20\\log|j\\omega|\\,=\\,20\\log\\omega\\,\\mathrm{dB}\n$$  \n\nThe phase angle of $j\\omega$ is constant and equal to $90^{\\circ}$ .The log-magnitude curve is a straight line with a slope of $20\\ \\mathrm{dB}$ 'decade. Figures 7–5(a) and (b) show frequency-response curves for $1/j\\omega$ and $j\\omega$ , respectively. We can clearly see that the differences in the frequency responses of the factors $1/j\\omega$ and $j\\omega$ lie in the signs of the slopes of the logmagnitude curves and in the signs of the phase angles. Both log magnitudes become equal to $\\mathrm{0~dB}$ at $\\omega=1$ .  \n\nIf the transfer function contains the factor $(1/j\\omega)^{n}$ or $(j\\omega)^{n}$ , the log magnitude becomes, respectively,  \n\n$$\n20\\log\\left|{\\frac{1}{(j\\omega)^{n}}}\\right|=-n\\,\\times\\,20\\log|j\\omega|\\,=\\,-20n\\log\\omega\\,\\mathrm{dB}\n$$  \n\nor  \n\n$$\n20\\log\\left|(j\\omega)^{n}\\right|\\,=\\,n\\,\\times\\,20\\log|j\\omega|\\,=\\,20n\\log\\omega\\;\\mathrm{dB}\n$$  \n\nThe slopes of the log-magnitude curves for the factors $(1/j\\omega)^{n}$ and ${\\left(j\\omega\\right)}^{n}$ are thus $-20n$ dB 'decade and $20n$ dB 'decade, respectively.The phase angle of $(1/j\\omega)^{n}$ is equal to $-90^{\\circ}\\times n$ over the entire frequency range, while that of $(j\\omega)^{n}$ is equal to $90^{\\circ}\\times n$ over the entire frequency range. The magnitude curves will pass through the point $\\mathrm{~0~dB},\\omega=1)$ .  \n\n![](images/e212cb207962036e382df85c782544c0d473e8a30eb64b9c5b3bbc75cc45b36b.jpg)  \nFigure 7–5 (a) Bode diagram of $G(j\\omega)\\,=\\,1/j\\omega;$ ;(b) Bode diagram of $G(j\\omega)\\,=\\,j\\omega$ .  \n\nFirst-Order Factors $(1\\,+\\,j\\omega T)^{\\mp1}$ .The log magnitude of the first-order factor $1/(1\\,+\\,j\\omega T)$ is  \n\n$$\n20\\,\\mathrm{log}\\left|\\frac{1}{1\\,+\\,j\\omega T}\\right|\\,=\\,-20\\,\\mathrm{log}\\,\\sqrt{1\\,+\\,\\omega^{2}T^{2}}\\,\\mathrm{dB}\n$$  \n\nFor low frequencies, such that $\\omega\\ll1/T$ , the log magnitude may be approximated by  \n\n$$\n-20\\log\\sqrt{1\\,+\\,\\omega^{2}T^{2}}\\doteq-20\\log1\\,=\\,0\\:\\mathrm{dB}\n$$  \n\nThus, the log-magnitude curve at low frequencies is the constant 0-dB line. For high frequencies, such that $\\omega\\gg1/T$ ,  \n\n$$\n-20\\log{\\sqrt{1\\,+\\,\\omega^{2}T^{2}}}\\doteq-20\\log\\omega T\\,\\mathrm{dB}\n$$  \n\nThis is an approximate expression for the high-frequency range. At $\\omega=1/T$ , the log magnitude equals 0 dB; at $\\omega\\,=\\,10/T$ , the log magnitude is $-20$ dB. Thus, the value of $-20\\log\\omega T$ dB decreases by $20\\,\\mathrm{dB}$ for every decade of $\\omega.\\operatorname{For}\\omega\\gg1/T$ ,the log-magnitude curve is thus a straight line with a slope of $-20\\;\\mathrm{dB}$ /decade (or $^{-6}$ dB /octave).  \n\nOur analysis shows that the logarithmic representation of the frequency-response curve of the factor $1/(1\\,+\\,j\\omega T)$ can be approximated by two straight-line asymptotes, one a straight line at 0 dB for the frequency range $0<\\omega<1/T$ and the other a straight line with slope $-20$ dB/decade (or $^{-6}$ dB 'octave) for the frequency range $1/T<\\omega<\\infty.$ .The exact log-magnitude curve, the asymptotes, and the exact phase-angle curve are shown in Figure 7–6.  \n\nThe frequency at which the two asymptotes meet is called the corner frequency or break frequency. For the factor $1/(1\\,+\\,j\\omega T)$ , the frequency $\\omega=1/T$ is the corner frequency, since at $\\omega=1/T$ the two asymptotes have the same value. (The low-frequency asymptotic expression at $\\omega=1/T$ is $20\\ \\mathrm{log}\\ 1\\ \\mathrm{dB}=\\,0$ dB, and the high-frequency asymptotic expression at $\\omega=1/T$ is also $20\\log1\\ \\mathrm{dB}=0$ dB.) The corner frequency divides the frequency-response curve into two regions: a curve for the low-frequency region and a curve for the high-frequency region.The corner frequency is very important in sketching logarithmic frequency-response curves.  \n\n![](images/2c5d79a81e4f11e27ba7eb86dfbc80fb378bf4eb88ad1dec759b049692c65e8a.jpg)  \nFigure 7–6 Log-magnitude curve, together with the asymptotes, and phase-angle curve of $1/(1\\,+\\,j\\omega T)$ .  \n\nThe exact phase angle $\\phi$ of the factor $1/(1\\,+\\,j\\omega T)$ is  \n\n$$\n\\phi=-\\mathrm{tan}^{-1}\\omega T\n$$  \n\nAt zero frequency, the phase angle is $0^{\\circ}$ .At the corner frequency, the phase angle is  \n\n$$\n\\phi=-\\mathsf{t a n}^{-1}\\frac{T}{T}=-\\mathsf{t a n}^{-1}1=-45^{\\circ}\n$$  \n\nAt infinity, the phase angle becomes $-90^{\\circ}$ . Since the phase angle is given by an inversetangent function, the phase angle is skew symmetric about the inflection point at $\\phi=-45^{\\circ}$ .  \n\nThe error in the magnitude curve caused by the use of asymptotes can be calculated. 1 The maximum error occurs at the corner frequency and is approximately equal to $^{-3}$ dB, since  \n\n$$\n-20\\log\\sqrt{1\\,+\\,1}\\,+\\,20\\log1\\,=\\,-10\\log2\\,=\\,-3.03\\;\\mathrm{dB}\n$$  \n\nThe error at the frequency one octave below the corner frequency—that is, at $\\omega=1/(2T)$ —is  \n\n$$\n-20\\log{\\sqrt{\\frac{1}{4}+1}}\\,+\\,20\\log1=-20\\log{\\frac{\\sqrt{5}}{2}}=-0.97\\,\\mathrm{dB}\n$$  \n\nThe error at the frequency one octave above the corner frequency—that is,at $\\omega=2/T.$ —is  \n\n$$\n-20\\log{\\sqrt{2^{2}+1}}\\,+\\,20\\log2\\,=-20\\log{\\frac{\\sqrt{5}}{2}}=-0.97\\,{\\mathrm{dB}}\n$$  \n\nThus, the error at one octave below or above the corner frequency is approximately equal to $-1$ dB. Similarly, the error at one decade below or above the corner frequency is approximately $-0.04$ dB. The error in decibels involved in using the asymptotic expression for the frequency-response curve of $1/(1\\,+\\,j\\omega T)$ is shown in Figure 7–7. The error is symmetric with respect to the corner frequency.  \n\nSince the asymptotes are quite easy to draw and are sufficiently close to the exact curve, the use of such approximations in drawing Bode diagrams is convenient in establishing the general nature of the frequency-response characteristics quickly with a minimum amount of calculation and may be used for most preliminary design work. If accurate frequency-response curves are desired, corrections may easily be made by referring to the curve given in Figure 7–7. In practice, an accurate frequency-response curve can be drawn by introducing a correction of $3\\;\\mathrm{dB}$ at the corner frequency and a correction of 1 dB at points one octave below and above the corner frequency and then connecting these points by a smooth curve.  \n\nNote that varying the time constant $T$ shifts the corner frequency to the left or to the right, but the shapes of the log-magnitude and the phase-angle curves remain the same.  \n\nThe transfer function $1/(1\\,+\\,j\\omega T)$ has the characteristics of a low-pass filter. For frequencies above $\\omega=1/T$ , the log magnitude falls off rapidly toward $-\\infty,$ .This is essentially due to the presence of the time constant. In the low-pass filter, the output can follow a sinusoidal input faithfully at low frequencies. But as the input frequency is increased, the output cannot follow the input because a certain amount of time is required for the system to build up in magnitude. Thus, at high frequencies, the amplitude of the output approaches zero and the phase angle of the output approaches $-90^{\\circ}$ .Therefore, if the input function contains many harmonics, then the low-frequency components are reproduced faithfully at the output, while the highfrequency components are attenuated in amplitude and shifted in phase.Thus, a firstorder element yields exact, or almost exact, duplication only for constant or slowly varying phenomena.  \n\nAn advantage of the Bode diagram is that for reciprocal factors—for example, the factor $1+j\\omega T$ —the log-magnitude and the phase-angle curves need only be changed in sign, since  \n\n$$\n20\\log|1\\,+\\,j\\omega T|\\,=\\,-20\\log\\left|\\frac{1}{1\\,+\\,j\\omega T}\\right|\n$$  \n\n![](images/09f687184bf724e6f05ad0b9fd36525b9e4d4ee1d7d702105d4d0960f8fe17aa.jpg)  \nFigure 7–7 Log-magnitude error in the asymptotic expression of the frequency-response curve of $1/(1\\,+\\,j\\omega T)$ .  \n\nand  \n\n$$\n\\left/1\\,+\\,j\\omega T=\\tan^{-1}\\omega T=-\\left/\\frac{1}{1\\,+\\,j\\omega T}\\right.\n$$  \n\nThe corner frequency is the same for both cases. The slope of the high-frequency asymptote of $1+j\\omega T$ is $20\\,\\mathrm{dB}$ 'decade,and the phase angle varies from $0^{\\circ}$ to $90^{\\circ}$ as the frequency $\\omega$ is increased from zero to infinity. The log-magnitude curve, together with the asymptotes, and the phase-angle curve for the factor $1+j\\omega T$ are shown in Figure 7–8.  \n\nTo draw a phase curve accurately, we have to locate several points on the curve.The phase angles of $(1\\,+\\,j\\omega T)^{\\mp1}$ are  \n\n$$\n\\begin{array}{c c c c}{\\displaystyle\\mp45^{\\circ}}&{\\mathrm{~at~}}&{\\displaystyle\\omega=\\frac{1}{T}}\\\\ {\\displaystyle\\mp26.6^{\\circ}}&{\\mathrm{~at~}}&{\\displaystyle\\omega=\\frac{1}{2T}}\\\\ {\\displaystyle\\mp5.7^{\\circ}}&{\\mathrm{~at~}}&{\\displaystyle\\omega=\\frac{1}{10T}}\\\\ {\\displaystyle\\mp63.4^{\\circ}}&{\\mathrm{~at~}}&{\\displaystyle\\omega=\\frac{2}{T}}\\\\ {\\displaystyle\\mp84.3^{\\circ}}&{\\mathrm{~at~}}&{\\displaystyle\\omega=\\frac{10}{T}}\\end{array}\n$$  \n\nFor the case where a given transfer function involves terms like $\\left(1\\,+\\,j\\omega T\\right)^{\\mp n}$ ,a similar asymptotic construction may be made.The corner frequency is still at $\\omega=1/T$ , and the asymptotes are straight lines.The low-frequency asymptote is a horizontal straight line  \n\n![](images/f2bb7d82219053cbd9b257fa5a8519513e41f2bf0051ecfb53edeb9bb1aaf37b.jpg)  \nFigure 7–8 Log-magnitude curve, together with the asymptotes, and phase-angle curve for $1+j\\omega T$ .  \n\nat $\\mathrm{0~dB}$ , while the high-frequency asymptote has the slope of $-20n$ dB 'decade or $20n$ dB 'decade. The error involved in the asymptotic expressions is $n$ times that for $(1+j\\omega T)^{\\mp1}$ .The phase angle is $n$ times that of $(\\bar{1}+j\\omega T)^{\\bar{\\mp}1}$ at each frequency point.  \n\nQuadratic Factors $\\left[1~+~2\\zeta\\!\\left(j\\omega/\\omega_{n}\\right)~+~\\left(j\\omega/\\omega_{n}\\right)^{2}\\right]^{\\mp1}$ .Control systems often possess quadratic factors of the form  \n\n$$\nG(j\\omega)\\,=\\,\\frac{1}{1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}\n$$  \n\nIf $\\zeta>1$ , this quadratic factor can be expressed as a product of two first-order factors with real poles. If $0<\\zeta<1$ , this quadratic factor is the product of two complexconjugate factors.Asymptotic approximations to the frequency-response curves are not accurate for a factor with low values of $\\zeta.$ . This is because the magnitude and phase of the quadratic factor depend on both the corner frequency and the damping ratio $\\zeta$ .  \n\nThe asymptotic frequency-response curve may be obtained as follows: Since  \n\n$$\n20\\log\\left|\\frac{1}{1+2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}}\\right|\\,=\\,-20\\log\\sqrt{\\bigg(1\\,-\\,\\frac{\\omega^{2}}{\\omega_{n}^{2}}\\bigg)^{2}\\,+\\,\\bigg(2\\zeta\\,\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}}\n$$  \n\nfor low frequencies such that $\\omega\\ll\\omega_{n}$ , the log magnitude becomes  \n\n$$\n-20\\log1=0\\,\\mathrm{dB}\n$$  \n\nThe low-frequency asymptote is thus a horizontal line at 0 dB. For high frequencies such that $\\omega\\gg\\omega_{n}$ , the log magnitude becomes  \n\n$$\n-20\\log{\\frac{\\omega^{2}}{\\omega_{n}^{2}}}=-40\\log{\\frac{\\omega}{\\omega_{n}}}\\,\\mathrm{dB}\n$$  \n\nThe equation for the high-frequency asymptote is a straight line having the slope $-40$ dB 'decade, since  \n\n$$\n-40\\log{\\frac{10\\omega}{\\omega_{n}}}=-40\\,-\\,40\\log{\\frac{\\omega}{\\omega_{n}}}\n$$  \n\nThe high-frequency asymptote intersects the low-frequency one at $\\omega\\,=\\,\\omega_{n}$ , since at this frequency  \n\n$$\n-40\\log{\\frac{\\omega_{n}}{\\omega_{n}}}=-40\\log1\\,=\\,0\\,\\mathrm{dB}\n$$  \n\nThis frequency, $\\omega_{n}$ , is the corner frequency for the quadratic factor considered.  \n\nThe two asymptotes just derived are independent of the value of $\\zeta$ . Near the frequency $\\omega\\,=\\,\\omega_{n}$ , a resonant peak occurs, as may be expected from Equation (7–7). The damping ratio $\\zeta$ determines the magnitude of this resonant peak. Errors obviously exist in the approximation by straight-line asymptotes. The magnitude of the error depends on the value of $\\zeta.$ . It is large for small values of $\\zeta.$ . Figure 7–9 shows the exact log-magnitude curves, together with the straight-line asymptotes and the exact phase-angle curves for the quadratic factor given by Equation (7–7) with several values of rection at a sufficient number of frequency points may be obtained from Figure $\\zeta$ . If corrections are desired in the asymptotic curves, the necessary amounts of corD$^{7-9}$ .The phase angle of the quadratic factor $\\dot{\\left[1\\right.}+\\left.2\\zeta\\(j\\dot{\\omega}/\\omega_{n})\\,+\\,\\left(j\\omega/\\omega_{n}\\right)^{2}\\right]^{-1}$ is  \n\n![](images/f5e9c15c8ef56dc59c0e396c4afc6a643690dc380409fe37a2a3d3b37fa0e019.jpg)  \nFigure 7–9 Log-magnitude curves, together with the asymptotes, and phase-angle curves of the quadratic transfer function given by Equation (7–7).  \n\n$$\n\\phi=\\left[\\frac{1}{1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}}=-\\tan^{-1}\\!\\left[\\,\\frac{2\\zeta\\,\\frac{\\omega}{\\omega_{n}}}{1\\,-\\,\\bigg(\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}}\\right]\\,\n$$  \n\nThe phase angle is a function of both $\\omega$ and $\\zeta$ . At $\\omega\\,=\\,0$ , the phase angle equals $0^{\\circ}$ . At the corner frequency $\\omega\\,=\\,\\omega_{n}$ , the phase angle is $-90^{\\circ}$ regardless of $\\zeta$ , since  \n\n$$\n\\phi=-\\mathsf{t a n}^{-1}\\left(\\frac{2\\zeta}{0}\\right)=-\\mathsf{t a n}^{-1}\\infty=-90^{\\circ}\n$$  \n\nAt $\\omega=\\infty$ , the phase angle becomes $-180^{\\circ}$ . The phase-angle curve is skew symmetric about the inflection point—the point where $\\phi=-90^{\\circ}$ .There are no simple ways to sketch such phase curves.We need to refer to the phase-angle curves shown in Figure 7–9.  \n\nThe frequency-response curves for the factor  \n\n$$\n1\\,+\\,2\\zeta\\!\\left(\\,j\\,\\frac{\\omega}{\\omega_{n}}\\right)\\,+\\,\\left(\\,j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}\n$$  \n\ncan be obtained by merely reversing the sign of the log magnitude and that of the phase angle of the factor  \n\n$$\n\\frac{1}{1\\,+\\,2\\zeta\\!\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}\n$$  \n\nTo obtain the frequency-response curves of a given quadratic transfer function, we must first determine the value of the corner frequency $\\omega_{n}$ and that of the damping ratio $\\zeta$ .Then, by using the family of curves given in Figure 7–9, the frequency-response curves can be plotted.  \n\n# The Resonant Frequency $\\pmb{\\omega}_{r}$ and the Resonant Peak Value $M_{r}$ .The magnitude of  \n\n$$\nG(j\\omega)=\\frac{1}{1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}\n$$  \n\nis  \n\n$$\n\\left|G(j\\omega)\\right|=\\frac{1}{\\sqrt{\\left(1\\,-\\frac{\\omega^{2}}{\\omega_{n}^{2}}\\right)^{2}\\,+\\,\\left(2\\zeta\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}}\n$$  \n\nIf $\\left|G(j\\omega)\\right|$ has a peak value at some frequency, this frequency is called the resonant frequency. Since the numerator of $\\left|G(j\\omega)\\right|$ is constant, a peak value of $\\left|G(j\\omega)\\right|$ will occur when  \n\n$$\ng(\\omega)\\,=\\,\\left(1\\,-\\,\\frac{\\omega^{2}}{\\omega_{n}^{2}}\\right)^{2}\\,+\\,\\left(2\\zeta\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}\n$$  \n\nis a minimum. Since Equation (7–10) can be written  \n\n$$\ng(\\omega)\\,=\\,\\bigg[\\frac{\\omega^{2}\\,-\\,\\omega_{n}^{2}\\big(1\\,-\\,2\\zeta^{2}\\big)}{\\omega_{n}^{2}}\\bigg]^{2}\\,+\\,4\\zeta^{2}\\big(1\\,-\\,\\zeta^{2}\\big)\n$$  \n\nthe minimum value of $g(\\omega)$ occurs at $\\omega\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,2\\zeta^{2}}$ .Thus the resonant frequency $\\omega_{r}$ is  \n\n$$\n\\omega_{r}=\\,\\omega_{n}\\sqrt{1\\,-\\,2\\zeta^{2}}\\,,\\qquad\\mathrm{for}\\,0\\le\\zeta\\le\\,0.707\n$$  \n\nAs the damping ratio $\\zeta$ approaches zero, the resonant frequency approaches $\\omega_{n}$ . For $0<\\zeta\\le0.707$ , the resonant frequency $\\omega_{r}$ is less than the damped natural frequency $\\omega_{d}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}$ ,which is exhibited in the transient response. From Equation (7–12), it can be seen that for $\\zeta>0.707$ ,there is no resonant peak.The magnitude $\\left|G(j\\omega)\\right|$ decreases monotonically with increasing frequency $\\omega$ . (The magnitude is less than 0 dB for all values of $\\omega>0,$ . Recall that, for $0.7<\\zeta<1$ , the step response is oscillatory, but the oscillations are well damped and are hardly perceptible.)  \n\n![](images/f037043ca8fc8efbd9ab147122cf300fd578fccb5bf3ca1ef6200fb363848f22.jpg)  \nFigure 7–10 $M_{r}$ -versus$\\cdot\\zeta$ curve for the second-order system $\\dot{1}/[1\\,+\\,2\\zeta\\bigl(j\\omega/\\omega_{n}\\bigr)\\,+$ A BD$\\left(j\\omega/\\omega_{n}\\right)^{2}]$ .  \n\nFor $0\\le\\zeta\\le0.707$ , the magnitude of the resonant peak, $M_{r}=|G(j\\omega_{r})|$ , can be found from Equations (7–12) and (7–9). For $0\\le\\zeta\\le0.707$ ,  \n\n$$\nM_{r}=\\left.\\left|G(j\\omega)\\right|_{\\mathrm{max}}=\\left|G(j\\omega_{r})\\right|=\\frac{1}{2\\zeta\\sqrt{1-\\zeta^{2}}}\n$$  \n\nFor $\\zeta>0.707$ ,  \n\n$$\nM_{r}=1\n$$  \n\nAs $\\zeta$ approaches zero, $M_{r}$ approaches infinity.This means that if the undamped system is excited at its natural frequency, the magnitude of $G(j\\omega)$ becomes infinity. The relationship between $M_{r}$ and $\\zeta$ is shown in Figure 7–10.  \n\nThe phase angle of $G(j\\omega)$ at the frequency where the resonant peak occurs can be obtained by substituting Equation (7–12) into Equation (7–8). Thus, at the resonant frequency $\\omega_{r}$ ,  \n\n$$\n\\underline{{{/G(j\\omega_{r})}}}=-\\tan^{-1}\\frac{\\sqrt{1-2\\zeta^{2}}}{\\zeta}=-90^{\\circ}\\,+\\,\\sin^{-1}\\frac{\\zeta}{\\sqrt{1-\\zeta^{2}}}\n$$  \n\nGeneral Procedure for Plotting Bode Diagrams. MATLAB provides an easy way to plot Bode diagrams. (The MATLAB approach is presented later in this section.) Here, however, we consider the case where we want to draw Bode diagrams manually without using MATLAB.  \n\nFirst rewrite the sinusoidal transfer function $G(j\\omega)H(j\\omega)$ as a product of basic factors discussed above.Then identify the corner frequencies associated with these basic factors. Finally,draw the asymptotic log-magnitude curves with proper slopes between the corner frequencies. The exact curve, which lies close to the asymptotic curve, can be obtained by adding proper corrections.  \n\nThe phase-angle curve of $G(j\\omega)H(j\\omega)$ can be drawn by adding the phase-angle curves of individual factors.  \n\nThe use of Bode diagrams employing asymptotic approximations requires much less time than other methods that may be used for computing the frequency response of a transfer function. The ease of plotting the frequency-response curves for a given transfer function and the ease of modification of the frequency-response curve as compensation is added are the main reasons why Bode diagrams are very frequently used in practice.  \n\n$$\nG(j\\omega)=\\frac{10(j\\omega+3)}{(j\\omega)(j\\omega+2)\\big[(j\\omega)^{2}+j\\omega+2\\big]}\n$$  \n\nMake corrections so that the log-magnitude curve is accurate.  \n\nTo avoid any possible mistakes in drawing the log-magnitude curve,it is desirable to put $G(j\\omega)$ in the following normalized form, where the low-frequency asymptotes for the first-order factors and the second-order factor are the 0-dB line:  \n\n$$\nG(j\\omega)=\\frac{7.5\\bigg(\\displaystyle\\frac{j\\omega}{3}+1\\bigg)}{(j\\omega)\\bigg(\\displaystyle\\frac{j\\omega}{2}+1\\bigg)\\bigg[\\displaystyle\\frac{(j\\omega)^{2}}{2}+\\frac{j\\omega}{2}+1\\bigg]}\n$$  \n\nThis function is composed of the following factors:  \n\n$$\n7.5,~~~~~(j\\omega)^{-1},~~~~~1+j\\frac{\\omega}{3},~~~~~\\bigg(1+j\\frac{\\omega}{2}\\bigg)^{-1},~~~~~\\bigg[1+j\\frac{\\omega}{2}+\\frac{(j\\omega)^{2}}{2}\\bigg]^{-1}\n$$  \n\nThe corner frequencies of the third, fourth, and fifth terms are $\\omega\\,=\\,3,\\,\\omega\\,=\\,2$ , and $\\omega=\\sqrt{2}$ ,respectively. Note that the last term has the damping ratio of 0.3536.  \n\nTo plot the Bode diagram, the separate asymptotic curves for each of the factors are shown 1 1 in Figure 7–11.The composite curve is then obtained by algebraically adding the individual curves, also shown in Figure 7–11. Note that when the individual asymptotic curves are added at each frequency, the slope of the composite curve is cumulative. Below $\\omega={\\sqrt{2}}$ 2, the plot has the slope of $-20$ dB 'decade.At the first corner frequency $\\omega=\\sqrt{2}$ 2, the slope changes to $-60$ dB 'decade and continues to the next corner frequency $\\omega=2$ , where the slope becomes $-80$ dB 'decade. At the last corner frequency $\\omega=3$ , the slope changes to $-60$ dB 'decade.  \n\nOnce such an approximate log-magnitude curve has been drawn, the actual curve can be obtained by adding corrections at each corner frequency and at frequencies one octave below and above the corner frequencies. For first-order factors $(1+\\,j\\omega T)^{\\mp1}$ , the corrections are $\\pm3$ dB at the corner frequency and $\\pm1$ dB at the frequencies one octave below and above the corner frequency. Corrections necessary for the quadratic factor are obtained from Figure 7–9.The exact log-magnitude curve for $G(j\\omega)$ is shown by a dashed curve in Figure 7–11.  \n\nNote that any change in the slope of the magnitude curve is made only at the corner frequencies of the transfer function $G(j\\omega)$ . Therefore, instead of drawing individual magnitude curves and adding them up, as shown, we may sketch the magnitude curve without sketching individual curves. We may start drawing the lowest-frequency portion of the straight line (that is, the straight line with the slope $-20\\ \\mathrm{dB}$ 'decade for $\\omega<\\sqrt{2}$ ). As the frequency is increased, we get the effect of the complex-conjugate poles (quadratic term) at the corner frequency $\\omega={\\sqrt{2}}$ .The complex-conjugate poles cause the slopes of the magnitude curve to change from $-20$ to $-60\\;\\mathrm{dB}$ 'decade. At the next corner frequency, $\\omega=2$ , the effect of the pole is to change the slope to $-80$ dB 'decade. Finally, at the corner frequency $\\omega=3$ , the effect of the zero is to change the slope from $-80$ to $-60\\;\\mathrm{dB}$ 'decade.  \n\nFor plotting the complete phase-angle curve, the phase-angle curves for all factors have to be sketched. The algebraic sum of all phase-angle curves provides the complete phase-angle curve, as shown in Figure 7–11.  \n\n![](images/81729fa49083464edf3717c7fbcb48bd2a6a2d054eac6ea5a10058db62164bad.jpg)  \nFigure 7–11 Bode diagram of the system considered in Example 7–3.  \n\nMinimum-Phase Systems and Nonminimum-Phase Systems. Transfer functions having neither poles nor zeros in the right-half $s$ plane are minimum-phase transfer functions, whereas those having poles and/or zeros in the right-half $s$ plane are nonminimum-phase transfer functions. Systems with minimum-phase transfer functions are called minimum-phase systems, whereas those with nonminimum-phase transfer functions are called nonminimum-phase systems.  \n\nFor systems with the same magnitude characteristic, the range in phase angle of the minimum-phase transfer function is minimum among all such systems, while the range in phase angle of any nonminimum-phase transfer function is greater than this minimum.  \n\nIt is noted that for a minimum-phase system, the transfer function can be uniquely determined from the magnitude curve alone. For a nonminimum-phase system, this is not the case. Multiplying any transfer function by all-pass filters does not alter the magnitude curve, but the phase curve is changed.  \n\nConsider as an example the two systems whose sinusoidal transfer functions are, respectively,  \n\n$$\nG_{1}(j\\omega)=\\frac{1\\,+\\,j\\omega T}{1\\,+\\,j\\omega T_{1}},\\qquad G_{2}(j\\omega)=\\frac{1\\,-\\,j\\omega T}{1\\,+\\,j\\omega T_{1}},\\qquad0<T<T_{1}\n$$  \n\n![](images/c512db1f1fc6e9d91ef5663e44a9485d64fa7e50ba86dcb8bd6a989f54fbb0a0.jpg)  \nFigure 7–12 Pole–zero configurations of a minimum-phase system $G_{1}(s)$ and nonminimum-phase system $G_{2}(s)$ .  \n\nThe pole–zero configurations of these systems are shown in Figure 7–12.The two sinusoidal transfer functions have the same magnitude characteristics, but they have different phase-angle characteristics, as shown in Figure 7–13.These two systems differ from each other by the factor  \n\n$$\nG(j\\omega)=\\frac{1-j\\omega T}{1+j\\omega T}\n$$  \n\nThe magnitude of the factor $(1\\,-\\,j\\omega T)/(1\\,+\\,j\\omega T)$ is always unity. But the phase angle equals $-2\\tan^{-1}\\omega T$ and varies from $0^{\\circ}$ to $-180^{\\circ}$ as $\\omega$ is increased from zero to infinity.  \n\nAs stated earlier, for a minimum-phase system, the magnitude and phase-angle characteristics are uniquely related. This means that if the magnitude curve of a system is specified over the entire frequency range from zero to infinity, then the phase-angle curve is uniquely determined, and vice versa. This, however, does not hold for a nonminimum-phase system.  \n\nNonminimum-phase situations may arise in two different ways. One is simply when a system includes a nonminimum-phase element or elements. The other situation may arise in the case where a minor loop is unstable.  \n\nFor a minimum-phase system, the phase angle at $\\omega=\\infty$ becomes $-90^{\\circ}(q\\mathrm{~-~}p)$ ,where $p$ and $q$ are the degrees of the numerator and denominator polynomials of the transfer function, respectively. For a nonminimum-phase system, the phase angle at $\\omega=\\infty$ differs from $-90^{\\circ}(q\\mathrm{~-~}p)$ . In either system, the slope of the log-magnitude curve at $\\omega=\\infty$ is equal to $-20(q\\mathrm{~-~}p)$ dB 'decade. It is therefore possible to detect whether the system is minimum phase by examining both the slope of the high-frequency asymptote of the log-magnitude curve and the phase angle at $\\omega=\\infty.$ If the slope of the log-magnitude curve as $\\omega$ approaches infinity is $-20(q\\mathrm{~-~}p)$ dB 'decade and the phase angle at $\\omega=\\infty$ is equal to $-90^{\\circ}(q\\mathrm{~-~}p)$ , then the system is minimum phase.  \n\n![](images/0ac440e44622a73d577aefaee209099a982f645cb41ccd5e87c2d75065e52ca5.jpg)  \nFigure 7–13 Phase-angle characteristics of the systems $G_{1}(s)$ and $G_{2}(s)$ shown in Figure 7–12.   \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nNonminimum-phase systems are slow in responding because of their faulty behavior at the start of a response. In most practical control systems, excessive phase lag should be carefully avoided.In designing a system,if fast speed of response is of primary importance, we should not use nonminimum-phase components. (A common example of nonminimum-phase elements that may be present in control systems is transport lag or dead time.)  \n\nIt is noted that the techniques of frequency-response analysis and design to be presented in this and the next chapter are valid for both minimum-phase and nonminimum-phase systems.  \n\nTransport Lag. Transport lag, which is also called dead time, is of nonminimumphase behavior and has an excessive phase lag with no attenuation at high frequencies. Such transport lags normally exist in thermal, hydraulic, and pneumatic systems.  \n\nConsider the transport lag given by  \n\n$$\nG(j\\omega)\\,=\\,e^{-j\\omega T}\n$$  \n\nThe magnitude is always equal to unity, since  \n\n$$\n\\left|G(j\\omega)\\right|\\,=\\,\\left|\\cos\\omega T\\,-\\,j\\sin\\omega T\\right|\\,=\\,1\n$$  \n\nTherefore, the log magnitude of the transport lag $e^{-j\\omega T}$ is equal to 0 dB. The phase angle of the transport lag is  \n\n$$\n\\begin{array}{l}{{\\displaystyle\\left/G(j\\omega)=-\\omega T\\ \\ \\ \\ \\ \\mathrm{(radians)}\\right.}}\\\\ {{\\displaystyle=-57.3\\;\\omega T\\ \\ \\ \\ \\ \\ \\mathrm{(degrees)}}}\\end{array}\n$$  \n\nThe phase angle varies linearly with the frequency $\\omega,$ .The phase-angle characteristic of transport lag is shown in Figure 7–14.  \n\n![](images/5313428bf911a96488746152e72058f611f1cc6098aa8e4b451adfce34157567.jpg)  \nFigure 7–14 Phase-angle characteristic of transport lag.  \n\n# EXAMPLE 7–4  \n\nDraw the Bode diagram of the following transfer function:  \n\n$$\nG(j\\omega)=\\frac{e^{-j\\omega L}}{1+j\\omega T}\n$$  \n\nThe log magnitude is  \n\n$$\n{\\begin{array}{r l}&{20\\log\\left|G(j\\omega)\\right|\\,=\\,20\\log\\left|e^{-j\\omega L}\\right|\\,+\\,20\\log\\left|{\\frac{1}{1\\,+\\,j\\omega T}}\\right|}\\\\ &{=\\,0\\,+\\,20\\log\\left|{\\frac{1}{1\\,+\\,j\\omega T}}\\right|}\\end{array}}\n$$  \n\nThe phase angle of $G(j\\omega)$ is  \n\n$$\n\\begin{array}{c}{{\\displaystyle{\\frac{\\slash G(j\\omega)}{\\slash{G}(j\\omega)}=\\ \\Big\\langle e^{-j\\omega L}\\ +\\ \\middle/{\\frac{1}{1\\ +\\ j\\omega T}}}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{=-\\omega L-\\ \\tan^{-1}{\\omega T}}}}\\end{array}\n$$  \n\nThe log-magnitude and phase-angle curves for this transfer function with $L\\,=\\,0.5$ and $T=1$ are shown in Figure 7–15.  \n\n![](images/aa0829ca390f9e4ee783f6401bd838fab93a8248c35330067fb98f1065c3ee0d.jpg)  \nFigure 7–15 Bode diagram for the system $e^{-\\bar{j}\\omega L}/(1\\,+\\,j\\omega T)$ with $L=0.5$ and $T=1$  \n\nRelationship between System Type and Log-Magnitude Curve. Consider the unity-feedback control system.The static position, velocity, and acceleration error constants describe the low-frequency behavior of type 0, type 1, and type 2 systems, respectively. For a given system, only one of the static error constants is finite and significant. (The larger the value of the finite static error constant, the higher the loop gain is as $\\omega$ approaches zero.)  \n\nThe type of the system determines the slope of the log-magnitude curve at low frequencies. Thus, information concerning the existence and magnitude of the steadystate error of a control system to a given input can be determined from the observation of the low-frequency region of the log-magnitude curve.  \n\nDetermination of Static Position Error Constants. Consider the unity-feedback control system shown in Figure 7–16. Assume that the open-loop transfer function is given by  \n\n$$\nG(s)=\\frac{K\\bigl(T_{a}s\\,+\\,1\\bigr)\\bigl(T_{b}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{m}s\\,+\\,1\\bigr)}{s^{N}\\bigl(T_{1}s\\,+\\,1\\bigr)\\bigl(T_{2}s\\,+\\,1\\bigr)\\cdots\\bigl(T_{p}s\\,+\\,1\\bigr)}\n$$  \n\nor  \n\n$$\nG(j\\omega)\\,=\\frac{K\\bigl(T_{a}j\\omega\\,+\\,1\\bigr)\\bigl(T_{b}j\\omega\\,+\\,1\\bigr)\\cdots\\bigl(T_{m}j\\omega\\,+\\,1\\bigr)}{(j\\omega)^{N}\\bigl(T_{1}j\\omega\\,+\\,1\\bigr)\\bigl(T_{2}j\\omega\\,+\\,1\\bigr)\\cdots\\bigl(T_{p}j\\omega\\,+\\,1\\bigr)}\n$$  \n\nFigure 7–17 shows an example of the log-magnitude plot of a type 0 system. In such a system, the magnitude of $G(j\\omega)$ equals $K_{p}$ at low frequencies, or  \n\n$$\n\\operatorname*{lim}_{\\omega\\to0}G(j\\omega)\\,=\\,K\\,=\\,K_{p}\n$$  \n\nIt follows that the low-frequency asymptote is a horizontal line at $20\\log K_{p}$ dB.  \n\n![](images/810d97e0a1249d4e1c9f5980e1dbbd3a7c198e69e77113bfdc89be03dcaf15f1.jpg)  \nFigure 7–16 Unity-feedback control system.  \n\n![](images/d983d2e4a5657a67d3fd3a13b13ed38c0b501200c07320a5a6d6e4d087812eda.jpg)  \nFigure 7–17 Log-magnitude curve of a type 0 system.  \n\nDetermination of Static Velocity Error Constants. Consider the unity-feedback control system shown in Figure 7–16.Figure 7–18 shows an example of the log-magnitude plot of a type 1 system. The intersection of the initial –20 -dB 'decade segment (or its extension) with the line $\\omega=1$ has the magnitude $20\\log K_{v}$ .This may be seen as follows: In a type 1 system  \n\n$$\nG(j\\omega)=\\frac{K_{v}}{j\\omega},~~~~~\\mathrm{for}\\;\\omega\\;\\ll\\;1\n$$  \n\nThus,  \n\n$$\n20\\log\\left|\\frac{K_{v}}{j\\omega}\\right|_{\\omega=1}=20\\log K_{v}\n$$  \n\nThe intersection of the initial –20 -dB 'decade segment (or its extension) with the 0-dB line has a frequency numerically equal to $K_{v}$ . To see this, define the frequency at this intersection to be $\\omega_{1}$ ; then  \n\n$$\n\\left|\\frac{K_{v}}{j\\omega_{1}}\\right|\\,=\\,1\n$$  \n\nor  \n\n$$\nK_{v}\\,=\\,\\omega_{1}\n$$  \n\nAs an example, consider the type 1 system with unity feedback whose open-loop transfer function is  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(J s\\,+\\,F)}\n$$  \n\nIf we define the corner frequency to be $\\omega_{2}$ and the frequency at the intersection of the –40 -dB 'decade segment (or its extension) with 0-dB line to be $\\omega_{3}$ , then  \n\n$$\n\\omega_{2}=\\frac{F}{J},~~~~~~\\omega_{3}^{2}=\\frac{K}{J}\n$$  \n\n![](images/5fe9b60b032de816a1ebbcf26ef774f1ea7313b9b3cb8449c4dd46ea3d9bedb5.jpg)  \nFigure 7–18 Log-magnitude curve of a type 1 system.   \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nSince  \n\n$$\n\\omega_{1}=K_{v}=\\frac{K}{F}\n$$  \n\nit follows that  \n\n$$\n\\omega_{1}\\omega_{2}=\\omega_{3}^{2}\n$$  \n\nor  \n\n$$\n\\frac{\\omega_{1}}{\\omega_{3}}=\\frac{\\omega_{3}}{\\omega_{2}}\n$$  \n\nOn the Bode diagram,  \n\n$$\n\\log\\omega_{1}\\,-\\,\\log\\omega_{3}\\,=\\,\\log\\omega_{3}\\,-\\,\\log\\omega_{2}\n$$  \n\nThus, the $\\omega_{3}$ point is just midway between the $\\omega_{2}$ and $\\omega_{1}$ points.The damping ratio $\\zeta$ of the system is then  \n\n$$\n\\zeta={\\frac{F}{2{\\sqrt{K J}}}}={\\frac{\\omega_{2}}{2\\omega_{3}}}\n$$  \n\nDetermination of Static Acceleration Error Constants. Consider the unityfeedback control system shown in Figure 7–16. Figure 7–19 shows an example of the log-magnitude plot of a type 2 system. The intersection of the initial $-40{\\mathrm{-dB}}$ 'decade segment (or its extension) with the $\\omega=1$ line has the magnitude of $20\\log K_{a}$ . Since at low frequencies  \n\n$$\nG(j\\omega)\\,=\\,\\frac{K_{a}}{(j\\omega)^{2}},\\;\\;\\;\\;\\;\\;\\mathrm{for}\\;\\omega\\,\\ll\\,1\n$$  \n\nit follows that  \n\n$$\n20\\log\\left|\\frac{K_{a}}{(j\\omega)^{2}}\\right|_{\\omega=1}=20\\log K_{a}\n$$  \n\n![](images/90aa4b77adc991f3375757d3b1462cb980712ee81dc350ee01acf1da5dfc0cbf.jpg)  \nFigure 7–19 Log-magnitude curve of a type 2 system.  \n\nThe frequency $\\omega_{a}$ at the intersection of the initial –40 -dB 'decade segment (or its extension) with the 0-dB line gives the square root of $K_{a}$ numerically.This can be seen from the following:  \n\n$$\n20\\log\\left|\\frac{K_{a}}{(j\\omega_{a})^{2}}\\right|\\,=\\,20\\log1\\,=\\,0\n$$  \n\nwhich yields  \n\n$$\n\\omega_{a}=\\sqrt{K_{a}}\n$$  \n\nPlotting Bode Diagrams with MATLAB. The command bode computes magnitudes and phase angles of the frequency response of continuous-time, linear, timeinvariant systems.  \n\nWhen the command bode (without left-hand arguments) is entered in the computer, MATLAB produces a Bode plot on the screen. Most commonly used bode commands are  \n\nbode(num,den)bode(num,den,w)bode(A,B,C,D) bode(A,B,C,D,w) bode(A,B,C,D,iu,w) bode(sys)  \n\nWhen invoked with left-hand arguments, such as  \n\n$$\n[\\mathrm{mag,phase,w}]=\\mathrm{bode(num,den,w)}\n$$  \n\nbode returns the frequency response of the system in matrices mag ,phase , and w. No plot is drawn on the screen.The matrices mag and phase contain magnitudes and phase angles of the frequency response of the system, evaluated at user-specified frequency points.The phase angle is returned in degrees.The magnitude can be converted to decibels with the statement  \n\n$$\n\\mathrm{magdB}=20^{*}|\\mathrm{og}10(\\mathrm{mag})\n$$  \n\nOther Bode commands with left-hand arguments are  \n\n$$\n\\begin{array}{l}{{[\\mathrm{mag,phase,w}]=\\mathrm{bode(num,den)}}}\\\\ {{[\\mathrm{mag,phase,w}]=\\mathrm{bode(num,den,w)}}}\\\\ {{[\\mathrm{mag,phase,w}]=\\mathrm{bode(A,B,C,D)}}}\\\\ {{[\\mathrm{mag,phase,w}]=\\mathrm{bode(A,B,C,D,w)}}}\\\\ {{[\\mathrm{mag,phase,w}]=\\mathrm{bode(A,B,C,D,iu,w)}}}\\\\ {{[\\mathrm{mag,phase,w}]=\\mathrm{bode(sys)}}}\\end{array}\n$$  \n\nTo specify the frequency range, use the command logspace(d1,d2) or logspace $\\left(\\mathrm{d}1,\\mathrm{d}2,\\boldsymbol{\\mathsf{n}}\\right)$ . logspace(d1,d2) generates a vector of 50 points logarithmically equally spaced between decades $1\\,0^{{\\mathrm{d}}1}$ and $10^{\\mathrm{d2}}$ . (50 points include both endpoints. There are 48 points between the endpoints.) To generate 50 points between 0.1 rad 'sec and 100 rad 'sec, enter the command  \n\n$$\n\\mathsf{w}=\\mathsf{l o g s p a c e}(-1,2)\n$$  \n\nlogspace(dl,d2,n) generates $n$ points logarithmically equally spaced between decades $10^{\\mathrm{d1}}$ and $10^{\\mathrm{d2}}$ . ( $\\boldsymbol{n}$ points include both endpoints.) For example, to generate 100 points including both endpoints between 1 rad 'sec and 1000 rad 'sec,enter the following command:  \n\n$$\n\\mathsf{w}=|\\mathrm{ogspace}(0,3,100)\n$$  \n\nTo incorporate the user-specified frequency points when plotting Bode diagrams, the bode command must include the frequency vector w, such as bode(num,den,w) and [mag,phase,w] $=$ bode(A,B,C,D,w) .  \n\nEXAMPLE 7–5 Consider the following transfer function:  \n\n$$\nG(s)={\\frac{25}{s^{2}+4s+25}}\n$$  \n\nPlot a Bode diagram for this transfer function. When the system is defined in the form  \n\n$$\nG(s)={\\frac{\\operatorname{num}(s)}{\\operatorname{den}(s)}}\n$$  \n\nuse the command bode(num,den) to draw the Bode diagram. [When the numerator and denominator contain the polynomial coefficients in descending powers of $s$ ,bode(num,den) draws the Bode diagram.] MATLAB Program 7–1 shows a program to plot the Bode diagram for this system.The resulting Bode diagram is shown in Figure 7–20.  \n\n![](images/72316f2407a4f082ba1986c411c0b6486b953e55aeb7db32ef0db12f64ba3f37.jpg)  \n\n![](images/2a6483db2518784389218d49a5798cfe7f46242d145d1d5858a013fc729f954f.jpg)  \nFigure 7–20 Bode diagram of $G(s)={\\frac{25}{s^{2}+4s+25}}.$ .  \n\n#  \n\nConsider the system shown in Figure 7–21.The open-loop transfer function is  \n\n$$\nG(s)\\,=\\frac{9\\Bigl(s^{2}\\,+\\,0.2s\\,+\\,1\\Bigr)}{s\\Bigl(s^{2}\\,+\\,1.2s\\,+\\,9\\Bigr)}\n$$  \n\nPlot a bode diagram. MATLAB Program 7–2 plots a Bode diagram for the system.The resulting plot is shown in   \nFigure 7–22. The frequency range in this case is automatically determined to be from 0.01 to   \n10 rad 'sec.  \n\n![](images/006111934d4674c051e5b6b1065cc07c1ab067e57e081d898d9ba69f644b39b9.jpg)  \n\nFigure 7–21 Control system.  \n\n![](images/2ff22d8a32f6271fb793e11b479c7fb696d5e252ab66cc0255f189ef642057da.jpg)  \n\n![](images/16ef5966d1bb3096376ebd628c3f951af2bfd36862bbc65a18d24a196dcfad3a.jpg)  \n\nFigure 7–22   \nBode diagram of   \n$G(s)=\\frac{\\bar{9}\\bigl(s^{2}+0.2s\\,+\\,1\\bigr)}{s\\bigl(s^{2}+1.2s\\,+\\,9\\bigr)}.$  \n\nIf it is desired to plot the Bode diagram from 0.01 to 1000 rad 'sec, enter the following command:  \n\n$$\n\\mathsf{w}=\\mathsf{l o g s p a c e}(-2,3,100)\n$$  \n\nThis command generates 100 points logarithmically equally spaced between 0.01 and 100 rad 'sec. (Note that such a vector wspecifies the frequencies in radians per second at which the frequency response will be calculated.)  \n\nIf we use the command bode(num,den,w)  \n\nthen the frequency range is as the user specified, but the magnitude range and phase-angle range will be automatically determined. See MATLAB Program 7–3 and the resulting plot in Figure 7–23.  \n\nMATLAB Program 7–3   \n$\\mathsf{n u m}=[9\\phantom{-}1.8$ 9];   \n$\\mathrm{den}=[1\\;\\;\\;1\\,.2\\;\\;9\\;\\;0]$ ;  \nw$=$ logspace(-2,3,100);   \nbode(num,den,w)  \ntitle('Bode Diagram of $\\mathrm{G}(\\mathsf{s})=9(\\mathsf{s}\\land2+0.2\\mathsf{s}+1)/[\\mathsf{s}(\\mathsf{s}\\land2+1.2\\mathsf{s}+9)]^{\\intercal})$   \nFigure 7–23   \nBode diagram of   \n$G(s)=\\frac{\\breve{9(s^{2}+0.2s\\,+\\,1)}}{s(s^{2}+1.2s\\,+\\,9)}.$  \n\n![](images/ebf3f125e30a2826ecf4fed13037e498c99ad934675acf6d14e7e44e780f44b3.jpg)  \n\nObtaining Bode Diagrams of Systems Defined in State Space. Consider the system defined by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}+\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ( $n$ -vector)  \n\n$\\mathbf{y}=$ output vector ( m-vector)   \n${\\textbf{u}}=$ control vector ( $r$ -vector)   \n$\\textbf{A}=$ state matrix ( ${\\boldsymbol{n}}\\times{\\boldsymbol{n}}$ matrix)   \n$\\textbf{B}=$ control matrix ( ${\\boldsymbol{n}}\\times{\\boldsymbol{r}}$ matrix)   \n$\\mathbf{C}=$ output matrix ( $\\vert m\\times n$ matrix)   \n$\\textbf{D}=$ direct transmission matrix ( ${\\boldsymbol{m}}\\times{\\boldsymbol{r}}$ matrix)  \n\nA Bode diagram for this system may be obtained by entering the command  \n\nor others listed earlier in this section.  \n\nThe command bode(A,B,C,D) produces a series of Bode plots, one for each input of the system, with the frequency range automatically determined. (More points are used when the response is changing rapidly.)  \n\nThe command bode(A,B,C,D,iu) , where iu is the i th input of the system, produces the Bode diagrams from the input iu to all the outputs $\\left({\\widehat{y}}_{1},y_{2},\\dots,y_{m}\\right)$ of the system, with a frequency range automatically determined. (The scalar iu is an index into the inputs of the system and specifies which input is to be used for plotting Bode diagrams). If the control vector $\\mathbf{u}$ has three inputs such that  \n\n$$\n\\mathbf{u}={\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\\\ {u_{3}}\\end{array}\\right]}\n$$  \n\nthen iu must be set to either 1, 2, or 3.  \n\nIf the system has only one input $u$ , then either of the following commands may be used:  \n\nor  \n\nEXAMPLE 7–7 Consider the following system:  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{c c}{0}&{1}\\\\ {-25}&{-4}\\end{array}\\right]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\left[\\begin{array}{c}{0}\\\\ {25}\\end{array}\\right]\\!\\!u}\\\\ {y=[1}&{0]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nThis system has one input $u$ and one output $y$ . By using the command  \n\nand entering MATLAB Program 7–4 into the computer, we obtain the Bode diagram shown in Figure 7–24.  \n\n![](images/ac70aa8d1a2f2aede690566d819d65d55afb3bf9deb33371922263c3dd324974.jpg)  \n\nIf we replace the command bode(A,B,C,D) in MATLAB Program 7–4 with  \n\nthen MATLAB will produce the Bode diagram identical to that shown in Figure 7–24.  \n\n# 7–3 POLAR PLOTS  \n\n![](images/c1189f033b21d9487daab5191ee0a92a7d8d9d7813d7d501d7ddff324ae2b85f.jpg)  \nFigure 7–24 Bode diagram of the system considered in Example 7–7.   \nFrequency (rad/sec)  \n\nThe polar plot of a sinusoidal transfer function $G(j\\omega)$ is a plot of the magnitude of $G(j\\omega)$ versus the phase angle of $G(j\\omega)$ on polar coordinates as $\\omega$ is varied from zero to infinity.Thus, the polar plot is the locus of vectors $\\left|G(j\\omega)\\right|\\underbar{|G(j\\omega)|}$ as $\\omega$ is varied from zero to infinity. Note that in polar plots a positive (negative) phase angle is measured counterclockwise (clockwise) from the positive real axis.The polar plot is often called the Nyquist plot.An example of such a plot is shown in Figure 7–25. Each point on the polar plot of $G(j\\omega)$ represents the terminal point of a vector at a particular value of $\\omega$ . In the polar plot, it is important to show the frequency graduation of the locus. The projections of $G(j\\omega)$ on the real and imaginary axes are its real and imaginary components.  \n\n![](images/89cfaae37e6ca8e97696dc9beb32cb0185081afca6c20e0cf14ee1bdb0643420.jpg)  \n\nMATLAB may be used to obtain a polar plot $G(j\\omega)$ or to obtain $\\left|G(j\\omega)\\right|$ and $\\underline{{\\langle G(j\\omega)\\rangle}}$ accurately for various values of $\\omega$ in the frequency range of interest.  \n\nAn advantage in using a polar plot is that it depicts the frequency-response characteristics of a system over the entire frequency range in a single plot. One disadvantage is that the plot does not clearly indicate the contributions of each individual factor of the open-loop transfer function.  \n\nIntegral and Derivative Factors $(j\\omega)^{\\mp1}$ .The polar plot of $G(j\\omega)\\,=\\,1/j\\omega$ is the negative imaginary axis, since  \n\n$$\nG(j\\omega)=\\frac{1}{j\\omega}=-j\\,\\frac{1}{\\omega}=\\frac{1}{\\omega}\\,\\frac{-90^{\\circ}}{-90^{\\circ}}\n$$  \n\nThe polar plot of $G(j\\omega)\\,=\\,j\\omega$ is the positive imaginary axis.  \n\nFirst-Order Factors $(1\\,+\\,j\\omega T)^{\\mp1}$ .For the sinusoidal transfer function  \n\n$$\nG(j\\omega)=\\frac{1}{1\\,+\\,j\\omega T}=\\frac{1}{\\sqrt{1\\,+\\,\\omega^{2}T^{2}}}\\:\\bigg\\lbrack{-\\mathrm{tan}^{-1}\\omega T}\\:\n$$  \n\nthe values of $G(j\\omega)$ at $\\omega\\,=\\,0$ and $\\omega=1/T$ are, respectively,  \n\n$$\nG(j0)\\,=\\,1\\,\\underline{{{/0^{\\circ}}}}\\quad\\mathrm{and}\\quad G\\biggl(j\\,\\frac{1}{T}\\biggr)\\,=\\,\\frac{1}{\\sqrt{2}}\\,\\,\\underline{{{/-45^{\\circ}}}}\n$$  \n\nIf $\\omega$ approaches infinity, the magnitude of $G(j\\omega)$ approaches zero and the phase angle approaches $-90^{\\circ}$ .The polar plot of this transfer function is a semicircle as the frequency $\\omega$ is varied from zero to infinity, as shown in Figure 7–26(a).The center is located at 0.5 on the real axis, and the radius is equal to 0.5.  \n\nTo prove that the polar plot of the first-order factor $G(j\\omega)\\,=\\,1/(1\\,+\\,j\\omega T)$ is a semicircle, define  \n\n$$\nG(j\\omega)\\,=\\,X\\,+\\,j Y\n$$  \n\n![](images/1d4e580b524d243abb2e33b4f3cae5bcfab652381d64a5292878e8c339bcc775.jpg)  \nFigure 7–26 (a) Polar plot of $1/(1\\,+\\,j\\omega T)$ ; (b) plot of $G(j\\omega)$ in $X{-}Y$ plane.   \n(a)  \n\n![](images/20bfdca30f3ce56ebddb58f89a946b9de504c7e8afa07fbcefd2641511bba134.jpg)  \n(b)  \n\nwhere  \n\n$$\n\\begin{array}{l}{{X=\\displaystyle\\frac{1}{1\\,+\\,\\omega^{2}T^{2}}=\\mathrm{{\\real\\,part\\,of}\\,}G(j\\omega)}}\\\\ {{Y=\\displaystyle\\frac{-\\omega T}{1\\,+\\,\\omega^{2}T^{2}}=\\mathrm{{\\imaginary\\,part\\,of}\\,}G(j\\omega)}}\\end{array}\n$$  \n\n![](images/ce431be91ae3b12f4f48417e72ac6485b14b72e2ba555d050cd4112e5afbbf6f.jpg)  \n\nFigure 7–27 Polar plot of $1+j\\omega T$ .  \n\nThen we obtain  \n\n$$\n\\left(X-\\frac{1}{2}\\right)^{2}+Y^{2}=\\left(\\frac{1}{2}\\frac{1-\\omega^{2}T^{2}}{1+\\omega^{2}T^{2}}\\right)^{2}+\\left(\\frac{-\\omega T}{1+\\omega^{2}T^{2}}\\right)^{2}=\\left(\\frac{1}{2}\\right)^{2}\n$$  \n\nThus, in the $X{-}Y$ plane $G(j\\omega)$ is a circle with center at $\\begin{array}{r}{X=\\frac{1}{2},Y=0}\\end{array}$ and with radius $\\frac{1}{2}$ ,as shown in Figure 7–26(b). The lower semicircle corresponds to $0\\leq\\omega\\leq\\infty$ , and the upper semicircle corresponds to $-\\infty\\leq\\omega\\leq0$ .  \n\nThe polar plot of the transfer function $1+j\\omega T$ is simply the upper half of the straight line passing through point (1,0) in the complex plane and parallel to the imaginary axis, as shown in Figure 7–27. The polar plot of $1+j\\omega T$ has an appearance completely different from that of $1/(1\\,+\\,j\\omega T)$ .  \n\nQuadratic Factors $\\left[1~+~2\\zeta(j\\omega/\\omega_{n})~+~(j\\omega/\\omega_{n})^{2}\\right]^{\\mp1}$ .The low- and high-frequency portions of the polar plot of the following sinusoidal transfer function  \n\n$$\nG(j\\omega)\\,=\\frac{1}{\\,1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}},\\qquad\\mathrm{for}\\,\\zeta>0\n$$  \n\nare given, respectively, by  \n\n$$\n\\operatorname*{lim}_{\\omega\\to0}G(j\\omega)=1\\big/0^{\\circ}\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{\\omega\\to\\infty}G(j\\omega)=0\\big/-180^{\\circ}\n$$  \n\nThe polar plot of this sinusoidal transfer function starts at $1\\,\\mathrm{\\textmu}^{\\mathrm{1}}$ and ends at $0\\,{\\Big/}{\\underline{{-180^{\\circ}}}}$ as $\\omega$ increases from zero to infinity. Thus, the high-frequency portion of $G(j\\omega)$ is tangent to the negative real axis.  \n\n![](images/1fee274d3f5e70c4d01d3d65c6a56c965ed93a4c357842c07ebb2162b677fb66.jpg)  \n\n# Figure 7–28 Polar plots of $\\frac{1}{1\\,+\\,2\\zeta\\!\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}$ for $\\zeta>0$ .  \n\nExamples of polar plots of the transfer function just considered are shown in Figure 7–28. The exact shape of a polar plot depends on the value of the damping ratio $\\zeta$ , but the general shape of the plot is the same for both the underdamped case ($1>\\zeta>0$ )and overdamped case $(\\zeta>1)$ ).  \n\nFor the underdamped case at $\\omega\\,=\\,\\omega_{n}$ , we have $G(j\\omega_{n})\\,=\\,1/(j2\\zeta)$ , and the phase angle at $\\omega\\,=\\,\\omega_{n}$ is $-90^{\\circ}$ . Therefore, it can be seen that the frequency at which the $G(j\\omega)$ locus intersects the imaginary axis is the undamped natural frequency $\\omega_{n}$ . In the polar plot, the frequency point whose distance from the origin is maximum corresponds to the resonant frequency $\\omega_{r}$ . The peak value of $G(j\\omega)$ is obtained as the ratio of the magnitude of the vector at the resonant frequency $\\omega_{r}$ to the magnitude of the vector at $\\omega\\,=\\,0.$ The resonant frequency $\\omega_{r}$ is indicated in the polar plot shown in Figure 7–29.  \n\nFor the overdamped case, as $\\zeta$ increases well beyond unity, the $G(j\\omega)$ locus approaches a semicircle. This may be seen from the fact that, for a heavily damped system, the characteristic roots are real, and one is much smaller than the other. Since, for sufficiently large $\\zeta$ , the effect of the larger root (larger in the absolute value) on the response becomes very small, the system behaves like a first-order one.  \n\n![](images/34ad120ccd184951591fa4e812bdd11cce86e6219eae1353fdfc8e6510cdf840.jpg)  \nFigure 7–29 Polar plot showing the resonant peak and resonant frequency $\\omega_{r}$ .  \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nPolar plot of $1\\,+\\,2\\dot{\\zeta}\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)^{2}\\,\\mathrm{for}\\;\\zeta>0.$  \n\n![](images/b2527a72eb97b6bffb53ca0b954f56c1ccf88f1b417c8f3ae66bdd52353ded4d.jpg)  \n\nNext, consider the following sinusoidal transfer function:  \n\n$$\n\\begin{array}{c}{\\displaystyle G(j\\omega)\\,=\\,1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}\\\\ {\\displaystyle=\\,\\left(1\\,-\\,\\frac{\\omega^{2}}{\\omega_{n}^{2}}\\right)\\,+\\,j\\bigg(\\frac{2\\zeta\\omega}{\\omega_{n}}\\bigg)}\\end{array}\n$$  \n\nThe low-frequency portion of the curve is  \n\n$$\n\\operatorname*{lim}_{\\omega\\to0}G(j\\omega)\\,=\\,1\\,\\underline{{{/0^{\\circ}}}}\n$$  \n\nand the high-frequency portion is  \n\n$$\n\\operatorname*{lim}_{\\omega\\to\\infty}G(j\\omega)\\,=\\,\\infty\\,\\underline{{{/180^{\\circ}}}}\n$$  \n\nSince the imaginary part of $G(j\\omega)$ is positive for $\\omega>0$ and is monotonically increasing, and the real part of $G(j\\omega)$ is monotonically decreasing from unity, the general shape of the polar plot of $G(j\\omega)$ is as shown in Figure 7–30. The phase angle is between $0^{\\circ}$ and $180^{\\circ}$ .  \n\n# EXAMPLE 7–8  \n\nConsider the following second-order transfer function:  \n\n$$\nG(s)={\\frac{1}{s(T s\\,+\\,1)}}\n$$  \n\nSketch a polar plot of this transfer function.  \n\nSince the sinusoidal transfer function can be written  \n\n$$\nG(j\\omega)=\\frac{1}{j\\omega(1\\,+\\,j\\omega T)}=-\\,\\frac{T}{1\\,+\\,\\omega^{2}T^{2}}\\,-\\,j\\,\\frac{1}{\\omega\\big(1\\,+\\,\\omega^{2}T^{2}\\big)}\n$$  \n\nthe low-frequency portion of the polar plot becomes  \n\n$$\n\\operatorname*{lim}_{\\omega\\to0}G(j\\omega)\\,=\\,-T\\,-\\,j\\infty\n$$  \n\nand the high-frequency portion becomes  \n\n$$\n\\operatorname*{lim}_{\\omega\\to\\infty}G(j\\omega)\\,=\\,0\\,-\\,j0\n$$  \n\nFigure 7–31   \nPolar plot of   \n$1/\\[j\\omega(1\\,+\\,j\\omega T)].$ .  \n\n![](images/438c68ed86feff0960be163e299d122fff81e3d4fb14bc3daca9ce169ad6b040.jpg)  \n\nThe general shape of the polar plot of $G(j\\omega)$ is shown in Figure 7–31. The $G(j\\omega)$ plot is asymptotic to the vertical line passing through the point $(-T,0)$ . Since this transfer function involves an integrator $(1/s)$ ,the general shape of the polar plot differs substantially from those of second-order transfer functions that do not have an integrator.  \n\nEXAMPLE 7–9 Obtain the polar plot of the following transfer function:  \n\n$$\nG(j\\omega)=\\frac{e^{-j\\omega L}}{1+j\\omega T}\n$$  \n\nSince $G(j\\omega)$ can be written  \n\n$$\nG(j\\omega)\\,=\\,\\bigl(e^{-j\\omega L}\\bigr)\\biggl(\\frac{1}{1\\,+\\,j\\omega T}\\biggr)\n$$  \n\nthe magnitude and phase angle are, respectively,  \n\n$$\n\\left|G(j\\omega)\\right|=\\left|e^{-j\\omega L}\\right|\\cdot\\left|\\frac{1}{1\\,+\\,j\\omega T}\\right|=\\frac{1}{\\sqrt{1\\,+\\,\\omega^{2}T^{2}}}\n$$  \n\nand  \n\n$$\n\\left/G(j\\omega)\\right.=\\;\\left.\\underline{{{\\rho}}}^{-j\\omega L}\\right.+\\;\\left.\\left/\\frac{1}{1\\,+\\,j\\omega T}=-\\omega L\\,-\\,\\tan^{-1}\\omega T\\right.\n$$  \n\nSince the magnitude decreases from unity monotonically and the phase angle also decreases monotonically and indefinitely, the polar plot of the given transfer function is a spiral, as shown in Figure 7–32.  \n\nFigure 7–32 Polar plot of $e^{-j\\omega L}/(1\\,+\\,j\\omega T).$  \n\n![](images/9f7f4a876eb64e20ed29e391411cff9627f7eba13b3d31392247b943caa21f14.jpg)  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{G(j\\omega)=\\frac{K\\left(1\\right.+\\left.j\\omega T_{a}\\right)\\left(1\\right.+\\left.j\\omega T_{b}\\right)\\cdots}{\\left.\\left(j\\omega\\right)^{\\lambda}\\left(1\\right.+\\left.j\\omega T_{1}\\right)\\left(1\\right.+\\left.j\\omega T_{2}\\right)\\cdots}}}\\\\ &{}&\\\\ &{}&{\\qquad=\\frac{b_{0}(j\\omega)^{m}\\,+\\,\\,b_{1}(j\\omega)^{m-1}\\,+\\,\\cdots}{a_{0}(j\\omega)^{n}\\,+\\,\\,a_{1}(j\\omega)^{n-1}\\,+\\,\\cdots}}\\end{array}\n$$  \n\nwhere $n>m$ or the degree of the denominator polynomial is greater than that of the numerator, will have the following general shapes:  \n\n1. For $\\lambda=0$ or type 0 systems: The starting point of the polar plot (which corresponds to $\\omega\\,=\\,0$ ) is finite and is on the positive real axis. The tangent to the polar plot at $\\omega\\,=\\,0$ is perpendicular to the real axis. The terminal point, which corresponds to $\\omega=\\infty$ , is at the origin, and the curve is tangent to one of the axes.  \n\n2. For $\\lambda=1$ or type 1 systems: the $j\\omega$ term in the denominator contributes $-90^{\\circ}$ to the total phase angle of $G(j\\omega)$ for $0\\leq\\omega\\leq\\infty$ .At $\\omega=0$ , the magnitude of $G(j\\omega)$ is infinity, and the phase angle becomes $-90^{\\circ}$ .At low frequencies, the polar plot is asymptotic to a line parallel to the negative imaginary axis.At $\\omega=\\infty$ , the magnitude becomes zero, and the curve converges to the origin and is tangent to one of the axes.  \n\n3. For $\\lambda\\,=\\,2$ or type 2 systems: The $(j\\omega)^{2}$ term in the denominator contributes $-180^{\\circ}$ to the total phase angle of $G(j\\omega)$ for $0\\leq\\omega\\leq\\infty$ . At $\\omega\\,=\\,0$ , the magnitude of $G(j\\omega)$ is infinity, and the phase angle is equal to $-180^{\\circ}$ . At low frequencies, the polar plot may be asymptotic to the negative real axis. At $\\omega=\\infty$ , the magnitude becomes zero, and the curve is tangent to one of the axes.  \n\nThe general shapes of the low-frequency portions of the polar plots of type 0, type 1, and type 2 systems are shown in Figure 7–33. It can be seen that, if the degree of the denominator polynomial of $G(j\\omega)$ is greater than that of the numerator, then the $G(j\\omega)$ loci converge to the origin clockwise.At $\\omega=\\infty$ , the loci are tangent to one or the other axes, as shown in Figure 7–34.  \n\n![](images/b7caa5eb7156d6bfbe3dafa248b019d71e90d3ad57d9126b695b7435fa2ed9c5.jpg)  \nFigure 7–33 Polar plots of type 0, type 1, and type 2 systems.  \n\n![](images/5c6b10c559014e4022e4c39bd7e45a8b2b533bb3dc88ac4ac04ee9a1887a69f3.jpg)  \nFigure 7–34 Polar plots in the high-frequency range.  \n\nNote that any complicated shapes in the polar plot curves are caused by the numerator dynamics—that is, by the time constants in the numerator of the transfer function. Figure 7–35 shows examples of polar plots of transfer functions with numerator dynamics. In analyzing control systems, the polar plot of $G(j\\omega)$ in the frequency range of interest must be accurately determined.  \n\nTable 7–1 shows sketches of polar plots of several transfer functions.  \n\n![](images/f8f6b8da308ce51445fc50c83e8f4fff5b238dc480f61b090c422785fa2e33b9.jpg)  \nFigure 7–35 Polar plots of transfer functions with numerator dynamics.  \n\nTable 7–1 Polar Plots of Simple Transfer Functions  \n\n![](images/2371f013054d48bcc6d5fc58449c1a8a5d7dcb2a01759ef47430610e93411b85.jpg)  \n\nDrawing Nyquist Plots with MATLAB. Nyquist plots, just like Bode diagrams, are commonly used in the frequency-response representation of linear, time-invariant, feedback control systems. Nyquist plots are polar plots, while Bode diagrams are rectangular plots. One plot or the other may be more convenient for a particular operation, but a given operation can always be carried out in either plot.  \n\nThe MATLAB command nyquist computes the frequency response for continuoustime, linear, time-invariant systems.When invoked without left-hand arguments, nyquist produces a Nyquist plot on the screen.  \n\nThe command nyquist(num,den)  \n\ndraws the Nyquist plot of the transfer function  \n\n$$\nG(s)={\\frac{\\operatorname{num}(s)}{\\operatorname{den}(s)}}\n$$  \n\nwhere num and den contain the polynomial coefficients in descending powers of $s$ .Other commonly used nyquist commands are  \n\nnyquist(num,den,w) nyquist(A,B,C,D) nyquist(A,B,C,D,w) nyquist(A,B,C,D,iu,w) nyquist(sys)  \n\nThe command involving the user-specified frequency vector w, such as  \n\nnyquist(num,den,w)  \n\ncalculates the frequency response at the specified frequency points in radians per second.  \n\nWhen invoked with left-hand arguments such as  \n\n$$\n\\begin{array}{r l}&{[\\mathrm{re},\\mathrm{im},\\mathrm{w}]=\\mathsf{n y q u i s t}(\\mathsf{n u m},\\mathsf{d e n})}\\\\ &{[\\mathrm{re},\\mathrm{im},\\mathrm{w}]=\\mathsf{n y q u i s t}(\\mathsf{n u m},\\mathsf{d e n},\\mathsf{w})}\\\\ &{[\\mathrm{re},\\mathrm{im},\\mathrm{w}]=\\mathsf{n y q u i s t}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D})}\\\\ &{[\\mathrm{re},\\mathrm{im},\\mathrm{w}]=\\mathsf{n y q u i s t}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{w})}\\\\ &{[\\mathrm{re},\\mathrm{im},\\mathsf{w}]=\\mathsf{n y q u i s t}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{i u},\\mathsf{w})}\\\\ &{[\\mathrm{re},\\mathrm{im},\\mathsf{w}]=\\mathsf{n y q u i s t}(\\mathsf{s y s})}\\end{array}\n$$  \n\nMATLAB returns the frequency response of the system in the matrices re ,im, and w.No plot is drawn on the screen. The matrices re and im contain the real and imaginary parts of the frequency response of the system, evaluated at the frequency points specified in the vector w. Note that re and im have as many columns as outputs and one row for each element in w.  \n\nEXAMPLE 7–10 Consider the following open-loop transfer function:  \n\n$$\nG(s)={\\frac{1}{s^{2}+0.8s\\,+\\,1}}\n$$  \n\nDraw a Nyquist plot with MATLAB. Since the system is given in the form of the transfer function, the command  \n\nnyquist(num,den)  \n\nmay be used to draw a Nyquist plot. MATLAB Program 7–5 produces the Nyquist plot shown in Figure 7–36. In this plot, the ranges for the real axis and imaginary axis are automatically determined.  \n\nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\n![](images/f3d6ce0c42c1d2e7d87469d4f545dacdbd10bea6ba39fd2b49d8082fab18ac34.jpg)  \n\nFigure 7–36   \nNyquist plot of   \n$G(s)={\\frac{1}{s^{2}+0.8s\\,+\\,1}}.$ .  \n\n![](images/12fff9024ff4dfcbcbbd1e13e07d061470ab4a37c106c50f9d033cb2f56bc116.jpg)  \nNyquist Plot of $G(s)=1/(s^{2}+0.8s+1)$  \n\nIf we wish to draw the Nyquist plot using manually determined ranges—for example, from $^{-2}$ to 2 on the real axis and from $^{-2}$ to 2 on the imaginary axis—enter the following command into the computer:  \n\n$$\n\\begin{array}{l}{{\\sf v=[-2\\mathrm{~\\bf~2~}\\cdot2\\mathrm{~\\bf~2~}\\cdot}}\\\\ {{\\sf a x i s(v);}}\\end{array}\n$$  \n\nor, combining these two lines into one,  \n\n$$\n\\mathsf{a x i s}([-2\\textit{\\textbf{2}}\\!\\cdot\\!2\\textit{\\textbf{2}}]);\n$$  \n\nSee MATLAB Program 7–6 and the resulting Nyquist plot shown in Figure 7–37.  \n\n![](images/9a11f88812ff25a251ef1e8a3440a0f4cceb3c7883c469e56714aee16a7c54a4.jpg)  \n\n![](images/1b8c75b4c295a118eda654f22458a15afbc2c1efe65417ea210d6b409b58654b.jpg)  \n\nCaution. In drawing a Nyquist plot,where a MATLAB operation involves “Divide by zero,” the resulting Nyquist plot may have an erroneous or undesirable appearance. For example, if the transfer function $G(s)$ is given by  \n\n$$\nG(s)={\\frac{1}{s(s+1)}}\n$$  \n\nthen the MATLAB command  \n\n$$\n\\begin{array}{l}{\\mathrm{num}=[1];}\\\\ {\\mathrm{den}=[1\\ \\ 1\\ \\ 0];}\\\\ {\\mathrm{nyquist}(\\mathrm{num},\\mathrm{den})}\\end{array}\n$$  \n\nproduces an undesirable Nyquist plot. An example of an undesirable Nyquist plot is shown in Figure 7–38. If such an undesirable Nyquist plot appears on the computer,  \n\n![](images/b7dfe755c5dfec9d0e6fb09cdae2f3a2c12c4111c077139089944869f9fac5c5.jpg)  \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nFigure 7–38   \nUndesirable Nyquist plot.  \n\nthen it can be corrected if we specify the axis(v) . For example, if we enter the axis command  \n\n$$\n\\mathsf{v}=[-2\\mathsf{\\begin{array}{c c c}{2}&{-5}&{5}\\end{array}}];\\,\\mathsf{a x i s}(\\mathsf{v})\n$$  \n\nin the computer,then a desirable form of Nyquist plot can be obtained.See Example 7–11.  \n\n# EXAMPLE 7–11  \n\nDraw a Nyquist plot for the following $G(s)$ :  \n\n$$\nG(s)={\\frac{1}{s(s+1)}}\n$$  \n\nMATLAB Program 7–7 will produce a desirable form of Nyquist plot on the computer, even though a warning message “Divide by zero” may appear on the screen.The resulting Nyquist plot is shown in Figure 7–39.  \n\n![](images/4da20e227b793e9c86c165241a2d4fc13d6d3a2e50c2e5a0380ba99acd4a0d78.jpg)  \n\nNotice that the Nyquist plot shown in Figure 7–39 includes the loci for both $\\omega>0$ and $\\omega<0$ .If we wish to draw the Nyquist plot for only the positive frequency region $[\\omega>0]$ ), then we need to use the command  \n\n![](images/0c1f045324e5739fc072302d09e7ca018a1d99cfe32428b45f2af450fc3172ac.jpg)  \nFigure 7–39 Nyquist plot of $G(s)={\\frac{1}{s(s+1)}}.$  \n\nA MATLAB program using this nyquist command is shown in MATLAB Program 7–8. The resulting Nyquist plot is presented in Figure 7–40.  \n\n![](images/01eabb86f6e949aa563a22eed7ce4bcf71336fe7dca8616c9f975cf949783904.jpg)  \n\nDrawing Nyquist Plots of a System Defined in State Space. Consider the system defined by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}+\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ( $n$ -vector)  \n\n$\\mathbf{y}=$ output vector ( m-vector)   \n${\\textbf{u}}=$ control vector ( $\\dot{\\boldsymbol{r}}$ -vector)   \n$\\mathbf{A}=$ state matrix ( ${\\boldsymbol{n}}\\times{\\boldsymbol{n}}$ matrix)   \n$\\mathbf{B}=$ control matrix ( ${\\boldsymbol{n}}\\times{\\boldsymbol{r}}$ matrix)   \n$\\mathbf{C}=$ output matrix ( $m\\times n$ matrix)   \n$\\mathbf{D}=$ direct transmission matrix ( $m\\times r$ matrix)  \n\nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method Nyquist plots for this system may be obtained by the use of the command This command produces a series of Nyquist plots, one for each input and output combination of the system.The frequency range is automatically determined. The command  \n\nproduces Nyquist plots from the single input iu to all the outputs of the system, with the frequency range determined automatically.The scalar iu is an index into the inputs of the system and specifies which input to use for the frequency response.  \n\nThe command  \n\nuses the user-supplied frequency vector w. The vector wspecifies the frequencies in radians per second at which the frequency response should be calculated.  \n\nEXAMPLE 7–12 Consider the system defined by  \n\n$$\n{\\begin{array}{r l}{{\\left[\\!\\!\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\!\\!\\right]}={\\left[\\!\\!\\begin{array}{l l}{\\;~~0}&{1{\\displaystyle\\prod_{-25}}x_{1}{\\\\\\ {-25}}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}+{\\left[\\!\\!\\begin{array}{l}{0}\\\\ {25}\\end{array}\\!\\!\\right]}u}\\\\ {y={\\left[\\!\\!\\begin{array}{l l}{1}&{0{\\displaystyle\\right]}{\\left[\\!\\!\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}+\\left[\\!\\!\\begin{array}{l}{0}\\end{array}\\!\\!\\right]}u}\\end{array}}\n$$  \n\nDraw a Nyquist plot.  \n\nThis system has a single input $u$ and a single output $y_{\\cdot}$ . A Nyquist plot may be obtained by entering the command  \n\nor  \n\nMATLAB Program 7–9 will provide the Nyquist plot. (Note that we obtain the identical result by using either of these two commands.) Figure 7–41 shows the Nyquist plot produced by MATLAB Program 7–9.  \n\n![](images/d594dfde9e327d257c5a4a238d23ac1780e4447ad31965130b0475a332390189.jpg)  \n\nFigure 7–41   \nNyquist plot of   \nsystem considered in Example 7–12.  \n\n![](images/ccef4934295105b5b82ea3e7164d623e2d4934ea2fe8f68315c563f14793a58a.jpg)  \n\nEXAMPLE 7–13 Consider the system defined by  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}{\\overline{{\\right]}}}={\\left[\\begin{array}{l l}{-1}&{-1}\\\\ {6.5}&{0{\\biggr]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1{\\prod_{x_{2}}}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{0{\\prod_{u_{2}}}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThis system involves two inputs and two outputs. There are four sinusoidal output–input relationships: $Y_{1}(j\\omega)/U_{1}(j\\omega),\\,Y_{2}(j\\omega)/U_{1}(j\\omega),\\,Y_{1}(j\\omega)/U_{2}(j\\omega)$ and $Y_{2}(j\\omega)/U_{2}(j\\omega)$ .Draw Nyquist plots for the system. (When considering input $u_{1}$ , we assume that input $u_{2}$ is zero, and vice versa.)  \n\nThe four individual Nyquist plots can be obtained by the use of the command MATLAB Program 7–10 produces the four Nyquist plots.They are shown in Figure 7–42.  \n\n![](images/4d2cc331fbe35520a7cea0ae6cdb1098bb93886bb0b9c12a91831efcebf0a4c1.jpg)  \n\n![](images/27d3a56dc8d3b126f5277ac89db2a3768d358a4c95239cccbbd0d137fe079dcf.jpg)  \n\n# 7–4 LOG-MAGNITUDE-VERSUS-PHASE PLOTS  \n\nAnother approach to graphically portraying the frequency-response characteristics is to use the log-magnitude-versus-phase plot, which is a plot of the logarithmic magnitude in decibels versus the phase angle or phase margin for a frequency range of interest. [The phase margin is the difference between the actual phase angle $\\phi$ and $-180^{\\circ}$ ; that is, $\\phi\\,-\\,\\left(-180^{\\circ}\\right)\\,=\\,180^{\\circ}\\,+\\,\\phi.]$ The curve is graduated in terms of the frequency $\\omega$ . Such log-magnitude-versus-phase plots are commonly called Nichols plots.  \n\nIn the Bode diagram, the frequency-response characteristics of $G(j\\omega)$ are shown on semilog paper by two separate curves, the log-magnitude curve and the phase-angle curve, while in the log-magnitude-versus-phase plot, the two curves in the Bode diagram are combined into one. In the manual approach the log-magnitude-versus-phase plot can easily be constructed by reading values of the log magnitude and phase angle from the Bode diagram. Notice that in the log-magnitude-versus-phase plot a change in the gain constant of $G(j\\omega)$ merely shifts the curve up (for increasing gain) or down (for decreasing gain), but the shape of the curve remains the same.  \n\nAdvantages of the log-magnitude-versus-phase plot are that the relative stability of the closed-loop system can be determined quickly and that compensation can be worked out easily.  \n\nThe log-magnitude-versus-phase plot for the sinusoidal transfer function $G(j\\omega)$ and that for $1/G(j\\omega)$ are skew symmetrical about the origin, since  \n\n$$\n\\left|\\frac{1}{G(j\\omega)}\\right|\\mathrm{in\\,dB}=-\\bigl|G(j\\omega)\\bigr|\\,\\mathrm{in\\,dB}\n$$  \n\n![](images/f36945dd40aba96cdf54ea2744eadefb8eda294a5eddcff15aa60b38bfc3f960.jpg)  \nFigure 7–43  \n\nThree representations of the frequency response of $\\frac{1}{1\\,+\\,2\\zeta\\!\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}$ ,for $\\zeta>0$ .  \n\n(a) Bode diagram; (b) polar plot; (c) log-magnitude-versus-phase plot.  \n\nand  \n\n$$\n\\left/\\frac{1}{G(j\\omega)}=-\\big/G(j\\omega)\\right.\n$$  \n\nFigure 7–43 compares frequency-response curves of  \n\n$$\nG(j\\omega)=\\frac{1}{1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{\\omega_{n}}\\bigg)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{n}}\\right)^{2}}\n$$  \n\nin three different representations. In the log-magnitude-versus-phase plot, the vertical distance between the points $\\omega=0$ and $\\omega=\\omega_{r}$ ,where $\\omega_{r}$ is the resonant frequency,is the peak value of $G(j\\omega)$ in decibels.  \n\nSince log-magnitude and phase-angle characteristics of basic transfer functions have been discussed in detail in Sections 7–2 and 7–3, it will be sufficient here to give examples of some log-magnitude-versus-phase plots.Table 7–2 shows such examples. (However, more on Nichols charts will be discussed in Section 7–6.)  \n\n![](images/ecee349d72bfeade45b9bbcc33ba9e9be0b04cd96266f6306d05616ace371bba.jpg)  \n\n# 7–5 NYQUIST STABILITY CRITERION  \n\nThe Nyquist stability criterion determines the stability of a closed-loop system from its open-loop frequency response and open-loop poles.  \n\nThis section presents mathematical background for understanding the Nyquist stability criterion. Consider the closed-loop system shown in Figure 7–44.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)H(s)}\n$$  \n\nFigure 7–44 Closed-loop system.  \n\n![](images/83c8cc8bf4371c4c97f6ef00a81310ed24e417d16c8e6bd7671d9e4ba4927ffa.jpg)  \n\nFor stability, all roots of the characteristic equation  \n\n$$\n1\\,+\\,G(s)H(s)\\,=\\,0\n$$  \n\nmust lie in the left-half $s$ plane.[It is noted that,although poles and zeros of the open-loop transfer function $G(s)H(s)$ may be in the right-half $s$ plane, the system is stable if all the poles of the closed-loop transfer function (that is, the roots of the characteristic equation) are in the left-half $s$ plane.] The Nyquist stability criterion relates the open-loop frequency response $G(j\\omega)H(j\\omega)$ to the number of zeros and poles of $1\\,+\\,G(s)H(s)$ that lie in the right-half $s$ plane.This criterion,derived by H.Nyquist,is useful in control engineering because the absolute stability of the closed-loop system can be determined graphically from open-loop frequency-response curves, and there is no need for actually determining the closed-loop poles. Analytically obtained open-loop frequency-response curves, as well as those experimentally obtained,can be used for the stability analysis.This is convenient because, in designing a control system, it often happens that mathematical expressions for some of the components are not known; only their frequency-response data are available.  \n\nThe Nyquist stability criterion is based on a theorem from the theory of complex variables.To understand the criterion, we shall first discuss mappings of contours in the complex plane.  \n\nWe shall assume that the open-loop transfer function $G(s)H(s)$ is representable as a ratio of polynomials in $s.$ . For a physically realizable system, the degree of the denominator polynomial of the closed-loop transfer function must be greater than or equal to that of the numerator polynomial.This means that the limit of $G(s)H(s)$ as $s$ approaches infinity is zero or a constant for any physically realizable system.  \n\nPreliminary Study. The characteristic equation of the system shown in Figure 7–44 is  \n\n$$\nF(s)\\,=\\,1\\,+\\,G(s)H(s)\\,=\\,0\n$$  \n\nWe shall show that, for a given continuous closed path in the $s$ plane that does not go through any singular points, there corresponds a closed curve in the $F(s)$ plane. The number and direction of encirclements of the origin of the $F(s)$ plane by the closed curve play a particularly important role in what follows, for later we shall correlate the number and direction of encirclements with the stability of the system.  \n\nConsider, for example, the following open-loop transfer function:  \n\n$$\nG(s)H(s)=\\frac{2}{s-1}\n$$  \n\nThe characteristic equation is  \n\n$$\n\\begin{array}{l}{{F(s)\\,=\\,1\\,+\\,G(s)H(s)}}\\\\ {{\\,}}\\\\ {{=\\,1\\,+\\,{\\frac{2}{s\\,-\\,1}}={\\frac{s\\,+\\,1}{s\\,-\\,1}}=0}}\\end{array}\n$$  \n\n![](images/e30a0a7576cb4e59df64d208a8feef66b94f95d6b21e6bebaaadf43948ce2eae.jpg)  \nFigure 7–45 Conformal mapping of the $s$ -plane grids into the $F(s)$ plane, where $\\bar{F}(s)\\,=\\,(s\\,+\\,1)/(s\\,-\\,1).$  \n\n![](images/d5aa42aff10ee9e9fa327edd00624fc4765480f0a416ffd25d5a0682b14578f7.jpg)  \n\nThe function $F(s)$ is analytic #everywhere in the $s$ plane except at its singular points. For each point of analyticity in the $s$ plane, there corresponds a point in the $F(s)$ plane. For example, if $s\\,=\\,2\\,+\\,j1$ , then $F(s)$ becomes  \n\n$$\nF(2\\,+\\,j1)=\\frac{2\\,+\\,j1\\,+\\,1}{2\\,+\\,j1\\,-\\,1}=2\\,-\\,j1\n$$  \n\nThus, point $s\\,=\\,2\\,+\\,j1$ in the $s$ plane maps into point $2\\mathrm{~-~}j1$ in the $F(s)$ plane.  \n\nThus,as stated previously,for a given continuous closed path in the $s$ plane,which does not go through any singular points, there corresponds a closed curve in the $F(s)$ plane.  \n\nFor the characteristic equation $F(s)$ given by Equation (7–15), the conformal mapping of the lines $\\omega=0,\\pm1,\\pm2$ and the lines $\\sigma=0,\\pm{1},\\pm{2}$ [see Figure 7–45(a)] yield circles in the $F(s)$ plane, as shown in Figure 7–45(b). Suppose that representative point $s$ traces out a contour in the $s$ plane in the clockwise direction. If the contour in the $s$ plane encloses the pole of $F(s)$ , there is one encirclement of the origin of the $F(s)$ plane by the locus of $F(s)$ in the counterclockwise direction. [See Figure 7–46(a).] If the contour in the $s$ plane encloses the zero of $F(s)$ ,there is one encirclement of the origin of the $F(s)$ plane by the locus of $F(s)$ in the clockwise direction. [See Figure 7–46(b).] If the contour in the $s$ plane encloses both the zero and the pole or if the contour encloses neither the zero nor the pole, then there is no encirclement of the origin of the $F(s)$ plane by the locus of $F(s)$ .[See Figures 7–46(c) and (d).]  \n\nFrom the foregoing analysis, we can say that the direction of encirclement of the origin of the $F(s)$ plane by the locus of $F(s)$ depends on whether the contour in the $s$ plane encloses a pole or a zero. Note that the location of a pole or zero in the $s$ plane, whether in the right-half or left-half $s$ plane, does not make any difference, but the enclosure of a pole or zero does. If the contour in the $s$ plane encloses equal numbers of poles and zeros, then the corresponding closed curve in the $F(s)$ plane does not encircle the origin of the $F(s)$ plane.The foregoing discussion is a graphical explanation of the mapping theorem, which is the basis for the Nyquist stability criterion.  \n\n![](images/632c610ff4eec446a94529d6001e6ab8d145e1d6a9f7214993494671b97d0d15.jpg)  \nFigure 7–46 Closed contours in the $s$ plane and their corresponding closed curves in the $F(s)$ plane, where $F(s)\\,=\\,(s\\,+\\,1)/(s\\,-\\,1)$ .  \n\nMapping Theorem. Let $F(s)$ be a ratio of two polynomials in $s.$ . Let $P$ be the number of poles and $Z$ be the number of zeros of $F(s)$ that lie inside some closed contour in the $s$ plane, with multiplicity of poles and zeros accounted for. Let the contour be such that it does not pass through any poles or zeros of $F(s)$ .This closed contour in the $s$ plane is then mapped into the $F(s)$ plane as a closed curve. The total number $N$ of clockwise encirclements of the origin of the $F(s)$ plane, as a representative point $s$ traces out the entire contour in the clockwise direction, is equal to $Z\\mathrm{~-~}P$ . (Note that by this mapping theorem, the numbers of zeros and of poles cannot be found—only their difference.)  \n\nWe shall not present a formal proof of this theorem here, but leave the proof to Problem A–7–6 . Note that a positive number $N$ indicates an excess of zeros over poles of the function $F(s)$ and a negative $N$ indicates an excess of poles over zeros. In control system applications, the number $P$ can be readily determined for $F(s)\\,=\\,1\\,+\\,G(s)H(s)$ from the function $G(s)H(s)$ . Therefore, if $N$ is determined from the plot of $F(s)$ , the number of zeros in the closed contour in the $s$ plane can be determined readily.Note that the exact shapes of the $s$ -plane contour and $F(s)$ locus are immaterial so far as encirclements of the origin are concerned, since encirclements depend only on the enclosure of poles and/or zeros of $F(s)$ by the $s$ -plane contour.  \n\nApplication of the Mapping Theorem to the Stability Analysis of Closed-Loop Systems. For analyzing the stability of linear control systems, we let the closed contour in the $s$ plane enclose the entire right-half $s$ plane.The contour consists of the entire $j\\omega$ axis from $\\omega=-\\infty$ to $+\\infty$ and a semicircular path of infinite radius in the right-half $s$ plane. Such a contour is called the Nyquist path. (The direction of the path is clockwise.) The Nyquist path encloses the entire right-half $s$ plane and encloses all the zeros and poles of $1\\,+\\,G(s)H(s)$ that have positive real parts. [If there are no zeros of $1\\,+\\,G(s)H(s)$ in the right-half $s$ plane, then there are no closed-loop poles there, and the system is stable.] It is necessary that the closed contour, or the Nyquist path, not pass through any zeros and poles of $1\\,+\\,G(s)H(s).$ . If $G(s)H(s)$ has a pole or poles at the origin of the $s$ plane, mapping of the point $s\\,=\\,0$ becomes indeterminate. In such cases, the origin is avoided by taking a detour around it. (A detailed discussion of this special case is given later.)  \n\nIf the mapping theorem is applied to the special case in which $F(s)$ is equal to $1\\,+\\,G(s)H(s)$ , then we can make the following statement: If the closed contour in the $s$ plane encloses the entire right-half $s$ plane, as shown in Figure 7–47, then the number of right-half plane zeros of the function $F(s)\\,=\\,1\\,+\\,G(s)H(s)$ is equal to the number of poles of the function $F(s)\\,=\\,1\\,+\\,G(s)H(s)$ in the right-half $s$ plane plus the number of clockwise encirclements of the origin of the $1\\,+\\,G(s)H(s)$ plane by the corresponding closed curve in this latter plane.  \n\nBecause of the assumed condition that  \n\n$$\n\\operatorname*{lim}_{s\\to\\infty}\\bigl[1\\,+\\,G(s)H(s)\\bigr]\\,=\\,\\mathrm{constant}\n$$  \n\nthe function of $1\\,+\\,G(s)H(s)$ remains constant as $s$ traverses the semicircle of infinite radius. Because of this, whether the locus of $1\\,+\\,G(s)H(s)$ encircles the origin of the $1\\,+\\,G(s)H(s)$ plane can be determined by considering only a part of the closed contour in the $s$ plane—that is,the $j\\omega$ axis.Encirclements of the origin,if there are any,occur only while a representative point moves from $-j\\infty$ to $+j\\infty$ along the jvaxis, provided that no zeros or poles lie on the $j\\omega$ axis.  \n\n![](images/37892dd0b2ffffea4c015573d77ede44421210d3f447cdb5b60fec926a892bdc.jpg)  \nFigure 7–47 Closed contour in the $s$ plane.  \n\n![](images/5a4b756d9d6dd3001b2be5dcbf0d6a3bf21af2104dfd3a999ed322e037e2986c.jpg)  \nFigure 7–48 Plots of $1\\,+\\,G(j\\omega)H(j\\omega)$ in the $1\\,+\\,G H$ plane and $G H$ plane.  \n\nNote that the portion of the $1\\,+\\,G(s)H(s)$ contour from $\\omega=-\\infty$ to $\\omega=\\infty$ is simply $1\\,+\\,G(j\\omega)H(j\\omega)$ . Since $1\\,+\\,G(j\\omega)H(j\\omega)$ is the vector sum of the unit vector and the vector $G(j\\omega)H(j\\omega),1\\,+\\,G(j\\omega)H(\\mathrm{j}\\omega)$ is identical to the vector drawn from the $-1+j0$ point to the terminal point of the vector $G(j\\omega)H(j\\omega)$ , as shown in Figure 7–48. Encirclement of the origin by the graph of $1\\,+\\,G(j\\omega)H(j\\omega)$ is equivalent to encirclement of the $-1+j0$ point by just the $G(j\\omega)H(j\\omega)$ locus.Thus,the stability of a closedloop system can be investigated by examining encirclements of the $-1+j0$ point by the locus of $G(j\\omega)H(j\\omega)$ .The number of clockwise encirclements of the $-1+j0$ point can be found by drawing a vector from the $-1+j0$ point to the $G(j\\omega)H(j\\omega)$ locus, starting from $\\omega=-\\infty$ , going through $\\omega=0$ , and ending at $\\omega=+\\infty$ , and by counting the number of clockwise rotations of the vector.  \n\nPlotting $G(j\\omega)H(j\\omega)$ for the Nyquist path is straightforward.The map of the negative $j\\omega$ axis is the mirror image about the real axis of the map of the positive $j\\omega$ axis.That is, the plot of $G(j\\omega)H(j\\omega)$ and the plot of $G(-j\\omega)H(-j\\omega)$ are symmetrical with each other about the real axis.The semicircle with infinite radius maps into either the origin of the $G H$ plane or a point on the real axis of the $G H$ plane.  \n\nIn the preceding discussion, $G(s)H(s)$ has been assumed to be the ratio of two polynomials in $s$ . Thus, the transport lag $e^{-T s}$ has been excluded from the discussion. Note, however, that a similar discussion applies to systems with transport lag, although a proof of this is not given here.The stability of a system with transport lag can be determined from the open-loop frequency-response curves by examining the number of encirclements of the $-1+j0$ point, just as in the case of a system whose open-loop transfer function is a ratio of two polynomials in $s$ .  \n\nNyquist Stability Criterion. The foregoing analysis, utilizing the encirclement of the $-1+j0$ point by the $G(j\\omega)H(j\\omega)$ locus, is summarized in the following Nyquist stability criterion:  \n\nNyquist stability criterion [for a special case when $G(s)H(s)$ has neither poles nor zeros on the jvaxis ]:In the system shown in Figure 7–44,if the open-loop transfer function $G(s)H(s)$ has $k$ poles in the right-half $s$ plane and $\\operatorname*{lim}_{s\\rightarrow\\infty}G(s)H(s)=$ constant ,then for stability,the $G(j\\omega)H(j\\omega)$ locus,as $\\omega$ varies from $-\\infty$ to $\\infty$ ,must encircle the $-1+j0$ point $k$ times in the counterclockwise direction.  \n\n# Remarks on the Nyquist Stability Criterion  \n\n1. This criterion can be expressed as  \n\n$$\nZ=N+P\n$$  \n\nwhere $Z=$ number of zeros of $1\\,+\\,G(s)H(s)$ in the right-half $s$ plane $N=$ number of clockwise encirclements of the $-1+j0$ point $P=$ number of poles of $G(s)H(s)$ in the right-half $s$ plane  \n\nIf $P$ is not zero,for a stable control system,we must have $Z=0$ ,or $N=-P$ ,which means that we must have $P$ counterclockwise encirclements of the $-1+j0$ point.  \n\nIf $G(s)H(s)$ does not have any poles in the right-half $s$ plane, then $Z\\,=\\,N$ .Thus, for stability there must be no encirclement of the $-1+j0$ point by the $G(j\\omega)H(j\\omega)$ locus. In this case it is not necessary to consider the locus for the entire $j\\omega$ axis, only for the positive-frequency portion.The stability of such a system can be determined by seeing if the $-1+j0$ point is enclosed by the Nyquist plot of $G(j\\omega)H(j\\omega)$ .The region enclosed by the Nyquist plot is shown in Figure 7–49. For stability, the $-1+j0$ point must lie outside the shaded region.  \n\n2. We must be careful when testing the stability of multiple-loop systems since they may include poles in the right-half $s$ plane. (Note that although an inner loop may be unstable, the entire closed-loop system can be made stable by proper design.) Simple inspection of encirclements of the $-1+j0$ point by the $G(j\\omega)H(j\\omega)$ locus is not sufficient to detect instability in multiple-loop systems. In such cases, however, whether any pole of $1\\,+\\,G(s)H(s)$ is in the right-half $s$ plane can be determined easily by applying the Routh stability criterion to the denominator of $G(s)H(s)$ .  \n\nIf transcendental functions,such as transport lag $e^{-T s}$ ,are included in $G(s)H(s)$ ,they must be approximated by a series expansion before the Routh stability criterion can be applied.  \n\n3. If the locus of $G(j\\omega)H(j\\omega)$ passes through the $-1+j0$ point, then zeros of the characteristic equation, or closed-loop poles, are located on the $j\\omega$ axis.This is not desirable for practical control systems. For a well-designed closed-loop system, none of the roots of the characteristic equation should lie on the $j\\omega$ axis.  \n\n![](images/a323a716d6c12753f686a64a88b3373cb824d0740d427262e96daedce764e6b7.jpg)  \nFigure 7–49 Region enclosed by a Nyquist plot.  \n\nSpecial Case when $G(s)H(s)$ Involves Poles and /or Zeros on the jVAxis. In the previous discussion, we assumed that the open-loop transfer function $G(s)H(s)$ has neither poles nor zeros at the origin.We now consider the case where $G(s)H(s)$ involves poles and/or zeros on the $j\\omega$ axis.  \n\nSince the Nyquist path must not pass through poles or zeros of $G(s)H(s)$ , if the function $G(s)H(s)$ has poles or zeros at the origin (or on the $j\\omega$ axis at points other than the origin), the contour in the $s$ plane must be modified. The usual way of modifying the contour near the origin is to use a semicircle with the infinitesimal radius $\\varepsilon$ , as shown in Figure 7–50. [Note that this semicircle may lie in the right-half $s$ plane or in the left-half $s$ plane. Here we take the semicircle in the right-half $s$ plane.] A representative point $s$ moves along the negative $j\\omega$ axis from $-j\\infty$ to $j0-$ .From $s\\,=\\,j0-$ to $s\\,=\\,j0+$ ,the point moves along the semicircle of radius $\\varepsilon$ (where $\\varepsilon\\ll1$ )and then moves along the positive $j\\omega$ axis from $j0+$ to $j\\infty.$ . From $s=j\\infty$ , the contour follows a semicircle with infinite radius, and the representative point moves back to the starting point, $s=-j\\infty.$ The area that the modified closed contour avoids is very small and approaches zero as the radius $\\varepsilon$ approaches zero.Therefore, all the poles and zeros, if any, in the right-half $s$ plane are enclosed by this contour.  \n\nConsider, for example, a closed-loop system whose open-loop transfer function is given by  \n\n![](images/87ea970a5e3f37a8c55a4bd9f25d2b3794cffd329c2515c5ce3e7d8dbf7b36cb.jpg)  \nFigure 7–50 Contour near the origin of the $s$ plane and closed contour in the $s$ plane avoiding poles and zeros at the origin.  \n\n$$\nG(s)H(s)\\,=\\frac{K}{s(T s\\,+\\,1)}\n$$  \n\nThe points corresponding to $s\\,=\\,j0+$ and $s\\,=\\,j0-$ on the locus of $G(s)H(s)$ in the $G(s)H(s)$ plane are $-j\\infty$ and $j\\infty$ , respectively. On the semicircular path with radius $\\varepsilon$ (where $\\varepsilon\\ll1$ ), the complex variable $s$ can be written  \n\n$$\ns\\,=\\,\\varepsilon e^{j\\theta}\n$$  \n\nwhere $\\theta$ varies from $-90^{\\circ}$ to $+90^{\\circ}$ .Then $G(s)H(s)$ becomes  \n\n$$\nG(\\varepsilon e^{j\\theta})H(\\varepsilon e^{j\\theta})=\\frac{K}{\\varepsilon e^{j\\theta}}=\\frac{K}{\\varepsilon}\\,e^{-j\\theta}\n$$  \n\n![](images/24c049783503c5537add801c703e5d56848b2aeeac7b17a3e709cd9c86916dcc.jpg)  \nFigure 7–51 $s$ -Plane contour and the $G(s)H(s)$ locus in the $G H$ plane, where $\\dot{G}(s)H(s)\\,=\\,K/\\bigl[s(T s\\,+\\,1)\\bigr].$ .  \n\nThe value $K/\\varepsilon$ approaches infinity as $\\varepsilon$ approaches zero, and $-\\theta$ varies from $90^{\\circ}$ to $-90^{\\circ}$ as a representative point $s$ moves along the semicircle in the $s$ plane. Thus, the points $G(j0-)H(j0-)=j\\infty$ and $G(j0{+})H(j0{+})\\,=\\,-j\\infty$ are joined by a semicircle of infinite radius in the right-half $G H$ plane.The infinitesimal semicircular detour around the origin in the $s$ plane maps into the $G H$ plane as a semicircle of infinite radius. Figure 7–51 shows the $s$ -plane contour and the $G(s)H(s)$ locus in the $G H$ plane. Points $A,B$ , and $C$ on the $s$ -plane contour map into the respective points $A^{\\prime},B^{\\prime}$ , and $C^{\\prime}$ on the $G(s)H(s)$ locus.As seen from Figure 7–51, points $D,E$ , and $F$ on the semicircle of infinite radius in the $s$ plane map into the origin of the $G H$ plane. Since there is no pole in the righthalf $s$ plane and the $G(s)H(s)$ locus does not encircle the $-1+j0$ point, there are no zeros of the function $1\\,+\\,G(s)H(s)$ in the right-half $s$ plane. Therefore, the system is stable.  \n\nFor an open-loop transfer function $G(s)H(s)$ involving a $1/s^{n}$ factor (where $n=2,3,\\dots)$ ), the plot of $G(s)H(s)$ has $n$ clockwise semicircles of infinite radius about the origin as a representative point $s$ moves along the semicircle of radius $\\varepsilon$ (where $\\varepsilon\\ll1$ ). For example, consider the following open-loop transfer function:  \n\n$$\nG(s)H(s)\\,=\\,\\frac{K}{s^{2}(T s\\,+\\,1)}\n$$  \n\nThen  \n\n$$\n\\operatorname*{lim}_{s\\rightarrow\\varepsilon e^{j\\theta}}G(s)H(s)={\\frac{K}{\\varepsilon^{2}e^{2j\\theta}}}={\\frac{K}{\\varepsilon^{2}}}\\,e^{-2j\\theta}\n$$  \n\nAs $\\theta$ varies from $-90^{\\circ}$ to $90^{\\circ}$ in the $s$ plane, the angle of $G(s)H(s)$ varies from $180^{\\circ}$ to $-180^{\\circ}$ , as shown in Figure 7–52. Since there is no pole in the right-half $s$ plane and the locus encircles the $-1+j0$ point twice clockwise for any positive value of $K$ , there are two zeros of $1\\,+\\,G(s)H(s)$ in the right-half $s$ plane. Therefore, this system is always unstable.  \n\n![](images/d6eae84bf0e86bd44760844d481175015959def2c5dff5daa200df531fa3f156.jpg)  \n\nFigure 7–52   \n$s$ -Plane contour and the   \n$G(s)H(s)$ locus in the $G H$   \nplane, where   \n$\\stackrel{.}{G}(s)\\dot{H}(s)\\,=\\,K/\\bigl[s^{2}(T s\\,+\\,1)\\bigr].$  \n\nNote that a similar analysis can be made if $G(s)H(s)$ involves poles and/or zeros on the $j\\omega$ axis.The Nyquist stability criterion can now be generalized as follows:  \n\nNyquist stability criterion [for a general case when $G(s)H(s)$ has poles and/or zeros on the jvaxis ]:In the system shown in Figure 7–44,if the open-loop transfer function $G(s)H(s)$ has $k$ poles in the right-half $s$ plane, then for stability the $G(s)H(s)$ locus, as a representative point $s$ traces on the modified Nyquist path in the clockwise direction, must encircle the $-1+j0$ point $k$ times in the counterclockwise direction.  \n\n# 7–6 STABILITY ANALYSIS  \n\nIn this section, we shall present several illustrative examples of the stability analysis of control systems using the Nyquist stability criterion.  \n\nIf the Nyquist path in the $s$ plane encircles $Z$ zeros and $P$ poles of $1\\,+\\,G(s)H(s)$ and does not pass through any poles or zeros of $1\\,+\\,G(s)H(s)$ as a representative point $s$ moves in the clockwise direction along the Nyquist path, then the corresponding contour in the $G(s)H(s)$ plane encircles the $-1+j0$ point $N\\,=\\,Z\\,-\\,P$ times in the clockwise direction. (Negative values of $N$ imply counterclockwise encirclements.)  \n\nIn examining the stability of linear control systems using the Nyquist stability criterion, we see that three possibilities can occur:  \n\n1. There is no encirclement of the $-1+j0$ point.This implies that the system is stable if there are no poles of $G(s)H(s)$ in the right-half $s$ plane; otherwise, the system is unstable.   \n2. There are one or more counterclockwise encirclements of the $-1+j0$ point.In this case the system is stable if the number of counterclockwise encirclements is the same as the number of poles of $G(s)H(s)$ in the right-half $s$ plane; otherwise, the system is unstable.   \n3. There are one or more clockwise encirclements of the $-1+j0$ point. In this case the system is unstable.  \n\nIn the following examples, we assume that the values of the gain $K$ and the time constants (such as $T,T_{1}$ ,and $T_{2}$ ) are all positive.  \n\nEXAMPLE 7–14 Consider a closed-loop system whose open-loop transfer function is given by  \n\n$$\nG(s)H(s)\\,=\\,\\frac{K}{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}\n$$  \n\nFigure 7–53 Polar plot of $G(j\\omega)H(j\\omega)$ considered in Example 7–14.  \n\nExamine the stability of the system.  \n\nA plot of $G(j\\omega)H(j\\omega)$ is shown in Figure 7–53. Since $G(s)H(s)$ does not have any poles in the right-half $s$ plane and the $-1+j0$ point is not encircled by the $G(j\\omega)H(j\\omega)$ locus, this system is stable for any positive values of $K,T_{1}$ ,and $T_{2}$ .  \n\n![](images/6f80f5efb04e62598aaf05032dd9943c821921c3c47231269b2f7fa4dcc14ad1.jpg)  \n\nEXAMPLE 7–15 Consider the system with the following open-loop transfer function:  \n\n$$\nG(s)H(s)\\,=\\frac{K}{s(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}\n$$  \n\nFigure 7–54   \nPolar plots of the   \nsystem considered in Example 7–15.  \n\nDetermine the stability of the system for two cases: (1) the gain $K$ is small and (2) $K$ is large. The Nyquist plots of the open-loop transfer function with a small value of $K$ and a large value of $K$ are shown in Figure 7–54.The number of poles of $G(s)H(s)$ in the right-half $s$ plane is zero.  \n\n![](images/d1dddbef999545d7ce0958067ee4397df71603e908a2c64364b8890af60732e4.jpg)  \n\nTherefore, for this system to be stable, it is necessary that $N\\,=\\,Z\\,=\\,0$ or that the $G(s)H(s)$ locus not encircle the $-1+j0$ point.  \n\nFor small values of $K$ , there is no encirclement of the $-1+j0$ point. Hence, the system is stable for small values of $K$ . For large values of $K$ , the locus of $G(s)H(s)$ encircles the $-1+j0$ point twice in the clockwise direction, indicating two closed-loop poles in the right-half $s$ plane, and the system is unstable. (For good accuracy, $K$ should be large. From the stability viewpoint, however, a large value of $K$ causes poor stability or even instability.To compromise between accuracy and stability, it is necessary to insert a compensation network into the system. Compensating techniques in the frequency domain are discussed in Sections 7–11 through 7–13.)  \n\nEXAMPLE 7–16 The stability of a closed-loop system with the following open-loop transfer function  \n\n$$\nG(s)H(s)={\\frac{K{(T_{2}s\\,+\\,1)}}{s^{2}(T_{1}s\\,+\\,1)}}\n$$  \n\ndepends on the relative magnitudes of $T_{1}$ and $T_{2}$ .Draw Nyquist plots and determine the stability of the system.  \n\nPlots of the locus $G(s)H(s)$ for three cases, $T_{1}<T_{2},T_{1}=T_{2}$ ,and $T_{1}>T_{2}$ ,are shown in Figure 7–55. For $T_{1}<T_{2}$ ,the locus of $G(s)H(s)$ does not encircle the $-1+j0$ point, and the closed-loop system is stable. For $T_{1}=T_{2}$ , the $G(s)H(s)$ locus passes through the $-1+j0$ point, which indicates that the closed-loop poles are located on the $j\\omega$ axis. For $T_{1}>T_{2}$ ,the locus of $G(s)H(s)$ encircles the $-1+j0$ point twice in the clockwise direction. Thus, the closed-loop system has two closed-loop poles in the right-half $s$ plane, and the system is unstable.  \n\n![](images/1365b256e6a4b9b60383d44b0630b57b2f56bf020b9dd8f27d992d1357f42a5b.jpg)  \n\nFigure 7–55   \nPolar plots of the   \nsystem considered in Example 7–16.  \n\n# EXAMPLE 7–17  \n\nConsider the closed-loop system having the following open-loop transfer function:  \n\n$$\nG(s)H(s)={\\frac{K}{s(T s-1)}}\n$$  \n\nDetermine the stability of the system.  \n\n![](images/eac7515c840ad1e1eb94fd4d606b102c034188fa3faa2a15a4727ae10499f374.jpg)  \n\nFigure 7–56   \nPolar plot of the   \nsystem considered in Example 7–17.  \n\nThe function $G(s)H(s)$ has one pole $\\mathit{\\Pi}_{S}=1/T\\mathit{\\Omega}$ )in the right-half $s$ plane.Therefore, $P\\,=\\,1,$ .The Nyquist plot shown in Figure 7–56 indicates that the $G(s)H(s)$ plot encircles the $-1+j0$ point once clockwise.Thus, $N=1.$ . Since $Z=N+P$ , we find that $Z=2.$ .This means that the closedloop system has two closed-loop poles in the right-half $s$ plane and is unstable.  \n\n# EXAMPLE 7–18  \n\nInvestigate the stability of a closed-loop system with the following open-loop transfer function:  \n\n$$\nG(s)H(s)\\,=\\frac{K(s\\,+\\,3)}{s(s\\,-\\,1)}\\qquad(K>1)\n$$  \n\nThe open-loop transfer function has one pole ($s=1$ )in the right-half $s$ plane, or $P\\,=\\,1.$ The open-loop system is unstable. The Nyquist plot shown in Figure 7–57 indicates that the $-1+j0$ point is encircled by the $G(s)H(s)$ locus once in the counterclockwise direction. Therefore, $N=-1.$ Thus, $Z$ is found from $Z=N+P$ to be zero, which indicates that there is no zero of $1\\,+\\,G(s)H(s)$ in the right-half $s$ plane, and the closed-loop system is stable. This is one of the examples for which an unstable open-loop system becomes stable when the loop is closed.  \n\n![](images/eb9f7368c80c75e8c450338426e891486e6b935d8750c0668a78a31e7ff3e831.jpg)  \n\nFigure 7–57   \nPolar plot of the   \nsystem considered in Example 7–18.  \n\n![](images/c4ee3c155d88381a7b50d3dea691c4c361e09a43687671ae239d28fd5acdfe4b.jpg)  \nFigure 7–58 Polar plot of a conditionally stable system.  \n\nConditionally Stable Systems. Figure 7–58 shows an example of a $G(j\\omega)H(j\\omega)$ locus for which the closed-loop system can be made unstable by varying the open-loop gain. If the open-loop gain is increased sufficiently, the $G(j\\omega)H(j\\omega)$ locus encloses the $-1+j0$ point twice,and the system becomes unstable.If the open-loop gain is decreased sufficiently, again the $G(j\\omega)H(j\\omega)$ locus encloses the $-1+j0$ point twice. For stable operation of the system considered here, the critical point $-1+j0$ must not be located in the regions between $O A$ and $B C$ shown in Figure 7–58. Such a system that is stable only for limited ranges of values of the open-loop gain for which the $-1+j0$ point is completely outside the $G(j\\omega)H(j\\omega)$ locus is a conditionally stable system.  \n\nA conditionally stable system is stable for the value of the open-loop gain lying between critical values, but it is unstable if the open-loop gain is either increased or decreased sufficiently.Such a system becomes unstable when large input signals are applied, since a large signal may cause saturation, which in turn reduces the open-loop gain of the system. It is advisable to avoid such a situation.  \n\nMultiple-Loop System. Consider the system shown in Figure 7–59.This is a multiple-loop system.The inner loop has the transfer function  \n\n$$\nG(s)\\,=\\,\\frac{G_{2}(s)}{1\\,+\\,G_{2}(s)H_{2}(s)}\n$$  \n\n![](images/6274b117a2560a93d0c87c28ae792bac2ac37e37c2d82166d9f7bde15f87e97a.jpg)  \nFigure 7–59 Multiple-loop system.   \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nIf $G(s)$ is unstable,the effects of instability are to produce a pole or poles in the right-half $s$ plane.Then the characteristic equation of the inner loop, $1\\,+\\,G_{2}(s)H_{2}(s)\\,=\\,0,$ has a zero or zeros in the right-half $s$ plane. If $G_{2}(s)$ and $H_{2}(s)$ have $P_{1}$ poles here, then the number $Z_{1}$ of right-half plane zeros of $1\\,+\\,G_{2}(s)H_{2}(s)$ can be found from $Z_{1}=N_{1}+P_{1}$ ,where $N_{1}$ is the number of clockwise encirclements of the $-1+j0$ point by the $G_{2}(s)H_{2}(s)$ locus. Since the open-loop transfer function of the entire system is given by $G_{1}(s)G(s)H_{1}(s)$ , the stability of this closed-loop system can be found from the Nyquist plot of $G_{1}(s)G(s)H_{1}(s)$ and knowledge of the right-half plane poles of $G_{1}(s)G(s)H_{1}(s)$ .  \n\nNotice that if a feedback loop is eliminated by means of block diagram reductions, there is a possibility that unstable poles are introduced; if the feedforward branch is eliminated by means of block diagram reductions, there is a possibility that right-half plane zeros are introduced.Therefore, we must note all right-half plane poles and zeros as they appear from subsidiary loop reductions. This knowledge is necessary in determining the stability of multiple-loop systems.  \n\nConsider the control system shown in Figure 7–60.The system involves two loops. Determine the range of gain $K$ for stability of the system by the use of the Nyquist stability criterion. (The gain $K$ is positive.)  \n\nTo examine the stability of the control system,we need to sketch the Nyquist locus of $G(s)$ ,where  \n\n$$\nG(s)\\,=\\,G_{1}(s)G_{2}(s)\n$$  \n\nHowever, the poles of $G(s)$ are not known at this point.Therefore, we need to examine the minor loop if there are right-half $s$ -plane poles. This can be done easily by use of the Routh stability criterion. Since  \n\n$$\nG_{2}(s)\\,=\\,{\\frac{1}{s^{3}\\,+\\,s^{2}\\,+\\,1}}\n$$  \n\nthe Routh array becomes as follows:  \n\n$$\n\\begin{array}{c c c}{{s^{3}}}&{{1}}&{{0}}\\\\ {{s^{2}}}&{{1}}&{{1}}\\\\ {{s^{1}}}&{{-1}}&{{0}}\\\\ {{s^{0}}}&{{1}}&{{}}\\end{array}\n$$  \n\nNotice that there are two sign changes in the first column. Hence, there are two poles of $G_{2}(s)$ in the right-half $s$ plane.  \n\nOnce we find the number of right-half $s$ plane poles of $G_{2}(s)$ ,we proceed to sketch the Nyquist locus of $G(s)$ , where  \n\n$$\nG(s)\\,=\\,G_{1}(s)G_{2}(s)\\,=\\frac{K(s\\,+\\,0.5)}{s^{3}\\,+\\,s^{2}\\,+\\,1}\n$$  \n\n![](images/f008ac4df13cb1c304b8ad10be2f8c0db97d30b6e5244a66096716d08fd46b80.jpg)  \n\nFigure 7–60 Control system.  \n\nOur problem is to determine the range of the gain $K$ for stability. Hence, instead of plotting Nyquist loci of $G(j\\omega)$ for various values of $K$ , we plot the Nyquist locus of $G(j\\omega)/K.$ Figure 7–61 shows the Nyquist plot or polar plot of $G(j\\omega)/K$ .  \n\nSince $G(s)$ has two poles in the right-half $s$ plane, we have $P=2$ .Noting that  \n\n$$\nZ=N+P\n$$  \n\nfor stability, we require $Z=0$ or $N=-2.$ . That is, the Nyquist locus of $G(j\\omega)$ must encircle the $-1+j0$ point twice counterclockwise. From Figure 7–61, we see that, if the critical point lies between 0 and $-0.5$ , then the $G(j\\omega)/K$ locus encircles the critical point twice counterclockwise. Therefore, we require  \n\n$$\n-0.5K<-1\n$$  \n\nThe range of the gain $K$ for stability is  \n\n$$\n2<K\n$$  \n\n![](images/aea171b78d96d4a72cb3d4b5cf6a72b995d5b47432c3e076d2fa707e4ab0dd31.jpg)  \nFigure 7–61 Polar plot of $G(j\\omega)/K$ .  \n\nNyquist Stability Criterion Applied to Inverse Polar Plots. In the previous analyses, the Nyquist stability criterion was applied to polar plots of the open-loop transfer function $G(s)H(s)$ .  \n\nIn analyzing multiple-loop systems, the inverse transfer function may sometimes be used in order to permit graphical analysis; this avoids much of the numerical calculation. (The Nyquist stability criterion can be applied equally well to inverse polar plots. The mathematical derivation of the Nyquist stability criterion for inverse polar plots is the same as that for direct polar plots.)  \n\nThe inverse po $G(j\\omega)H(j\\omega)$ is a graph of $1/\\!\\left[G(j\\omega)H(j\\omega)\\right]$ as a function of $\\omega$ . For example, if $G(j\\omega)H(j\\omega)$ is  \n\nthen  \n\n$$\n\\begin{array}{l}{{G(j\\omega)H(j\\omega)=\\displaystyle\\frac{j\\omega T}{1\\,+\\,j\\omega T}}}\\\\ {{{}}}\\\\ {{\\displaystyle\\frac{1}{G(j\\omega)H(j\\omega)}=\\frac{1}{j\\omega T}+1}}\\end{array}\n$$  \n\nThe inverse polar plot for $\\omega\\geq0$ is the lower half of the vertical line starting at the point $(1,0)$ on the real axis.  \n\nThe Nyquist stability criterion applied to inverse plots may be stated as follows: For $1/\\!\\left[G(s)H(s)\\right]$ a closed-loop system to be stable, the encirclement, if any, of the locus $s$ along the Nyquist path) ust be counterc $-1+j0$ point by the CDth f such encircleme ts must be equal to the number of poles of $1/\\!\\left[G(s)H(s)\\right]$ [that is, the zeros of $G(s)H(s)]$ ] that lie in the right-half $s$ plane. [The number of zeros of $G(s)H(s)$ in the right-half $s$ plane may be determined by the use of the Routh stability criterion.] If the open-loop transfer function $G(s)H(s)$ has no zeros in the righthalf $s$ plane, then for a closed-loop system to be stable, the number of encirclements of the $-1+j0$ point by the $1/\\!\\left[G(s)\\mathring{H}(s)\\right]$ locus must be zero.  \n\nNote that although the Nyquist stability criterion can be applied to inverse polar plots, if experimental frequency-response data are incorporated, counting the number of encirclements of the $1/\\bar{\\left[G(s)H(s)\\right]}$ locus ficult because the phase shift corresponding to the infinite semicircular path in the $s$ plane is difficult to measure. For example, if the open-loop transfer function $G(s)H(s)$ involves transport lag such that  \n\n$$\nG(s)H(s)={\\frac{K e^{-j\\omega L}}{s(T s\\,+\\,1)}}\n$$  \n\nthen the number of encirclements of the $-1+j0$ point by the $1/\\!\\left[G(s)H(s)\\right]$ locus becomes infinite, and the Nyquist stability criterion cannot be applied to the inverse polar plot of such an open-loop transfer function.  \n\nIn general, if experimental frequency-response data cannot be put into analytical form, both the $G(j\\omega)H(j\\omega)$ and $1/\\!\\left[G(j\\omega)H(j\\omega)\\right]$ st be plotted. In addition, the number of right-half plane zeros of G(s)H(s) must be determined. It is more difficult to determine the right-half plane zeros of $G(s)H(s)$ (in other words, to determine whether a given component is minimum phase) than it is to determine the right-half plane poles of $G(s)H(s)$ (in other words, to determine whether the component is stable).  \n\nDepending on whether the data are graphical or analytical and whether nonminimum-phase components are included, an appropriate stability test must be used for multiple-loop systems. If the data are given in analytical form or if mathematical expressions for all the components are known, the application of the Nyquist stability criterion to inverse polar plots causes no difficulty, and multiple-loop systems may be analyzed and designed in the inverse $G H$ plane. (See Problem A–7–15 .)  \n\n# 7–7 RELATIVE STABILITY ANALYSIS  \n\nRelative Stability. In designing a control system, we require that the system be stable. Furthermore, it is necessary that the system have adequate relative stability.  \n\nIn this section, we shall show that the Nyquist plot indicates not only whether a system is stable,but also the degree of stability of a stable system.The Nyquist plot also gives information as to how stability may be improved, if this is necessary.  \n\nIn the following discussion, we shall assume that the systems considered have unity feedback. Note that it is always possible to reduce a system with feedback elements to a unity-feedback system, as shown in Figure 7–62. Hence, the extension of relative stability analysis for the unity-feedback system to nonunity-feedback systems is possible.  \n\nWe shall also assume that, unless otherwise stated, the systems are minimum-phase systems; that is, the open-loop transfer function has neither poles nor zeros in the righthalf $s$ plane.  \n\nRelative Stability Analysis by Conformal Mapping. One of the important problems in analyzing a control system is to find all closed-loop poles or at least those closest to the $j\\omega$ axis (or the dominant pair of closed-loop poles). If the open-loop frequency-response characteristics of a system are known, it may be possible to estimate the closed-loop poles closest to the $j\\omega$ axis. It is noted that the Nyquist locus $G(j\\omega)$ need not be an analytically known function of $\\omega$ . The entire Nyquist locus may be experimentally obtained.The technique to be presented here is essentially graphical and is based on a conformal mapping of the $s$ plane into the $G(s)$ plane.  \n\n![](images/2ad7aa0f242c427a5c7d99c26e00d2c5d383eb78107f39eb889ff021dd7c02a2.jpg)  \nFigure 7–62 Modification of a system with feedback elements to a unityfeedback system.  \n\n![](images/cfc2cf1a162120ee0a39b2b991f7891025dbc254994d8681286534271abd29c9.jpg)  \nFigure 7–63 Conformal mapping of $s$ -plane grids into the $G(s)$ plane.  \n\n![](images/53f437581b17c23d2ad5b39d5e74eec4fea81099fd4f7d3c6029c62cac80d332.jpg)  \n\nConsider the conformal mapping of constant$\\sigma$ lines (lines $s\\,=\\,\\sigma\\,+\\,j\\omega$ , where $\\sigma$ is constant and $\\omega$ varies) and constant$\\omega$ lines (lines $s\\,=\\,\\sigma\\,+\\,j\\omega$ , where $\\omega$ is constant and $\\sigma$ varies) in the $s$ plane.The $\\sigma=0$ line (the $j\\omega$ axis) in the $s$ plane maps into the Nyquist plot in the $G(s)$ plane.The constant$\\sigma$ lines in the $s$ plane map into curves that are similar to the Nyquist plot and are in a sense parallel to the Nyquist plot, as shown in Figure 7–63.The constant$\\omega$ lines in the $s$ plane map into curves, also shown in Figure 7–63.  \n\nAlthough the shapes of constant$\\sigma$ and constant$\\omega$ loci in the $G(s)$ plane and the closeness of approach of the $G(j\\omega)$ locus to the $-1+j0$ point depend on a particular $G(s)$ , the closeness of approach of the $G(j\\omega)$ locus to the $-1+j0$ point is an indication of the relative stability of a stable system. In general, we may expect that the closer the $G(j\\omega)$ locus is to the $-1+j0$ point, the larger the maximum overshoot is in the step transient response and the longer it takes to damp out.  \n\n![](images/ed767cf6707df8b690fd922ed8c247a275204c24727fde14f8c418718a05d588.jpg)  \nFigure 7–64 Two systems with two closed-loop poles each.  \n\nConsider the two systems shown in Figures 7–64(a) and (b). (In Figure 7–64, the $\\times\\mathbf{\\dot{s}}$ indicate closed-loop poles.) System (a) is obviously more stable than system (b) because the closed-loop poles of system (a) are located farther left than those of system (b). Figures 7–65(a) and (b) show the conformal mapping of $s$ -plane grids into the $G(s)$ plane. The closer the closed-loop poles are located to the $j\\omega$ axis, the closer the $G(j\\omega)$ locus is to the $-1+j0$ point.  \n\n![](images/66c6ee28d46117cf8558186ec4d2306c9078d072bfa045387aa8280e827c9c80.jpg)  \nFigure 7–65 Conformal mappings of $s$ -plane grids for the systems shown in Figure 7–64 into the $G(s)$ plane.  \n\nFigure 7–66   \nPolar plots of   \n$\\frac{K(1\\,+\\,j\\omega T_{a})(1\\,+\\,j\\omega T_{b})\\cdots}{(j\\omega)(1\\,+\\,j\\omega T_{1})(1\\,+\\,j\\omega T_{2})\\cdots}.$  \n\nPhase and Gain Margins. Figure 7–66 shows the polar plots of $G(j\\omega)$ for three different values of the open-loop gain $K$ . For a large value of the gain $K$ , the system is unstable.As the gain is decreased to a certain value, the $G(j\\omega)$ locus passes through the $-1+j0$ point.This means that with this gain value the system is on the verge of instability, and the system will exhibit sustained oscillations. For a small value of the gain $K$ ,the system is stable.  \n\nIn general, the closer the $G(j\\omega)$ locus comes to encircling the $-1\\,+\\,j0$ point, the more oscillatory is the system response.The closeness of the $G(j\\omega)$ locus to the $-1+j0$ point can be used as a measure of the margin of stability. (This does not apply, however, to conditionally stable systems.) It is common practice to represent the closeness in terms of phase margin and gain margin.  \n\nPhase margin: The phase margin is that amount of additional phase lag at the gain crossover frequency required to bring the system to the verge of instability.The gain crossover frequency is the frequency at which $\\left|G(j\\omega)\\right|$ ,magnitude of the ope loop transfer function, is unity.The phase margin $\\gamma$ is 180° plus the phase angle $\\phi$ of the open-loop transfer function at the gain crossover frequency, or  \n\n$$\n\\gamma=180^{\\circ}+\\phi\n$$  \n\n![](images/a1f8c8fee7c8bf4439840bcbc63ae2d1cf2db6a48b8f20432ca8bbe80b177eba.jpg)  \n\nFigures 7–67(a), (b), and (c) illustrate the phase margin of both a stable system and an unstable system in Bode diagrams,polar plots,and log-magnitude-versus-phase plots. In the polar plot, a line may be drawn from the origin to the point at which the unit circle crosses the $G(j\\omega)$ locus. If this line lies below (above) the negative real axis, then the angle $\\gamma$ is positive (negative).The angle from the negative real axis to this line is the phase margin.The phase margin is positive for $\\gamma>0$ and negative for $\\gamma<0$ . For a minimumphase system to be stable, the phase margin must be positive. In the logarithmic plots, the critical point in the complex plane corresponds to the 0-dB and $-180^{\\circ}$ lines.  \n\n![](images/ec909bccc0d6b5b580a249cc6481087d5e88ff8db0468cf85041f90fdb618f32.jpg)  \nFigure 7–67 Phase and gain margins of stable and unstable systems. (a) Bode diagrams; (b) polar plots; (c) log-magnitudeversus-phase plots.   \n(c)  \n\nGain m gin: The gain margin is the r cal of the magnitude $\\left|G(j\\omega)\\right|$ at the frequency at which the phase angle is $-180^{\\circ}$ °. Defining the phase crossover frequency $\\omega_{1}$ to be the frequency at which the phase angle of the open-loop transfer function equals $-180^{\\circ}$ gives the gain margin $K_{g}$ :  \n\n$$\nK_{g}=\\frac{1}{\\left|G(j\\omega_{1})\\right|}\n$$  \n\nIn terms of decibels,  \n\n$$\nK_{g}\\,\\mathrm{dB}\\,=\\,20\\log K_{g}=-20\\log\\left|G\\!\\left(j\\omega_{1}\\right)\\right|\n$$  \n\nThe gain margin expressed in decibels is positive if $K_{g}$ is greater than unity and negative if $K_{g}$ is smaller than unity.Thus, a positive gain margin (in decibels) means that the system is stable, and a negative gain margin (in decibels) means that the system is unstable.The gain margin is shown in Figures 7–67(a), (b), and (c).  \n\nFor a stable minimum-phase system,the gain margin indicates how much the gain can be increased before the system becomes unstable. For an unstable system, the gain margin is indicative of how much the gain must be decreased to make the system stable.  \n\nThe gain margin of a first- or second-order system is infinite since the polar plots for such systems do not cross the negative real axis. Thus, theoretically, first- or secondorder systems cannot be unstable. (Note, however, that so-called first- or second-order systems are only approximations in the sense that small time lags are neglected in deriving the system equations and are thus not truly first- or second-order systems. If these small lags are accounted for, the so-called first- or second-order systems may become unstable.)  \n\nIt is noted that for a nonminimum-phase system with unstable open loop the stability condition will not be satisfied unless the $G(j\\omega)$ plot encircles the $-1+j0$ point. Hence, such a stable nonminimum-phase system will have negative phase and gain margins.  \n\nIt is also important to point out that conditionally stable systems will have two or more phase crossover frequencies, and some higher-order systems with complicated numerator dynamics may also have two or more gain crossover frequencies, as shown in Figure 7–68. For stable systems having two or more gain crossover frequencies, the phase margin is measured at the highest gain crossover frequency.  \n\nA Few Comments on Phase and Gain Margins. The phase and gain margins of a control system are a measure of the closeness of the polar plot to the $-1+j0$ point. Therefore, these margins may be used as design criteria.  \n\nIt should be noted that either the gain margin alone or the phase margin alone does not give a sufficient indication of the relative stability. Both should be given in the determination of relative stability.  \n\nFor a minimum-phase system, both the phase and gain margins must be positive for the system to be stable. Negative margins indicate instability.  \n\nProper phase and gain margins ensure us against variations in the system components and are specified for definite positive values.The two values bound the behavior of the closed-loop system near the resonant frequency. For satisfactory performance, the phase margin should be between $30^{\\circ}$ and $60^{\\circ}$ , and the gain margin should be greater than 6 dB. With these values, a minimum-phase system has guaranteed stability, even if the openloop gain and time constants of the components vary to a certain extent.Although the phase and gain margins give only rough estimates of the effective damping ratio of the closed-loop system, they do offer a convenient means for designing control systems or adjusting the gain constants of systems.  \n\n![](images/c677f9cdb5aee23469ed33c48e23c3fed2bf7133734661dc58d0230d3e9ddf57.jpg)  \nFigure 7–68 Polar plots showing more than two phase or gain crossover frequencies.  \n\nObtain the phase and gain margins of the system shown in Figure 7–69 for the two cases where $K=10$ and $K=100$ .  \n\nFor minimum-phase systems, the magnitude and phase characteristics of the openloop transfer function are definitely related.The requirement that the phase margin be between $30^{\\circ}$ and $60^{\\circ}$ means that in a Bode diagram the slope of the log-magnitude curve at the gain crossover frequency should be more gradual than $-40$ dB 'decade. In most practical cases, a slope of $-20$ dB 'decade is desirable at the gain crossover frequency for stability. If it is $-40$ dB 'decade, the system could be either stable or unstable. (Even if the system is stable,however,the phase margin is small.) If the slope at the gain crossover frequency is $-60$ dB 'decade or steeper, the system is most likely unstable.  \n\nFor nonminimum-phase systems, the correct interpretation of stability margins requires careful study.The best way to determine the stability of nonminimum-phase systems is to use the Nyquist diagram approach rather than Bode diagram approach.  \n\nFigure 7–69 Control system.  \n\n![](images/e30bfa33b720cd07a6673fa542f04d39f312fcb298f7fd08d4c4e1e64187b3ab.jpg)  \n\n![](images/c743b1aa75f1ed58ccfe061361c0ea3df36fa9332669118d30fa3febd174c58e.jpg)  \nFigure 7–70 Bode diagrams of the system shown in Figure 7–69; (a) with $K=10$ and (b) with $K=100$ .  \n\nThe phase and gain margins can easily be obtained from the Bode diagram.A Bode diagram of the given open-loop transfer function with $K=10$ is shown in Figure 7–70(a).The phase and gain margins for $K=10$ are  \n\n$$\n\\mathrm{Phase\\;margin}\\,=\\,21^{\\circ},\\qquad\\mathrm{Gain\\;margin}\\,=\\,8\\;\\mathrm{dB}\n$$  \n\nTherefore, the system gain may be increased by $\\mathrm{8\\dB}$ before the instability occurs. Increasing the gain from $K=10$ to $K=100$ shifts the 0-dB axis down by $20\\:\\mathrm{dB}$ , as shown in Figure 7–70(b).The phase and gain margins are  \n\n$$\n\\mathrm{Phase\\;margin}\\,=\\,-30^{\\circ},\\qquad\\mathrm{Gain\\;margin}\\,=\\,-12\\,\\mathrm{dB}\n$$  \n\nThus, the system is stable for $K=10$ ,but unstable for $K=100$ .  \n\nNotice that one of the very convenient aspects of the Bode diagram approach is the ease with which the effects of gain changes can be evaluated.Note that to obtain satisfactory performance,we must increase the phase margin to $30^{\\circ}\\sim60^{\\circ}$ .This can be done by decreasing the gain $K$ . Decreasing $K$ is not desirable, however, since a small value of $K$ will yield a large error for the ramp input. This suggests that reshaping of the open-loop frequency-response curve by adding compensation may be necessary. Compensation techniques are discussed in detail in Sections 7–11 through 7–13.  \n\nObtaining Gain Margin, Phase Margin, Phase-Crossover Frequency, and GainCrossover Frequency with MATLAB. The gain margin,phase margin,phase-crossover frequency,and gain-crossover frequency can be obtained easily with MATLAB.The command to be used is  \n\n$$\n[\\mathrm{Gm},\\mathrm{pm},\\mathrm{wcp},\\mathrm{wcg}]=\\mathrm{margin}(\\mathrm{sys})\n$$  \n\nEXAMPLE 7–21  \n\nFigure 7–71 Closed-loop system.  \n\nwhere Gm is the gain margin, pm is the phase margin, wcp is the phase-crossover frequency, and wcg is the gain-crossover frequency. For details of how to use this command, see Example 7–21.  \n\nDraw a Bode diagram of the open-loop transfer function $G(s)$ of the closed-loop system shown in Figure 7–71. Determine the gain margin, phase margin, phase-crossover frequency, and gaincrossover frequency with MATLAB.  \n\nA MATLAB program to plot a Bode diagram and to obtain the gain margin, phase margin, phase-crossover frequency, and gain-crossover frequency is shown in MATLAB Program 7–11. The Bode diagram of $G(s)$ is shown in Figure 7–72.  \n\n![](images/6cd9f933bc3ab816655558f02a3ed1188b140726b9b761eb07e898a9a501f82c.jpg)  \n\n![](images/c4720a7dab442b8e1342f2ad2f01846aeb771308ce50fe2ea2f632d249f08ba4.jpg)  \n\n![](images/24733665eee46e777b845bd46b060b6473b6e9167220301d35a4d8b2606ba63b.jpg)  \nFigure 7–72 Bode diagram of $G(s)$ shown in Figure 7–71.  \n\nResonant Peak Magnitude $M_{r}$ and Resonant Frequency $\\pmb{\\omega}_{r}$ .Consider the standard second-order system shown in Figure 7–73.The closed-loop transfer function is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nwhere $\\zeta$ and $\\omega_{n}$ are the damping ratio and the undamped natural frequency, respectively. The closed-loop frequency response is  \n\n$$\n\\frac{C(j\\omega)}{R(j\\omega)}=\\frac{1}{\\left(1\\,-\\frac{\\omega^{2}}{\\omega_{n}^{2}}\\right)\\,+\\,j2\\zeta\\,\\frac{\\omega}{\\omega_{n}}}=M e^{j\\alpha}\n$$  \n\nwhere  \n\n$$\n{\\cal M}={\\frac{1}{\\sqrt{\\left(1-{\\frac{\\omega^{2}}{\\omega_{n}^{2}}}\\right)^{2}+\\left(2\\zeta\\,{\\frac{\\omega}{\\omega_{n}}}\\right)^{2}}}},\\qquad\\alpha=-\\tan^{-1}{\\frac{2\\zeta\\,{\\frac{\\omega}{\\omega_{n}}}}{1-{\\frac{\\omega^{2}}{\\omega_{n}^{2}}}}}\n$$  \n\nAs given by Equation (7–12), for $0\\le\\zeta\\le0.707$ , the maximum value of $M$ occurs at the frequency $\\omega_{r}$ , where  \n\n$$\n\\omega_{r}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,2\\zeta^{2}}\n$$  \n\nThe frequency $\\omega_{r}$ is the resonant frequency. At the resonant frequency, the value of $M$ is maximum and is given by Equation (7–13), rewritten  \n\n$$\nM_{r}=\\frac{1}{2\\zeta\\sqrt{1-\\zeta^{2}}}\n$$  \n\nwhere $M_{r}$ is defined as the resonant peak magnitude . The resonant peak magnitude is related to the damping of the system.  \n\nThe magnitude of the resonant peak gives an indication of the relative stability of the system. A large resonant peak magnitude indicates the presence of a pair of dominant closed-loop poles with small damping ratio, which will yield an undesirable transient response.A smaller resonant peak magnitude, on the other hand, indicates the absence of a pair of dominant closed-loop poles with small damping ratio, meaning that the system is well damped.  \n\nRemember that $\\omega_{r}$ is real only if $\\zeta<0.707.$ .Thus, there is no closed-loop resonance if $\\zeta>0.707$ . [The value of $M_{r}$ is unity only if $\\zeta>0.707$ . See Equation (7–14).] Since the values of $M_{r}$ and $\\omega_{r}$ can be easily measured in a physical system, they are quite useful for checking agreement between theoretical and experimental analyses.  \n\n![](images/e30f9b5f3311d9800f0f2acfe3606d01a3900f14e514836f6040954068d210d9.jpg)  \nFigure 7–73 Standard secondorder system.  \n\nIt is noted, however, that in practical design problems the phase margin and gain margin are more frequently specified than the resonant peak magnitude to indicate the degree of damping in a system.  \n\nCorrelation between Step Transient Response and Frequency Response in the Standard Second-Order System. The maximum overshoot in the unit-step response of the standard second-order system, as shown in Figure 7–73, can be exactly correlated with the resonant peak magnitude in the frequency response. Hence, essentially the same information about the system dynamics is contained in the frequency response as is in the transient response.  \n\nFor a unit-step input,the output of the system shown in Figure 7–73 is given by Equation (5–12), or  \n\n$$\nc(t)\\,=\\,1\\,-\\,e^{-\\zeta\\omega_{n}t}\\biggl(\\cos\\omega_{d}t\\,+\\,\\frac{\\zeta}{\\sqrt{1\\,-\\,\\zeta^{2}}}\\sin\\omega_{d}t\\biggl),\\mathrm{~\\\\\\~for\\,}t\\ge0\n$$  \n\nwhere  \n\n$$\n\\omega_{d}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}\n$$  \n\nOn the other hand, the maximum overshoot $M_{p}$ 2 for the unit-step response is given by Equation (5–21), or  \n\n$$\nM_{p}\\,=\\,e^{-(\\zeta/\\sqrt{1-\\zeta^{2}})\\pi}\n$$  \n\nThis maximum overshoot occurs in the transient response that has the damped natural frequency $\\omega_{d}\\,=\\,\\omega_{n}\\sqrt{1\\,-\\,\\zeta^{2}}$ .The maximum overshoot becomes excessive for values of $\\zeta<0.4$ .  \n\nSince the second-order system shown in Figure 7–73 has the open-loop transfer function  \n\n$$\nG(s)={\\frac{\\omega_{n}^{2}}{s(s+2\\zeta\\omega_{n})}}\n$$  \n\nfor sinusoidal operation, the magnitude of $G(j\\omega)$ becomes unity when  \n\n$$\n\\omega\\,=\\,\\omega_{n}\\sqrt{\\sqrt{1\\,+\\,4\\zeta^{4}}\\,-\\,2\\zeta^{2}}\n$$  \n\nwhich can be obtai equating $\\left|G(j\\omega)\\right|$ to unity and solving for $\\omega$ .At this frequency, the phase angle of $G(j\\omega)$ is  \n\n$$\n\\begin{array}{r}{\\boxed{G(j\\omega)=-\\boxed{j\\omega}-\\boxed{j\\omega+2\\zeta\\omega_{n}}=-90^{\\circ}-\\tan^{-1}\\frac{\\sqrt{\\sqrt{1+4\\zeta^{4}}-2\\zeta^{2}}}{2\\zeta}}}\\end{array}\n$$  \n\nThus, the phase margin $\\gamma$ is  \n\n$$\n{\\begin{array}{r l}&{\\gamma=180^{\\circ}+\\,\\underbrace{\\left|G(j\\omega)\\right|}_{\\mathrm{tan}^{-1}{\\frac{\\sqrt{\\sqrt{1+4\\zeta^{4}}-2\\zeta^{2}}}{2\\zeta}}}}\\\\ &{=\\,\\tan^{-1}{\\frac{2\\zeta}{\\sqrt{\\sqrt{1+4\\zeta^{4}}-2\\zeta^{2}}}}}\\end{array}}\n$$  \n\nEquation (7–21) gives the relationship between the damping ratio $\\zeta$ and the phase margin $\\gamma$ . (Notice that the phase margin $\\gamma$ is a function only of the damping ratio $\\zeta$ .)  \n\n![](images/6aff73ab036baf87fe41f3ee2f85e08225d1520c8ac18c854b249940d864bd91.jpg)  \nFigure 7–74 Curve $\\gamma$ (phase margin) versus $\\zeta$ for the system shown in Figure 7–73.  \n\nIn the following,we shall summarize the correlation between the step transient response and frequency response of the standard second-order system given by Equation (7–16):  \n\n1. The phase margin and the damping ratio are directly related.Figure 7–74 shows a plot of the phase margin $\\gamma$ as a function of the damping ratio $\\zeta$ .It is noted that for the standard second-order system shown in Figure 7–73,the phase margin $\\gamma$ and the damping ratio $\\zeta$ are related approximately by a straight line for $0\\leq\\zeta\\leq0.6,$ , as follows:  \n\n$$\n\\zeta=\\frac{\\gamma}{100^{\\circ}}\n$$  \n\nThus a phase margin of $60^{\\circ}$ corresponds to a damping ratio of 0.6. For higher-order systems having a dominant pair of closed-loop poles, this relationship may be used as a rule of thumb in estimating the relative stability in the transient response (that is, the damping ratio) from the frequency response.  \n\n2. Referring to Equations (7–17) and (7–19), we see that the values of $\\omega_{r}$ and $\\omega_{d}$ are almost the same for small values of $\\zeta$ .Thus, for small values of $\\zeta$ , the value of $\\omega_{r}$ is indicative of the speed of the transient response of the system. 3. From Equations (7–18) and (7–20), we note that the smaller the value of $\\zeta$ is, the larger the values of tion of $\\zeta$ is shown in Figure 7–75.A close relationship between $M_{r}$ and $M_{p}$ are.The correlation between $M_{r}$ $M_{r}$ and A and $M_{p}$ $M_{p}$ as a funccan be Bseen for $\\zeta>0.4.$ For very small values of $\\zeta,M_{r}$ becomes very large $\\left[M_{r}\\gg1\\right]$ , while the value of $M_{p}$ does not exceed 1.  \n\nCorrelation between Step Transient Response and Frequency Response in General Systems. The design of control systems is very often carried out on the basis of the frequency response.The main reason for this is the relative simplicity of this approach compared with others. Since in many applications it is the transient response of the system to aperiodic inputs rather than the steady-state response to sinusoidal inputs that is of primary concern, the question of correlation between transient response and frequency response arises.  \n\n![](images/91b2069d738b80434326b5883915e31385baba81296a3ad5de614ef6726bcdcb.jpg)  \nFigure 7–75 Curves $M_{r}$ versus $\\zeta$ and $M_{p}$ versus $\\zeta$ for the system shown in Figure 7–73.  \n\nFor the standard second-order system shown in Figure 7–73, mathematical relationships correlating the step transient response and frequency response can be obtained easily.The time response of the standard second-order system can be predicted exactly from a knowledge of the $M_{r}$ and $\\omega_{r}$ of its closed-loop frequency response.  \n\nFor nonstandard second-order systems and higher-order systems, the correlation is more complex, and the transient response may not be predicted easily from the frequency response because additional zeros and/or poles may change the correlation between the step transient response and the frequency response existing for the standard second-order system. Mathematical techniques for obtaining the exact correlation are available, but they are very laborious and of little practical value.  \n\nThe applicability of the transient-response–frequency-response correlation existing for the standard second-order system shown in Figure 7–73 to higher-order systems depends on the presence of a dominant pair of complex-conjugate closed-loop poles in the latter systems. Clearly, if the frequency response of a higher-order system is dominated by a pair of complex-conjugate closed-loop poles, the transient-response– frequency-response correlation existing for the standard second-order system can be extended to the higher-order system.  \n\nFor linear, time-invariant, higher-order systems having a dominant pair of complexconjugate closed-loop poles, the following relationships generally exist between the step transient response and frequency response:  \n\n1. The value of formance is usually obtained if the value of $M_{r}$ is indicative of the relative stability. Satisfactory transient perB$M_{r}$ is in the range $1.0<M_{r}<1.4$ $\\mathrm{0\\dB<}M_{r}<3$ dB , whi corresponds to an effective da ing ratio of $0.4<\\zeta<0.7.$ . For values of $M_{r}$ greater than 1.5, the step transient response may exhibit several overshoots. (Note that, in general, a large value of $M_{r}$ corresponds to a large overshoot in the step transient response. If the system is subjected to noise signals whose frequencies are near the resonant frequency $\\omega_{r}$ , the noise will be amplified in the output and will present serious problems.) 2. The magnitude of the resonant frequency $\\omega_{r}$ is indicative of the speed of the transient response.The larger the value of $\\omega_{r}$ , the faster the time response is. In other words, the rise time varies inversely with $\\omega_{r}$ . In terms of the open-loop frequency response, the damped natural frequency in the transient response is somewhere between the gain crossover frequency and phase crossover frequency.  \n\n3. The resonant peak frequency $\\omega_{r}$ and the damped natural frequency $\\omega_{d}$ for the step transient response are very close to each other for lightly damped systems.  \n\nThe three relationships just listed are useful for correlating the step transient response with the frequency response of higher-order systems, provided that they can be approximated by the standard second-order system or a pair of complex-conjugate closed-loop poles. If a higher-order system satisfies this condition, a set of time-domain specifications may be translated into frequency-domain specifications. This simplifies greatly the design work or compensation work of higher-order systems.  \n\nIn addition to the phase margin, gain margin, resonant peak $M_{r}$ , and resonant frequency $\\omega_{r}$ , there are other frequency-domain quantities commonly used in performance specifications.They are the cutoff frequency, bandwidth, and the cutoff rate.These will be defined in what follows.  \n\nCutoff Frequency and Bandwidth. Referring to Figure 7–76, the frequency $\\omega_{b}$ at which the magnitude of the closed-loop frequency response is $3\\;\\mathrm{dB}$ below its zero-frequency value is called the cutoff frequency .Thus  \n\n$$\n\\left|{\\frac{C(j\\omega)}{R(j\\omega)}}\\right|\\,<\\,\\left|{\\frac{C(j0)}{R(j0)}}\\right|\\,-\\,3\\;\\mathrm{dB},\\qquad\\mathrm{for}\\;\\omega>\\omega_{b}\n$$  \n\nFor systems in which $\\left|C(j0)/R(j0)\\right|\\,=\\,0\\;\\mathrm{dB}$ ,  \n\n$$\n\\left|{\\frac{C(j\\omega)}{R(j\\omega)}}\\right|\\,<-3\\,\\mathrm{dB},\\qquad\\mathrm{for}\\,\\omega>\\,\\omega_{b}\n$$  \n\nThe closed-loop system filters out the signal components whose frequencies are greater than the cutoff frequency and transmits those signal components with frequencies lower than the cutoff frequency.  \n\nThe frequency range $0\\leq\\omega\\leq\\omega_{b}$ in which the magnitude of $C(j\\omega)/R(j\\omega)$ is greater than $-3\\;\\mathrm{dB}$ is called the bandwidth of the system. The bandwidth indicates the frequency where the gain starts to fall off from its low-frequency value.Thus, the bandwidth indicates how well the system will track an input sinusoid. Note that for a given $\\omega_{n}$ , the rise time increases with increasing damping ratio $\\zeta$ . On the other hand, the bandwidth decreases with the increase in $\\zeta$ .Therefore, the rise time and the bandwidth are inversely proportional to each other.  \n\n![](images/a6c0ac2e2eaadc46e8ce7cedef11726c606be5c5efaa59969a99d06565425710.jpg)  \nFigure 7–76 Plot of a closed-loop frequency response curve showing cutoff frequency $\\omega_{b}$ and bandwidth.   \nv  \n\nThe specification of the bandwidth may be determined by the following factors:  \n\n1. The ability to reproduce the input signal.A large bandwidth corresponds to a small rise time,or fast response.Roughly speaking,we can say that the bandwidth is proportional to the speed of response. (For example, to decrease the rise time in the step response by a factor of 2, the bandwidth must be increased by approximately a factor of 2.)  \n\n2. The necessary filtering characteristics for high-frequency noise.  \n\nFor the system to follow arbitrary inputs accurately, it must have a large bandwidth. From the viewpoint of noise,however,the bandwidth should not be too large.Thus,there are conflicting requirements on the bandwidth,and a compromise is usually necessary for good design. Note that a system with large bandwidth requires high-performance components, so the cost of components usually increases with the bandwidth.  \n\nCutoff Rate. The cutoff rate is the slope of the log-magnitude curve near the cutoff frequency.The cutoff rate indicates the ability of a system to distinguish the signal from noise.  \n\nIt is noted that a closed-loop frequency response curve with a steep cutoff characteristic may have a large resonant peak magnitude, which implies that the system has a relatively small stability margin.  \n\nEXAMPLE 7–22 Consider the following two systems:  \n\n$$\n\\mathrm{System~I:}\\quad\\frac{C(s)}{R(s)}=\\frac{1}{s\\,+\\,1},\\qquad\\mathrm{System~II:}\\quad\\frac{C(s)}{R(s)}=\\frac{1}{3s\\,+\\,1}\n$$  \n\nCompare the bandwidths of these two systems.Show that the system with the larger bandwidth has a faster speed of response and can follow the input much better than the one with the smaller bandwidth.  \n\nFigure 7–77(a) shows the closed-loop frequency-response curves for the two systems. (Asymptotic curves are shown by dashed lines.) We find that the bandwidth of system I is $0\\leq\\omega\\leq1$ rad 'sec and that of system II is $0\\leq\\omega\\leq0.33\\,\\mathrm{rad/sec}.$ . Figures 7–77(b) and (c) show, respectively, the unit-step response and unit-ramp response curves for the two systems.Clearly,system I,whose bandwidth is three times wider than that of system II,has a faster speed of response and can follow the input much better.  \n\n![](images/1d0d3746234a08ca3bfc797b6830fc764f471fa541d26039675e5faa8cdedebe.jpg)  \nFigure 7–77 Comparison of dynamic characteristics of the two systems considered in Example 7–22. (a) Closed-loop frequency-response curves; (b) unit-step response curves; (c) unit-ramp response curves.  \n\nMATLAB Approach to Get Resonant Peak, Resonant Frequency, and Bandwidth. The resonant peak is the value of the maximum magnitude (in decibels) of the closed-loop frequency response.The resonant frequency is the frequency that yields the maximum magnitude. MATLAB commands to be used for obtaining the resonant peak and resonant frequency are as follows:  \n\n[mag,phase,w] $=$ bode(num,den,w); or [mag,phase,w] $=$ bode(sys,w); $[\\mathsf{M}\\mathsf{p},\\mathsf{k}]=\\mathsf{m a x}(\\mathsf{m a g});$   \nresonant_peak $=20^{*}|\\log10(M{\\sf p}),$ ;  \nresonant_frequency $\\prime=\\mathsf{w}(\\mathsf{k})$  \n\nThe bandwidth can be obtained by entering the following lines in the program:  \n\n$\\begin{array}{r l}&{\\mathsf{n}=1\\,;}\\\\ &{\\mathsf{w h i l e\\ 20^{*}l o g\\,1\\,0(m a g(n))}>=-3\\,;\\,\\mathsf{n}=\\mathsf{n}+1\\,;}\\end{array}$   \nend   \nbandwidth $=\\mathsf{W}(\\mathsf{n})$  \n\nFor a detailed MATLAB program, see Example 7–23.  \n\n# EXAMPLE 7–23  \n\nConsider the system shown in Figure 7–78.Using MATLAB,obtain a Bode diagram for the closedloop transfer function. Obtain also the resonant peak, resonant frequency, and bandwidth. MATLAB Program 7–12 produces a Bode diagram for the closed-loop system as well as the resonant peak, resonant frequency, and bandwidth. The resulting Bode diagram is shown in  \n\n![](images/5ee23b7aa17aae5f153625ecaa75795234146f04e4e7e0612a95f54d16f3b1a3.jpg)  \n\nFigure 7–78 Closed-loop system.  \n\n![](images/036575555f2bc25f488bd773e6d4502b12b6689d12110874687ca16232920052.jpg)  \nFigure 7–79 Bode diagram of the closed-loop transfer function of the system shown in Figure 7–78.  \n\n![](images/1c1d4d85d77501f43e76801bb015504e2857713ad23dfa53b85bad5335a2d669.jpg)  \n\nFigure 7–79.The resonant peak is obtained as 5.2388 dB.The resonant frequency is 0.7906 rad 'sec.   \nThe bandwidth is 1.2649 rad 'sec. These values can be verified from Figure 7–78.  \n\n# 7–8 CLOSED-LOOP FREQUENCY RESPONSE OF UNITYFEEDBACK SYSTEMS  \n\nClosed-Loop Frequency Response. For a stable, unity-feedback closed-loop system, the closed-loop frequency response can be obtained easily from that of the openloop frequency response. Consider the unity-feedback system shown in Figure 7–80(a). The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{G(s)}{1+G(s)}\n$$  \n\nIn the Nyquist or polar plot shown in Figure 7–80(b), the vector where angle of the vector to the Nyquist locu $\\omega_{1}$ is the frequency at point $\\overrightarrow{O A}$ !pre is $\\big/G\\big(j\\omega_{1}\\big)$ A $1+G\\!\\left(j\\omega_{1}\\right)$ B.$A$ The vector . The length of the vector . T $\\overrightarrow{P A}$ !,fore, the ratio of the vector from the $\\overrightarrow{O A}$ $\\overrightarrow{O A}$ !$\\overrightarrow{O A}$ re is $\\left|G\\!\\left(j\\omega_{1}\\right)\\right|$ @!,$-1+j0$ to $\\overrightarrow{P A}$ $G\\!\\left(j\\omega_{1}\\right)$ !reprepoint d the ,sents the closed-loop frequency response, or  \n\n$$\n\\frac{\\overrightarrow{O A}}{\\overrightarrow{P A}}=\\frac{G\\!\\left(j\\omega_{1}\\right)}{1+G\\!\\left(j\\omega_{1}\\right)}=\\frac{C\\!\\left(j\\omega_{1}\\right)}{R\\!\\left(j\\omega_{1}\\right)}\n$$  \n\n![](images/4711e2e54b9049b1c1dec4f5a05210c5d6f769cc33b60dbed786efc2da1bfa3a.jpg)  \nFigure 7–80 (a) Unity-feedback system; (b) determination of closed-loop frequency response from open-loop frequency response.   \n(a)  \n\n![](images/9abacd7bde444d86860c4c5bfbf3c8db0d50fe6ea856ba807a210fc3b4e6110a.jpg)  \n(b)  \n\nThe magnitude of the closed-loop transfer function at tudes of the angle formed by the vectors $\\overrightarrow{O A}$ !to $\\overrightarrow{P A}$ !. The phase angle of the closed-loop transfer function at $\\overrightarrow{O A}$ !to $\\overrightarrow{P A}$ !—that is $\\phi\\,-\\,\\theta$ $\\omega\\,=\\,\\omega_{1}$ , shown in Figure 7–80(b). By is the ratio of the magni$\\omega\\,=\\,\\omega_{1}$ is measuring the magnitude and phase angle at different frequency points, the closed-loop frequency-response curve can be obtained.  \n\nLet us define the magnitude of the closed-loop frequency response as $M$ and the phase angle as $\\alpha$ , or  \n\n$$\n\\frac{C(j\\omega)}{R(j\\omega)}=M e^{j\\alpha}\n$$  \n\nIn the following, we shall find the constant-magnitude loci and constant-phase-angle loci. Such loci are convenient in determining the closed-loop frequency response from the polar plot or Nyquist plot.  \n\nConstant-Magnitude Loci ( $M$ circles). To obtain the constant-magnitude loci, let us first note that $G(j\\omega)$ is a complex quantity and can be written as follows:  \n\n$$\nG(j\\omega)\\,=\\,X\\,+\\,j Y\n$$  \n\nwhere $X$ and $Y$ are real quantities.Then $M$ is given by  \n\n$$\nM=\\frac{|X+j Y|}{|1+X+j Y|}\n$$  \n\nand $M^{2}$ is  \n\n$$\nM^{2}={\\frac{X^{2}+Y^{2}}{(1\\,+\\,X)^{2}\\,+\\,Y^{2}}}\n$$  \n\nHence  \n\n$$\nX^{2}(1\\,-\\,M^{2})\\,-\\,2M^{2}X\\,-\\,M^{2}\\,+\\,(1\\,-\\,M^{2})Y^{2}\\,=\\,0\n$$  \n\nIf $M=1$ , then from Equation (7–22), we obtain $X=-{\\frac{1}{2}}$ .This is the equation of a straight line parallel to the $Y$ axis and passing throu int $(-\\frac{1}{2},0)$ .  \n\nIf $M\\ne1$ Equation (7–22) can be written  \n\n$$\nX^{2}+\\frac{2M^{2}}{M^{2}-1}\\,X\\,+\\frac{M^{2}}{M^{2}-1}+\\,Y^{2}=0\n$$  \n\nIf the term $M^{2}/(M^{2}\\mathrm{~-~}1)^{2}$ is added to both sides of this last equation, we obtain  \n\n$$\n\\left(X\\,+\\,\\frac{M^{2}}{M^{2}\\,-\\,1}\\right)^{2}\\,+\\,Y^{2}=\\frac{M^{2}}{\\left(M^{2}\\,-\\,1\\right)^{2}}\n$$  \n\nEquation (7–23 oa circle with center at $X=-M^{2}/(M^{2}\\,-\\,1),Y\\,=\\,0$ and with radius $\\bar{|M/(M^{2}\\ -1)|}$ .  \n\nThe constant Mloci on the $G(s)$ plane are thus a family of circles.The center and radius of the circle for a given value of $M$ can be easily calculated. For example, for $M=1.3$ , the center is at $(-2.45,0)$ and the radius is 1.88. A family of constant $M$ circles is shown in Figure 7–81. It is seen that as $M$ becomes larger compared with 1, the $M$ circles become smaller and converge to the $-1+j0$ point. For $M>1$ , the centers of the $M$ circles lie to the left of the $-1+j0$ point. Similarly, as $M$ becomes smaller compared with 1, the $M$ circle becomes smaller and converges to the origin. For $0<M<1$ ,the centers of the of points equidistant from the origin and from the $M$ circles lie to the right of the origin. A B$-1+j0$ $M=1$ point.As stated earlier, it is corresponds to the locus a straight line passing through the point $\\left(-\\textstyle{\\frac{1}{2}},0\\right)$ and parallel to the imaginary axis. (The constant $M$ circles corresponding to $M>1$ lie to the left of the $M=1$ line, and those corresponding to $0<M<1$ lie to the right of the $M=1$ line.) The $M$ circles are symmetrical with respect to the straight line corresponding to $M=1$ and with respect to the real axis.  \n\n![](images/6e948b8da9e10bf147605f5ffb10ed54a0dae26c7aa53eed26072084a5888367.jpg)  \nFigure 7–81 A family of constant $M$ circles.  \n\nConstant-Phase-Angle Loci ( $N$ Circles). We shall obtain the phase angle $\\alpha$ in terms of $X$ and $Y$ . Since  \n\n$$\n\\Big/e^{j\\alpha}=\\;\\Bigg/\\frac{X\\,+\\,j Y}{1\\,+\\,X\\,+\\,j Y}\n$$  \n\nthe phase angle $\\alpha$ is  \n\n$$\n\\alpha=\\tan^{-1}\\!\\left({\\frac{Y}{X}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{Y}{1\\,+\\,X}}\\right)\n$$  \n\nIf we define  \n\n$$\n\\tan\\alpha\\,=\\,N\n$$  \n\nthen  \n\n$$\nN=\\tan\\!\\left[\\tan^{-1}\\!\\left({\\frac{Y}{X}}\\right)\\,-\\,\\tan^{-1}\\!\\left({\\frac{Y}{1\\,+\\,X}}\\right)\\right]\n$$  \n\nSince  \n\n$$\n\\tan(A\\,-\\,B)={\\frac{\\tan A\\,-\\,\\tan B}{1\\,+\\,\\tan A\\tan B}}\n$$  \n\nwe obtain  \n\n$$\nN={\\frac{{\\cfrac{Y}{X}}-{\\cfrac{Y}{1+X}}}{1+{\\cfrac{Y}{X}}\\left({\\cfrac{Y}{1+X}}\\right)}}={\\frac{Y}{X^{2}+X+Y^{2}}}\n$$  \n\nor  \n\n$$\nX^{2}\\,+\\,X\\,+\\,Y^{2}\\,-\\,{\\frac{1}{N}}\\,Y\\,=\\,0\n$$  \n\nThe addition of $\\textstyle{\\left({\\frac{1}{4}}\\right)}\\,+\\,1/(2N)^{2}$ to both sides of this last equation yields  \n\n$$\n\\left(X+{\\frac{1}{2}}\\right)^{2}+\\left(Y\\,-{\\frac{1}{2N}}\\right)^{2}={\\frac{1}{4}}+\\left({\\frac{1}{2N}}\\right)^{2}\n$$  \n\nThis is an equation of a circle with center at $X=-{\\frac{1}{2}}$ ,$Y\\,=\\,1/(2N)$ and with radius $\\sqrt{{\\frac{1}{4}}\\,+\\,1/(2N)^{2}}$ .For example, if $\\alpha=30^{\\circ}$ , then $N\\,=\\,\\tan\\alpha\\,=\\,0.577$ , and the center and the radius of the circle corresponding to $\\alpha=30^{\\circ}$ are found to be $(-0.5,0.866)$ and unity, respectively. Since Equation (7–24) is satisfied when $X=Y=0$ and $X=-1$ ,$Y=0$ regardless of the value of $N$ , each circle passes through the origin and the $-1+j0$ point. The constant $\\alpha$ loci can be drawn easily,once the value of $N$ is given.A family of constant $N$ circles is shown in Figure 7–82 with $\\alpha$ as a parameter.  \n\nIt should be noted that the constant $N$ locus for a given value of $\\alpha$ is actually not the entire circle, but only an arc. In other words, the $\\alpha=30^{\\circ}$ and $\\alpha=-150^{\\circ}$ arcs are parts of the same circle.This is so because the tangent of an angle remains the same if $\\pm180^{\\circ}$ (or multiples thereof) is added to the angle.  \n\nThe use of the $M$ and $N$ circles enables us to find the entire closed-loop frequency response from the open-loop frequency response $G(j\\omega)$ without calculating the magnitude and phase of the closed-loop transfer function at each frequency.The intersections of the $G(j\\omega)$ locus and the $M$ circles and $N$ circles give the values of $M$ and $N$ at frequency points on the $G(j\\omega)$ locus.  \n\n![](images/da864e7cd414420b41da4308ac4458f8706ba32bf71964bdccad8795f90f40e3.jpg)  \nFigure 7–82 A family of constant $N$ circles.  \n\nThe $N$ circles are multivalued in the sense that the circle for $\\alpha\\,=\\,\\alpha_{1}$ and that for $\\alpha\\,=\\,\\alpha_{1}\\pm180^{\\circ}n$ $\\langle n=1,2,\\ldots\\rangle$ )are the same. In using the $N$ circles for the determination of the phase angle of closed-loop systems, we must interpret the proper value of $\\alpha$ . To avoid any error, start at zero frequency, which corresponds to $\\alpha=0^{\\circ}$ , and proceed to higher frequencies.The phase-angle curve must be continuous.  \n\nGraphically, the intersections of the $G(j\\omega)$ locus and $M$ circles give the values of $M$ at the frequencies denoted on the $G(j\\omega)$ locus. Thus, the constant $M$ circle with the smallest radius that is tangent to the $G(j\\omega)$ locus gives the value of the resonant peak magnitude $M_{r}$ . If it is desired to keep the resonant peak value less than a certain value, then the system should not enclose the critical point $\\langle-1+j0$ point )and, at the same time, there should be no intersections with the particular $M$ circle and the $G(j\\omega)$ locus.  \n\nFigure 7–83(a) shows the $G(j\\omega)$ locus superimposed on a family of $M$ circles. Figure 7–83(b) shows the $G(j\\omega)$ locus superimposed on a family of $N$ circles. From these plots, it is possible to obtain the closed-loop frequency response by inspection. It is seen that the $M=1.1$ circle intersects the $G(j\\omega)$ locus at frequency point $\\omega\\,=\\,\\omega_{1}$ . This means that at this frequency the magnitude of the closed-loop transfer function is 1.1. In Figure 7–83(a), the $M=2$ circle is just tangent to the $\\mathrm{G}(j\\omega)$ locus.Thus, there is only one point on th $G(j\\omega)$ locus for which $\\left|C(j\\omega)/R(j\\omega)\\right|$ is equal t 2. Figure 7–83(c) ws the closed-loop frequency-response curve for the system.The upper curve is the M-versusfrequency $\\omega$ curve, and the lower curve is the phase angle $\\alpha$ -versus-frequency $\\omega$ curve.  \n\nThe resonant peak value is the value of $M$ corresponding to the $M$ circle of smallest radius that is tangent to the $G(j\\omega)$ locus.Thus, in the Nyquist diagram, the resonant peak value $M_{r}$ and the resonant frequency $\\omega_{r}$ can be found from the $M$ -circle tangency to the $G(j\\omega)$ locus. (In the present example, $M_{r}=2$ and $\\omega_{r}\\,=\\,\\omega_{4}$ .)  \n\n![](images/c21978feaa65827659ea5286f11183695440e54ad9c7c6ac061d51508ffee5f8.jpg)  \nFigure 7–83 (a) $G(j\\omega)$ locus superimposed on a family of $M$ circles; (b) $G(j\\omega)$ locus superimposed on a family of $N$ circles; (c) closed-loop frequency-response curves.  \n\nNichols Chart. In dealing with design problems, we find it convenient to construct the $M$ and $N$ loci in the log-magnitude-versus-phase plane. The chart consisting of the $M$ and $N$ loci in the log-magnitude-versus-phase diagram is called the Nichols chart. The $G(j\\omega)$ locus drawn on the Nichols chart gives both the gain characteristics and phase characteristics of the closed-loop transfer function at the same time.The Nichols chart is shown in Figure 7–84, for phase angles between $0^{\\circ}$ and $-240^{\\circ}$ .  \n\nNote that the critical point ($-1+j0$ point )is mapped to the Nichols chart as the point $(0\\,\\mathrm{dB},-180^{\\circ})$ . The Nichols chart contains curves of constant closed-loop magnitude and phase angle. The designer can graphically determine the phase margin, gain margin, resonant peak magnitude, resonant frequency, and bandwidth of the closedloop system from the plot of the open-loop locus, $G(j\\omega)$ .  \n\nThe Nichols chart is symmetric about the $-180^{\\circ}$ axis. The $M$ and $N$ loci repeat for every $360^{\\circ}$ , and there is symmetry at every $180^{\\circ}$ interval.The $M$ loci are centered about the critical point $(0\\,\\mathrm{dB},-180^{\\circ})$ .The Nichols chart is useful for determining the frequency response of the closed loop from that of the open loop. If the open-loop frequency-response curve is superimposed on the Nichols chart, the intersections of the open-loop frequency-response curve $G(j\\omega)$ and the $M$ and $N$ loci give the values of the magnitude $M$ and phase angle $\\alpha$ of the closed-loop frequency response at each frequency point. If the $G(j\\omega)$ locus does not intersect the $M=M_{r}$ locus, but is tangent to it, then the resonant peak value of $M$ of the closed-loop frequency response is given by $M_{r}$ .The resonant frequency is given by the frequency at the point of tangency.  \n\nAs an example, consider the unity-feedback system with the following open-loop transfer function:  \n\n$$\nG(j\\omega)\\,=\\frac{K}{s(s\\,+\\,1)(0.5s\\,+\\,1)},\\qquad K\\,=\\,1\n$$  \n\nTo find the closed-loop frequency response by use of the Nichols chart, the $G(j\\omega)$ locus is constructed in the log-magnitude-versus-phase plane by use of MATLAB or from the Bode diagram.Figure 7–85(a) shows the $G(j\\omega)$ locus together with the $M$ and $N$ loci. The closed-loop frequency-response curves may be constructed by reading the magnitudes and phase angles at various frequency points on the $G(j\\omega)$ locus from the $M$ and $N$ loci, as shown in Figure 7–85(b). Since the largest magnitude contour touched by the $G(j\\omega)$ locus is $5\\;\\mathrm{dB}$ , the resonant peak magnitude $M_{r}$ is 5 dB. The corresponding resonant peak frequency is 0.8 rad 'sec.  \n\n![](images/3a7a6f31f54eb80669224be3b758972e1eb027e2cd9a31fbb3f1d72bd8431e27.jpg)  \nFigure 7–84 Nichols chart.  \n\nNotice that the phase crossover point is the point where the $G(j\\omega)$ locus intersects the $-180^{\\circ}$ axis (for the present system, $\\omega=1.4$ rad 'sec), and the gain crossover point is the point where the locus intersects the 0-dB axis (for the present system, $\\omega\\,=\\,0.76\\;\\mathrm{rad/sec})$ ). The phase margin is the horizontal distance (measured in degrees) between the gain crossover point and the critical point $(0\\,\\mathrm{dB},-180^{\\circ})$ . The gain margin is the distance (in decibels) between the phase crossover point and the critical point.  \n\nThe bandwidth of the closed-loop system can easily be found from the $G(j\\omega)$ locus in the Nichols diagram. The frequency at the intersection of the $G(j\\omega)$ locus and the $M=-3$ dB locus gives the bandwidth.  \n\nIf the open-loop gain $K$ is varied, the shape of the $G(j\\omega)$ locus in the log-magnitudeversus-phase diagram remains the same, but it is shifted up (for increasing $K$ ) or down (for decreasing $K$ ) along the vertical axis. Therefore, the $G(j\\omega)$ locus intersects the $M$ and $N$ loci differently, resulting in a different closed-loop frequency-response curve. For a small value of the gain $K$ ,the $G(j\\omega)$ locus will not be tangent to any of the $M$ loci,which means that there is no resonance in the closed-loop frequency response.  \n\n![](images/9ed6a3fdce4467274c40aaa77c93e6674a47a8d10f01fcfa020fbc2204651290.jpg)  \nFigure 7–85 (a) Plot of $G(j\\omega)$ superimposed on Nichols chart; (b) closed-loop frequency-response curves.  \n\nConsider the unity-feedback control system whose open-loop transfer function is  \n\n$$\nG(j\\omega)\\,=\\,\\frac{K}{j\\omega(1\\,+\\,j\\omega)}\n$$  \n\nDetermine the value of the gain $K$ so that $M_{r}=1.4$ .The first step in the determination of the gain $K$ is to sketch the polar plot of  \n\n$$\n\\frac{G(j\\omega)}{K}=\\frac{1}{j\\omega(1\\,+\\,j\\omega)}\n$$  \n\nFigure 7–86 shows the $M_{r}=1.4$ locus and the $G(j\\omega)/K$ locus. Changing the gain has no effect on the phase angle, but merely moves the curve vertically up for $K>1$ and down for $K<1$ .  \n\nIn Figure 7–86, the $G(j\\omega)/K$ locus must be raised by 4 dB in order that it be tangent to the desired $M_{r}$ locus and that the entire $G(j\\omega)/K$ locus be outside the $M_{r}=1.4$ locus.The amount of vertical shift of the $G(j\\omega)/K$ locus determines the gain necessary to yield the desired value of $M_{r}$ .Thus, by solving  \n\n$$\n20\\log K=4\n$$  \n\nwe obtain  \n\n$$\nK=1.59\n$$  \n\n![](images/e64cef37b28d4ded77326db5484400ad3f7b2b4279bf19a4ebdfb2cb2b30c7e9.jpg)  \n\nFigure 7–86   \nDetermination of the gain $K$ using the   \nNichols chart.  \n\nThe first step in the analysis and design of a control system is to derive a mathematical model of the plant under consideration.Obtaining a model analytically may be quite difficult.We may have to obtain it by means of experimental analysis.The importance of the frequency-response methods is that the transfer function of the plant, or any other component of a system, may be determined by simple frequency-response measurements.  \n\nIf the amplitude ratio and phase shift have been measured at a sufficient number of frequencies within the frequency range of interest, they may be plotted on the Bode diagram.Then the transfer function can be determined by asymptotic approximations.We build up asymptotic log-magnitude curves consisting of several segments. With some trial-and-error juggling of the corner frequencies, it is usually possible to find a very close fit to the curve. (Note that if the frequency is plotted in cycles per second rather than radians per second, the corner frequencies must be converted to radians per second before computing the time constants.)  \n\nSinusoidal-Signal Generators. In performing a frequency-response test, suitable sinusoidal-signal generators must be available. The signal may have to be in mechanical, electrical, or pneumatic form. The frequency ranges needed for the test are approximately 0.001 to $10\\;\\mathrm{Hz}$ for large-time-constant systems and 0.1 to $1000\\ \\mathrm{Hz}$ for small-time-constant systems. The sinusoidal signal must be reasonably free from harmonics or distortion.  \n\nFor very low frequency ranges (below $0.01\\ \\mathrm{Hz}$ ), a mechanical signal generator (together with a suitable pneumatic or electrical transducer if necessary) may be used. For the frequency range from 0.01 to $1000\\ \\mathrm{Hz}$ , a suitable electrical-signal generator (together with a suitable transducer if necessary) may be used.  \n\nDetermination of Minimum-Phase Transfer Functions from Bode Diagrams. As stated previously, whether a system is minimum phase can be determined from the frequency-response curves by examining the high-frequency characteristics.  \n\nTo determine the transfer function, we first draw asymptotes to the experimentally obtained log-magnitude curve. The asymptotes must have slopes of multiples of $\\pm20$ dB 'decade. If the slope of the experimentally obtained log-magnitude curve changes from $-20$ to $-40$ dB 'decade at $\\omega\\,=\\,\\omega_{1}$ , it is c that a factor $1\\bar{/}[1\\,+\\,j(\\omega/\\omega_{1})]$ exists in the transfer function. If the slope changes by $-40$ dB 'decade at $\\omega=\\omega_{2}$ , there must be a quadratic factor of the form  \n\n$$\n\\frac{1}{1\\,+\\,2\\zeta\\!\\left(j\\,\\frac{\\omega}{\\omega_{2}}\\right)\\,+\\,\\left(j\\,\\frac{\\omega}{\\omega_{2}}\\right)^{2}}\n$$  \n\nin the transfer function. The undamped natural frequency of this quadratic factor is equal to the corner frequency $\\omega_{2}$ . The damping ratio $\\zeta$ can be determined from the experimentally obtained log-magnitude curve by measuring the amount of resonant peak near the corner frequency $\\omega_{2}$ and comparing this with the curves shown in Figure 7–9.  \n\nOnce the factors of the transfer function $G(j\\omega)$ have been determined, the gain can be determined from the low-frequency portion of the log-magnitude curve. Since such terms as $1\\,+\\,j(\\omega/\\omega_{1})$ and $1\\,+\\,2\\zeta\\!\\left(j\\omega/\\omega_{2}\\right)\\,+\\,\\left(j\\omega/\\omega_{2}\\right)^{2}$ become unity as $\\omega$ approaches zero, at very low frequencies the sinusoidal transfer function $G(j\\omega)$ can be written  \n\n$$\n\\operatorname*{lim}_{\\omega\\rightarrow0}G(j\\omega)=\\frac{K}{(j\\omega)^{\\lambda}}\n$$  \n\nIn many practical systems, $\\lambda$ equals $0,1$ , or 2.  \n\n1. For $\\lambda=0$ , or type 0 systems,  \n\n$$\nG(j\\omega)\\,=\\,K,\\qquad\\mathrm{for}\\,\\omega\\,\\ll\\,1\n$$  \n\nor  \n\n$$\n20\\log\\bigl|G(j\\omega)\\bigr|\\,=\\,20\\log K,\\qquad\\mathrm{for}\\,\\omega\\,\\ll\\,1\n$$  \n\nThe low-frequency asymptote is a horizontal line at $20\\log K$ dB. The value of $K$ can thus be found from this horizontal asymptote.  \n\n2. For $\\lambda=1$ , or type 1 systems,  \n\n$$\nG(j\\omega)=\\frac{K}{j\\omega},~~~~~\\mathrm{for}\\;\\omega\\;\\ll\\;1\n$$  \n\nor  \n\n$$\n20\\log\\left|{\\cal G}(j\\omega)\\right|\\,=\\,20\\log K\\,-\\,20\\log\\omega,\\qquad\\mathrm{for}\\,\\omega\\,\\ll\\,1\n$$  \n\nwhich indicates that the low-frequency asymptote has the slope $-20$ dB 'decade. The frequency at which the low-frequency asymptote (or its extension) intersects the 0-dB line is numerically equal to $K$ .  \n\n3. For $\\lambda=2$ , or type 2 systems,  \n\n$$\nG(j\\omega)\\,=\\frac{K}{(j\\omega)^{2}},\\;\\;\\;\\;\\;\\;\\mathrm{for}\\;\\omega\\,\\ll\\,1\n$$  \n\nor  \n\n$$\n20\\log\\left|{\\cal G}(j\\omega)\\right|\\,=\\,20\\log K\\,-\\,40\\log\\omega,\\qquad\\mathrm{for}\\,\\omega\\,\\ll\\,1\n$$  \n\nThe slope of the low-frequency asymptote is $-40~\\mathrm{dB}$ 'decade. The frequency at which this asymptote (or its extension) intersects the 0-dB line is numerically equal to $\\sqrt{K}$ .  \n\nExamples of log-magnitude curves for type 0, type 1, and type 2 systems are shown in Figure 7–87, together with the frequency to which the gain $K$ is related.  \n\nThe experimentally obtained phase-angle curve provides a means of checking the transfer function obtained from the log-magnitude curve. For a minimum-phase system, the experimental phase-angle curve should agree reasonably well with the theoretical phase-angle curve obtained from the transfer function just determined.These two phaseangle curves should agree exactly in both the very low and very high frequency ranges. If the experimentally obtained phase angle at very high frequencies (compared with the corner frequencies) is not equal to $-90^{\\circ}(q\\mathrm{~-~}p)$ ,where $p$ and $q$ are the degrees of the numerator and denominator polynomials of the transfer function, respectively, then the transfer function must be a nonminimum-phase transfer function.  \n\n![](images/5a82fa3c94f3c54004ac8233b0bd821d2194fe56852335ba1c5c2e5ef550ac41.jpg)  \nFigure 7–87 (a) Log-magnitude curve of a type 0 system; (b) logmagnitude curves of type 1 systems; (c) log-magnitude curves of type 2 systems. (The slopes shown are in dB 'decade.)  \n\nNonminimum-Phase Transfer Functions. If,at the high-frequency end,the computed phase lag is $180^{\\circ}$ less than the experimentally obtained phase lag, then one of the zeros of the transfer function should have been in the right-half $s$ plane instead of the left-half $s$ plane.  \n\nIf the computed phase lag differed from the experimentally obtained phase lag by a constant rate of change of phase,then transport lag,or dead time,is present.If we assume the transfer function to be of the form  \n\n$$\nG(s)e^{-T s}\n$$  \n\nwhere $G(s)$ is a ratio of two polynomials in $s$ , then  \n\n$$\n\\begin{array}{r l}&{\\underset{\\omega\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{d}{d\\omega}\\,\\frac{\\displaystyle{d}}{\\displaystyle{d}\\omega}\\frac{\\displaystyle{d}(j\\omega)e^{-j\\omega T}}{\\displaystyle{d}\\omega}=\\operatorname*{lim}_{\\omega\\rightarrow\\infty}\\frac{d}{d\\omega}\\big[\\frac{\\displaystyle{d}(j\\omega)}{\\displaystyle{d}\\omega}+\\,\\frac{\\displaystyle{\\big/}e^{-j\\omega T}}{\\displaystyle{d}\\omega}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\,\\underset{\\omega\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{d}{d\\omega}\\big[\\frac{\\displaystyle{d}(j\\omega)}{\\displaystyle{d}\\omega}-\\,\\omega T\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\,0\\,-\\,T=-T}\\end{array}\n$$  \n\nwhere we used the fact that $\\operatorname*{lim}_{\\omega\\to\\infty}\\underline{{\\langle G(j\\omega)\\rangle}}=$ constant. Thus, from this last equation, we can evaluate the magnitude of the transport lag $T$ .  \n\n# A Few Remarks on the Experimental Determination of Transfer Functions  \n\n2. The frequency response of measuring equipment used to measure the system output must have a nearly flat magnitude-versus-frequency curve. In addition, the phase angle must be nearly proportional to the frequency.  \n\n3. Physical systems may have several kinds of nonlinearities. Therefore, it is necessary to consider carefully the amplitude of input sinusoidal signals. If the amplitude of the input signal is too large, the system will saturate, and the frequency-response test will yield inaccurate results. On the other hand, a small signal will cause errors due to dead zone. Hence, a careful choice of the amplitude of the input sinusoidal signal must be made. It is necessary to sample the waveform of the system output to make sure that the waveform is sinusoidal and that the system is operating in the linear region during the test period. (The waveform of the system output is not sinusoidal when the system is operating in its nonlinear region.)  \n\n4. If the system under consideration is operating continuously for days and weeks, then normal operation need not be stopped for frequency-response tests. The sinusoidal test signal may be superimposed on the normal inputs.Then,for linear systems, the output due to the test signal is superimposed on the normal output. For the determination of the transfer function while the system is in normal operation, stochastic signals (white noise signals) also are often used. By use of correlation functions, the transfer function of the system can be determined without interrupting normal operation.  \n\nDetermine the transfer function of the system whose experimental frequency-response curves are as shown in Figure 7–88.  \n\nThe first step in determining the transfer function is to approximate the log-magnitude curve by asymptotes with slopes $\\pm20$ dB 'decade and multiples thereof, as shown in Figure 7–88. We then estimate the corner frequencies. For the system shown in Figure 7–88, the following form of the transfer function is estimated:  \n\n$$\nG(j\\omega)\\,=\\,\\frac{K(1\\,+\\,0.5j\\omega)}{j\\omega(1\\,+\\,j\\omega)\\bigg[1\\,+\\,2\\zeta\\bigg(j\\,\\frac{\\omega}{8}\\bigg)\\,+\\,\\bigg(j\\,\\frac{\\omega}{8}\\bigg)^{2}\\bigg]}\n$$  \n\nThe value of the damping ratio $\\zeta$ is estimated by examining the peak resonance near $\\omega=6\\,\\mathrm{rad/sec}.$ Referring to Figure $7{-}9,\\zeta$ is determined to be 0.5.The gain $K$ is numerically equal to the frequency at the intersection of the extension of the low-frequency asymptote that has $20\\,\\mathrm{dB}.$ /decade slope and the 0-dB line.The value of $K$ is thus found to be 10.Therefore, $G(j\\omega)$ is tentatively determined as  \n\n$$\nG(j\\omega)=\\frac{10(1+0.5j\\omega)}{j\\omega(1\\,+\\,j\\omega)\\bigg[1+\\left(j\\,\\frac{\\omega}{8}\\right)\\,+\\,\\left(j\\,\\frac{\\omega}{8}\\right)^{2}\\bigg]}\n$$  \n\nor  \n\n$$\nG(s)\\,=\\frac{320(s\\,+\\,2)}{s(s\\,+\\,1)(s^{2}\\,+\\,8s\\,+\\,64)}\n$$  \n\n![](images/423eac81f919aa8dc83066107d9491f3b5a4b6eeb384c4dddb0b49f26055ea4d.jpg)  \nFigure 7–88 Bode diagram of a system. (Solid curves are experimentally obtained curves.)  \n\nThis transfer function is tentative because we have not examined the phase-angle curve yet.  \n\nOnce the corner frequencies are noted on the log-magnitude curve, the corresponding phaseangle curve for each component factor of the transfer function can easily be drawn.The sum of these component phase-angle curves is that of the assumed transfer function. The phase-angle curve for $G(j\\omega)$ is denoted by $\\underline{{\\boldsymbol{\\langle G\\rangle}}}$ in Figure 7–88. From Figure 7–88, we clearly notice a discrepancy between the computed phase-angle curve and the experimentally obtained phaseangle curve. The difference between the two curves at very high frequencies appears to be a constant rate of change. Thus, the discrepancy in the phase-angle curves must be caused by transport lag.  \n\nHence, we assume the complete transfer function to be $G(s)e^{-T s}$ . Since the discrepancy between the computed and experimental phase angles is $-0.2\\omega$ rad for very high frequencies, we can determine the value of $T$ as follows:  \n\n$$\n\\operatorname*{lim}_{\\omega\\rightarrow\\infty}{\\frac{d}{d\\omega}}\\,\\Big[G(j\\omega)e^{-j\\omega T}=-T=-0.2\n$$  \n\nor  \n\n$$\nT\\,=\\,0.2\\;\\mathrm{sec.}\n$$  \n\nThe presence of transport lag can thus be determined, and the complete transfer function determined from the experimental curves is  \n\n$$\nG(s)e^{-T s}=\\frac{320(s+2)e^{-0.2s}}{s(s+1)(s^{2}+8s\\,+\\,64)}\n$$  \n\nIn Chapter 6 we presented root-locus analysis and design. The root-locus method was shown to be very useful to reshape the transient-response characteristics of closedloop control systems. The root-locus approach gives us direct information on the transient response of the closed-loop system.The frequency-response approach,on the other hand, gives us this information only indirectly. However, as we shall see in the remaining three sections of this chapter, the frequency-response approach is very useful in designing control systems.  \n\nFor any design problem,the designer will do well to use both approaches to the design and choose the compensator that most closely produces the desired closed-loop response.  \n\nIn most control systems design, transient-response performance is usually very important. In the frequency-response approach, we specify the transient-response performance in an indirect manner.That is, the transient-response performance is specified in terms of the phase margin, gain margin, resonant peak magnitude (they give a rough estimate of the system damping);the gain crossover frequency,resonant frequency,bandwidth (they give a rough estimate of the speed of transient response); and static error constants (they give the steady-state accuracy). Although the correlation between the transient response and frequency response is indirect, the frequency-domain specifications can be easily met in the Bode diagram approach.  \n\nAfter the open loop has been designed, the closed-loop poles and zeros can be determined.Then, the transient-response characteristics must be checked to see whether the designed system satisfies the requirements in the time domain. If it does not, then the compensator must be modified and the analysis repeated until a satisfactory result is obtained.  \n\nDesign in the frequency domain is simple and straightforward. The frequencyresponse plot indicates clearly the manner in which the system should be modified, although the exact quantitative prediction of the transient-response characteristics cannot be made. The frequency-response approach can be applied to systems or components whose dynamic characteristics are given in the form of frequency-response data. Note that because of difficulty in deriving the equations governing certain components, such as pneumatic and hydraulic components, the dynamic characteristics of such components are usually determined experimentally through frequency-response tests.The experimentally obtained frequency-response plots can be combined easily with other such plots when the Bode diagram approach is used. Note also that in dealing with highfrequency noises we find that the frequency-response approach is more convenient than other approaches.  \n\nThere are basically two approaches in the frequency-domain design. One is the polar plot approach and the other is the Bode diagram approach. When a compensator is added, the polar plot does not retain the original shape, and, therefore, we need to draw a new polar plot,which will take time and is thus inconvenient.On the other hand,a Bode diagram of the compensator can be simply added to the original Bode diagram, and thus plotting the complete Bode diagram is a simple matter.Also, if the open-loop gain is varied, the magnitude curve is shifted up or down without changing the slope of the curve, and the phase curve remains the same. For design purposes, therefore, it is best to work with the Bode diagram.  \n\nA common approach to the design based on the Bode diagram is that we first adjust the open-loop gain so that the requirement on the steady-state accuracy is met.Then the magnitude and phase curves of the uncompensated open loop (with the open-loop gain just adjusted) are plotted. If the specifications on the phase margin and gain margin are not satisfied, then a suitable compensator that will reshape the open-loop transfer function is determined. Finally, if there are any other requirements to be met, we try to satisfy them, unless some of them are mutually contradictory.  \n\nInformation Obtainable from Open-Loop Frequency Response. The lowfrequency region (the region far below the gain crossover frequency) of the locus indicates the steady-state behavior of the closed-loop system.The medium-frequency region (the region near the gain crossover frequency) of the locus indicates relative stability. The high-frequency region (the region far above the gain crossover frequency) indicates the complexity of the system.  \n\nRequirements on Open-Loop Frequency Response. We might say that,in many practical cases, compensation is essentially a compromise between steady-state accuracy and relative stability.  \n\nTo have a high value of the velocity error constant and yet satisfactory relative stability, we find it necessary to reshape the open-loop frequency-response curve.  \n\nThe gain in the low-frequency region should be large enough, and near the gain crossover frequency, the slope of the log-magnitude curve in the Bode diagram should be $-20$ dB 'decade.This slope should extend over a sufficiently wide frequency band to assure a proper phase margin. For the high-frequency region, the gain should be attenuated as rapidly as possible to minimize the effects of noise.  \n\nExamples of generally desirable and undesirable open-loop and closed-loop frequency-response curves are shown in Figure 7–89.  \n\nReferring to Figure 7–90, we see that the reshaping of the open-loop frequencyresponse curve may be done if the high-frequency portion of the locus follows the $G_{1}(j\\omega)$ locus,while the low-frequency portion of the locus follows the $G_{2}(j\\omega)$ locus.The reshaped locus $G_{c}(j\\omega)G(j\\omega)$ should have reasonable phase and gain margins or should be tangent to a proper $M$ circle, as shown.  \n\n![](images/ab8e8486441a6778abaef6c79c6b0e80f5bc50dadd5c768372392fd61382dfed.jpg)  \nFigure 7–89 (a) Examples of desirable and undesirable open-loop frequency-response curves; (b) examples of desirable and undesirable closed-loop frequency-response curves.  \n\n![](images/886a2e002794d628e6d69ae32d1ce9a1e22d00a652106865e9ac45c4012f6cd7.jpg)  \nFigure 7–90 Reshaping of the open-loop frequency-response curve.  \n\nBasic Characteristics of Lead, Lag, and Lag–Lead Compensation. Lead compensation essentially yields an appreciable improvement in transient response and a small change in steady-state accuracy.It may accentuate high-frequency noise effects.Lag compensation, on the other hand, yields an appreciable improvement in steady-state accuracy at the expense of increasing the transient-response time. Lag compensation will suppress the effects of high-frequency noise signals. Lag–lead compensation combines the characteristics of both lead compensation and lag compensation.The use of a lead or lag compensator raises the order of the system by 1 (unless cancellation occurs between the zero of the compensator and a pole of the uncompensated open-loop transfer function).The use of a lag–lead compensator raises the order of the system by 2 [unless cancellation occurs between zero(s) of the lag–lead compensator and pole(s) of the uncompensated open-loop transfer function], which means that the system becomes more complex and it is more difficult to control the transient-response behavior.The particular situation determines the type of compensation to be used.  \n\n# 7–11 LEAD COMPENSATION  \n\nWe shall first examine the frequency characteristics of the lead compensator. Then we present a design technique for the lead compensator by use of the Bode diagram.  \n\nCharacteristics of Lead Compensators. Consider a lead compensator having the following transfer function:  \n\n$$\nK_{c}\\alpha\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}=K_{c}\\,{\\frac{s\\,+\\,{\\frac{1}{T}}}{s\\,+\\,{\\frac{1}{\\alpha T}}}}\\qquad(0<\\alpha<1)\n$$  \n\nwhere $\\alpha$ is the attenuation factor of the lead compensator. It has a zero at $s=-1/T$ and a pole at $s=-1/(\\alpha T)$ .Since $0<\\alpha<1$ ,we see that the zero is always located to the right of the pole in the complex plane. Note that for a small value of $\\alpha$ the pole is located far to the left. The minimum value of $\\alpha$ is limited by the physical construction of the lead compensator.The minimum value of $\\alpha$ is usually taken to be about 0.05. (This means that the maximum phase lead that may be produced by a lead compensator is about $65^{\\circ}$ .) [See Equation (7–25).]  \n\n![](images/beffc5a3b7b48f9b52b98d661e7c97d26d0e1110e8df422ddd022ac3cd08c687.jpg)  \nFigure 7–91 Polar plot of a lead compensator $\\alpha(j\\omega T\\,+\\,1)/(j\\omega\\alpha T\\,+\\,1),$ ,where $0<\\alpha<1$ .  \n\nFigure 7–91 shows the polar plot of  \n\n$$\nK_{c}\\alpha\\frac{j\\omega T\\,+\\,1}{j\\omega\\alpha T\\,+\\,1}\\qquad(0<\\alpha<1)\n$$  \n\nwith $K_{c}=1.$ .For a given value of $\\alpha$ , the angle between the positive real axis and the tangent line drawn from the origin to the semicircle gives the maximum phase-lead angle, $\\phi_{m}$ .We shall call the frequency at the tangent point $\\omega_{m}$ .From Figure 7–91 the phase angle at $\\omega\\,=\\,\\omega_{m}$ is $\\phi_{m}$ ,where  \n\n$$\n\\sin\\phi_{m}=\\displaystyle\\frac{\\displaystyle\\frac{1-\\alpha}{2}}{\\displaystyle\\frac{1+\\alpha}{2}}=\\displaystyle\\frac{1-\\alpha}{1+\\alpha}\n$$  \n\nEquation (7–25) relates the maximum phase-lead angle and the value of $\\alpha$ .  \n\nFigure 7–92 shows the Bode diagram of a lead compensator when $K_{c}=1$ and $\\alpha=0.1$ .The corner frequencies for the lead compensator are $\\omega=1/T$ and $\\omega=1/(\\alpha T)=10/T$ .By examining Figure 7–92, we see that $\\omega_{m}$ is the geometric mean of the two corner frequencies, or  \n\n$$\n\\log\\omega_{m}={\\frac{1}{2}}\\left(\\log{\\frac{1}{T}}+\\log{\\frac{1}{\\alpha T}}\\right)\n$$  \n\nFigure 7–92   \nBode diagram of a   \nlead compensator   \n$\\alpha(j\\omega T\\,+\\,1)/(j\\omega\\alpha T\\,+\\,1)$ ,where $\\alpha=0.1$ .  \n\n![](images/35b69fa11ff6868a3b87f57c6dccb6fdb6d4791c74656d33fcedf919d41c9175.jpg)  \nvin rad/sec  \n\nHence,  \n\n$$\n\\omega_{m}={\\frac{1}{\\sqrt{\\alpha}T}}\n$$  \n\nAs seen from Figure 7–92, the lead compensator is basically a high-pass filter. (The high frequencies are passed, but low frequencies are attenuated.)  \n\nLead Compensation Techniques Based on the Frequency-Response Approach. The primary function of the lead compensator is to reshape the frequency-response curve to provide sufficient phase-lead angle to offset the excessive phase lag associated with the components of the fixed system.  \n\nConsider the system shown in Figure 7–93.Assume that the performance specifications are given in terms of phase margin, gain margin, static velocity error constants, and so on.The procedure for designing a lead compensator by the frequency-response approach may be stated as follows:  \n\n1. Assume the following lead compensator:  \n\n$$\nG_{c}(s)=K_{c}\\alpha\\frac{T s+1}{\\alpha T s+1}=K_{c}\\frac{s+\\displaystyle\\frac{1}{T}}{s+\\displaystyle\\frac{1}{\\alpha T}}\\qquad(0<\\alpha<1)\n$$  \n\nDefine  \n\n$$\nK_{c}\\alpha\\,=\\,K\n$$  \n\nThen  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}\n$$  \n\nThe open-loop transfer function of the compensated system is  \n\n$$\nG_{c}(s)G(s)\\,=\\,K\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}\\,G(s)\\,=\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}\\,K G(s)\\,=\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}\\,G_{1}(s)\n$$  \n\nwhere  \n\n$$\nG_{1}(s)\\,=\\,K G(s)\n$$  \n\nDetermine gain $K$ to satisfy the requirement on the given static error constant.  \n\n2. Using the gain $K$ thus determined, draw a Bode diagram of $G_{1}(j\\omega)$ ,the gainadjusted but uncompensated system. Evaluate the phase margin. 3. Determine the necessary phase-lead angle to be added to the system. Add an additional $5^{\\circ}$ to $12^{\\circ}$ to the phase-lead angle required, because the addition of the lead compensator shifts the gain crossover frequency to the right and decreases the phase margin.  \n\n![](images/12634ec98f06e7a885d5988261efa141a455dd299800af3f909d99a1de4ec5bc.jpg)  \nFigure 7–93 Control system.  \n\n4. Determine the attenuation factor 1 by use of Equation (7–25). Determine the $-2\\dot{0}\\log\\left(\\dot{1}/\\sqrt{\\alpha}\\right)$ frequency where the magnitude of the uncompensated system .Select this frequency as the new gain crossover frequency. This A B$G_{1}(j\\omega)$ is equal to frequency corresponds to $\\omega_{m}=\\mathbf{\\hat{1}}/(\\sqrt{\\alpha}T),$ ,and the maximum phase shift $\\phi_{m}$ occurs at this frequency.  \n\n5. Determine the corner frequencies of the lead compensator as follows:  \n\n$$\n\\begin{array}{l}{{\\mathrm{Zero~of~lead~compensator}}{\\mathrm{:~}}\\quad\\omega={\\frac{1}{T}}}\\\\ {{\\mathrm{Pole~of~lead~compensator}}{\\mathrm{:~}}\\quad\\ \\omega={\\frac{1}{\\alpha T}}}\\end{array}\n$$  \n\n6. Using the value of $K$ determined in step 1 and that of $\\alpha$ determined in step 4, calculate constant $K_{c}$ from  \n\n$$\nK_{c}=\\frac{K}{\\alpha}\n$$  \n\n7. Check the gain margin to be sure it is satisfactory. If not, repeat the design process by modifying the pole–zero location of the compensator until a satisfactory result is obtained.  \n\n# EXAMPLE 7–26  \n\nConsider the system shown in Figure 7–94.The open-loop transfer function is  \n\n$$\nG(s)={\\frac{4}{s(s+2)}}\n$$  \n\nIt is desired to design a compensator for the system so that the static velocity error constant $K_{v}$ is $20\\;\\mathrm{sec}^{-1}$ ,the phase margin is at least $50^{\\circ}$ , and the gain margin is at least $10\\:\\mathrm{dB}$ .We shall use a lead compensator of the form  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\alpha\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}=K_{c}{\\frac{s+{\\frac{1}{T}}}{s+{\\frac{1}{\\alpha T}}}}\n$$  \n\nThe compensated system will have the open-loop transfer function $G_{c}(s)G(s)$ .Define  \n\n$$\nG_{1}(s)\\,=\\,K G(s)\\,=\\frac{4K}{s(s\\,+\\,2)}\n$$  \n\nwhere $K=K_{c}\\alpha$ .  \n\n![](images/1a1d9481c546fc331c26a9d372e8871e08904620c3bc3ff3c71ab4e978e0cd11.jpg)  \nFigure 7–94 Control system.  \n\nThe first step in the design is to adjust the gain $K$ to meet the steady-state performance specification or to provide the required static velocity error constant. Since this constant is given as $20\\;\\mathrm{sec}^{-1}$ , we obtain  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\to0}s G_{c}(s)G(s)\\,=\\,\\operatorname*{lim}_{s\\to0}s\\,{\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}\\,G_{1}(s)\\,=\\,\\operatorname*{lim}_{s\\to0}{\\frac{s4K}{s(s\\,+\\,2)}}\\,=\\,2K\\,=\\,20\n$$  \n\nor  \n\n$$\nK=10\n$$  \n\nWith $K=10$ , the compensated system will satisfy the steady-state requirement. We shall next plot the Bode diagram of  \n\n$$\nG_{1}(j\\omega)\\,=\\frac{40}{j\\omega(j\\omega\\,+\\,2)}=\\frac{20}{j\\omega(0.5j\\omega\\,+\\,1)}\n$$  \n\nFigure 7–95 shows the magnitude and phase-angle curves of $G_{1}(j\\omega)$ . From this plot, the phase and gain margins of the system are found to be $17^{\\circ}$ and $+\\infty$ dB, respectively. (A phase margin of $17^{\\circ}$ implies that the system is quite oscillatory.Thus, satisfying the specification on the steady state yields a poor transient-response performance.) The specification calls for a phase margin of at least $50^{\\circ}$ . We thus find the additional phase lead necessary to satisfy the relative stability requirement is $33^{\\circ}$ .To achieve a phase margin of $50^{\\circ}$ without decreasing the value of $K$ ,the lead compensator must contribute the required phase angle.  \n\nNoting that the addition of a lead compensator modifies the magnitude curve in the Bode diagram, we realize that the gain crossover frequency will be shifted to the right.We must offset the increased phase lag of $G_{1}(j\\omega)$ due to this increase in the gain crossover frequency. Considering the shift of the gain crossover frequency, we may assume that $\\phi_{m}$ ,the maximum phase lead required, is approximately $38^{\\circ}$ . (This means that $5^{\\circ}$ has been added to compensate for the shift in the gain crossover frequency.)  \n\nSince  \n\n$$\n\\sin\\phi_{m}=\\frac{1-\\alpha}{1+\\alpha}\n$$  \n\n![](images/32536b106c11eef7b968498c975928e3bda62eb648879c5167178cc572f11766.jpg)  \nFigure 7–95 Bode diagram for $G_{1}(j\\omega)\\,=\\,10G(j\\omega)$ $=40/{\\left[j\\omega(j\\omega+2)\\right]}$   \nvin rad/sec  \n\n$\\phi_{m}=38^{\\circ}$ corresponds to $\\alpha=0.24$ .Once the attenuation factor $\\alpha$ has been determined on the 1 1 basis of the required phase-lead angle,the next step is to determine the corner frequencies and angle $\\omega=1/(\\alpha T)$ $\\phi_{m}$ occurs at the geometric mean of the two corner frequencies,or of the lead compensator. To do so, we first note that the maximum phase-lead $\\omega=1/(\\sqrt{\\alpha}T)$ A A B.B[See Equa$\\omega=1/T$ tion (7–26).] The amount of the modification in the magnitude curve at $\\omega\\,=\\,1/(\\sqrt{\\alpha}T)$ due to the inclusion of the term $(T s\\,+\\,1)/(\\alpha T s\\,+\\,1)$ is  \n\n$$\n\\left|{\\frac{1\\,+\\,j\\omega T}{1\\,+\\,j\\omega\\alpha T}}\\right|_{\\omega=1/({\\sqrt\\alpha}T)}=\\left|{\\frac{1+j{\\frac{1}{\\sqrt\\alpha}}}{1\\,+\\,j\\alpha{\\frac{1}{\\sqrt\\alpha}}}}\\right|={\\frac{1}{\\sqrt\\alpha}}\n$$  \n\nNote that  \n\n$$\n{\\frac{1}{\\sqrt{\\alpha}}}={\\frac{1}{\\sqrt{0.24}}}={\\frac{1}{0.49}}=6.2\\,\\mathrm{dB}\n$$  \n\ngain crossover frequency $\\left|G_{1}(j\\omega)\\right|\\,=-6.2\\,\\mathrm{dE}$ A 1 Bcorre $\\omega_{c}$ onds to . Noting that this frequency corresponds to $\\omega=9$ 1 rad 'sec.We shall select this frequency $1/(\\sqrt{\\alpha}T)$ A Bnew ,or $\\bar{\\omega}_{c}=1/\\big(\\sqrt{\\alpha}T\\big)$ ,we obtain  \n\n$$\n\\frac{1}{T}=\\sqrt{\\alpha}\\omega_{c}=4.41\n$$  \n\nand  \n\n$$\n\\frac{1}{\\alpha T}=\\frac{\\omega_{c}}{\\sqrt{\\alpha}}=18.4\\\n$$  \n\nThe lead compensator thus determined is  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\,\\frac{s\\,+\\,4.41}{s\\,+\\,18.4}=\\,K_{c}\\alpha\\,\\frac{0.227s\\,+\\,1}{0.054s\\,+\\,1}\n$$  \n\nwhere the value of $K_{c}$ is determined as  \n\n$$\nK_{c}=\\frac{K}{\\alpha}=\\frac{10}{0.24}=41.7\n$$  \n\nThus, the transfer function of the compensator becomes  \n\n$$\nG_{c}(s)\\,=\\,41.7\\,\\frac{s\\,+\\,4.41}{s\\,+\\,18.4}=\\,10\\,\\frac{0.227s\\,+\\,1}{0.054s\\,+\\,1}\n$$  \n\nNote that  \n\n$$\n\\frac{G_{c}(s)}{K}\\,G_{1}(s)\\,=\\frac{G_{c}(s)}{10}\\,10G(s)\\,=\\,G_{c}(s)G(s)\n$$  \n\nThe magnitude curve and phase-angle curve for $G_{c}(j\\omega)/10$ are shown in Figure 7–96. The compensated system has the following open-loop transfer function:  \n\n$$\nG_{c}(s)G(s)\\,=\\,41.7\\,\\frac{s\\,+\\,4.41}{s\\,+\\,18.4}\\frac{4}{s(s\\,+\\,2)}\n$$  \n\n![](images/3126321453f14ad8f8cadba7629a577f0f243a8ddf1ef0c7100d62a5941a8e92.jpg)  \nFigure 7–96 Bode diagram for the compensated system.   \nvin rad/sec  \n\nThe solid curves in Figure 7–96 show the magnitude curve and phase-angle curve for the compensated system.Note that the bandwidth is approximately equal to the gain crossover frequency. The lead compensator causes the gain crossover frequency to increase from 6.3 to 9 rad 'sec. The increase in this frequency means an increase in bandwidth.This implies an increase in the speed of response.The phase and gain margins are seen to be approximately $50^{\\circ}$ and $+\\infty$ dB, respectively. The compensated system shown in Figure 7–97 therefore meets both the steady-state and the relative-stability requirements.  \n\nNote that for type 1 systems, such as the system just considered, the value of the static velocity error constant $K_{v}$ is merely the value of the frequency corresponding to the intersection of the extension of the initial $-20{\\mathrm{-dB}}$ 'decade slope line and the 0-dB line, as shown in Figure 7–96. Note also that we have changed the slope of the magnitude curve near the gain crossover frequency from $-40$ dB 'decade to $-20$ dB 'decade.  \n\n![](images/fc452bdb003568256a3036b5558efb229851261c6314fcc745e1b57070a24317.jpg)  \nFigure 7–97 Compensated system.  \n\nFigure 7–98 shows the polar plots of the gain-adjusted but uncompensated open-loop transfer function $G_{1}(j\\omega)\\,=\\,10\\,G(j\\omega)$ and the compensated open-loop transfer function $G_{c}(j\\omega)G(j\\omega)$ .From Figure 7–98, we see that the resonant frequency of the uncompensated system is about 6 rad 'sec and that of the compensated system is about 7 rad 'sec. (This also indicates that the bandwidth has been increased.)  \n\nFrom Figure 7–98, we find that the value of the resonant peak $M_{r}$ for the uncompensated system with $K=10$ is 3.The value of $M_{r}$ for the compensated system is found to be 1.29.This clearly shows that the compensated system has improved relative stability.  \n\nNote that, if the phase angle of $G_{1}(j\\omega)$ decreases rapidly near the gain crossover frequency, lead compensation becomes ineffective because the shift in the gain crossover frequency to the right makes it difficult to provide enough phase lead at the new gain crossover frequency. This means that, to provide the desired phase margin, we must use a very small value for $\\alpha$ .The value of $\\alpha$ , however, should not be too small (smaller than 0.05) nor should the maximum phase lead $\\phi_{m}$ be too large (larger than $65^{\\circ}$ ), because such values will require an additional gain of excessive value. [If more than $65^{\\circ}$ is needed, two (or more) lead networks may be used in series with an isolating amplifier.]  \n\nFinally, we shall examine the transient-response characteristics of the designed system. We shall obtain the unit-step response and unit-ramp response curves of the compensated and uncompensated systems with MATLAB. Note that the closed-loop transfer functions of the uncompensated and compensated systems are given, respectively, by  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{4}{s^{2}+2s+4}}\n$$  \n\nand  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{166.8s\\,+\\,735.588}{s^{3}\\,+\\,20.4s^{2}\\,+\\,203.6s\\,+\\,735.588}\n$$  \n\n![](images/910ce93c441e01d9194b4dff425d8795380d3e094be6cb94cb02a3040f256f93.jpg)  \nFigure 7–98 Polar plots of the gain-adjusted but uncompensated open-loop transfer function $G_{1}$ and compensated openloop transfer function $G_{c}G$ .  \n\nMATLAB programs for obtaining the unit-step response and unit-ramp response curves are given in MATLAB Program 7–13. Figure 7–99 shows the unit-step response curves of the system before and after compensation. Also, Figure 7–100 depicts the unit-ramp response curves before and after compensation.These response curves indicate that the designed system is satisfactory.  \n\n![](images/aef1088b0c7cd70c0577b65b0b429b109aec99b323b25ab9423652cd38f5ee5d.jpg)  \n\nIt is noted that the closed-loop poles for the compensated system are located as follows:  \n\n$$\n\\begin{array}{l}{{s\\,=\\,-6.9541\\,\\pm\\,j8.0592}}\\\\ {{}}\\\\ {{s\\,=\\,-6.4918}}\\end{array}\n$$  \n\nBecause the dominant closed-loop poles are located far from the $j\\omega$ axis, the response damps out quickly.  \n\n![](images/eaa0f3495bf3fda725f9059727147fede61e533b7c8b0ef4107b85a192cf10ea.jpg)  \nFigure 7–99 Unit-step response curves of the compensated and uncompensated systems.  \n\n![](images/58bf4b826d569fd711d0fd0ab94cd49b3c6f410624dda503fa6a2fbda0f958a6.jpg)  \nFigure 7–100 Unit-ramp response curves of the compensated and uncompensated systems.  \n\n# 7–12 LAG COMPENSATION  \n\nIn this section we first discuss the Nyquist plot and Bode diagram of the lag compensator.   \nThen we present lag compensation techniques based on the frequency-response approach.  \n\nCharacteristics of Lag Compensators. Consider a lag compensator having the following transfer function:  \n\n$$\nG_{c}(s)=K_{c}\\beta\\frac{T s+1}{\\beta T s+1}=K_{c}\\frac{s+\\frac{1}{T}}{s+\\frac{1}{\\beta T}}\\qquad(\\beta>1)\n$$  \n\n![](images/30efdcf291ece137670035201c37a791ac07277bf372b31f41b570b08bee9ed2.jpg)  \nFigure 7–101 Polar plot of a lag compensator $K_{c}\\beta\\bar{(j\\omega{\\cal T}\\,+\\,1)}/(j\\omega\\beta{\\cal T}\\,+\\,1).$  \n\nFigure 7–102   \nBode diagram of a   \nlag compensator   \n$\\beta(j\\omega T\\,+\\,1)/(j\\omega\\beta T\\,+\\,1).$ ,with $\\beta=10$ .  \n\n![](images/409f809e98efae84c0267d7702621b0a0634282ac2886ebe995e61c49f21239b.jpg)  \nvin rad/sec  \n\nIn the complex plane, a lag compensator has a zero at $s=-1/T$ and a pole at $s=-1/(\\beta T)$ .The pole is located to the right of the zero.  \n\nFigure 7–101 shows a polar plot of the lag compensator. Figure 7–102 shows a Bode diagram of the compensator, where $K_{c}=1$ and $\\beta=10$ .The corner frequencies of the lag compensator are at $\\omega=1/T$ and $\\omega\\,=\\,1/(\\beta T)$ .As seen from Figure 7–102, where the values of $K_{c}$ and $\\beta$ are set equal to 1 and 10, respectively, the magnitude of the lag compensator becomes 10 (or 20 dB) at low frequencies and unity (or 0 dB) at high frequencies.Thus, the lag compensator is essentially a low-pass filter.  \n\nLag Compensation Techniques Based on the Frequency-Response Approach. The primary function of a lag compensator is to provide attenuation in the highfrequency range to give a system sufficient phase margin. The phase-lag characteristic is of no consequence in lag compensation.  \n\nThe procedure for designing lag compensators for the system shown in Figure 7–93 by the frequency-response approach may be stated as follows:  \n\n1. Assume the following lag compensator:  \n\n$$\nG_{c}(s)=K_{c}\\beta\\frac{T s+1}{\\beta T s+1}=K_{c}\\frac{s+\\frac{1}{T}}{s+\\frac{1}{\\beta T}}\\qquad(\\beta>1)\n$$  \n\nDefine  \n\n$$\nK_{c}\\beta\\,=\\,K\n$$  \n\nThen  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}\n$$  \n\nThe open-loop transfer function of the compensated system is  \n\n$$\nG_{c}(s)G(s)\\,=\\,K\\,{\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}}\\,G(s)\\,=\\,{\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}}\\,K G(s)\\,=\\,{\\frac{T s\\,+\\,1}{\\beta T s\\,+\\,1}}\\,G_{1}(s)\n$$  \n\nwhere  \n\n$$\nG_{1}(s)\\,=\\,K G(s)\n$$  \n\nDetermine gain $K$ to satisfy the requirement on the given static velocity error constant.  \n\n2. If the gain-adjusted but uncompensated system $G_{1}(j\\omega)\\,=\\,K G(j\\omega)$ does not satisfy the specifications on the phase and gain margins, then find the frequency point where the phase angle of the open-loop transfer function is equal to $-180^{\\circ}$ plus the required phase margin. The required phase margin is the specified phase margin plus $5^{\\circ}$ to $12^{\\circ}$ . (The addition of $5^{\\circ}$ to $12^{\\circ}$ compensates for the phase lag of the lag compensator.) Choose this frequency as the new gain crossover frequency.  \n\n3. To prevent detrimental effects of phase lag due to the lag compensator, the pole and zero of the lag compensator must be located substantially lower than the new gain crossover frequency. Therefore, choose the corner frequency $\\omega=1/T$ (corresponding to the zero of the lag compensator) 1 octave to 1 decade below the new gain crossover frequency. (If the time constants of the lag compensator do not become too large, the corner frequency $\\omega=1/T$ may be chosen 1 decade below the new gain crossover frequency.)  \n\nNotice that we choose the compensator pole and zero sufficiently small.Thus the phase lag occurs at the low-frequency region so that it will not affect the phase margin.  \n\n4. Determine the attenuation necessary to bring the magnitude curve down to 0 dB at the new gain crossover frequency. Noting that this attenuation is $-20\\log\\beta$ ,determine the value of $\\beta$ . Then the other corner frequency (corresponding to the pole of the lag compensator) is determined from $\\omega\\,=\\,1/(\\beta T)$ .  \n\n5. Using the value of $K$ determined in step 1 and that of $\\beta$ determined in step 4, calculate constant $K_{c}$ from  \n\n$$\nK_{c}=\\frac{K}{\\beta}\n$$  \n\nEXAMPLE 7–27 Consider the system shown in Figure 7–103.The open-loop transfer function is given by  \n\n$$\nG(s)=\\frac{1}{s(s\\,+\\,1)(0.5s\\,+\\,1)}\n$$  \n\nIt is desired to compensate the system so that the static velocity error constant $K_{v}$ is $5\\;\\mathrm{sec}^{-1}$ , the phase margin is at least $40^{\\circ}$ , and the gain margin is at least 10 dB.  \n\nWe shall use a lag compensator of the form  \n\n$$\nG_{c}(s)=K_{c}\\beta\\frac{T s+1}{\\beta T s+1}=K_{c}\\frac{s+\\frac{1}{T}}{s+\\frac{1}{\\beta T}}\\qquad(\\beta>1)\n$$  \n\nDefine  \n\n$$\nK_{c}\\beta=K\n$$  \n\nDefine also  \n\n$$\nG_{1}(s)\\,=\\,K G(s)\\,=\\frac{K}{s(s\\,+\\,1)(0.5s\\,+\\,1)}\n$$  \n\nThe first step in the design is to adjust the gain $K$ to meet the required static velocity error constant.Thus,  \n\n$$\n\\begin{array}{l}{{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s\\,\\displaystyle\\frac{T s\\,+1}{\\beta T s\\,+1}G_{1}(s)\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{1}(s)\\,}}\\\\ {{\\,}}\\\\ {{=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}\\displaystyle\\frac{s K}{s(s\\,+1)(0.5s\\,+\\,1)}=K\\,=\\,5}}\\end{array}\n$$  \n\nor  \n\n$$\nK=5\n$$  \n\nWith $K=5$ ,the compensated system satisfies the steady-state performance requirement. We shall next plot the Bode diagram of  \n\n$$\nG_{1}(j\\omega)=\\frac{5}{j\\omega(j\\omega+1)(0.5j\\omega+1)}\n$$  \n\n![](images/f5541c7194b546708e59a007cac98704a821630c1f277925e00bf3c500ed8ba3.jpg)  \nFigure 7–103 Control system.  \n\n![](images/517f66d69ff6232c0f77e2f656165914085651b6d9d48d8875249296b07d8780.jpg)  \nFigure 7–104 Bode diagrams for $G_{1}$ (gain-adjusted but uncompensated open-loop transfer function), $G_{c}$ (compensator), and $G_{c}G$ (compensated open-loop transfer function).  \n\nThe magnitude curve and phase-angle curve of $G_{1}(j\\omega)$ are shown in Figure 7–104. From this plot, the phase margin is found to be $-20^{\\circ}$ , which means that the gain-adjusted but uncompensated system is unstable.  \n\nNoting that the addition of a lag compensator modifies the phase curve of the Bode diagram, we must allow $5^{\\circ}$ to $12^{\\circ}$ to the specified phase margin to compensate for the modification of the phase curve.Since the frequency corresponding to a phase margin of $40^{\\circ}$ is 0.7 rad 'sec,the new gain crossover frequency (of the compensated system) must be chosen near this value. To avoid overly large time constants for the lag compensator,we shall choose the corner frequency $\\omega=1/T$ (which corresponds to the zero of the lag compensator) to be 0.1 rad 'sec.Since this corner frequency is not too far below the new gain crossover frequency,the modification in the phase curve may not be small.Hence,we add about $12^{\\circ}$ to the given phase margin as an allowance to account for the lag angle introduced by the lag compensator.The required phase margin is now $52^{\\circ}$ .The phase angle of the uncompensated open-loop transfer function is $-128^{\\circ}$ at about $\\omega=0.5$ rad 'sec. So we choose the new gain crossover frequency to be 0.5 rad 'sec.To bring the magnitude curve down to $0\\,\\mathrm{dB}$ at this new gain crossover frequency,the lag compensator must give the necessary attenuation, which in this case is $-20$ dB. Hence,  \n\n$$\n20\\log{\\frac{1}{\\beta}}=-20\n$$  \n\nor  \n\n$$\n\\beta=10\n$$  \n\nThe other corner frequency $\\omega\\,=\\,1(\\beta T)$ , which corresponds to the pole of the lag compensator, is then determined as  \n\n$$\n{\\frac{1}{\\beta T}}=0.01\\;\\mathrm{rad/sec}\n$$  \n\nThus, the transfer function of the lag compensator is  \n\n$$\nG_{c}(s)\\,=\\,K_{c}(10)\\,\\frac{10s\\,+\\,1}{100s\\,+\\,1}=K_{c}\\frac{s\\,+\\,\\frac{1}{10}}{s\\,+\\,\\frac{1}{100}}\n$$  \n\nSince the gain $K$ was determined to be 5 and $\\beta$ was determined to be 10, we have  \n\n$$\nK_{c}=\\frac{K}{\\beta}=\\frac{5}{10}=0.5\n$$  \n\nThe open-loop transfer function of the compensated system is  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{5(10s\\,+\\,1)}{s(100s\\,+\\,1)(s\\,+\\,1)(0.5s\\,+\\,1)}\n$$  \n\nThe magnitude and phase-angle curves of $G_{c}(j\\omega)G(j\\omega)$ are also shown in Figure 7–104.  \n\nThe phase margin of the compensated system is about $40^{\\circ}$ , which is the required value. The gain margin is about 11 dB, which is quite acceptable.The static velocity error constant is $5\\,\\mathrm{sec}^{-1}$ ,as required. The compensated system, therefore, satisfies the requirements on both the steady state and the relative stability.  \n\nNote that the new gain crossover frequency is decreased from approximately 1 to 0.5 rad 'sec. This means that the bandwidth of the system is reduced.  \n\nTo further show the effects of lag compensation,the log-magnitude-versus-phase plots of the gainadjusted but uncompensated system $G_{1}(j\\omega)$ and of the compensated system $G_{c}(j\\omega)G(j\\omega)$ are shown in Figure 7–105.The plot of $G_{1}(j\\omega)$ clearly shows that the gain-adjusted but uncompensated system is unstable.The addition of the lag compensator stabilizes the system.The plot of $G_{c}(j\\omega)G(j\\omega)$ is tangent to the $M=3$ dB locus. Thus, the resonant peak value is 3 dB, or 1.4, and this peak occurs at $\\omega=0.5$ rad 'sec.  \n\nCompensators designed by different methods or by different designers (even using the same approach) may look sufficiently different.Any of the well-designed systems, however, will give similar transient and steady-state performance.The best among many alternatives may be chosen from the economic consideration that the time constants of the lag compensator should not be too large.  \n\n![](images/6e2103966370442b9a6dbf2a0b128755a71b577c16ccfa005dd0965f869ce7d0.jpg)  \nFigure 7–105 Log-magnitudeversus-phase plots of $G_{1}$ (gain-adjusted but uncompensated open-loop transfer function) and $G_{c}G$ (compensated openloop transfer function).   \n1  \n\nFinally, we shall examine the unit-step response and unit-ramp response of the compensated system and the original uncompensated system without gain adjustment. The closed-loop transfer functions of the compensated and uncompensated systems are  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{50s\\:+\\:5}{50s^{4}\\:+\\:150.5s^{3}\\:+\\:101.5s^{2}\\:+\\:51s\\:+\\:5}\n$$  \n\nand  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{1}{0.5s^{3}\\,+\\,1.5s^{2}\\,+\\,s\\,+\\,1}\n$$  \n\nrespectively. MATLAB Program 7–14 will produce the unit-step and unit-ramp responses of the compensated and uncompensated systems.The resulting unit-step response curves and unit-ramp response curves are shown in Figures 7–106 and 7–107, respectively. From the response curves we find that the designed system satisfies the given specifications and is satisfactory.  \n\nMATLAB Program 7–14   \n%\\*\\*\\*\\*\\*Unit-step response\\*\\*\\*\\*\\*   \n$\\mathsf{n u m}=[1]_{.}$ ;  \n$\\mathsf{d e n}=[0.5\\ \\ 1.5\\ \\ 1\\ \\ 1]\\,,$ ;  \nnum $=[50\\ \\cdot5]$ ;  \ndenc $=$ [50  150.5  101.5  51  5];   \n$\\mathrm{t}=0{:}0.1:40_{\\cdot}$ ;  \n[c1,x $\\mathsf{l}\\,,\\mathrm{t}]=\\mathsf{s t e p}(\\mathsf{n u m},\\mathrm{de}$ n,t);   \n$\\left[\\mathrm{c}2,\\mathrm{x}2,\\mathrm{t}\\right]=$ step(numc,denc,t);   \n$\\mathsf{p l o t}(\\mathsf{t},\\mathsf{c}\\,|\\,,^{\\prime}.^{\\prime},\\mathsf{t},\\mathsf{c}\\,2,^{\\prime}-^{\\prime})$ )  \ngrid   \ntitle('Unit-Step Responses of Compensated and Uncompensated Systems') xlabel('t Sec')   \nylabel('Outputs')   \ntext(12.7,1.27,'Compensated system')   \ntext(12.2,0.7,'Uncompensated system')   \n%\\*\\*\\*\\*\\*Unit-ramp response\\*\\*\\*\\*\\*   \n$\\mathsf{n u m}\\mathbb{1}=[1]$ ;  \nde ${\\mathsf{n}}1=[0.5\\ \\ 1.5\\ \\ 1\\ \\ 1\\ \\ 0]\\,;$   \nnum $1{\\,\\mathrm{c}}=[50\\ \\ 5]$ ;  \n$\\mathsf{d e n}\\,1\\,\\mathsf{c}=[50\\ \\ 1\\,50.5\\ \\ 1\\,01\\,.5\\ \\ 5\\ \\ 5\\ \\ 0]\\,;$   \n$\\mathrm{t}=0{:}0.1:20_{_\\cdot}$ ;  \n[y1,z1,t] $=$ step(num1,den1,t);   \n$[\\mathsf{y}2,\\!z2,\\!\\mathrm{t}]=$ step(num1c,den1c,t);   \n$\\mathrm{plot(t,y1,\\Sigma^{\\prime},t,y2,\\Sigma^{\\prime}-',t,t,'--)}$ ;  \ngrid   \ntitle('Unit-Ramp Responses of Compensated and Uncompensated Systems') xlabel('t Sec')   \nylabel('Outputs')   \ntext(8.3,3,'Compensated system')   \ntext(8.3,5,'Uncompensated system')  \n\n![](images/25a6bf4b3352ff00ffebb6484d1222b0e899a0159a727e7280128aeec5df5bd2.jpg)  \nFigure 7–106 Unit-step response curves for the compensated and uncompensated systems (Example 7–27).  \n\n![](images/9eb401e1bf17c564ff29ab9873aa7dbaa9fa7c9f749402db41cd51b681bf2fd2.jpg)  \nUnit-Ramp Responses of Compensated  and Uncompensated Systems   \nFigure 7–107 Unit-ramp response curves for the compensated and uncompensated systems (Example 7–27).  \n\nNote that the zero and poles of the designed closed-loop system are as follows:  \n\n$$\n{\\mathrm{sles~at~}}s=-0.2859\\pm\\,j0.5196,\\qquad s=-0.1228,\\qquad s=-2.3155\n$$  \n\nThe dominant closed-loop poles are very close to the $j\\omega$ axis with the result that the response is slow. Also, a pair of the closed-loop pole at $s\\,=\\,-0.1228$ and the zero at $s=-0.1$ produces a slowly decreasing tail of small amplitude.  \n\n# A Few Comments on Lag Compensation.  \n\n1. Lag compensators are essentially low-pass filters. Therefore, lag compensation permits a high gain at low frequencies (which improves the steady-state performance) and reduces gain in the higher critical range of frequencies so as to improve the phase margin. Note that in lag compensation we utilize the attenuation characteristic of the lag compensator at high frequencies rather than the phaselag characteristic. (The phase-lag characteristic is of no use for compensation purposes.)  \n\n2. Suppose that the zero and pole of a lag compensator are located at $s\\,=-z$ and $s=-p$ ,respectively.Then the exact locations of the zero and pole are not critical provided that they are close to the origin and the ratio $z/p$ is equal to the required multiplication factor of the static velocity error constant.  \n\nIt should be noted, however, that the zero and pole of the lag compensator should not be located unnecessarily close to the origin, because the lag compensator will create an additional closed-loop pole in the same region as the zero and pole of the lag compensator.  \n\nThe closed-loop pole located near the origin gives a very slowly decaying transient response, although its magnitude will become very small because the zero of the lag compensator will almost cancel the effect of this pole. However, the transient response (decay) due to this pole is so slow that the settling time will be adversely affected.  \n\nIt is also noted that in the system compensated by a lag compensator the transfer function between the plant disturbance and the system error may not involve a zero that is near this pole. Therefore, the transient response to the disturbance input may last very long.  \n\n3. The attenuation due to the lag compensator will shift the gain crossover frequency to a lower frequency point where the phase margin is acceptable. Thus, the lag compensator will reduce the bandwidth of the system and will result in slower transient response. [The phase angle curve of $G_{c}(j\\omega)G(j\\omega)$ is relatively unchanged near and above the new gain crossover frequency.]  \n\n4. Since the lag compensator tends to integrate the input signal, it acts approximately as a proportional-plus-integral controller. Because of this, a lag-compensated system tends to become less stable. To avoid this undesirable feature, the time constant $T$ should be made sufficiently larger than the largest time constant of the system.  \n\n5. Conditional stability may occur when a system having saturation or limiting is compensated by use of a lag compensator.When the saturation or limiting takes place in the system, it reduces the effective loop gain.Then the system becomes less stable and unstable operation may even result, as shown in Figure 7–108. To avoid this, the system must be designed so that the effect of lag compensation becomes significant only when the amplitude of the input to the saturating element is small. (This can be done by means of minor feedback-loop compensation.)  \n\n![](images/4b2c474977f494e47a7f1faaba5532fe5eb0acd688f4c3bbd81840f3b93e5819.jpg)  \nFigure 7–108 Bode diagram of a conditionally stable system.  \n\n# 7–13 LAG–LEAD COMPENSATION  \n\nWe shall first examine the frequency-response characteristics of the lag–lead compensator. Then we present the lag–lead compensation technique based on the frequencyresponse approach.  \n\nCharacteristic of Lag–Lead Compensator. Consider the lag–lead compensator given by  \n\n$$\nG_{c}(s)=K_{c}\\left({\\frac{s+{\\frac{1}{T_{1}}}}{s+{\\frac{\\gamma}{T_{1}}}}}\\right)\\left({\\frac{s+{\\frac{1}{T_{2}}}}{s+{\\frac{1}{\\beta T_{2}}}}}\\right)\n$$  \n\nwhere $\\gamma>1$ and $\\beta>1.$ .The term  \n\n$$\n\\frac{s+\\displaystyle\\frac{1}{T_{1}}}{s+\\displaystyle\\frac{\\gamma}{T_{1}}}=\\frac{1}{\\gamma}\\left(\\displaystyle\\frac{T_{1}s+1}{\\displaystyle\\frac{T_{1}}{\\gamma}\\,s+1}\\right)\\qquad(\\gamma>1)\n$$  \n\nproduces the effect of the lead network, and the term  \n\n$$\n{\\frac{s\\,+{\\frac{1}{T_{2}}}}{s\\,+{\\frac{1}{\\beta T_{2}}}}}=\\beta\\biggl({\\frac{T_{2}s\\,+\\,1}{\\beta T_{2}s\\,+\\,1}}\\biggr)\\qquad(\\beta>1)\n$$  \n\nproduces the effect of the lag network.  \n\n![](images/58887c4efadbc76c69300282388ea6f68e4e44ebade53253a9775393a10bc516.jpg)  \nFigure 7–109 Polar plot of a lag–lead compensator given by Equation (7–27), with $K_{c}=1$ and $\\gamma=\\beta$ .  \n\nIn designing a lag–lead compensator, we frequently chose $\\gamma\\,=\\,\\beta$ . (This is not necessary. We can, of course, choose $\\gamma\\neq\\beta.$ ) In what follows, we shall consider the case where $\\gamma\\,=\\,\\beta.$ .The polar plot of the lag–lead compensator with $K_{c}=1$ and $\\gamma=\\beta$ becomes as shown in Figure 7–109. It can be seen that, for $0\\,<\\,\\omega\\,<\\,\\omega_{1}$ ,the compensator acts as a lag compensator, while for $\\omega_{1}<\\omega<\\infty$ it acts as a lead compensator. The frequency $\\omega_{1}$ is the frequency at which the phase angle is zero. It is given by  \n\n$$\n\\omega_{1}={\\frac{1}{\\sqrt{T_{1}T_{2}}}}\n$$  \n\n(To derive this equation, see Problem A–7–21 .)  \n\nFigure 7–110 shows the Bode diagram of a lag–lead compensator when $K_{c}=1$ ,$\\gamma=\\beta=10$ ,and $T_{2}=10T_{1}$ .Notice that the magnitude curve has the value 0 dB at the low- and high-frequency regions.  \n\n![](images/ac46248bb53dd80a4181375ca18045de3b3343efa65c7cc6b1b8abd2e63cc9ac.jpg)  \nFigure 7–110 Bode diagram of a lag–lead compensator given by Equation (7–27) with $K_{c}=1$ ,$\\gamma=\\beta=10$ ,and $T_{2}=10T_{1}$ .  \n\nLag–Lead Compensation Based on the Frequency-Response Approach. The design of a lag–lead compensator by the frequency-response approach is based on the combination of the design techniques discussed under lead compensation and lag compensation.  \n\nLet us assume that the lag–lead compensator is of the following form:  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\,{\\frac{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}{\\displaystyle{\\left({\\frac{T_{1}}{\\beta}}\\,s\\,+\\,1\\right)}(\\beta T_{2}s\\,+\\,1)}}\\,=\\,K_{c}\\,{\\frac{\\displaystyle{\\left(s\\,+\\,{\\frac{1}{T_{1}}}\\right)}\\left(s\\,+\\,{\\frac{1}{T_{2}}}\\right)}{\\displaystyle{\\left(s\\,+\\,{\\frac{\\beta}{T_{1}}}\\right)}\\left(s\\,+\\,{\\frac{1}{\\beta T_{2}}}\\right)}}\n$$  \n\nwhere $\\beta>1.$ The phase-lead portion of the lag–lead compensator (the portion involving $T_{1}$ ) alters the frequency-response curve by adding phase-lead angle and increasing the phase margin at the gain crossover frequency.The phase-lag portion (the portion involving $T_{2}$ ) provides attenuation near and above the gain crossover frequency and thereby allows an increase of gain at the low-frequency range to improve the steady-state performance.  \n\nWe shall illustrate the details of the procedures for designing a lag–lead compensator by an example.  \n\n# EXAMPLE 7–28  \n\nConsider the unity-feedback system whose open-loop transfer function is  \n\n$$\nG(s)={\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}}\n$$  \n\nIt is desired that the static velocity error constant be $10\\;\\mathrm{sec}^{-1}$ ,the phase margin be $50^{\\circ}$ , and the gain margin be $10\\;\\mathrm{dB}$ or more.  \n\nAssume that we use the lag–lead compensator given by Equation (7–28).[Note that the phaselead portion increases both the phase margin and the system bandwidth (which implies increasing the speed of response).The phase-lag portion maintains the low-frequency gain.]  \n\nThe open-loop transfer function of the compensated system is $G_{c}(s)G(s)$ .Since the gain $K$ of the plant is adjustable, let us assume that $K_{c}=1$ .Then, $\\operatorname*{lim}_{s\\rightarrow0}G_{c}(s)\\,=\\,1$ From the requirement on the static velocity error constant, we obtain  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)\\,\\frac{K}{s(s\\,+\\,1)(s\\,+\\,2)}=\\frac{K}{2}=10\n$$  \n\nHence,  \n\n$$\nK=20\n$$  \n\nWe shall next draw the Bode diagram of the uncompensated system with $K=20$ ,as shown in Figure 7–111. The phase margin of the gain-adjusted but uncompensated system is found to be $-32^{\\circ}$ , which indicates that the gain-adjusted but uncompensated system is unstable.  \n\nThe next step in the design of a lag–lead compensator is to choose a new gain crossover frequency.From the phase-angle curve for $G(j\\omega)$ ,we notice that $/G(j\\omega)=-180^{\\circ}$ at $\\omega=1.5\\,\\mathrm{rad/sec}$ .It is convenient to choose the new gain crossover frequency to be 1.5 rad 'sec so that the phaselead angle required at $\\omega=1.5\\;\\mathrm{rad/sec}$ is about $50^{\\circ}$ , which is quite possible by use of a single lag–lead network.  \n\nOnce we choose the gain crossover frequency to be 1.5 rad 'sec, we can determine the corner frequency of the phase-lag portion of the lag–lead compensator. Let us choose the corner frequency $\\omega=1/T_{2}$ (which corresponds to the zero of the phase-lag portion of the compensator) to be 1 decade below the new gain crossover frequency, or at $\\omega=0.15$ rad 'sec.  \n\n![](images/de06cc4ae5745524bee92365c3be37a13e714f5f2043f6a56e12b5891af1bd6b.jpg)  \nFigure 7–111 Bode diagrams for $G$ (gain-adjusted but uncompensated open-loop transfer function), $G_{c}$ (compensator), and $G_{c}G$ (compensated open-loop transfer function).  \n\nRecall that for the lead compensator the maximum phase-lead angle $\\phi_{m}$ is given by Equation (7–25), where $\\alpha$ is $1/\\beta$ in the present case. By substituting $\\alpha=1/\\beta$ in Equation (7–25), we have  \n\n$$\n\\sin\\phi_{m}=\\frac{1-\\displaystyle\\frac{1}{\\beta}}{1+\\displaystyle\\frac{1}{\\beta}}=\\frac{\\beta-1}{\\beta+1}\n$$  \n\nNotice that $\\beta=10$ corresponds to $\\phi_{m}=54.9^{\\circ}$ . Since we need a $50^{\\circ}$ phase margin, we may choose $\\beta=10.$ . (Note that we will be using several degrees less than the maximum angle, $54.9^{\\circ}$ .) Thus,  \n\n$$\n\\beta=10\n$$  \n\nThen the corner frequency $\\omega\\,=\\,1/\\beta T_{2}$ (which corresponds to the pole of the phase-lag portion of the compensator) becomes $\\omega\\,=\\,0.015$ rad 'sec. The transfer function of the phase-lag portion of the lag–lead compensator then becomes  \n\n$$\n\\frac{s\\,+\\,0.15}{s\\,+\\,0.015}=10\\biggl(\\frac{6.67s\\,+\\,1}{66.7s\\,+\\,1}\\biggr)\n$$  \n\nThe phase-lead portion can be determined as follows: Since the new gain crossover frequency is $\\omega=1.5$ rad 'sec, from Figure $7–111,G(j1.5)$ is found to be 13 dB. Hence, if the lag–lead compensator contributes $-13\\ \\mathrm{dB}$ at $\\omega=1.5$ rad 'sec, then the new gain crossover frequency is as desired. From this requirement, it is possible to draw a straight line of slope $20\\;\\mathrm{dB}_{\\cdot}$ 'decade, passing through the point (1.5 rad 'sec, $-13$ dB). The intersections of this line and the 0-dB line and –20 -dB line determine the corner frequencies. Thus, the corner frequencies for the lead portion are $\\omega\\,=\\,0.7~\\mathrm{rad/sec}$ and $\\omega=7$ rad 'sec. Thus, the transfer function of the lead portion of the lag–lead compensator becomes  \n\n$$\n{\\frac{s\\,+\\,0.7}{s\\,+\\,7}}={\\frac{1}{10}}\\left({\\frac{1.43s\\,+\\,1}{0.143s\\,+\\,1}}\\right)\n$$  \n\nCombining the transfer functions of the lag and lead portions of the compensator, we obtain the transfer function of the lag–lead compensator. Since we chose $K_{c}=1$ ,we have  \n\n$$\nG_{c}(s)\\,=\\,{\\bigg(}{\\frac{s\\,+\\,0.7}{s\\,+\\,7}}{\\bigg)}{\\bigg(}{\\frac{s\\,+\\,0.15}{s\\,+\\,0.015}}{\\bigg)}\\,=\\,{\\bigg(}{\\frac{1.43s\\,+\\,1}{0.143s\\,+\\,1}}{\\bigg)}{\\bigg(}{\\frac{6.67s\\,+\\,1}{66.7s\\,+\\,1}}{\\bigg)}\n$$  \n\nThe magnitude and phase-angle curves of the lag–lead compensator just designed are shown in Figure 7–111.The open-loop transfer function of the compensated system is  \n\n$$\n\\begin{array}{r l}{G_{c}(s)G(s)\\,=\\frac{(s\\,+\\,0.7)(s\\,+\\,0.15)20}{(s\\,+\\,7)(s\\,+\\,0.015)s(s\\,+\\,1)(s\\,+\\,2)}}\\\\ {=\\frac{10(1.43s\\,+\\,1)(6.67s\\,+\\,1)}{s(0.143s\\,+\\,1)(66.7s\\,+\\,1)(s\\,+\\,1)(0.5s\\,+\\,1)}}\\end{array}\n$$  \n\nThe magnitude and phase-angle curves of the system of Equation (7–29) are also shown in Figure 7–111.The phase margin of the compensated system is $50^{\\circ}$ , the gain margin is $16\\;\\mathrm{dB}$ , and the static velocity error constant is $10\\;\\mathrm{sec}^{-1}$ . All the requirements are therefore met, and the design has been completed.  \n\nFigure 7–112 shows the polar plots of $G(j\\omega)$ (gain-adjusted but uncompensated open-loop transfer function) and $G_{c}(j\\omega)G(j\\omega)$ (compensated open-loop transfer function).The $G_{c}(j\\omega)G(j\\omega)$ locus is tangent to the $M=1.2$ circle at about $\\omega=2$ rad 'sec. Clearly, this indicates that the compensated system has satisfactory relative stability. The bandwidth of the compensated system is slightly larger than 2 rad 'sec.  \n\n![](images/601bf9647156290a7337ebd137ca4db37ed3c7630e021d7d413833e02e0f3688.jpg)  \nFigure 7–112 Polar plots of $G$ (gain adjusted) and $G_{c}G$ .  \n\n![](images/089b35b2be67f7a601d1eded750135c56597c22d1b71085e7af1b424b4f2903c.jpg)  \nFigure 7–113 Unit-step response of the compensated system (Example 7–28).  \n\nIn the following we shall examine the transient-response characteristics of the compensated system. (The gain-adjusted but uncompensated system is unstable.) The closed-loop transfer function of the compensated system is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{95.381s^{2}\\,+\\,81s\\,+\\,10}{4.7691s^{5}\\,+\\,47.7287s^{4}\\,+\\,110.3026s^{3}\\,+\\,163.724s^{2}\\,+\\,82s\\,+\\,10}}\n$$  \n\nThe unit-step and unit-ramp response curves obtained with MATLAB are shown in Figures 7–113 and 7–114, respectively.  \n\n![](images/434457be089007c021621ac28b4c8ac6f7b6939c3c4d67176da32bdc14ce7397.jpg)  \nFigure 7–114 Unit-ramp response of the compensated system (Example 7–28).   \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\n$$\nZ\\mathrm{eros\\;at}\\;s=-0.1499,\\qquad s=-0.6993\n$$  \n\n$$\ns=-0.1785,\\qquad s=-0.5425,\\qquad s=-7.4923\n$$  \n\nThe pole at $s=-0.1785$ and zero at $s=-0.1499$ are located very close to each other. Such a pair of pole and zero produces a long tail of small amplitude in the step response,as seen in Figure 7–113. Also,the pole at $s=-0.5425$ and zero at $s=-0.6993$ are located fairly close to each other.This pair adds amplitude to the long tail.  \n\nSummary of Control Systems Design by Frequency-Response Approach. The last three sections presented detailed procedures for designing lead, lag, and lag–lead compensators by the use of simple examples. We have shown that the design of a compensator to satisfy the given specifications (in terms of the phase margin and gain margin) can be carried out in the Bode diagram in a simple and straightforward manner. It is noted that not every system can be compensated with a lead, lag, or lag–lead compensator. In some cases compensators with complex poles and zeros may be used. For systems that cannot be designed by use of the root-locus or frequencyresponse methods, the pole-placement method may be used. (See Chapter 10.) In a given design problem if both conventional design methods and the pole-placement method can be used, conventional methods (root-locus or frequency-response methods) usually result in a lower-order stable compensator. Note that a satisfactory design of a compensator for a complex system may require a creative application of all available design methods.  \n\n# Comparison of Lead, Lag, and Lag–Lead Compensation  \n\n1. Lead compensation is commonly used for improving stability margins. Lag compensation is used to improve the steady-state performance. Lead compensation achieves the desired result through the merits of its phase-lead contribution,whereas lag compensation accomplishes the result through the merits of its attenuation property at high frequencies.  \n\n2. In some design problems both lead compensation and lag compensation may satisfy the specifications. Lead compensation yields a higher gain crossover frequency than is possible with lag compensation. The higher gain crossover frequency means a larger bandwidth.A large bandwidth means reduction in the settling time. The bandwidth of a system with lead compensation is always greater than that with lag compensation.Therefore, if a large bandwidth or fast response is desired, lead compensation should be employed. If, however, noise signals are present, then a large bandwidth may not be desirable, since it makes the system more susceptible to noise signals because of an increase in the high-frequency gain. Hence, lag compensation should be used for such a case.  \n\n3. Lead compensation requires an additional increase in gain to offset the attenuation inherent in the lead network.This means that lead compensation will require a larger gain than that required by lag compensation.A larger gain, in most cases, implies larger space, greater weight, and higher cost.  \n\n4. Lead compensation may generate large signals in the system. Such large signals are not desirable because they will cause saturation in the system.  \n\n5. Lag compensation reduces the system gain at higher frequencies without reducing the system gain at lower frequencies. Since the system bandwidth is reduced, the system has a slower speed to respond. Because of the reduced high-frequency gain, the total system gain can be increased, and thereby low-frequency gain can be increased and the steady-state accuracy can be improved. Also, any highfrequency noises involved in the system can be attenuated.   \n6. Lag compensation will introduce a pole-zero combination near the origin that will generate a long tail with small amplitude in the transient response.   \n7. If both fast responses and good static accuracy are desired,a lag–lead compensator may be employed.By use of the lag–lead compensator,the low-frequency gain can be increased (which means an improvement in steady-state accuracy), while at the same time the system bandwidth and stability margins can be increased.   \n8. Although a large number of practical compensation tasks can be accomplished with lead, lag, or lag–lead compensators, for complicated systems, simple compensation by use of these compensators may not yield satisfactory results. Then, different compensators having different pole–zero configurations must be employed.  \n\n![](images/8fcf3cd30aa34109026c27b70336001adb3e07b5a933e80422ddfcf44a444ce1.jpg)  \nFigure 7–115 Unit-step response curves and unit-ramp response curves. (a) Uncompensated system; (b) lead compensated system; (c) lag compensated system; (d) lag–lead compensated system.  \n\nGraphical Comparison. Figure 7–115(a) shows a unit-step response curve and unit-ramp response curve of an uncompensated system.Typical unit-step response and unit-ramp response curves for the compensated system using a lead, lag, and lag–lead compensator, respectively, are shown in Figures 7–115(b), (c), and (d).The system with a lead compensator exhibits the fastest response, while that with a lag compensator exhibits the slowest response, but with marked improvements in the unit-ramp response. The system with a lag–lead compensator will give a compromise; reasonable improvements in both the transient response and steady-state response can be expected. The response curves shown depict the nature of improvements that may be expected from using different types of compensators.  \n\nFeedback Compensation. A tachometer is one of the rate feedback devices. Another common rate feedback device is the rate gyro. Rate gyros are commonly used in aircraft autopilot systems.  \n\nVelocity feedback using a tachometer is very commonly used in positional servo systems. It is noted that, if the system is subjected to noise signals, velocity feedback may generate some difficulty if a particular velocity feedback scheme performs differentiation of the output signal. (The result is the accentuation of the noise effects.)  \n\nCancellation of Undesirable Poles. Since the transfer function of elements in cascade is the product of their individual transfer functions, it is possible to cancel some undesirable poles or zeros by placing a compensating element in cascade, with its poles and zeros being adjusted to cancel the undesirable poles or zeros of the original system. For example, a large time constant $T_{1}$ may be canceled by use of the lead network $\\left(T_{1}s\\,+\\,1\\right)\\!\\dot{/}\\!\\left(T_{2}s\\,+\\,1\\right)$ as follows:  \n\n![](images/84f33fbef8d2fc897925559329c43040bddf58eac963e39544f0256ae8c4d920.jpg)  \nFigure 7–116 Step-response curves showing the effect of canceling a large time constant.  \n\n$$\n\\left({\\frac{1}{T_{1}s\\,+\\,1}}\\right)\\left({\\frac{T_{1}s\\,+\\,1}{T_{2}s\\,+\\,1}}\\right)\\,={\\frac{1}{T_{2}s\\,+\\,1}}\n$$  \n\nIf $T_{2}$ is much smaller than $T_{1}$ ,we can effectively eliminate the large time constant $T_{1}$ .Figure 7–116 shows the effect of canceling a large time constant in step transient response.  \n\nIf an undesirable pole in the original system lies in the right-half $s$ plane, this compensation scheme should not be used since, although mathematically it is possible to cancel the undesirable pole with an added zero, exact cancellation is physically impossible because of inaccuracies involved in the location of the poles and zeros. A pole in the right-half $s$ plane not exactly canceled by the compensator zero will eventually lead to unstable operation, because the response will involve an exponential term that increases with time.  \n\nIt is noted that if a left-half plane pole is almost canceled but not exactly canceled, as is almost always the case, the uncanceled pole-zero combination will cause the response to have a small amplitude but long-lasting transient-response component. If the cancellation is not exact but is reasonably good, then this component will be small.  \n\nIt should be noted that the ideal control system is not the one that has a transfer function of unity. Physically, such a control system cannot be built since it cannot instantaneously transfer energy from the input to the output. In addition, since noise is almost always present in one form or another, a system with a unity transfer function is not desirable. A desired control system, in many practical cases, may have one set of dominant complex-conjugate closed-loop poles with a reasonable damping ratio and undamped natural frequency.The determination of the significant part of the closed-loop pole-zero configuration, such as the location of the dominant closed-loop poles, is based on the specifications that give the required system performance.  \n\nCancellation of Undesirable Complex-Conjugate Poles. If the transfer function of a plant contains one or more pairs of complex-conjugate poles, then a lead, lag, or lag–lead compensator may not give satisfactory results. In such a case, a network that has two zeros and two poles may prove to be useful. If the zeros are chosen so as to cancel the undesirable complex-conjugate poles of the plant, then we can essentially replace the undesirable poles by acceptable poles. That is, if the undesirable complexconjugate poles are in the left-half $s$ plane and are in the form  \n\n$$\n\\frac{1}{s^{2}\\,+\\,2\\zeta_{1}\\omega_{1}s\\,+\\,\\omega_{1}^{2}}\n$$  \n\nthen the insertion of a compensating network having the transfer function  \n\n$$\n\\frac{s^{2}\\,+\\,2\\zeta_{1}\\omega_{1}s\\,+\\,\\omega_{1}^{2}}{s^{2}\\,+\\,2\\zeta_{2}\\omega_{2}s\\,+\\,\\omega_{2}^{2}}\n$$  \n\nwill result in an effective change of the undesirable complex-conjugate poles to acceptable poles. Note that even though the cancellation may not be exact, the compensated system will exhibit better response characteristics. (As stated earlier, this approach cannot be used if the undesirable complex-conjugate poles are in the righthalf $s$ plane.)  \n\nFamiliar networks consisting only of $R C$ components whose transfer functions possess two zeros and two poles are the bridged${\\cal T}$ networks. Examples of bridged${\\cal T}$ networks and their transfer functions are shown in Figure 7–117. (The derivations of the transfer functions of the bridged${\\cal T}$ networks were given in Problem A–3–5 .)  \n\n![](images/b6ac992b6aba5ccdc430171694efec6d1f8e6f6c77f338ed718d6083513ed52e.jpg)  \nFigure 7–117 Bridged$\\boldsymbol{\\cdot}\\boldsymbol{T}$ networks.  \n\nConcluding Comments. In the design examples presented in this chapter, we have been primarily concerned only with the transfer functions of compensators. In actual design problems, we must choose the hardware. Thus, we must satisfy additional design constraints such as cost, size, weight, and reliability.  \n\nThe system designed may meet the specifications under normal operating conditions but may deviate considerably from the specifications when environmental changes are considerable. Since the changes in the environment affect the gain and time constants of the system, it is necessary to provide automatic or manual means to adjust the gain to compensate for such environmental changes, for nonlinear effects that were not taken into account in the design, and also to compensate for manufacturing tolerances from unit to unit in the production of system components. (The effects of manufacturing tolerances are suppressed in a closed-loop system; therefore, the effects may not be critical in closed-loop operation but critical in open-loop operation.) In addition to this, the designer must remember that any system is subject to small variations due mainly to the normal deterioration of the system.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–7–1. Consider a system whose closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{10(s\\,+\\,1)}{(s\\,+\\,2)(s\\,+\\,5)}\n$$  \n\nClearly, the closed-loop poles are located at $s=-2$ and $s=-5$ , and the system is not oscillatory. Show that the closed-loop frequency response of this system will exhibit a resonant peak, although the damping ratio of the closed-loop poles is greater than unity.  \n\nSolution. Figure 7–118 shows the Bode diagram for the system. The resonant peak value is approximately 3.5 dB. (Note that, in the absence of a zero, the second-order system with $\\zeta>0.7$ will not exhibit a resonant peak; however, the presence of a closed-loop zero will cause such a peak.)  \n\n![](images/d19ac419c91aa6b6d259fe21aaabe1e4a29caa7db8382c69b4e7ea96aa19f9fa.jpg)  \n\nFigure 7–118   \nBode diagram for   \n$10(1\\,+\\,j\\bar{\\omega})/\\big[(2\\,+\\,j\\omega)(5\\,+\\,j\\omega)\\big].$  \n\nA–7–2. Consider the system defined by  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\ \\ 0}&{\\ \\ 1{\\biggr]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{1}&{1{\\biggr]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ {\\ \\ 0}&{1{\\biggr]}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{\\ 0}\\\\ {0}&{1{\\biggr]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nObtain the sinusoidal transfer functions $Y_{1}(j\\omega)/U_{1}(j\\omega),\\;Y_{2}(j\\omega)/U_{1}(j\\omega),\\;Y_{1}(j\\omega)/U_{2}(j\\omega)$ ,and $Y_{2}(j\\omega)/U_{2}(j\\omega)$ .In deriving $Y_{1}(j\\omega)/U_{1}(j\\omega)$ and $Y_{2}(j\\omega)/U_{1}(j\\omega)$ ,we assume that $U_{2}(j\\omega)\\,=\\,0\\nonumber$ .Similarly, in obtaining $Y_{1}(j\\omega)/U_{2}(j\\omega)$ and $Y_{2}(j\\omega)/U_{2}(j\\omega)$ ,we assume that $U_{1}(j\\omega)\\,=\\,0$ .  \n\nSolution. The transfer matrix expression for the system defined by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\mathbb{B}}\\mathbf{u}}\\\\ {\\dot{\\mathbf{y}}=\\mathbf{C}\\mathbf{x}+\\mathbf{\\mathbb{D}}\\mathbf{u}}\\end{array}\n$$  \n\nis given by  \n\n$$\n\\mathbf{Y}(s)\\,=\\,\\mathbf{G}(s)\\mathbf{U}(s)\n$$  \n\nwhere $\\mathbf{G}(s)$ is the transfer matrix and is given by  \n\n$$\n\\mathbf{G}(s)\\,=\\,\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\,+\\,\\mathbf{D}\n$$  \n\nFor the system considered here, the transfer matrix becomes  \n\n$$\n\\begin{array}{r l}{\\mathbf{C}(s\\mathbf{I}-\\mathbf{A})^{-1}\\mathbf{B}+\\mathbf{D}=\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]\\left[\\begin{array}{l l}{s}&{-1}\\\\ {s}&{s+4}\\end{array}\\right]^{-1}\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{1}\\end{array}\\right]}\\\\ {=\\frac{1}{s^{2}+4s+25}\\left[\\begin{array}{l l}{s+4}&{1}\\\\ {-25}&{s}\\end{array}\\right]\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{1-}\\end{array}\\right]}\\\\ {=\\left[\\begin{array}{l l}{\\frac{s+4}{s^{2}+4s+25}}&{\\frac{s+5}{s^{2}+4s+25}}\\\\ {\\frac{-25}{s^{2}+4s+25}}&{\\frac{s-25}{s^{2}+4s+25}}\\end{array}\\right]}\\end{array}\n$$  \n\nHence  \n\n$$\n\\begin{array}{r}{\\left[Y_{1}(s)\\right]=\\left[\\frac{s\\mathrm{~}+\\mathrm{~}4}{s^{2}\\mathrm{~}+\\mathrm{~}4s\\mathrm{~}+\\mathrm{~}25}\\quad\\frac{s\\mathrm{~}+\\mathrm{~}5}{s^{2}\\mathrm{~}+\\mathrm{~}4s\\mathrm{~}+\\mathrm{~}25}\\right]\\!\\!\\left[U_{1}(s)\\right]}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mathrm{~}25}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~}\\overline{{s^{2}\\mathrm{~}+\\mathrm{~}4s\\mathrm{~}+\\mathrm{~}25}}\\,\\bigg[\\frac{\\mathrm{~}}{\\mathrm{~}U_{2}(s)}\\bigg]}\\end{array}\n$$  \n\nAssuming that $U_{2}(j\\omega)\\,=\\,0$ , we find $Y_{1}(j\\omega)/U_{1}(j\\omega)$ and $Y_{2}(j\\omega)/U_{1}(j\\omega)$ as follows:  \n\n$$\n\\begin{array}{r}{\\frac{Y_{1}(j\\omega)}{U_{1}(j\\omega)}=\\frac{j\\omega\\ +\\ 4}{(j\\omega)^{2}\\ +\\ 4j\\omega\\ +\\ 25}}\\\\ {\\frac{Y_{2}(j\\omega)}{U_{1}(j\\omega)}=\\frac{\\ -25}{\\ (j\\omega)^{2}\\ +\\ 4j\\omega\\ +\\ 25}}\\end{array}\n$$  \n\nSimilarly, assuming that $U_{1}(j\\omega)\\,=\\,0$ , we find $Y_{1}(j\\omega)/U_{2}(j\\omega)$ and $Y_{2}(j\\omega)/U_{2}(j\\omega)$ as follows:  \n\n$$\n\\begin{array}{r}{\\frac{Y_{1}(j\\omega)}{U_{2}(j\\omega)}=\\frac{j\\omega\\ +\\ 5}{(j\\omega)^{2}\\ +\\ 4j\\omega\\ +\\ 25}}\\\\ {\\frac{Y_{2}(j\\omega)}{U_{2}(j\\omega)}=\\frac{j\\omega\\ -\\ 25}{(j\\omega)^{2}\\ +\\ 4j\\omega\\ +\\ 25}}\\end{array}\n$$  \n\nNotice that $Y_{2}(j\\omega)/U_{2}(j\\omega)$ is a nonminimum-phase transfer function.  \n\nA–7–3. Referring to Problem A–7–2, plot Bode diagrams for the system, using MATLAB.  \n\nSolution. MATLAB Program 7–15 produces Bode diagrams for the system. There are four sets of Bode diagrams: two for input 1 and two for input 2. These Bode diagrams are shown in Figure 7–119.  \n\n![](images/4826a68e67a477200b1168b5396c573370a49120cb02f59cee17010d33c5b02f.jpg)  \n\n![](images/92ed5ee08970b681428af3b4e07b6b6a1d8e35691a9b0cbb98f1f77c908129a6.jpg)  \nFigure 7–119 Bode diagrams.  \n\nFigure 7–120 Closed-loop system.  \n\n![](images/7639bb9cc20f68d470bc8a5a6159f452ff3add7741f53102c9364cccfcc0bafa.jpg)  \n\nA–7–4. Using MATLAB, plot Bode diagrams for the closed-loop system shown in Figure 7–120 for $K=1$ ,$K=10$ ,and $K=20$ .Plot three magnitude curves in one diagram and three phase-angle curves in another diagram.  \n\nSolution. The closed-loop transfer function of the system is given by  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{K}{s(s+1)(s+5)\\,+\\,K}}}\\\\ {\\displaystyle{=\\,\\frac{K}{s^{3}\\,+\\,6s^{2}\\,+\\,5s\\,+\\,K}}}\\end{array}\n$$  \n\nHence the numerator and denominator of $C(s)/R(s)$ are  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=\\left[\\mathsf{K}\\right]}\\\\ {\\mathsf{d e n}=\\left[\\mathbb{1}\\ \\mathrm{~6~}\\,5\\ \\mathsf{K}\\right]}\\end{array}\n$$  \n\nA possible MATLAB program is shown in MATLAB Program 7–16.The resulting Bode diagrams are shown in Figures 7–121(a) and (b).  \n\n![](images/db4fde5a5e35565e89b835ca7ce42e7ce43c5406cf4df5abbec0ad659a9ef843.jpg)  \n\n![](images/e340599343643852b0eeb6445db115b67e2ebe594490770477cca17c909ee758.jpg)  \nFigure 7–121 Bode diagrams: (a) Magnitudeversus-frequency curves; (b) phaseangle-versusfrequency curves.  \n\nA–7–5. Prove that the polar plot of the sinusoidal transfer function  \n\n$$\nG(j\\omega)=\\frac{j\\omega T}{1\\,+\\,j\\omega T},\\;\\;\\;\\;\\;\\;\\mathrm{for}\\;0\\leq\\omega\\leq\\infty\n$$  \n\nis a semicircle. Find the center and radius of the circle.  \n\nSolution. The given sinusoidal transfer function $G(j\\omega)$ can be written as follows:  \n\n$$\nG(j\\omega)\\,=\\,X\\,+\\,j Y\n$$  \n\nwhere  \n\n$$\nX={\\frac{\\omega^{2}T^{2}}{1\\,+\\,\\omega^{2}T^{2}}},\\qquad Y={\\frac{\\omega T}{1\\,+\\,\\omega^{2}T^{2}}}\n$$  \n\nThen  \n\n$$\n\\left(X\\,-\\frac{1}{2}\\right)^{2}\\,+\\,Y^{2}=\\frac{{\\left(\\omega^{2}T^{2}\\,-\\,1\\right)}^{2}}{4{\\left(1\\,+\\,\\omega^{2}T^{2}\\right)}^{2}}+\\frac{\\omega^{2}T^{2}}{{\\left(1\\,+\\,\\omega^{2}T^{2}\\right)}^{2}}=\\frac{1}{4}\n$$  \n\nHence, we see that the plot of $G(j\\omega)$ is a circle centered at (0.5,0) with radius equal to 0.5. The upper semicircle corresponds to $0\\,\\leq\\,\\omega\\,\\leq\\,\\infty$ , and the lower semicircle corresponds to $-\\infty\\leq\\omega\\leq0$ .  \n\nA–7–6. Prove the following mapping theorem: Let $F(s)$ be a ratio of polynomials in $s$ . Let $P$ be the number of poles and $Z$ be the number of zeros of $F(s)$ that lie inside a closed contour in the $s$ plane, with multiplicity accounted for. Let the closed contour be such that it does not pass through any poles or zeros of $F(s)$ .The closed contour in the $s$ plane then maps into the $F(s)$ plane as a closed curve.The number $N$ of clockwise encirclements of the origin of the $F(s)$ plane, as a representative point $s$ traces out the entire contour in the $s$ plane in the clockwise direction, is equal to $Z\\mathrm{~-~}P$ .  \n\nSolution. To prove this theorem, we use Cauchy’s theorem and the residue theorem. Cauchy’s theorem states that the integral of $F(s)$ around a closed contour in the $s$ plane is zero if $F(s)$ is analytic #within and on the closed contour, or  \n\n$$\n\\oint\\!F(s)\\,d s\\,=\\,0\n$$  \n\nSuppose that $F(s)$ is given by  \n\n$$\nF(s)\\,=\\frac{(s\\,+\\,z_{1})^{k_{1}}(s\\,+\\,z_{2})^{k_{2}}\\cdots}{(s\\,+\\,p_{1})^{m_{1}}(s\\,+\\,p_{2})^{m_{2}}\\cdots}X(s)\n$$  \n\nwhere $X(s)$ is analytic in the closed contour in the $s$ plane and all the poles and zeros are located in the contour.Then the ratio $F^{\\prime}(s)/F(s)$ can be written  \n\n$$\n{\\frac{F^{\\prime}(s)}{F(s)}}=\\left({\\frac{k_{1}}{s+z_{1}}}+{\\frac{k_{2}}{s+z_{2}}}+\\cdots\\right)-\\left({\\frac{m_{1}}{s+p_{1}}}+{\\frac{m_{2}}{s+p_{2}}}+\\cdots\\right)+{\\frac{X^{\\prime}(s)}{X(s)}}\n$$  \n\nThis may be seen from the following consideration: If ${\\hat{F}}(s)$ is given by  \n\n$$\n\\hat{F}(s)\\,=\\,(s\\,+\\,z_{1})^{k}X(s)\n$$  \n\nthen ${\\hat{F}}(s)$ has a zero of $k$ th order at $s=-z_{1}$ . Differentiating $F(s)$ with respect to $s$ yields  \n\n$$\n{\\hat{F}}^{\\prime}(s)\\,=\\,k{\\left(s\\,+\\,z_{1}\\right)}^{k-1}X(s)\\,+\\,{\\left(s\\,+\\,z_{1}\\right)}^{k}X^{\\prime}(s)\n$$  \n\nHence,  \n\n$$\n{\\frac{{\\hat{F}}^{\\prime}(s)}{{\\hat{F}}(s)}}={\\frac{k}{s\\,+\\,z_{1}}}+{\\frac{X^{\\prime}(s)}{X(s)}}\n$$  \n\nWe see that by taking the ratio $\\hat{F}^{\\prime}(s)/\\hat{F}\\left(s\\right)$ , the $k$ th-order zero of ${\\hat{F}}(s)$ becomes a simple pole of $\\hat{F}^{\\prime}(s)/\\hat{F}\\left(s\\right)$ .  \n\n#For the definition of an analytic function, see the footnote on page 447.  \n\nIf the last term on the right-hand side of Equation (7–31) does not contain any poles or zeros in the closed contour in the $s$ plane, $F^{\\prime}(s)/F(s)$ is analytic in this contour except at point $s=-z_{1}$ .Then, referring to Equation (7–30) and using the residue theorem, which states that the integral of $F^{\\prime}(s)/F(s)$ taken in the clockwise direction around a closed contour in the $s$ plane is equal to $-2\\pi j$ times the residues at the simple poles of $F^{\\prime}(s)/F(s)$ , or  \n\n$$\n\\begin{array}{r}{\\oint\\!{\\frac{F^{\\prime}(s)}{F(s)}}\\,d s=-2\\pi j\\Big(\\sum\\mathrm{\\residues}\\Big)}\\end{array}\n$$  \n\nwe have  \n\n$$\n\\oint\\!{\\frac{F^{\\prime}(s)}{F(s)}}\\,d s=-2\\pi j\\big[\\big(k_{1}+k_{2}+\\cdots\\big)-\\big(m_{1}+m_{2}+\\cdots\\big)\\big]=-2\\pi j(Z-P)\n$$  \n\nwhere $Z=k_{1}+k_{2}+\\cdots=$ total number of zeros of $F(s)$ enclosed in the closed contour in the $s$ plane  \n\n[The $k$ multiple zeros (or poles) are considered $k$ zeros (or poles) located at the same point.] Since $F(s)$ is a complex quantity, $F(s)$ can be written  \n\n$$\nF(s)\\,=\\,|F|e^{j\\theta}\n$$  \n\nand  \n\n$$\n\\ln F(s)\\,=\\,\\ln|F|\\,+\\,j\\theta\n$$  \n\nNoting that $F^{\\prime}(s)/F(s)$ can be written  \n\n$$\n{\\frac{F^{\\prime}(s)}{F(s)}}={\\frac{d\\ln F(s)}{d s}}\n$$  \n\nwe obtain  \n\n$$\n{\\frac{F^{\\prime}(s)}{F(s)}}={\\frac{d\\ln|F|}{d s}}+j\\,{\\frac{d\\theta}{d s}}\n$$  \n\nIf the closed contour in the $s$ plane is mapped into the closed contour $\\boldsymbol{\\mathit{\\Pi}}_{T}$ in the $F(s)$ plane, then  \n\n$$\n\\oint\\frac{F^{\\prime}(s)}{F(s)}\\,d s\\,=\\,\\oint_{\\cal P}d\\ln|F|\\,+\\,j\\oint_{\\cal P}d\\theta\\,=\\,j\\int\\,d\\theta\\,=\\,2\\pi j({\\cal P}\\,-\\,Z)\n$$  \n\nThe integral $\\oint_{T}d\\ln|F|$ is zero since the magnitude $\\ln\\vert F\\vert$ is the same at the initial point and the final point of the contour $T.$ Thus we obtain  \n\n$$\n{\\frac{\\theta_{2}\\,-\\,\\theta_{1}}{2\\pi}}=P\\,-\\,Z\n$$  \n\nThe angular difference between the final and initial values of $\\theta$ is equal to the total change in the phase angle of $F^{\\prime}(s)/F(s)$ as a representative point in the $s$ plane moves along the closed contour. Noting that $N$ is the number of clockwise encirclements of the origin of the $F(s)$ plane and $\\theta_{2}\\mathrm{~-~}\\theta_{1}$ is zero or a multiple of $2\\pi$ rad, we obtain  \n\n$$\n\\frac{\\theta_{2}\\,-\\,\\theta_{1}}{2\\pi}=-N\n$$  \n\n![](images/be009b0bc5367092b40aeb855dffc4ee55fa7ae55e76301c10801146b64686f8.jpg)  \nFigure 7–122 Determination of encirclement of the origin of $F(s)$ plane.  \n\nThus, we have the relationship  \n\n$$\nN\\,=\\,Z\\,-\\,P\n$$  \n\nThis proves the theorem.  \n\nNote that by this mapping theorem,the exact numbers of zeros and of poles cannot be found— only their difference. Note also that, from Figures 7–122(a) and (b), we see that if $\\theta$ does not change through $2\\pi$ rad, then the origin of the $F(s)$ plane cannot be encircled.  \n\nA–7–7. The Nyquist plot (polar plot) of the open-loop frequency response of a unity-feedback control system is shown in Figure 7–123(a). Assuming that the Nyquist path in the $s$ plane encloses the entire right-half $s$ plane, draw a complete Nyquist plot in the $G$ plane.Then answer the following questions:  \n\n(a) If the open-loop transfer function has no poles in the right-half $s$ plane, is the closed-loop system stable?   \n(b) If the open-loop transfer function has one pole and no zeros in right-half $s$ plane,is the closedloop system stable?   \n(c) If the open-loop transfer function has one zero and no poles in the right-half $s$ plane, is the closed-loop system stable?  \n\n![](images/1c71a5bed38fa24c23914296a0723688fec7e2c5579d949f193daddf0d3df96e.jpg)  \nFigure 7–123 (a) Nyquist plot; (b) complete Nyquist plot in the $G$ plane.   \nChapter 7 /Control Systems Analysis and Design by the Frequency-Response Method  \n\nSolution. Figure 7–123(b) shows a complete Nyquist plot in the $G$ plane.The answers to the three questions are as follows:  \n\n(a) The closed-loop system is stable, because the critical point $(-1+j0)$ is not encircled by the Nyquist plot.That is, since $P=0$ and $N=0$ , we have $Z=N\\,+\\,P\\,=\\,0$ .  \n(b) The open-loop transfer function has one pole in the right-half $s$ plane. Hence, $P=1$ . (The open-loop system is unstable.) For the closed-loop system to be stable, the Nyquist plot must encircle the critical point $(-1+j0)$ once counterclockwise. However, the Nyquist plot does not encircle the critical point.Hence, $N=0,$ .Therefore, $Z=N+P=1.$ The closed-loop system is unstable.   \n(c) Since the open-loop transfer function has one zero, but no poles, in the right-half $s$ plane, we have $Z\\,=\\,N\\,+\\,P\\,=\\,0.$ Thus, the closed-loop system is stable. (Note that the zeros of the open-loop transfer function do not affect the stability of the closed-loop system.)  \n\nA–7–8. Is a closed-loop system with the following open-loop transfer function and with $K=2$ stable?  \n\n$$\nG(s)H(s)={\\frac{K}{s(s+1)(2s+1)}}\n$$  \n\nFind the critical value of the gain $K$ for stability.  \n\nSolution. The open-loop transfer function is  \n\n$$\n\\begin{array}{r}{G(j\\omega)H(j\\omega)=\\frac{K}{j\\omega\\left(j\\omega\\mathrm{~+~}1\\right)\\left(2j\\omega\\mathrm{~+~}1\\right)}}\\\\ {=\\frac{K}{{-3\\omega^{2}\\mathrm{~+~}j\\omega\\big(1\\mathrm{~-~}2\\omega^{2}\\big)}}}\\end{array}\n$$  \n\nThis open-loop transfer function has no poles in the right-half $s$ plane. Thus, for stability, the $-1+j0$ point should not be encircled by the Nyquist plot. Let us find the point where the Nyquist plot crosses the negative real axis. Let the imaginary part of $G(j\\omega)H(j\\omega)$ be zero, or  \n\n$$\n1\\,-\\,2\\omega^{2}\\,=\\,0\n$$  \n\nfrom which  \n\n$$\n\\omega=\\pm\\,\\frac{1}{\\sqrt{2}}\n$$  \n\nSubstituting $\\omega=1/\\sqrt{2}$ into $G(j\\omega)H(j\\omega)$ , we obtain  \n\n$$\nG\\biggl(j\\,{\\frac{1}{\\sqrt{2}}}\\biggr)H\\biggl(j\\,{\\frac{1}{\\sqrt{2}}}\\biggr)=-\\,{\\frac{2K}{3}}\n$$  \n\nThe critical value of the gain $K$ is obtained by equating $-2K/3$ to $-1$ , or  \n\n$$\n-\\,{\\frac{2}{3}}\\,K=-1\n$$  \n\nHence,  \n\n$$\nK=\\frac{3}{2}\n$$  \n\nThe system is stable if $0<K<\\frac{3}{2}$ .Hence, the system with $K=2$ is unstable.  \n\nFigure 7–124 Closed-loop system.  \n\n![](images/fd0ad8c381c897f344db0b4696c3773c1dbbbff790b65f6c8b15b0fe39e3bfc4.jpg)  \n\nA–7–9. Consider the closed-loop system shown in Figure 7–124. Determine the critical value of $K$ for stability by the use of the Nyquist stability criterion.  \n\nSolution. The polar plot of  \n\n$$\nG(j\\omega)=\\frac{K}{j\\omega\\,-\\,1}\n$$  \n\nis a circle with center at $-K/2$ on the negative real axis and radius $K/2$ , as shown in Figure 7–125(a). As $\\omega$ is increased from $-\\infty$ to $\\infty$ , the $G(j\\omega)$ locus makes a counterclockwise rotation. In this system, $P=1$ because there is one pole of $G(s)$ in the right-half $s$ plane. For the closedloop system to be stable, $Z$ must be equal to zero.Therefore, $N=Z\\,-\\,P$ must be equal to $-1$ , or there must be one counterclockwise encirclement of the $-1+j0$ point for stability. (If there is no encirclement of the $-1+j0$ point, the system is unstable.) Thus, for stability, $K$ must be greater than unity,and $K=1$ gives the stability limit.Figure 7–125(b) shows both stable and unstable cases of $G(j\\omega)$ plots.  \n\n![](images/97bedc9af0d5117978fa9481a6f5d9e9bbf05a3aaab1fe592cceef03218f426a.jpg)  \nFigure 7–125 (a) Polar plot of $K/(j\\omega\\,-\\,1)$ ;(b) polar plots of $K/(j\\omega\\,-\\,1)$ for stable and unstable cases.  \n\nA–7–10. Consider a unity-feedback system whose open-loop transfer function is  \n\n$$\nG(s)=\\frac{K e^{-0.8s}}{s+1}\n$$  \n\nUsing the Nyquist plot, determine the critical value of $K$ for stability.  \n\nSolution. For this system,  \n\n$$\n\\begin{array}{r l}{\\lefteqn{G(j\\omega)=\\frac{K e^{-0.8j\\omega}}{j\\omega\\;+\\;1}}}\\\\ &{=\\frac{K(\\cos0.8\\omega-\\;j\\sin0.8\\omega)(1\\;-\\;j\\omega)}{1\\;+\\;\\omega^{2}}}\\\\ &{=\\frac{K}{1\\;+\\;\\omega^{2}}[(\\cos0.8\\omega-\\;\\omega\\sin0.8\\omega)\\;-\\;j(\\sin0.8\\omega\\;+\\;\\omega\\cos0.8\\omega)]}\\end{array}\n$$  \n\nThe imaginary part of $G(j\\omega)$ is equal to zero if  \n\n$$\n\\sin0.8\\omega\\,+\\,\\omega\\cos0.8\\omega\\,=\\,0\n$$  \n\nHence,  \n\n$$\n\\omega=-\\mathrm{tan}\\,0.8\\omega\n$$  \n\nSolving this equation for the smallest positive value of $\\omega$ , we obtain  \n\n$$\n\\omega=2.4482\n$$  \n\nSubstituting $\\omega=2.4482$ into $G(j\\omega)$ , we obtain  \n\n$$\nG(j2.4482)=\\frac{K}{1\\,+\\,2.4482^{2}}\\left(\\cos1.9586\\,-\\,2.4482\\sin1.9586\\right)\\,=-0.378K\n$$  \n\nThe critical value of $K$ for stability is obtained by letting $G(j2.4482)$ equal $-1$ . Hence,  \n\n$$\n0.378K=1\n$$  \n\nor  \n\n$$\nK=2.65\n$$  \n\nFigure 7–126 shows the Nyquist or polar plots of $2.65e^{-0.8j\\omega}/(1\\,+\\,j\\omega)$ and $2.65/(1\\,+\\,j\\omega)$ .The firstorder system without transport lag is stable for all values of $K$ , but the one with a transport lag of 0.8 sec becomes unstable for $K>2.65$ .  \n\n![](images/b84f6607a2ef371049174f9dda0b3aa0d061ca22a991fafc196bb68021492edd.jpg)  \nFigure 7–126 Polar plots of $2.65e^{-0.8j\\omega}/(1\\,+\\,j\\omega)$ and $2.65/(1\\,+\\,j\\omega)$ .  \n\nA–7–11. Consider a unity-feedback system with the following open-loop transfer function:  \n\n$$\nG(s)=\\frac{20\\!\\left(s^{2}+s\\,+\\,0.5\\right)}{s(s\\,+\\,1)(s\\,+\\,10)}\n$$  \n\nDraw a Nyquist plot with MATLAB and examine the stability of the closed-loop system.  \n\nSolution. MATLAB Program 7–17 produces the Nyquist diagram shown in Figure 7–127. From this figure, we see that the Nyquist plot does not encircle the $-1+j0$ point. Hence, $N=0$ in the Nyquist stability criterion. Since no open-loop poles lie in the right-half $s$ plane, ${P}=0,$ .Therefore, $Z=N+P=0.$ The closed-loop system is stable.  \n\n![](images/aaf6e508348b4ebde8037d6a6a8babd3f3ef4b7ed82560b832e560f4cb103768.jpg)  \n\nFigure 7–127   \nNyquist plot of   \n$G(s)={\\frac{20(s^{2}+s\\,+\\,0.5)}{s(s\\,+\\,1)(s\\,+\\,10)}}.$  \n\n![](images/344c607cd67ea8300fb51e97656c1d741ceabfa54ba4eb0125ed5a7153eee261.jpg)  \n\nA–7–12. Consider the same system as discussed in Problem A–7–11 . Draw the Nyquist plot for only the positive-frequency region.  \n\nSolution. Drawing a Nyquist plot for only the positive-frequency region can be done by the use of the following command:  \n\n$$\n[\\mathsf{r e,i m,w}]=\\mathsf{n y q u i s t(n u m,d e n,w)}\n$$  \n\nThe frequency region may be divided into several subregions by using different increments. For example, the frequency region of interest may be divided into three subregions as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathsf{w}1=0.1\\,;0.1\\,;10;}\\\\ &{\\mathsf{w}2=1\\,0;2;100;}\\\\ &{\\mathsf{w}3=1\\,00;10;500;}\\\\ &{\\mathsf{w}}&{=[\\mathsf{w}1\\;\\;\\mathsf{w}2\\;\\;\\mathsf{w}3]}\\end{array}\n$$  \n\nMATLAB Program 7–18 uses this frequency region. Using this program, we obtain the Nyquist plot shown in Figure 7–128.  \n\n![](images/7ddfa3495d15f134c6af84ce09dcdcaaa448551bce9429ee803673f1487b3901.jpg)  \n\n![](images/11b9dbbb93a91c173bf2c7379badaa4a06534f3a4cabfe39c6589103f8334bfe.jpg)  \nFigure 7–128 Nyquist plot for the positive-frequency region.  \n\nA–7–13. Referring to Problem A–7–12 , plot the polar locus of $G(s)$ where  \n\n$$\nG(s)=\\frac{20\\!\\left(s^{2}+s\\,+\\,0.5\\right)}{s(s\\,+\\,1)(s\\,+\\,10)}\n$$  \n\nLocate on the polar locus frequency points where $\\omega=~0.2,0.3,0.5,1,2,6,10$ , and 20 rad 'sec.   \nAlso, find the magnitudes and phase angles of $G(j\\omega)$ at the specified frequency points.  \n\nSolution. In MATLAB Program 7–19 we used the frequency vector w, which consists of three frequency subvectors: w1 ,w2 , and w3 . Instead of such a w, we may simply use the  \n\nfrequency vector $\\mathsf{W}\\,=\\,\\mathsf{l o g s c a l e}(\\mathsf{d}_{1},\\,\\mathsf{d}_{2},\\,\\mathsf{n})$ . MATLAB Program 7–19 uses the following frequency vector:  \n\n$$\n\\mathsf{w}=\\mathsf{l o g s c a l e}(-1,2,100)\n$$  \n\nThis MATLAB program plots the polar locus and locates the specified frequency points on the polar locus, as shown in Figure 7–129.  \n\n![](images/7036653d3e4dcc94a19f91e29ba687ce4a20dec3c40ea188eaaf533f216a6277.jpg)  \n\n![](images/8216d2a6345dc788ae3f74d8df253f7f8c8384590e5d28e74c187a02c22b2566.jpg)  \nFigure 7–129 Polar plot of $G(j\\omega)$ given in Problem A–7–13 .  \n\nA–7–14. Consider a unity-feedback, positive-feedback system with the following open-loop transfer function:  \n\n$$\nG(s)=\\frac{s^{2}\\,+\\,4s\\,+\\,6}{s^{2}\\,+\\,5s\\,+\\,4}\n$$  \n\nDraw a Nyquist plot.  \n\nSolution. The Nyquist plot of the positive-feedback system can be obtained by defining num and den as  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=[-1\\mathsf{\\Omega}-4\\mathsf{\\Omega}-6]}\\\\ {\\mathsf{d e n}=[1\\mathsf{\\Omega}5\\mathsf{\\Omega}4]}\\end{array}\n$$  \n\nand using the command nyquist(num,den) . MATLAB Program 7–20 produces the Nyquist plot, as shown in Figure 7–130.  \n\nThis system is unstable, because the $-1+j0$ point is encircled once clockwise. Note that this is a special case where the Nyquist plot passes through $-1+j0$ point and also encircles this point once clockwise.This means that the closed-loop system is degenerate; the system behaves as if it were an unstable first-order system.See the following closed-loop transfer function of the positivefeedback system:  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{s^{2}\\,+\\,4s\\,+\\,6}{s^{2}\\,+\\,5s\\,+\\,4\\,-\\,\\left(s^{2}\\,+\\,4s\\,+\\,6\\right)}}}\\\\ {\\displaystyle{=\\frac{s^{2}\\,+\\,4s\\,+\\,6}{s\\,-\\,2}}}\\end{array}\n$$  \n\n![](images/3b7917a055fc7c3a26024cd83ca940b8ceb772b4913fe5156feb20aaf8b72122.jpg)  \n\n![](images/2acf82c73743ddc4d41ca3131f70ed961f4ec42a3215255af622308a0dd4a743.jpg)  \nFigure 7–130 Nyquist plot for positive-feedback system.  \n\nNote that the Nyquist plot for the positive-feedback case is a mirror image about the imaginary axis of the Nyquist plot for the negative-feedback case.This may be seen from Figure 7–131, which was obtained by use of MATLAB Program 7–21. (Note that the positive-feedback case is unstable, but the negative-feedback case is stable.)  \n\n![](images/dc27f992c90af1d92a88228628500bd042e1e72a758a76b4847667eb38f77c0d.jpg)  \n\n![](images/be0bf3a4cfb413efaac184bf2d935d23f337f6117fb8ac54578e434924a9f732.jpg)  \nFigure 7–131 Nyquist plots for positive-feedback system and negativefeedback system.  \n\nA–7–15. Consider the control system shown in Figure 7–60. (Refer to Example 7–19.) Using the inverse polar plot, determine the range of gain $K$ for stability.  \n\nSolution. Since  \n\n$$\nG_{2}(s)\\,=\\,{\\frac{1}{s^{3}\\,+\\,s^{2}\\,+\\,1}}\n$$  \n\nwe have  \n\n$$\nG(s)\\,=\\,G_{1}(s)G_{2}(s)\\,=\\frac{K(s\\,+\\,0.5)}{s^{3}\\,+\\,s^{2}\\,+\\,1}\n$$  \n\nHence, the inverse of the feedforward transfer function is  \n\n$$\n{\\frac{1}{G(s)}}={\\frac{s^{3}\\,+\\,s^{2}\\,+\\,1}{K(s\\,+\\,0.5)}}\n$$  \n\nNotice that $1/G(s)$ has a pole at $s\\,=\\,-0.5$ . It does not have any pole in the right-half $s$ plane. Therefore, the Nyquist stability equation  \n\n$$\nZ=N+P\n$$  \n\nreduces to $Z\\,=\\,N$ since $P\\,=\\,0.$ . The reduced equation states that the number $Z$ of the zeros of   \n$1\\,+\\,\\left[1/G(s)\\right]$ in the right-half $s$ lane is equal to $N$ umber of clockwise encirclements of   \nthe –1+j0 point. For stability, Nmust be equal to zero, or there should be no encirclement. Fig  \nure 7–132 shows the Nyquist plot or polar plot of $K/G(j\\omega)$ .Notice that since  \n\n$$\n\\begin{array}{r l}&{\\frac{K}{G(j\\omega)}=\\Bigg[\\frac{(j\\omega)^{3}\\:+\\:(j\\omega)^{2}\\:+\\:1}{j\\omega\\:+\\:0.5}\\Bigg]\\Big(\\frac{0.5\\:-\\:j\\omega}{0.5\\:-\\:j\\omega}\\Big)}\\\\ &{\\qquad\\qquad=\\frac{0.5\\:-\\:0.5\\omega^{2}\\:-\\:\\omega^{4}\\:+\\:j\\omega(-1\\:+\\:0.5\\omega^{2})}{0.25\\:+\\:\\omega^{2}}}\\end{array}\n$$  \n\nFigure 7–132 Polar plot of $K/G(j\\omega)$ .  \n\n![](images/96d363586ee4e81157ad3e97b6c2f7d5fd5719d8ab2116bf0501cc88993d520d.jpg)  \n\nthe $K/G(j\\omega)$ locus crosses the negative real axis at $\\omega=\\sqrt{2}$ ,and the crossing point at the negative real axis is $^{-2}$ .  \n\nFrom Figure 7–132, we see that if the critical point lies in the region between $^{-2}$ and $-\\infty$ , then the critical point is not encircled. Hence, for stability, we require  \n\n$$\n-1<\\frac{-2}{K}\n$$  \n\nThus, the range of gain $K$ for stability is  \n\n$$\n2<K\n$$  \n\nwhich is the same result as we obtained in Example 7–19.  \n\n# A–7–16.  \n\nFigure 7–133 shows a block diagram of a space-vehicle control system. Determine the gain $K$ such that the phase margin is $50^{\\circ}$ .What is the gain margin in this case?  \n\nSolution. Since  \n\n$$\nG(j\\omega)=\\frac{K(j\\omega+2)}{(j\\omega)^{2}}\n$$  \n\nwe have  \n\n$$\n\\big/G(j\\omega)=\\,\\big/j\\omega\\,+\\,2\\,-\\,2\\,\\big/j\\omega=\\,\\tan^{-1}\\frac{\\omega}{2}\\,-\\,180^{\\circ}\n$$  \n\nThe requirement that the phase margin be $50^{\\circ}$ means that $\\big/G\\big(j\\omega_{c}\\big)$ must be equal to $-130^{\\circ}$ , where $\\omega_{c}$ is the gain crossover frequency, or  \n\n$$\n\\big/G\\big(j\\omega_{c}\\big)=-130^{\\circ}\n$$  \n\nFigure 7–133   \nSpace-vehicle control system.  \n\n![](images/0601d6dd7b782894a525b0d9690c89693737dcb63e7b56d26239bb68230b476f.jpg)  \n\nHence, we set  \n\n$$\n\\tan^{-1}{\\frac{\\omega_{c}}{2}}=50^{\\circ}\n$$  \n\nfrom which we obtain  \n\n$$\n\\omega_{c}\\,=\\,2.3835\\,\\mathrm{rad/sec}\n$$  \n\nSince the phase curve never crosses the $-180^{\\circ}$ line, the gain margin is $+\\infty$ dB. Noting that the magnitude of $G(j\\omega)$ must be equal to 0 dB at $\\omega\\,=\\,2.3835$ , we have  \n\n$$\n\\left|\\frac{K(j\\omega\\,+\\,2)}{(j\\omega)^{2}}\\right|_{\\omega=2.3835}\\,=\\,1\n$$  \n\nfrom which we get  \n\n$$\nK={\\frac{2.3835^{2}}{\\sqrt{2^{2}+2.3835^{2}}}}=1.8259\n$$  \n\nThis $K$ value will give the phase margin of $50^{\\circ}$ .  \n\nA–7–17. For the standard second-order system  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\omega_{n}^{2}}{s^{2}+2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}}\n$$  \n\nshow that the bandwidth $\\omega_{b}$ is given by  \n\n$$\n\\omega_{b}\\,=\\,\\omega_{n}\\bigl(1\\,-\\,2\\zeta^{2}\\,+\\,\\sqrt{4\\zeta^{4}\\,-\\,4\\zeta^{2}\\,+\\,2}\\bigr)^{1/2}\n$$  \n\nNote that $\\omega_{b}/\\omega_{n}$ is a function only of $\\zeta$ . Plot a curve of $\\omega_{b}/\\omega_{n}$ versus $\\zeta$  \n\nlution. The width $\\omega_{b}$ is determined from $\\left|C\\!\\left(j\\omega_{b}\\right)\\middle/R\\!\\left(j\\omega_{b}\\right)\\right|=-3$ dB. Quite often, instead of $^{-3}$ dB, we use $-3.01$ dB, which is equal to 0.707.Thus,  \n\n$$\n\\left|\\frac{C(j\\omega_{b})}{R(j\\omega_{b})}\\right|\\,=\\,\\left|\\frac{\\omega_{n}^{2}}{\\left(j\\omega_{b}\\right)^{2}\\,+\\,2\\zeta\\omega_{n}\\bigl(j\\omega_{b}\\bigr)\\,+\\,\\omega_{n}^{2}}\\right|\\,=\\,0.707\n$$  \n\nThen  \n\n$$\n\\frac{\\omega_{n}^{2}}{\\sqrt{\\left(\\omega_{n}^{2}\\mathrm{~-~}\\omega_{b}^{2}\\right)^{2}\\mathrm{~+~}\\left(2\\zeta\\omega_{n}\\omega_{b}\\right)^{2}}}=0.707\n$$  \n\nfrom which we get  \n\n$$\n\\omega_{n}^{4}\\,=\\,0.5\\big[\\big(\\omega_{n}^{2}\\,-\\,\\omega_{b}^{2}\\big)^{2}\\,+\\,4\\zeta^{2}\\omega_{n}^{2}\\omega_{b}^{2}\\big]\n$$  \n\n![](images/7f724d5fdff12a9f9b6093887db666b1fc654fa0c2bfcb88a5a5b3002fe4681e.jpg)  \nFigure 7–134 Curve of $\\omega_{b}/\\omega_{n}$ versus $\\zeta$ , where $\\omega_{b}$ is the bandwidth.  \n\nBy dividing both sides of this last equation by $\\omega_{n}^{4}$ , we obtain  \n\n$$\n1\\,=\\,0.5\\Bigg\\{\\Bigg[1\\,-\\,\\Bigg(\\frac{\\omega_{b}}{\\omega_{n}}\\Bigg)^{2}\\Bigg]^{2}\\,+\\,4\\zeta^{2}\\bigg(\\frac{\\omega_{b}}{\\omega_{n}}\\bigg)^{2}\\Bigg\\}\n$$  \n\nSolving this last equation for $\\left(\\omega_{b}/\\omega_{n}\\right)^{2}$ yields  \n\n$$\n\\left(\\frac{\\omega_{b}}{\\omega_{n}}\\right)^{2}=-2\\zeta^{2}+1\\,\\pm\\,\\sqrt{4\\zeta^{4}-\\,4\\zeta^{2}\\,+\\,2}\n$$  \n\nSince $\\left(\\omega_{b}/\\omega_{n}\\right)^{2}>0$ , we take the plus sign in this last equation.Then  \n\n$$\n\\omega_{b}^{2}\\,=\\,\\omega_{n}^{2}\\bigl(1\\,-\\,2\\zeta^{2}\\,+\\,\\sqrt{4\\zeta^{4}\\,-\\,4\\zeta^{2}\\,+\\,2}\\bigr)\n$$  \n\nor  \n\n$$\n\\omega_{b}\\,=\\,\\omega_{n}\\bigl(1\\,-\\,2\\zeta^{2}\\,+\\,\\sqrt{4\\zeta^{4}\\,-\\,4\\zeta^{2}\\,+\\,2}\\bigr)^{1/2}\n$$  \n\nFigure 7–134 shows a curve relating $\\omega_{b}/\\omega_{n}$ versus $\\zeta$ .  \n\nA–7–18. A Bode diagram of the open-loop transfer function $G(s)$ of a unity-feedback control system is shown in Figure 7–135. It is known that the open-loop transfer function is minimum phase. From the diagram, it can be seen that there is a pair of complex-conjugate poles at $\\omega=2$ rad 'sec. Determine the damping ratio of the quadratic term involving these complex-conjugate poles. Also, determine the transfer function $G(s)$ .  \n\nSolution. Referring to Figure 7–9 and examining the Bode diagram of Figure 7–135, we find the damping ratio $\\zeta$ and undamped natural frequency $\\omega_{n}$ of the quadratic term to be  \n\n$$\n\\zeta=0.1,\\qquad\\omega_{n}=2\\,\\mathrm{rad/sec}\n$$  \n\n![](images/5038e457c30805586ea09d8e497239f84c3878289fb82e9186be0bb429e83147.jpg)  \nFigure 7–135 Bode diagram of the open-loop transfer function of a unityfeedback control system.  \n\nNoting that there is another corner frequency at $\\omega=0.5$ rad 'sec and the slope of the magnitude curve in the low-frequency region is $-40$ dB 'decade, $G(j\\omega)$ can be tentatively determined as follows:  \n\n$$\nG(j\\omega)=\\frac{K\\Big(\\frac{j\\omega}{0.5}+1\\Big)}{(j\\omega)^{2}\\Bigg[\\left(\\frac{j\\omega}{2}\\right)^{2}+0.1(j\\omega)\\,+\\,1\\Bigg]}\n$$  \n\nSince, from Figure 7 we find $\\left|G(j0.1)\\right|=40\\;\\mathrm{dB}$ ,he gain value $K$ can be determined to be unity.Also, the calculated phase curve, $\\underline{{\\langle G(j\\omega)\\rangle}}$ versus v, agrees with the given phase curve. Hence, the transfer function $G(s)$ can be determined to be  \n\n$$\nG(s)\\,=\\frac{4(2s\\,+\\,1)}{s^{2}(s^{2}\\,+\\,0.4s\\,+\\,4)}\n$$  \n\nA–7–19. A closed-loop control system may include an unstable element within the loop.When the Nyquist stability criterion is to be applied to such a system, the frequency-response curves for the unstable element must be obtained.  \n\nHow can we obtain experimentally the frequency-response curves for such an unstable element? Suggest a possible approach to the experimental determination of the frequency response of an unstable linear element.  \n\nSolution. One possible approach is to measure the frequency-response characteristics of the unstable element by using it as a part of a stable system.  \n\nFigure 7–136 Control system.  \n\n![](images/da220c751a981ec42861497181adb0c7fd3aaeec153fd6c0d86f31af367c0cb1.jpg)  \n\nConsider the system shown in Figure 7–136. Suppose that the element $G_{1}(s)$ is unstable.The complete system may be made stable by choosing a suitable linear element $G_{2}(s)$ .We apply a sinusoidal signal at the input.At steady state, all signals in the loop will be sinusoidal.We measure the signals $e(t)$ , the input to the unstable element, and $x(t)$ , the output of the unstable element. By changing the frequency [and possibly the amplitude for the convenience of measuring $e(t)$ and $x(t)]$ of the input sinusoid and repeating this process, it is possible to obtain the frequency response of the unstable linear element.  \n\nA–7–20. Show that the lead network and lag network inserted in cascade in an open loop act as proportional-plus-derivative control (in the region of small $\\omega$ ) and proportional-plus-integral control (in the region of large $\\omega$ ), respectively.  \n\nSolution. In the region of small $\\omega$ , the polar plot of the lead network is approximately the same as that of the proportional-plus-derivative controller.This is shown in Figure 7–137(a).  \n\nSimilarly, in the region of large $\\omega$ , the polar plot of the lag network approximates the proportional-plus-integral controller, as shown in Figure 7–137(b).  \n\nA–7–21. Consider a lag–lead compensator $G_{c}(s)$ defined by  \n\n$$\nG_{c}(s)=K_{c}\\frac{\\biggl(s+\\frac{1}{T_{1}}\\biggr)\\biggl(s+\\frac{1}{T_{2}}\\biggr)}{\\biggl(s+\\frac{\\beta}{T_{1}}\\biggr)\\biggl(s+\\frac{1}{\\beta T_{2}}\\biggr)}\n$$  \n\nShow that at frequency $\\omega_{1}$ ,where  \n\n$$\n\\omega_{1}={\\frac{1}{\\sqrt{T_{1}T_{2}}}}\n$$  \n\n![](images/073e504ac6e442417a1c9d86ed79160ae1a4ea051aeb71300fc620e43e3cdc25.jpg)  \nFigure 7–137 (a) Polar plots of a lead network and a proportional-plusderivative controller; (b) polar plots of a lag network and a proportional-plusintegral controller.  \n\nthe phase angle of $G_{c}(j\\omega)$ becomes zero. (This compensator acts as a lag compensator for $0<\\omega<\\omega_{1}$ and acts as a lead compensator for $\\omega_{1}<\\omega<\\infty.$ ) (Refer to Figure 7–109.)  \n\nSolution. The angle of $G_{c}(j\\omega)$ is given by  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\underline{{\\big/}G_{c}(j\\omega)}=\\underbrace{\\bigg/j\\omega+\\frac{1}{T_{1}}+\\int\\!\\!j\\omega+\\frac{1}{T_{2}}-\\int\\!\\!j\\omega+\\frac{\\beta}{T_{1}}-\\int\\!\\!j\\omega+\\frac{1}{\\beta T_{2}}}_{=\\mathrm{\\tan}^{-1}\\omega T_{1}+\\mathrm{\\tan}^{-1}\\omega T_{2}-\\mathrm{\\tan}^{-1}\\omega T_{1}/\\beta-\\mathrm{\\tan}^{-1}\\omega T_{2}\\beta}}}\\\\ &{\\quad\\quad=\\tan^{-1}\\!\\omega T_{1}+\\tan^{-1}\\!\\omega T_{2}-\\tan^{-1}\\!\\omega T_{1}/\\beta-\\tan^{-1}\\!\\omega T_{2}\\beta}\\end{array}\n$$  \n\nAt $\\omega\\,=\\,\\omega_{1}\\,=\\,1/\\sqrt{T_{1}T_{2}}$ ,we have  \n\n$$\n\\Big/G_{c}\\big(j\\omega_{1}\\big)=\\tan^{-1}\\sqrt{\\frac{T_{1}}{T_{2}}}+\\,\\tan^{-1}\\sqrt{\\frac{T_{2}}{T_{1}}}-\\,\\tan^{-1}\\frac{1}{\\beta}\\sqrt{\\frac{T_{1}}{T_{2}}}-\\,\\tan^{-1}\\!\\beta\\sqrt{\\frac{T_{2}}{T_{1}}}\n$$  \n\nSince  \n\n$$\n\\tan\\left(\\tan^{-1}\\!\\sqrt{\\frac{T_{1}}{T_{2}}}+\\tan^{-1}\\!\\sqrt{\\frac{T_{2}}{T_{1}}}\\right)=\\frac{\\sqrt{\\frac{T_{1}}{T_{2}}}+\\sqrt{\\frac{T_{2}}{T_{1}}}}{1-\\sqrt{\\frac{T_{1}}{T_{2}}}\\sqrt{\\frac{T_{2}}{T_{1}}}}=\\infty\n$$  \n\nor  \n\nand also  \n\n$$\n\\tan^{-1}\\!\\sqrt{\\frac{T_{1}}{T_{2}}}\\,+\\,\\tan^{-1}\\!\\sqrt{\\frac{T_{2}}{T_{1}}}=\\,90^{\\circ}\n$$  \n\n$$\n\\tan^{-1}{\\frac{1}{\\beta}}\\sqrt{\\frac{T_{1}}{T_{2}}}\\,+\\,\\tan^{-1}\\!\\beta\\sqrt{\\frac{T_{2}}{T_{1}}}=\\,90^{\\circ}\n$$  \n\nwe have  \n\n$$\n\\big/G_{c}(j\\omega_{1})=0^{\\circ}\n$$  \n\nThus, the angle of $G_{c}(j\\omega_{1})$ becomes $0^{\\circ}$ at $\\omega\\,=\\,\\omega_{1}\\,=\\,1/\\sqrt{T_{1}T_{2}}$ .  \n\nA–7–22. Consider the control system shown in Figure 7–138. Determine the value of gain $K$ such that the phase margin is $60^{\\circ}$ .What is the gain margin with this value of gain $K$ ?  \n\nSolution. The open-loop transfer function is  \n\n$$\n\\begin{array}{c}{G(s)\\,=\\,K\\,{\\frac{s\\,+\\,0.1}{s\\,+\\,0.5}}{\\frac{10}{s(s\\,+\\,1)}}}\\\\ {\\,=\\,{\\frac{K(10s\\,+\\,1)}{s^{3}\\,+\\,1.5s^{2}\\,+\\,0.5s}}}\\end{array}\n$$  \n\nFigure 7–138 Control system.  \n\n![](images/40198104ce80451d91e73cd2ee1d796da03f9d1199de2b4b04c649d8ff7f493b.jpg)  \n\nLet us plot the Bode diagram of $G(s)$ when $K=1,$ . MATLAB Program 7–22 may be used for this purpose. Figure 7–139 shows the Bode diagram produced by this program. From this diagram the required phase margin of $60^{\\circ}$ occurs at the frequency $\\omega=1.15$ rad 'sec.The magnitude of $G(j\\omega)$ at this frequency is found to be 14.5 dB.Then gain $K$ must satisfy the following equation:  \n\n$$\n20\\log K=-14.5\\,\\mathrm{dB}\n$$  \n\nor  \n\n$$\nK=0.188\n$$  \n\n![](images/0abd725557f830311573635fce67c61fd2223309ca85ea7402c93c23605531de.jpg)  \n\nThus, we have determined the value of gain $K$ . Since the angle curve does not cross the $-180^{\\circ}$ line, the gain margin is $+\\infty$ dB.  \n\nTo verify the results, let us draw a Nyquist plot of $G$ for the frequency range  \n\n$$\n\\mathsf{W}=0.5;0.01;1.15\n$$  \n\nThe end point of the locus $(\\omega=1.15\\,\\mathrm{rad/sec})$ )will be on a unit circle in the Nyquist plane.To check the phase margin, it is convenient to draw the Nyquist plot on a polar diagram, using polar grids. To draw the Nyquist plot on a polar diagram, first define a complex vector $z$ by  \n\n$$\nz={\\mathsf{r e}}+{\\mathsf{i}}{\\mathsf{i}}{\\mathsf{m}}={\\mathsf{r e}}^{\\mathsf{i}\\theta}\n$$  \n\nwhere rand $\\theta$ (theta) are given by  \n\n$$\n\\begin{array}{c}{{\\Gamma=\\mathsf{a b s}(z)}}\\\\ {{\\mathsf{t h e t a}=\\mathsf{a n g l e}(z)}}\\end{array}\n$$  \n\nThe abs means the square root of the sum of the real part squared and imaginary part squared;   \nangle means tan –1 (imaginary part/real part).  \n\n![](images/65d399084e8bec3cfadd6b41427bb47c49f6675feec746a8ad172a811f44c64e.jpg)  \nFigure 7–139 Bode diagram of $G(s)=\\frac{10s\\,+\\,1}{s(s\\,+\\,0.5)(s\\,+\\,1)}.$  \n\nIf we use the command polar(theta,r)  \n\nMATLAB will produce a plot in the polar coordinates. Subsequent use of the grid command draws polar grid lines and grid circles.  \n\nMATLAB Program 7–23 produces the Nyquist plot of $G(j\\omega)$ , where $\\omega$ is between 0.5 and 1.15 rad 'sec.The resulting plot is shown in Figure 7–140.Notice that point $G(j1.15)$ lies on the unit circle, and the phase angle of this point is $-120^{\\circ}$ . Hence, the phase margin is $60^{\\circ}$ . The fact that point $G(j1.15)$ is on the unit circle verifies that at $\\omega=1.15$ rad 'sec the magnitude is equal to 1 or 0 dB. (Thus, $\\omega=1.15$ is the gain crossover frequency.) Thus, $K=0.188$ gives the desired phase margin of $60^{\\circ}$ .  \n\n![](images/7b279f7b96c961dbe23de0fc1a15c33e19e668dcd35252d836c70568a57bfc2a.jpg)  \n\n![](images/cd611e0aa8fe12c0942fece8b77c2ee95b24fcd73b791c6e88ffeb8a3fe0452a.jpg)  \nFigure 7–140 Nyquist plot of $G(j\\omega)$ showing that the phase margin is $60^{\\circ}$  \n\nNote that in writing ‘text’ in the polar diagram we enter the text command as follows:  \n\n$$\n\\operatorname{text}(\\mathbf{x},\\mathbf{y},^{\\prime}\\mathbf{\\Omega}^{\\prime})\n$$  \n\nFor example, to write ‘Nyquist plot’ starting at point $(0.3,-1.7)$ , enter the command  \n\n$$\n\\mathrm{text}(0.3,-1.7,\"\\mathrm{Nyquist}\\;\\mathrm{plot}^{\\prime})\n$$  \n\nThe text is written horizontally on the screen.  \n\nA–7–23. If the open-loop transfer function $G(s)$ involves lightly damped complex-conjugant poles, then more than one $M$ locus may be tangent to the $G(j\\omega)$ locus. Consider the unity-feedback system whose open-loop transfer function is  \n\n$$\nG(s)\\,=\\frac{9}{s(s\\,+\\,0.5)(s^{2}\\,+\\,0.6s\\,+\\,10)}\n$$  \n\nDraw the Bode diagram for this open-loop transfer function.Draw also the log-magnitude-versusphase plot,and show that two $M$ loci are tangent to the $G(j\\omega)$ locus.Finally,plot the Bode diagram for the closed-loop transfer function.  \n\nSolution. Figure 7–141 shows the Bode diagram of $G(j\\omega)$ . Figure 7–142 shows the log-magnitude-versus-phase plot of $G(j\\omega)$ . It is seen that the $G(j\\omega)$ locus is tangent to the $M=8$ -dB locus at $\\omega=0.97$ rad 'sec, and it is tangent to the $M=-4$ -dB locus at $\\omega=2.8$ rad 'sec.  \n\n![](images/b2955c107a72d26ecd4c7286977f386428376c12395b8bd3d51df49589534017.jpg)  \nFigure 7–141 Bode diagram of $G(s)$ given by Equation (7–32).  \n\n![](images/ea0166745591317090e6737d35db653029376535a88bf964c8c9c82ca4fdccea.jpg)  \nFigure 7–142 Log-magnitudeversus-phase plot of $G(s)$ given by Equation (7–32).  \n\nFigure 7–143 shows the Bode diagram of the closed-loop transfer function. The magnitude curve of the closed-loop frequency response shows two resonant peaks. Note that such a case occurs when the closed-loop transfer function involves the product of two lightly damped secondorder terms and the two corresponding resonant frequencies are sufficiently separated from each other.As a matter of fact, the closed-loop transfer function of this system can be written  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{G(s)}{1\\,+\\,G(s)}}\\ ~~}}\\\\ {{\\displaystyle~~~~~~~=\\frac{9}{\\left(s^{2}\\,+\\,0.487s\\,+\\,1\\right)\\!\\left(s^{2}\\,+\\,0.613s\\,+\\,9\\right)}}}\\end{array}\n$$  \n\n![](images/c4ae7fbd6b102540fba91faf6613555181d080770cad71d422a98312c8cdfdd4.jpg)  \nFigure 7–143 Bode diagram of $G(s)/[1{\\overline{{+}}}\\ G(s)]$ ,where $G(s)$ is given by Equation (7–32).  \n\nClearly, the denominator of the closed-loop transfer function is a product of two lightly damped second-order terms (the damping ratios are 0.243 and 0.102), and the two resonant frequencies are sufficiently separated.  \n\nA–7–24. Consider the system shown in Figure 7–144(a). Design a compensator such that the closed-loop system will satisfy the requirements that the static velocity error constant $=20\\ {\\mathrm{sec}}^{-1}$ ,phase margin $=50^{\\circ}$ , and gain margin $\\geq10$ dB.  \n\nSolution. To satisfy the requirements, we shall try a lead compensator $G_{c}(s)$ of the form  \n\n$$\n\\begin{array}{l}{{G_{c}(s)\\,=\\,K_{c}\\alpha\\,\\displaystyle\\frac{T s\\,+\\,1}{\\alpha T s\\,+\\,1}}}\\\\ {{{}}}\\\\ {{{}}}\\\\ {{{\\,=\\,K_{c}\\displaystyle\\frac{s\\,+\\,\\displaystyle\\frac{1}{T}}{s+\\displaystyle\\frac{1}{\\alpha T}}}}}\\end{array}\n$$  \n\n(If the lead compensator does not work, then we need to employ a compensator of different form.) The compensated system is shown in Figure 7–144(b).  \n\nDefine  \n\n$$\nG_{1}(s)=K G(s)=\\frac{10K}{s(s^{\\mathrm{~+~}1})}\n$$  \n\nwhere $K\\,=\\,K_{c}\\alpha.$ . The first step in the design is to adjust the gain $K$ to meet the steady-state performance specification or to provide the required static velocity error constant. Since the static velocity error constant $K_{v}$ is given as $20\\;\\mathrm{sec}^{-1}$ , we have  \n\n$$\n{\\begin{array}{r l}&{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)}\\\\ &{\\qquad=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s\\frac{T s+1}{\\alpha T s+1}G_{1}(s)}\\\\ &{\\qquad=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}{\\frac{s10K}{s(s+1)}}}\\\\ &{\\qquad=10K=20}\\end{array}}\n$$  \n\nor  \n\nWith $K=2$ ,the compensated system will satisfy the steady-state requirement. We shall next plot the Bode diagram of  \n\n$$\nG_{1}(s)=\\frac{20}{s(s+1)}\n$$  \n\n![](images/af10db0dc1423db01b47f51a2758f14f9e8be1d1628604fa34a8af4cc181bfec.jpg)  \nFigure 7–144 (a) Control system; (b) compensated system.  \n\nMATLAB Program 7–24 produces the Bode diagram shown in Figure 7–145. From this plot, the phase margin is found to be $14^{\\circ}$ .The gain margin is $+\\infty$ dB.  \n\n![](images/79bc79fa956d5c32196eaefbee54954a835397015bd1b35ef248e0a86f5e120c.jpg)  \n\nSince the specification calls for a phase margin of $50^{\\circ}$ , the additional phase lead necessary to satisfy the phase-margin requirement is $36^{\\circ}$ .A lead compensator can contribute this amount.  \n\nNoting that the addition of a lead compensator modifies the magnitude curve in the Bode diagram, we realize that the gain crossover frequency will be shifted to the right.We must offset the increased phase lag of $G_{1}(j\\omega)$ due to this increase in the gain crossover frequency.Taking the shift of the gain crossover frequency into consideration, we may assume that $\\phi_{m}$ ,the maximum phase lead required, is approximately $41^{\\circ}$ . (This means that approximately $5^{\\circ}$ has been added to compensate for the shift in the gain crossover frequency.) Since  \n\n$$\n\\sin\\phi_{m}=\\frac{1-\\alpha}{1+\\alpha}\n$$  \n\n$\\phi_{m}=41^{\\circ}$ corresponds to $\\alpha\\,=\\,0.2077$ .Note that $\\alpha=0.21$ corresponds to $\\phi_{m}=40.76^{\\circ}$ . Whether we choose $\\phi_{m}=41^{\\circ}$ or $\\phi_{m}=40.76^{\\circ}$ does not make much difference in the final solution. Hence, let us choose $\\alpha=0.21$ .  \n\n![](images/4c25f86e1eb4013e8a40d1f279b2b5e3e2d0e0a820960274f1c1486fabc28e9d.jpg)  \nFigure 7–145 Bode diagram of $G_{1}(s)$ .  \n\nOnce the attenuation factor $\\alpha$ has been determined on the basis of the required phase-lead 1 angle, the next step is to determine the corner frequencies $\\omega=1/T$ and $\\omega=1/(\\alpha T)$ of the lead compensator. Notice that the maximum phase-lead angle $\\phi_{m}$ occurs at the geometric mean of the two corner frequencies, or $\\omega\\,=\\,1/\\big(\\sqrt{\\alpha}T\\big)$ .  \n\nThe amount of the modification in the magnitude curve at $\\omega\\,=\\,1/\\big(\\sqrt{\\alpha}T\\big)$ due to the inclusion of the term $(T s\\,+\\,1)/(\\alpha T s\\,+\\,1)$ is  \n\n$$\n{\\Bigg|}{\\frac{1\\,+\\,j\\omega T}{1\\,+\\,j\\omega\\alpha T}}{\\Bigg|}_{\\omega={\\frac{1}{\\sqrt{\\alpha}T}}}=\\left|{\\frac{1\\,+\\,j{\\frac{1}{\\sqrt{\\alpha}}}}{1\\,+\\,j\\alpha{\\frac{1}{\\sqrt{\\alpha}}}}}\\right|={\\frac{1}{\\sqrt{\\alpha}}}\n$$  \n\nNote that  \n\n$$\n\\frac{1}{\\sqrt{\\alpha}}=\\frac{1}{\\sqrt{0.21}}=6.7778\\,\\mathrm{dB}\n$$  \n\nWe need to find the frequency point where, when the lead compensator is added, the total magnitude becomes 0 dB.  \n\nFrom Figure 7–145 we see that the frequency point where the magnitude of $G_{1}(j\\omega)$ is $-6.7778$ dB occurs between $\\omega=1$ and 10 rad 'sec. Hence, we plot a new Bode diagram of $G_{1}(j\\omega)$ in the frequency range between $\\omega=1$ and 10 to locate the exact point where $G_{1}(j\\omega)\\,=\\,-6.7778$ dB. MATLAB Program 7–25 produces the Bode diagram in this frequency range, which is shown in Figure 7–146. From this diagram, we find the frequency point where $\\left|G_{1}(j\\omega)\\right|\\,=-6.7778$ dB occurs at $\\omega\\,=\\,6.5686$ rad 'sec. Let us select this frequency to be the new gain crossover frequency, or $\\omega_{c}\\,=\\,6.5686$ rad 'sec. Noting that this frequency corresponds to $\\bar{1}/(\\sqrt{\\alpha}T)$ ,or  \n\n$$\n\\omega_{c}={\\frac{1}{\\sqrt{\\alpha}T}}\n$$  \n\nwe obtain  \n\n$$\n{\\frac{1}{T}}=\\omega_{c}{\\sqrt{\\alpha}}=6.5686{\\sqrt{0.21}}=3.0101\n$$  \n\nand  \n\n$$\n\\frac{1}{\\alpha T}=\\frac{\\omega_{c}}{\\sqrt{\\alpha}}=\\frac{6.5686}{\\sqrt{0.21}}=14.3339\n$$  \n\n![](images/a889fd09666c453e6443f6bdac05fa1b633eed1d4b6960bb33ef57677afa47f5.jpg)  \n\n![](images/ed9fd6d4bd9dceea21243ba3a194c101238f7f993405631cac7fca2a57c184ce.jpg)  \nFigure 7–146 Bode diagram of $G_{1}(s)$ .  \n\nThe lead compensator thus determined is  \n\n$$\nG_{c}(s)\\,=\\,K_{c}\\,{\\frac{s\\,+\\,3.0101}{s\\,+\\,14.3339}}=K_{c}\\,\\alpha\\,{\\frac{0.3322s\\,+\\,1}{0.06976s\\,+\\,1}}\n$$  \n\nwhere $K_{c}$ is determined as  \n\n$$\nK_{c}=\\frac{K}{\\alpha}=\\frac{2}{0.21}=9.5238\n$$  \n\nThus, the transfer function of the compensator becomes  \n\n$$\nG_{c}(s)\\,=\\,9.5238\\,{\\frac{s\\,+\\,3.0101}{s\\,+\\,14.3339}}=2\\,{\\frac{0.3322s\\,+\\,1}{0.06976s\\,+\\,1}}\n$$  \n\nMATLAB Program 7–26 produces the Bode diagram of this lead compensator, which is shown in Figure 7–147.  \n\n![](images/6aaf9091c655efe9ea9b8cf940736d208f40bbf0f376bedd5476f1f152e2cd0e.jpg)  \n\n![](images/8988b0be06818d2d5980fc29c72ee93917c0128c6f47827bdcb8c1c4abd049b3.jpg)  \n\nFigure 7–147 Bode diagram of $G_{c}(s)$ .  \n\nThe open-loop transfer function of the designed system is  \n\n$$\n\\begin{array}{r l r}{G_{c}(s)G(s)\\space=\\space9.5238\\frac{s\\space+\\space3.0101}{s\\space+\\space14.3339}\\frac{10}{s\\space(s\\space+\\space1)}}&{}&\\\\ {\\space=\\space\\frac{95.238s\\space+\\space286.6759}{s^{3}\\space+\\space15.3339s^{2}\\space+\\space14.3339s}}&{}&\\end{array}\n$$  \n\nMATLAB Program 7–27 will produce the Bode diagram of $G_{c}(s)G(s)$ , which is shown in Figure 7–148.  \n\n![](images/fd7bad8d5b3a4a49b84ea6de090b9c2bcc2cf04fd82d5810ac726f9445200aec.jpg)  \n\n![](images/d4cdbb59d1b139ff6dffb1dab6ffcf19ad49947d6a146f31d6ebea41c3984c2f.jpg)  \nFigure 7–148 Bode diagram of $G_{c}(s)G(s)$ .  \n\nFrom MATLAB Program 7–27 and Figure 7–148 it is clearly seen that the phase margin is approximately $50^{\\circ}$ and the gain margin is $+\\infty$ dB. Since the static velocity error constant $K_{v}$ is $20\\ \\mathrm{sec}^{-1}$ , all the specifications are met. Before we conclude this problem, we need to check the transient-response characteristics.  \n\nUnit-Step Response: We shall compare the unit-step response of the compensated system with that of the original uncompensated system.  \n\nThe closed-loop transfer function of the original uncompensated system is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{10}{s^{2}\\,+\\,s\\,+\\,10}}\n$$  \n\nThe closed-loop transfer function of the compensated system is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{95.238s\\,+\\,286.6759}{s^{3}\\,+\\,15.3339s^{2}\\,+\\,110.5719s\\,+\\,286.6759}\n$$  \n\nMATLAB Program 7–28 produces the unit-step responses of the uncompensated and compensated systems. The resulting response curves are shown in Figure 7–149. Clearly, the compensated system exhibits a satisfactory response. Note that the closed-loop zero and poles are located as follows:  \n\n$s\\,=\\,-5.2880\\,\\pm\\,j5.6824,\\;\\;\\;s\\,=\\,-4.7579$  \n\nUnit-Ramp Response: It is worthwhile to check the unit-ramp response of the compensated system. Since $K_{v}\\,=\\,20\\,\\ \\mathrm{sec}^{-1}$ , the steady-state error following the unit-ramp input will be $1/K_{v}=0.05$ . The static velocity error constant of the uncompensated system is $10\\;\\mathrm{sec}^{-1}$ . Hence, the original uncompensated system will have twice as large a steady-state error in following the unit-ramp input.  \n\n![](images/ebc6d1c04287da7c31910f8a1d79c1471b3acde534c8dfec9c4b3e98ba499dec.jpg)  \n\n![](images/1914e3847ddcdab769eb86e7db64293648621ff882555ca94cae088ed2a5885c.jpg)  \nFigure 7–149 Unit-step responses of the uncompensated and compensated systems.  \n\nMATLAB Program 7–29 produces the unit-ramp response curves. [Note that the unit-ramp response is obtained as the unit-step response of $C(s)/s R(s).]$ The resulting curves are shown in Figure 7–150.The compensated system has a steady-state error equal to one-half that of the original uncompensated system.  \n\n![](images/c66c7793f3f7f39c250c4c1fecffd0f2013d0976b13f69ad70af77b39436f713.jpg)  \n\n![](images/5bac5009e8c700b0e5f2647db6a4aa4d09169cad8358b6276727973073609842.jpg)  \nFigure 7–150 Unit-ramp responses of the uncompensated and compensated systems.  \n\nA–7–25. Consider a unity-feedback system whose open-loop transfer function is  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s\\,+\\,1)(s\\,+\\,4)}\n$$  \n\nDesign a lag–lead compensator $G_{c}(s)$ such that the static velocity error constant is $10\\;\\mathrm{sec}^{-1}$ , the phase margin is $50^{\\circ}$ , and the gain margin is $10\\;\\mathrm{dB}$ or more.  \n\nSolution. We shall design a lag–lead compensator of the form  \n\n$$\nG_{c}(s)=K_{c}\\frac{\\biggl(s+\\frac{1}{T_{1}}\\biggr)\\biggl(s+\\frac{1}{T_{2}}\\biggr)}{\\biggl(s+\\frac{\\beta}{T_{1}}\\biggr)\\biggl(s+\\frac{1}{\\beta T_{2}}\\biggr)}\n$$  \n\nThen the open-loop transfer function of the compensated system is $G_{c}(s)G(s)$ . Since the gain $K$ of the plant is adjustable, let us assume that $K_{c}=1.$ . Then $\\operatorname*{lim}_{s\\rightarrow0}G_{c}(s)\\,=\\,1.$ From the requirement on the static velocity error constant, we obtain  \n\n$$\n\\begin{array}{l}{{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)G(s)\\,=\\,\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)\\,\\displaystyle\\frac{K}{s(s\\,+\\,1)(s\\,+\\,4)}}}\\\\ {{~~~}}\\\\ {{{}=\\displaystyle\\frac{K}{4}=10}}\\end{array}\n$$  \n\nHence,  \n\n$$\nK=40\n$$  \n\nWe shall first plot a Bode diagram of the uncompensated system with $K=40.$ MATLAB Program 7–30 may be used to plot this Bode diagram.The diagram obtained is shown in Figure 7–151.  \n\n![](images/4b7ccfcb18714097ba33eb8217e91825a665f67044278467a6fd650fe65e53fb.jpg)  \n\n![](images/ee1b182005068b37baffc4b9a1c793303f5cc7e10655ef42b20e7ffc37d660b5.jpg)  \n\nFigure 7–151   \nBode diagram of   \n$G(s)\\,=\\,\\bar{40}/\\bigl[s(s\\,+\\,1)(s\\,+\\,4)\\bigr]$ .  \n\nFrom Figure 7–151, the phase margin of the gain-adjusted but uncompensated system is found to be $-16^{\\circ}$ , which indicates that this system is unstable. The next step in the design of a lag–lead compensator is to choose a new gain crossover frequency. From the phase-angle curve for $G(j\\omega)$ , we notice that the phase crossover frequency is $\\omega=2$ rad 'sec. We may choose the new gain crossover frequency to be 2 rad 'sec so that the phase-lead angle required at $\\omega=2$ rad 'sec is about $50^{\\circ}$ . A single lag–lead compensator can provide this amount of phaselead angle quite easily.  \n\nOnce we choose the gain crossover frequency to be 2 rad 'sec, we can determine the corner frequencies of the phase-lag portion of the lag–lead compensator. Let us choose the corner frequency $\\omega=1/T_{2}$ (which corresponds to the zero of the phase-lag portion of the compensator) to be 1 decade below the new gain crossover frequency, or at $\\omega=0.2$ rad 'sec. For another corner frequency $\\omega\\,=\\,1/(\\beta T_{2})$ ,we need the value of $_{\\beta}$ . The value of $\\beta$ can be determined from the consideration of the lead portion of the compensator, as shown next.  \n\nFor the lead compensator, the maximum phase-lead angle $\\phi_{m}$ is given by  \n\n$$\n\\sin\\phi_{m}=\\frac{\\beta-1}{\\beta+1}\n$$  \n\nNotice that $\\beta=10$ corresponds to $\\phi_{m}=54.9^{\\circ}$ . Since we need a $50^{\\circ}$ phase margin, we may choose $\\beta=10.$ . (Note that we will be using several degrees less than the maximum angle, $54.9^{\\circ}$ .) Thus,  \n\n$$\n\\beta=10\n$$  \n\nThen the corner frequency $\\omega\\,=\\,1/(\\beta T_{2})$ (which corresponds to the pole of the phase-lag portion of the compensator) becomes  \n\n$$\n\\omega=0.02\n$$  \n\nThe transfer function of the phase-lag portion of the lag–lead compensator becomes  \n\n$$\n\\frac{s\\,+\\,0.2}{s\\,+\\,0.02}=10\\bigg(\\frac{5s\\,+\\,1}{50s\\,+\\,1}\\bigg)\n$$  \n\nThe phase-lead portion can be determined as follows: Since the new gain crossover frequency is $\\omega=2$ rad 'sec om Fi –151, $\\left|G(j2)\\right|$ is found to be 6 d nce, if the lag–lead compensator contributes $^{-6}$ dB at $\\omega=2$ rad 'sec,then the new gain crossover frequency is as desired.From this requirement, it is possible to draw a straight line of slope 20 dB/decade passing through the point (2 rad 'sec, $-6$ dB ). (Such a line has been manually drawn on Figure 7–151.) The intersections of this line and the 0-dB line and $-20$ -dB line determine the corner frequencies. From this consideration, the corner frequencies for the lead portion can be determined as $\\omega=0.4$ rad 'sec and $\\omega=4$ rad 'sec. Thus, the transfer function of the lead portion of the lag–lead compensator becomes  \n\n$$\n\\frac{s\\,+\\,0.4}{s\\,+\\,4}=\\frac{1}{10}\\left(\\frac{2.5s\\,+\\,1}{0.25s\\,+\\,1}\\right)\n$$  \n\nCombining the transfer functions of the lag and lead portions of the compensator, we can obtain the transfer function $G_{c}(s)$ of the lag–lead compensator. Since we chose $K_{c}=1$ , we have  \n\n$$\nG_{c}(s)\\,=\\frac{s\\,+\\,0.4}{s\\,+\\,4}\\frac{s\\,+\\,0.2}{s\\,+\\,0.02}=\\frac{(2.5s\\,+\\,1)(5s\\,+\\,1)}{(0.25s\\,+\\,1)(50s\\,+\\,1)}\n$$  \n\nThe Bode diagram of the lag–lead compensator $G_{c}(s)$ can be obtained by entering MATLAB Program 7–31 into the computer.The resulting plot is shown in Figure 7–152.  \n\n![](images/ffd658e80f932b40a1d3a57c70613abe47be34a55d161f70ede73ea1cbf184dc.jpg)  \nFigure 7–152 Bode diagram of the designed lag–lead compensator.  \n\n![](images/bcd609b154063b206af297bfea77bca2142ddeb1f0d0a3f302c703f057cafa6c.jpg)  \n\nThe open-loop transfer function of the compensated system is  \n\n$$\n\\begin{array}{c}{{G_{c}(s)G(s)=\\displaystyle\\frac{(s\\,+\\,0.4)(s\\,+\\,0.2)}{(s\\,+\\,4)(s\\,+\\,0.02)}\\displaystyle\\frac{40}{s(s\\,+\\,1)(s\\,+\\,4)}}}\\\\ {{=\\displaystyle\\frac{40s^{2}\\,+\\,24s\\,+\\,3.2}{s^{5}\\,+\\,9.02s^{4}\\,+\\,24.18s^{3}\\,+\\,16.48s^{2}\\,+\\,0.32s}}}\\end{array}\n$$  \n\nUsing MATLAB Program 7–32 the magnitude and phase-angle curves of the designed open-loop transfer function $G_{c}(s)G(s)$ can be obtained as shown in Figure 7–153. Note that the denominator polynomial den1 was obtained using the conv command, as follows:  \n\n![](images/a66589fe7c4b8a05b8c69569b9d0b03ece8943d0bd96cb62f1e053ccd71d1036.jpg)  \n\n![](images/32dc6abd2f645266a4bc39a06965a98bf1221583994bf7fe302c3ecfe429f57a.jpg)  \nFigure 7–153 Bode diagram of the open-loop transfer function $G_{c}(s)G(s)$ of the compensated system.  \n\n![](images/757959038924e928d4a33c73722801cf1c23a4c4d43b4c3ac8cccf8d34ce57e8.jpg)  \n\nSince the phase margin of the compensated system is $50^{\\circ}$ , the gain margin is $12\\;\\mathrm{dB}$ , and the static velocity error constant is $10\\;\\mathrm{sec}^{-1}$ , all the requirements are met.  \n\nWe shall next investigate the transient-response characteristics of the designed system.  \n\nUnit-Step Response: Noting that  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{40(s\\,+\\,0.4)(s\\,+\\,0.2)}{(s\\,+\\,4)(s\\,+\\,0.02)s(s\\,+\\,1)(s\\,+\\,4)}\n$$  \n\nwe have  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{C(s)}{R(s)}=\\frac{G_{c}(s)G(s)}{1+G_{c}(s)G(s)}}}}\\\\ {{\\displaystyle{\\qquad=\\frac{40(s+0.4)(s+0.2)}{(s+4)(s+0.02)s(s+1)(s+4)+40(s+0.4)(s+0.2)}}}}\\end{array}\n$$  \n\nTo determine the denominator polynomial with MATLAB, we may proceed as follows: Define  \n\n$$\n\\begin{array}{l}{{a(s)\\,=\\,(s\\,+\\,4)(s\\,+\\,0.02)\\,=\\,s^{2}\\,+\\,4.02s\\,+\\,0.08}}\\\\ {{\\,}}\\\\ {{b(s)\\,=\\,s(s\\,+\\,1)(s\\,+\\,4)\\,=\\,s^{3}\\,+\\,5s^{2}\\,+\\,4s}}\\\\ {{\\,}}\\\\ {{c(s)\\,=\\,40(s\\,+\\,0.4)(s\\,+\\,0.2)\\,=\\,40s^{2}\\,+\\,24s\\,+\\,3.2}}\\end{array}\n$$  \n\nThen we have  \n\n$$\n\\begin{array}{l}{{\\sf a}=[1\\ \\ 4.02\\ \\ 0.08]}\\\\ {{\\sf b}=[1\\ \\ 5\\ \\ 4\\ \\ 0]}\\\\ {{\\sf c}=[40\\ \\ 24\\ \\ 3.2]}\\end{array}\n$$  \n\nUsing the following MATLAB program, we obtain the denominator polynomial.  \n\n![](images/564e30cd5226ec79fe8ec1840401f1fb2b6f28c201c3876140ea01427a22dbce.jpg)  \n\nMATLAB Program 7–33 is used to obtain the unit-step response of the compensated system. The resulting unit-step response curve is shown in Figure 7–154. (Note that the gain-adjusted but uncompensated system is unstable.)  \n\n![](images/9407b5bf380dfe73982d9c9acd5a5b33279eba26bc69a13bb325c327b198726f.jpg)  \n\n![](images/73f4c290ef7b2a859ac2f5f98b2bff61cb2f023af0767d5edc660609437bca87.jpg)  \nFigure 7–154 Unit-step response curve of the compensated system.  \n\nUnit-Ramp Response: The unit-ramp response of the compensated system may be obtained by entering MATLAB Program 7–34 into the computer. Here we converted the unit-ramp response of $G_{c}G/(1\\,+\\,G_{c}G)$ into the unit-step response of $G_{c}G/[s(1\\,+\\,G_{c}G)]$ . The unit-ramp response curve obtained using this program is shown in Figure 7–155.  \n\n![](images/7c04181bfd695353b328cbb02df2238b191e077c43179f6c1d84233a200dfd23.jpg)  \n\n![](images/8ffe942b73827337ec6e6f77b95cb1f2790c92a60b777d792e33cb13f1831a82.jpg)  \nFigure 7–155 Unit-ramp response of the compensated system.  \n\n# PROBLEMS  \n\nB–7–1. Consider the unity-feedback system with the openloop transfer function:  \n\n$$\nG(s)=\\frac{10}{s+1}\n$$  \n\nObtain the steady-state output of the system when it is subjected to each of the following inputs:  \n\n$$\n\\begin{array}{l}{r(t)\\,=\\,\\sin\\bigl(t\\,+\\,30^{\\circ}\\bigr)}\\\\ {r(t)\\,=\\,2\\cos\\bigl(2t\\,-\\,45^{\\circ}\\bigr)}\\\\ {r(t)\\,=\\,\\sin\\bigl(t\\,+\\,30^{\\circ}\\bigr)\\,-\\,2\\cos\\bigl(2t\\,-\\,45^{\\circ}\\bigr)}\\end{array}\n$$  \n\nB–7–2. Consider the system whose closed-loop transfer function is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{K(T_{2}s\\,+\\,1)}{T_{1}s\\,+\\,1}}\n$$  \n\nObtain the steady-state output of the system when it is subjected to the input $r(t)\\,=\\,R\\sin\\omega t$ .  \n\nB–7–3. Using MATLAB, plot Bode diagrams of $G_{1}(s)$ and $G_{2}(s)$ given below.  \n\n$$\n\\begin{array}{l c r}{{G_{1}(s)={\\displaystyle\\frac{1\\,+\\,s}{1\\,+\\,2s}}}}\\\\ {{\\,}}\\\\ {{G_{2}(s)={\\displaystyle\\frac{1\\,-\\,s}{1\\,+\\,2s}}}}\\end{array}\n$$  \n\n$G_{1}(s)$ is a minimum-phase system and $G_{2}(s)$ is a nonminimum-phase system.  \n\nB–7–4. Plot the Bode diagram of  \n\n$$\nG(s)=\\frac{10\\bigl(s^{2}+0.4s\\,+\\,1\\bigr)}{s\\bigl(s^{2}+0.8s\\,+\\,9\\bigr)}\n$$  \n\nB–7–5. Given  \n\n$$\nG(s)\\,=\\,\\frac{\\omega_{n}^{2}}{s^{2}\\,+\\,2\\zeta\\omega_{n}s\\,+\\,\\omega_{n}^{2}}\n$$  \n\nshow that  \n\n$$\n\\left|G\\!\\left(j\\omega_{n}\\right)\\right|=\\frac{1}{2\\zeta}\n$$  \n\nB–7–6. Consider a unity-feedback control system with the following open-loop transfer function:  \n\n$$\nG(s)=\\frac{s\\,+\\,0.5}{s^{3}\\,+\\,s^{2}\\,+\\,1}\n$$  \n\nThis is a nonminimum-phase system. Two of the three open-loop poles are located in the right-half $s$ plane as follows:  \n\nOpen-loop poles at $s=-1.4656$  \n\n$$\n\\begin{array}{r}{s\\,=\\,0.2328\\,+\\,j0.7926}\\\\ {\\phantom{.0}}\\\\ {s\\,=\\,0.2328\\,-\\,j0.7926}\\end{array}\n$$  \n\nPlot the Bode diagram of $G(s)$ with MATLAB.Explain why the phase-angle curve starts from $0^{\\circ}$ and approaches $+180^{\\circ}$ .  \n\nB–7–7. Sketch the polar plots of the open-loop transfer function  \n\n$$\nG(s)H(s)=\\frac{K(T_{a}s\\,+\\,1)(T_{b}s\\,+\\,1)}{s^{2}(T s\\,+\\,1)}\n$$  \n\nfor the following two cases:  \n\n$$\n\\begin{array}{c c c}{{T_{a}>T>0,}}&{{}}&{{T_{b}>T>0}}\\\\ {{\\/T>T_{a}>0,}}&{{}}&{{T>T_{b}>0}}\\end{array}\n$$  \n\nB–7–8. Draw a Nyquist locus for the unity-feedback control system with the open-loop transfer function  \n\n$$\nG(s)={\\frac{K(1\\,-\\,s)}{s\\,+\\,1}}\n$$  \n\nUsing the Nyquist stability criterion, determine the stability of the closed-loop system.  \n\nB–7–9. A system with the open-loop transfer function  \n\n$$\nG(s)H(s)\\,=\\,\\frac{K}{s^{2}(T_{1}s\\,+\\,1)}\n$$  \n\nis inherently unstable.This system can be stabilized by adding derivative control. Sketch the polar plots for the open-loop transfer function with and without derivative control.  \n\nB–7–10. Consider the closed-loop system with the following open-loop transfer function:  \n\n$$\nG(s)H(s)={\\frac{10K(s+0.5)}{s^{2}(s+2)(s+10)}}\n$$  \n\nPlot both the direct and inverse polar plots of $G(s)H(s)$ with $K=1$ and $K=10$ . Apply the Nyquist stability criterion to the plots, and determine the stability of the system with these values of $K$ .  \n\nB–7–11. Consider the closed-loop system whose open-loop transfer function is  \n\n$$\nG(s)H(s)={\\frac{K e^{-2s}}{s}}\n$$  \n\nFind the maximum value of $K$ for which the system is stable.  \n\nB–7–12. Draw a Nyquist plot of the following $G(s)$ :  \n\n$$\nG(s)\\,=\\,\\frac{1}{s(s^{2}\\,+\\,0.8s\\,+\\,1)}\n$$  \n\nB–7–13. Consider a unity-feedback control system with the following open-loop transfer function:  \n\n$$\nG(s)\\,=\\,\\frac{1}{s^{3}\\,+\\,0.2s^{2}\\,+\\,s\\,+\\,1}\n$$  \n\nDraw a Nyquist plot of $G(s)$ and examine the stability of the system.  \n\nB–7–14. Consider a unity-feedback control system with the following open-loop transfer function:  \n\n$$\nG(s)\\,=\\,\\frac{s^{2}\\,+\\,2s\\,+\\,1}{s^{3}\\,+\\,0.2s^{2}\\,+\\,s\\,+\\,1}\n$$  \n\nDraw a Nyquist plot of $G(s)$ and examine the stability of the closed-loop system.  \n\nB–7–15. Consider the unity-feedback system with the following $G(s)$ :  \n\n$$\nG(s)={\\frac{1}{s(s-1)}}\n$$  \n\nSuppose that we choose the Nyquist path as shown in Figure 7–156. Draw the corresponding $G(j\\omega)$ locus in the $G(s)$ plane. Using the Nyquist stability criterion, determine the stability of the system.  \n\n![](images/3f7dd2912a892c264f09054638453e987a6703d4b67ac83e222a99cbb6581c4e.jpg)  \nFigure 7–156 Nyquist path.  \n\nB–7–16. Consider the closed-loop system shown in Figure   \n$7–157.G(s)$ has no poles in the right-half $s$ plane. If the Nyquist plot of $G(s)$ is as shown in Figure   \n7–158(a), is this system stable? If the Nyquist plot is as shown in Figure 7–158(b), is this   \nsystem stable?  \n\n![](images/cf066850b6fb4c879639f8b84384a187530aea98154a04068eabb74f2fe91dfe.jpg)  \n\nFigure 7–157 Closed-loop system.  \n\n![](images/0702bd78c88fb911340fd41f59bfd3298373a57a772504d4dd13730616dd711c.jpg)  \nFigure 7–158 Nyquist plots.  \n\nB–7–17. A Nyquist plot of a unity-feedback system with the feedforward transfer function $G(s)$ is shown in Figure 7–159. If $G(s)$ has one pole in the right-half $s$ plane, is the system stable?  \n\nIf $G(s)$ has no pole in the right-half $s$ plane, but has one zero in the right-half $s$ plane, is the system stable?  \n\n![](images/47182fc51ccf392ae096a43168c5318c8bb25a233d327a3b64392914a9994afe.jpg)  \n\nB–7–18. Consider the unity-feedback control system with the following open-loop transfer function $G(s)$ :  \n\n$$\nG(s)=\\frac{K(s\\,+\\,2)}{s(s\\,+\\,1)(s\\,+\\,10)}\n$$  \n\nPlot Nyquist diagrams of $G(s)$ for $K=1,10$ , and 100.  \n\nB–7–19. Consider a negative-feedback system with the following open-loop transfer function:  \n\n$$\nG(s)\\,=\\,\\frac{2}{s(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nPlot the Nyquist diagram of $G(s)$ .If the system were a positive-feedback one with the same open-loop transfer function $G(s)$ ,what would the Nyquist diagram look like?  \n\nB–7–20. Consider the control system shown in Figure 7–160. Plot Nyquist diagrams of $G(s)$ ,where  \n\n$$\n\\begin{array}{c}{{G(s)=\\displaystyle\\frac{10}{s\\big[(s+1)(s+5)\\,+\\,10k\\big]}}}\\\\ {{=\\displaystyle\\frac{10}{s^{3}\\,+\\,6s^{2}\\,+\\,(5\\,+\\,10k)s}}}\\end{array}\n$$  \n\nfor $k\\,=\\,0.3,\\,0.5$ , and 0.7.  \n\nB–7–22. Referring to Problem B–7–21 , it is desired to plot only $Y_{1}(j\\omega)/U_{1}(j\\omega)$ for $\\omega>0$ . Write a MATLAB program to produce such a plot.  \n\nIf it is desired to plot $Y_{1}(j\\omega)/U_{1}(j\\omega)$ for $-\\infty<\\omega<\\infty$ ,what changes must be made in the MATLAB program?  \n\nB–7–23. Consider the unity-feedback control system whose open-loop transfer function is  \n\n$$\nG(s)={\\frac{a s\\,+\\,1}{s^{2}}}\n$$  \n\nDetermine the value of $a$ so that the phase margin is $45^{\\circ}$ .  \n\nB–7–24. Consider the system shown in Figure 7–161. Draw a Bode diagram of the open-loop transfer function $G(s)$ .Determine the phase margin and gain margin.  \n\n![](images/b7a68ae8c514b3f7811b5ef6bdc37a5cddc4d62534c459e633550e8a56899675.jpg)  \n\nFigure 7–161 Control system.  \n\n![](images/0982106526d33f33441c5baea6fd3faa334685a395251aee3315dd2dcecaa53f.jpg)  \nFigure 7–160 Control system.  \n\nB–7–21. Consider the system defined by  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{-1}&{-1}\\\\ {6.5}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThere are four individual Nyquist plots involved in this system. Draw two Nyquist plots for the input $u_{1}$ in one diagram and two Nyquist plots for the input $u_{2}$ in another diagram. Write a MATLAB program to obtain these two diagrams.  \n\nB–7–25. Consider the system shown in Figure 7–162. Draw a Bode diagram of the open-loop transfer function $G(s)$ . Determine the phase margin and gain margin with MATLAB.  \n\n![](images/850c4d7b8b24d91fa79ffaf42ca8dc3c3ca3c7557ab5ee60a784fea03cb67692.jpg)  \n\nFigure 7–162 Control system.  \n\nB–7–26. Consider a unity-feedback control system with the open-loop transfer function  \n\nB–7–28. Consider a unity-feedback control system whose open-loop transfer function is  \n\n$$\nG(s)={\\frac{K}{s(s^{2}+s+4)}}\n$$  \n\n$$\nG(s)\\,=\\,\\frac{K}{s(s^{2}\\,+\\,s\\,+\\,0.5)}\n$$  \n\nDetermine the value of the gain $K$ such that the phase margin is $50^{\\circ}$ .What is the gain margin with this gain $K$ ?  \n\nB–7–27. Consider the system shown in Figure 7–163. Draw a Bode diagram of the open-loop transfer function, and determine the value of the gain $K$ such that the phase margin is $50^{\\circ}$ . What is the gain margin of this system with this gain $K$ ?  \n\n![](images/bd4c51a0d4308d65b4e0aab317efc1db80545b7ebb9360c67355e2b9084c2d2d.jpg)  \n\nFigure 7–163 Control system.  \n\nDetermine the value of the gain $K$ such that the resonant peak magnitude in the frequency response is 2 dB, or $M_{r}=2$ dB.  \n\nB–7–29. A Bode diagram of the open-loop transfer function $G(s)$ of a unity-feedback control system is shown in Figure 7–164. It is known that the open-loop transfer function is minimum phase. From the diagram, it can be seen that there is a pair of complex-conjugate poles at $\\omega=2$ rad 'sec. Determine the damping ratio of the quadratic term involving these complex-conjugate poles. Also, determine the transfer function $G(s)$ .  \n\n![](images/ad0d1b37cf7e296e1e19ebe9a5fb4b5ddaca37cc229e8038b59be44f7b12afb9.jpg)  \nFigure 7–164 Bode diagram of the open-loop transfer function of a unityfeedback control system.  \n\nB–7–30. Draw Bode diagrams of the PI controller given by  \n\n$$\nG_{c}(s)\\,=\\,5\\bigg(1\\,+\\frac{1}{2s}\\bigg)\n$$  \n\nand the PD controller given by  \n\n$$\nG_{c}(s)\\,=\\,5(1\\,+\\,0.5s)\n$$  \n\nB–7–31. Figure 7–165 shows a block diagram of a spacevehicle attitude-control system. Determine the proportional gain constant $K_{p}$ and derivative time $T_{d}$ such that the bandwidth of the closed-loop system is 0.4 to 0.5 rad 'sec. (Note that the closed-loop bandwidth is close to the gain crossover frequency.) The system must have an adequate phase margin. Plot both the open-loop and closed-loop frequency response curves on Bode diagrams.  \n\n![](images/0cd7bd73aad0a62cccf35656f7cd2209d4b8b29df5afd0079e36ecabd0ea88ab.jpg)  \n\nFigure 7–165 Block diagram of space-vehicle attitude-control system.  \n\nB–7–32. Referring to the closed-loop system shown in Figure 7–166, design a lead compensator $G_{c}(s)$ such that the phase margin is $45^{\\circ}$ ,gain margin is not less than 8 dB,and the static velocity error constant $K_{v}$ is $4.0\\ \\mathrm{sec}^{-1}$ . Plot unit-step and unit-ramp response curves of the compensated system with MATLAB.  \n\n![](images/3c14f0b18a9c413767b260b37fb5a2ac77addd4f21f37727a88175a44a9f4215.jpg)  \n\nFigure 7–166 Closed-loop system.  \n\nB–7–33. Consider the system shown in Figure 7–167. It is desired to design a compensator such that the static velocity error constant is $4~\\mathrm{sec}^{-1}$ , phase margin is $50^{\\circ}$ , and gain margin is $8\\ \\mathrm{dB}$ or more. Plot the unit-step and unitramp response curves of the compensated system with MATLAB.  \n\n![](images/649e9e33bcb9d1d342cb27485973f2904cec2f641094c279c592a1caf16d52a3.jpg)  \n\nFigure 7–167 Control system.  \n\nB–7–34. Consider the system shown in Figure 7–168. Design a lag–lead compensator such that the static velocity error constant $K_{v}$ is $20\\;\\mathrm{sec}^{-1}$ , phase margin is $60^{\\circ}$ , and gain margin is not less than 8 dB. Plot the unit-step and unitramp response curves of the compensated system with MATLAB.  \n\n![](images/5e73dfab549c7dbeefb97f36ce3a14dad974f201de3630aa45b8cba9bfc8ed93.jpg)  \n\nFigure 7–168 Control system.  \n\n# PID Controllers and Modified PID Controllers  \n\n# 8–1 INTRODUCTION  \n\nIn previous chapters, we occasionally discussed the basic PID controllers. For example, we presented electronic, hydraulic, and pneumatic PID controllers. We also designed control systems where PID controllers were involved.  \n\nIt is interesting to note that more than half of the industrial controllers in use today are PID controllers or modified PID controllers.  \n\nBecause most PID controllers are adjusted on-site, many different types of tuning rules have been proposed in the literature.Using these tuning rules,delicate and fine tuning of PID controllers can be made on-site.Also, automatic tuning methods have been developed and some of the PID controllers may possess on-line automatic tuning capabilities. Modified forms of PID control, such as I-PD control and multi-degrees-offreedom PID control, are currently in use in industry. Many practical methods for bumpless switching (from manual operation to automatic operation) and gain scheduling are commercially available.  \n\nThe usefulness of PID controls lies in their general applicability to most control systems. In particular, when the mathematical model of the plant is not known and therefore analytical design methods cannot be used, PID controls prove to be most useful. In the field of process control systems,it is well known that the basic and modified PID control schemes have proved their usefulness in providing satisfactory control, although in many given situations they may not provide optimal control.  \n\nIn this chapter we first present the design of a PID controlled system using Ziegler and Nichols tuning rules.We next discuss a design of PID controller with the conventional frequency-response approach, followed by the computational optimization approach to design PID controllers. Then we introduce modified PID controls such as PI-D control and I-PD control.Then we introduce multi-degrees-of-freedom control systems,which can satisfy conflicting requirements that single-degree-of-freedom control systems cannot. (For the definition of multi-degrees-of-freedom control systems, see Section 8–6.)  \n\nIn practical cases, there may be one requirement on the response to disturbance input and another requirement on the response to reference input. Often these two requirements conflict with each other and cannot be satisfied in the single-degree-offreedom case. By increasing the degrees of freedom, we are able to satisfy both. In this chapter we present two-degrees-of-freedom control systems in detail.  \n\nThe computational optimization approach presented in this chapter to design control systems (such as to search optimal sets of parameter values to satisfy given transient response specifications) can be used to design both single-degree-of-freedom control systems and multi-degrees-of-freedom control systems, provided a fairly precice mathematical model of the plant is known.  \n\nOutline of the Chapter. Section 8–1 has presented introductory material for the chapter. Section 8–2 deals with a design of a PID controller with Ziegler–Nichols Rules. Section 8–3 treats a design of a PID controller with the frequency-response approach. Section 8–4 presents a computational optimization approach to obtain optimal parameter values of PID controllers. Section 8–5 discusses multi-degrees-of-freedom control systems including modified PID control systems.  \n\n# 8–2 ZIEGLER–NICHOLS RULES FOR TUNINGPID CONTROLLERS  \n\nPID Control of Plants. Figure 8–1 shows a PID control of a plant. If a mathematical model of the plant can be derived, then it is possible to apply various design techniques for determining parameters of the controller that will meet the transient and steady-state specifications of the closed-loop system. However, if the plant is so complicated that its mathematical model cannot be easily obtained, then an analytical or computational approach to the design of a PID controller is not possible.Then we must resort to experimental approaches to the tuning of PID controllers.  \n\nThe process of selecting the controller parameters to meet given performance specifications is known as controller tuning. Ziegler and Nichols suggested rules for tuning PID controllers (meaning to set values $K_{p}$ ,$T_{i}$ ,and $T_{d.}$ ) based on experimental step responses or based on the value of $K_{p}$ that results in marginal stability when only proportional control action is used. Ziegler–Nichols rules, which are briefly presented in the following, are useful when mathematical models of plants are not known. (These rules can, of course, be applied to the design of systems with known mathematical models.) Such rules suggest a set of values of $K_{p},T_{i}$ ,and $T_{d}$ that will give a stable operation of the system. However, the resulting system may exhibit a large maximum overshoot in the step response, which is unacceptable. In such a case we need series of fine tunings until an acceptable result is obtained. In fact, the Ziegler–Nichols tuning rules give an educated guess for the parameter values and provide a starting point for fine tuning, rather than giving the final settings for $K_{p},T_{i}$ ,and $T_{d}$ in a single shot.  \n\n![](images/1ee1ec13f4c95884cf6cc564388d2467fff38445ca846d099936499b3c9aa6b9.jpg)  \nFigure 8–1 PID control of a plant.  \n\nZiegler–Nichols Rules for Tuning PID Controllers. Ziegler and Nichols proposed rules for determining values of the proportional gain $K_{p}$ ,integral time $T_{i}$ ,and derivative time $T_{d}$ based on the transient response characteristics of a given plant. Such determination of the parameters of PID controllers or tuning of PID controllers can be made by engineers on-site by experiments on the plant. (Numerous tuning rules for PID controllers have been proposed since the Ziegler–Nichols proposal.They are available in the literature and from the manufacturers of such controllers.)  \n\n![](images/5ec50a5a5acdc8d47543e18d5aceadae0dbf4de75ae62617e3f68afd1c068d12.jpg)  \nFigure 8–2 Unit-step response of a plant.  \n\nThere are two methods called Ziegler–Nichols tuning rules: the first method and the second method.We shall give a brief presentation of these two methods.  \n\n![](images/335e66a8711be7647d0e74caf0935022be94efdc19b51095121a21e62c2085dc.jpg)  \nFigure 8–3 S-shaped response curve.  \n\nFirst Method. In the first method, we obtain experimentally the response of the plant to a unit-step input, as shown in Figure 8–2. If the plant involves neither integrator(s) nor dominant complex-conjugate poles, then such a unit-step response curve may look S-shaped, as shown in Figure 8–3. This method applies if the response to a step input exhibits an S-shaped curve. Such step-response curves may be generated experimentally or from a dynamic simulation of the plant.  \n\nThe S-shaped curve may be characterized by two constants, delay time $L$ and time constant $T$ .The delay time and time constant are determined by drawing a tangent line at the inflection point of the S-shaped curve and determining the intersections of the tangent line with the time axis and line $c(t)\\,=\\,K$ ,as shown in Figure 8–3. The transfer function $C(s)/U(s)$ may then be approximated by a first-order system with a transport lag as follows:  \n\nTable 8–1 Ziegler–Nichols Tuning Rule Based on Step Response of Plant (First Method)   \n\n![](images/e8dfbd25b5945ce2f4e42c20c1b1f180bc00fca0ec907c5bcef36cf6be041e64.jpg)  \n\n$$\n\\frac{C(s)}{U(s)}={\\frac{K e^{-L s}}{T s\\,+\\,1}}\n$$  \n\nZiegler and Nichols suggested to set the values of $K_{p},T_{i}$ ,and $T_{d}$ according to the formula shown in Table 8–1.  \n\nNotice that the PID controller tuned by the first method of Ziegler–Nichols rules gives  \n\n$$\n\\begin{array}{l}{{G_{c}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)}}\\\\ {{\\qquad=\\,1.2\\,\\frac{T}{L}\\,\\bigg(1\\,+\\frac{1}{2L s}\\,+\\,0.5L s\\bigg)}}\\\\ {{\\qquad=\\,0.6T\\frac{\\displaystyle\\left(s+\\frac{1}{L}\\right)^{2}}{s}}}\\end{array}\n$$  \n\nThus, the PID controller has a pole at the origin and double zeros at $s=-1/L$ .  \n\nSecond Method. In the second method, we first set $T_{i}=\\infty$ and $T_{d}=0$ Using the proportional control action only (see Figure 8–4), increase $K_{p}$ from 0 to a critical value $K_{\\mathrm{cr}}$ at which the output first exhibits sustained oscillations. (If the output does not exhibit sustained oscillations for whatever value $K_{p}$ may take, then this method does not apply.) Thus, the critical gain $K_{\\mathrm{cr}}$ and the corresponding period $P_{\\mathrm{cr}}$ are experimentally determined (see Figure 8–5). Ziegler and Nichols suggested that we set the values of the parameters $K_{p},T_{i}$ ,and $T_{d}$ according to the formula shown in Table 8–2.  \n\n![](images/a25e0ec312d464b63d12fd33d93a06d51671ca59834314dafbc8df18afb03ba3.jpg)  \nFigure 8–4 Closed-loop system with a proportional controller.   \nChapter 8 /PID Controllers and Modified PID Controllers  \n\n![](images/1ba616c2fc6ba66f6844c3c5cc554448d36de1318b8a8e8440dcbcecf80a3658.jpg)  \nFigure 8–5 Sustained oscillation with period $P_{\\mathrm{cr}}$ .($\\boldsymbol{P}_{\\mathrm{cr}}$ is measured in sec.)  \n\nTable 8–2 Ziegler–Nichols Tuning Rule Based on Critical Gain $K_{\\mathrm{cr}}$ and Critical Period $P_{\\mathrm{cr}}$ (Second Method)   \n\n![](images/55b361569f75725072abad1a6ebadbbe196d70a6cb0482745c7881e9dead9428.jpg)  \n\nNotice that the PID controller tuned by the second method of Ziegler–Nichols rules gives  \n\n$$\n\\begin{array}{r l}{G_{c}(s)=\\displaystyle K_{p}\\bigg(1+\\frac{1}{T_{i}s}+T_{d}s\\bigg)}&{{}}\\\\ {\\,=\\,0.6K_{\\mathrm{cr}}\\bigg(1+\\frac{1}{0.5P_{\\mathrm{cr}}s}+0.125P_{\\mathrm{cr}}s\\bigg)}&{{}}\\\\ {\\,=\\,0.075K_{\\mathrm{cr}}P_{\\mathrm{cr}}\\frac{\\bigg(s+\\frac{4}{P_{\\mathrm{cr}}}\\bigg)^{2}}{s}}&{{}}\\end{array}\n$$  \n\nThus, the PID controller has a pole at the origin and double zeros at $s=-4/P_{\\mathrm{cr}}$  \n\nNote that if the system has a known mathematical model (such as the transfer function), then we can use the root-locus method to find the critical gain $K_{\\mathrm{cr}}$ and the frequency of the sustained oscillations $\\omega_{\\mathrm{cr}}$ , where $2\\pi/\\omega_{\\mathrm{cr}}=P_{\\mathrm{cr}}$ .These values can be found from the crossing points of the root-locus branches with the $j\\omega$ axis. (Obviously, if the root-locus branches do not cross the $j\\omega$ axis, this method does not apply.)  \n\nComments. Ziegler–Nichols tuning rules (and other tuning rules presented in the literature) have been widely used to tune PID controllers in process control systems where the plant dynamics are not precisely known. Over many years, such tuning rules proved to be very useful.Ziegler–Nichols tuning rules can,of course,be applied to plants whose dynamics are known. (If the plant dynamics are known, many analytical and graphical approaches to the design of PID controllers are available, in addition to Ziegler–Nichols tuning rules.)  \n\n# EXAMPLE 8–1  \n\nConsider the control system shown in Figure 8–6 in which a PID controller is used to control the system.The PID controller has the transfer function  \n\n$$\nG_{c}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}+\\,T_{d}s\\bigg)\n$$  \n\nAlthough many analytical methods are available for the design of a PID controller for the present system, let us apply a Ziegler–Nichols tuning rule for the determination of the values of parameters $K_{p}$ ,$T_{i}$ ,and $T_{d}$ .Then obtain a unit-step response curve and check to see if the designed system exhibits approximately $25\\%$ maximum overshoot. If the maximum overshoot is excessive ($40\\%$ or more), make a fine tuning and reduce the amount of the maximum overshoot to approximately $25\\%$ or less.  \n\nSince the plant has an integrator, we use the second method of Ziegler–Nichols tuning rules. By setting $T_{i}=\\infty$ and $T_{d}=0$ we obtain the closed-loop transfer function as follows:  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K_{p}}{s(s\\,+\\,1)(s\\,+\\,5)\\,+\\,K_{p}}\n$$  \n\nThe value of $K_{p}$ that makes the system marginally stable so that sustained oscillation occurs can be obtained by use of Routh’s stability criterion. Since the characteristic equation for the closed-loop system is  \n\n$$\ns^{3}\\,+\\,6s^{2}\\,+\\,5s\\,+\\,K_{p}=0\n$$  \n\nthe Routh array becomes as follows:  \n\n![](images/e534c25810f3f880cb6be88784d6510ba9cd10cc32131c1e5170cf31137adaa0.jpg)  \n\nFigure 8–6   \nPID-controlled system.  \n\nExamining the coefficients of the first column of the Routh table, we find that sustained oscillation will occur if $K_{p}\\,=\\,30.$ Thus, the critical gain $K_{\\mathrm{cr}}$ is  \n\n$$\nK_{\\mathrm{cr}}\\,=\\,30\n$$  \n\nWith gain $K_{p}$ set equal to $K_{\\mathrm{cr}}\\,(=\\,30)$ ,the characteristic equation becomes  \n\n$$\ns^{3}\\,+\\,6s^{2}\\,+\\,5s\\,+\\,30\\,=\\,0\n$$  \n\nTo find the frequency of the sustained oscillation, we substitute $s=j\\omega$ into this characteristic equation as follows:  \n\n$$\n(j\\omega)^{3}\\,+\\,6(j\\omega)^{2}\\,+\\,5(j\\omega)\\,+\\,30\\,=\\,0\n$$  \n\nor  \n\n$$\n6(5\\,-\\,\\omega^{2})\\,+\\,j\\omega\\bigl(5\\,-\\,\\omega^{2}\\bigr)\\,=\\,0\n$$  \n\nfrom which we find the frequency of the sustained oscillation to be $\\omega^{2}=5$ or $\\omega={\\sqrt{5}}$ Hence, the period of sustained oscillation is  \n\n$$\nP_{\\mathrm{cr}}=\\frac{2\\pi}{\\omega}=\\frac{2\\pi}{\\sqrt{5}}=2.8099\n$$  \n\nReferring to Table 8–2, we determine $K_{p},T_{i}$ ,and $T_{d}$ as follows:  \n\n$$\n\\begin{array}{r l}&{K_{p}=0.6K_{\\mathrm{cr}}=18}\\\\ &{~}\\\\ &{T_{i}=0.5P_{\\mathrm{cr}}=1.405}\\\\ &{~}\\\\ &{T_{d}=0.125P_{\\mathrm{cr}}=0.35124}\\end{array}\n$$  \n\nThe transfer function of the PID controller is thus  \n\n$$\n\\begin{array}{l}{\\displaystyle{G_{c}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)}}\\\\ {\\displaystyle{\\qquad=\\,18\\bigg(1\\,+\\frac{1}{1.405s}\\,+\\,0.35124s\\bigg)}}\\\\ {\\displaystyle{\\qquad=\\,\\frac{6.3223(s\\,+\\,1.4235)^{2}}{s}}}\\end{array}\n$$  \n\n![](images/8fabfeacf6e59d25aa5910289b4b79ba78d3c5c16126056481aa0e6af5e714bf.jpg)  \nFigure 8–7 Block diagram of the system with PID controller designed by use of the Ziegler–Nichols tuning rule (second method).  \n\nThe PID controller has a pole at the origin and double zero at $s\\,=\\,-1.4235$ .A block diagram of the control system with the designed PID controller is shown in Figure 8–7.  \n\nNext, let us examine the unit-step response of the system. The closed-loop transfer function $C(s)/R(s)$ is given by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{6.3223s^{2}+18s\\,+\\,12.811}{s^{4}\\,+\\,6s^{3}+\\,11.3223s^{2}\\,+\\,18s\\,+\\,12.811}\n$$  \n\nThe unit-step response of this system can be obtained easily with MATLAB. See MATLAB Program 8–1. The resulting unit-step response curve is shown in Figure 8–8. The maximum overshoot in the unit-step response is approximately $62\\%$ .The amount of maximum overshoot is excessive. It can be reduced by fine tuning the controller parameters. Such fine tuning can be made on the computer. We find that by keeping $K_{p}\\,=\\,18$ and by moving the double zero of the PID controller to $s\\,=\\,-0.65\\cdot$ —that is, using the PID controller  \n\n$$\nG_{c}(s)\\,=\\,18\\bigg(1\\,+\\,\\frac{1}{3.077s}\\,+\\,0.7692s\\bigg)\\,=\\,13.846\\,\\frac{(s\\,+\\,0.65)^{2}}{s}\n$$  \n\nthe maximum overshoot in the unit-step response can be reduced to approximately $18\\%$ (see Figure 8–9). If the proportional gain $K_{p}$ is increased to 39.42, without changing the location of the double zero $s=-0.65)$ ,that is, using the PID controller  \n\n$$\nG_{c}(s)=39.42\\bigg(1\\,+\\,{\\frac{1}{3.077s}}\\,+\\,0.7692s\\bigg)\\,=\\,30.322\\,{\\frac{(s\\,+\\,0.65)^{2}}{s}}\n$$  \n\n![](images/dceea6c1116ab1312645d41deb9026ad4bfdb3e3552711d0904c43bb0bc52a53.jpg)  \n\n![](images/1b8ddc9ce607dd955a02d18665bad58d6e4550e8f6d147df8fdef230ecacb7da.jpg)  \nFigure 8–8 Unit-step response curve of PIDcontrolled system designed by use of the Ziegler–Nichols tuning rule (second method).  \n\n![](images/27085ee693b242ae28257b2238c4f4fff8839a7ca2f317a8758e73e6500aecc7.jpg)  \nFigure 8–9 Unit-step response of the system shown in Figure 8–6 with PID controller having parameters $K_{p}\\,=\\,18$ ,$T_{i}=3.077$ ,and $T_{d}=0.7692$ .  \n\nthen the speed of response is increased, but the maximum overshoot is also increased to approximately $28\\%$ ,as shown in Figure 8–10.Since the maximum overshoot in this case is fairly close to $25\\%$ and the response is faster than the system with $G_{c}(s)$ given by Equation (8–1),we may consider $G_{c}(s)$ as given by Equation (8–2) as acceptable.Then the tuned values of $K_{p},T_{i}$ ,and $T_{d}$ become  \n\n$$\nK_{p}=39.42,\\qquad T_{i}=3.077,\\qquad T_{d}=0.7692\n$$  \n\nIt is interesting to observe that these values respectively are approximately twice the values suggested by the second method of the Ziegler–Nichols tuning rule.The important thing to note here is that the Ziegler–Nichols tuning rule has provided a starting point for fine tuning.  \n\nIt is instructive to note that, for the case where the double zero is located at $s=-1.4235$ ,increasing the value of $K_{p}$ increases the speed of response, but as far as the percentage maximum overshoot is concerned, varying gain $K_{p}$ has very little effect.The reason for this may be seen from  \n\n![](images/7f4f867e865721fc38b51836ceb074c29bba60d8d448c3a70518d357f25a997e.jpg)  \nFigure 8–10 Unit-step response of the system shown in Figure 8–6 with PID controller having parameters $K_{p}=39.42$ ,$T_{i}=3.077$ ,and $T_{d}=0.7692$ .  \n\n![](images/255c9a2f719e7ff92b63b8f26789db2d9d859f20e95a0070debe25a2076a88fe.jpg)  \n\nFigure 8–11 Root-locus diagram of system when PID controller has double zero at $s\\,=\\,-1.4235$ .  \n\nthe root-locus analysis. Figure 8–11 shows the root-locus diagram for the system designed by use of the second method of Ziegler–Nichols tuning rules. Since the dominant branches of root loci are along the $\\zeta=0.3$ lines for a considerable range of $K$ ,varying the value of $K$ (from 6 to 30) will not change the damping ratio of the dominant closed-loop poles very much. However, varying the location of the double zero has a significant effect on the maximum overshoot, because the damping ratio of the dominant closed-loop poles can be changed significantly.This can also be seen from the root-locus analysis.Figure 8–12 shows the root-locus diagram for the system where the PID controller has the double zero at $s=-0.65.$ .Notice the change of the root-locus configuration.This change in the configuration makes it possible to change the damping ratio of the dominant closed-loop poles.  \n\nIn Figure 8–12, notice that, in the case where the system has gain $K=30.322$ ,the closed-loop poles at $s=-2.35\\pm j4.82$ act as dominant poles.Two additional closed-loop poles are very near the double zero at $s=-0.65$ ,with the result that these closed-loop poles and the double zero almost cancel each other.The dominant pair of closed-loop poles indeed determines the nature of the response. On the other hand, when the system has $K=13.846$ ,the closed-loop poles at $s=-2.35\\pm j2.62$ are not quite dominant because the two other closed-loop poles near the double zero at $s=-0.65$ have considerable effect on the response.The maximum overshoot in the step response in this case $(18\\%)$ is much larger than the case where the system is of second order and having only dominant closed-loop poles. (In the latter case the maximum overshoot in the step response would be approximately $6\\%$ .)  \n\nIt is possible to make a third, a fourth, and still further trials to obtain a better response. But this will take a lot of computations and time. If more trials are desired, it is desirable to use the computational approach presented in Section 10–3. Problem A–8–12 solves this problem with the computational approach with MATLAB. It finds sets of parameter values that will yield the maximum overshoot of $10\\%$ or less and the settling time of 3 sec or less.A solution to the present problem obtained in Problem A–8–12 is that for the PID controller defined by  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\n![](images/b20aeb752abacb04ca8a09a9f2970d4ba9248254c62aaf115feccae90e07c0c8.jpg)  \nFigure 8–12 Root-locus diagram of system when PID controller has double zero at $s\\,=\\,-0.65$ .$K\\,=\\,13.846$ corresponds to $G_{c}(s)$ given by Equation (8–1) and $K\\,=\\,30.322$ corresponds to $G_{c}(s)$ given by Equation (8–2).  \n\nthe values of $K$ and $a$ are  \n\n$$\nK=29,\\ \\ \\ \\ \\ a=0.25\n$$  \n\nwith the maximum overshoot equal to $9.52\\%$ and settling time equal to 1.78 sec.Another possible solution obtained there is that  \n\n$$\nK\\,=\\,27,\\;\\;\\;\\;\\;\\;a\\,=\\,0.2\n$$  \n\nwith the $5.5\\%$ maximum overshoot and 2.89 sec of settling time. See Problem A–8–12 for details.  \n\n# 8–3 DESIGN OF PID CONTROLLERS WITH FREQUENCY-RESPONSE APPROACH  \n\nIn this section we present a design of a PID controller based on the frequency-response approach.  \n\nConsider the system shown in Figure 8–13.Using a frequency-response approach,design a PID controller such that the static velocity error constant is $\\bar{4}\\,\\mathrm{sec}^{-1}$ , phase margin is $50^{\\circ}$ or more, and gain margin is $10\\;\\mathrm{dB}$ or more. Obtain the unit-step and unit-ramp response curves of the PID controlled system with MATLAB.  \n\nLet us choose the PID controller to be  \n\n$$\nG_{c}(s)\\,=\\frac{K(a s\\,+\\,1)(b s\\,+\\,1)}{s}\n$$  \n\nFigure 8–13 Control system.  \n\n![](images/f2e816a83d1290774a44f618886870269824b4fc809b9dca66e8e422129a246c.jpg)  \n\nSince the static velocity error constant $K_{v}$ is specified as $4~\\mathrm{sec}^{-1}$ , we have  \n\n$$\n\\begin{array}{l}{{K_{v}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s)\\,\\frac{1}{s^{2}\\,+\\,1}=\\displaystyle\\operatorname*{lim}_{s\\rightarrow0}s\\,\\frac{K(a s\\,+\\,1)(b s\\,+\\,1)}{s}\\frac{1}{s^{2}\\,+\\,1}}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,=K\\,=\\,4}}\\end{array}\n$$  \n\nThus  \n\n$$\nG_{c}(s)\\,=\\frac{4(a s\\,+\\,1)(b s\\,+\\,1)}{s}\n$$  \n\nNext, we plot a Bode diagram of  \n\n$$\nG(s)\\,=\\,\\frac{4}{s(s^{2}\\,+\\,1)}\n$$  \n\nMATLAB Program 8–2 produces a Bode diagram of $G(s)$ .The resulting Bode diagram is shown in Figure 8–14.  \n\n![](images/3f850f6fa43420be0a95cec97d04f5aa6598eca856a94ebf0a3cbbe4e1aa1da0.jpg)  \nBode Diagram of $4/[s(s^{2}+1)]$  \n\n![](images/2da6ea4663ec9a5a4fc17c96d7f360995bdb278c8b278c2f9f0d44cd71990056.jpg)  \nFigure 8–14 Bode diagram of $4/[s(s^{2}+1)]$ .  \n\nWe need the phase margin of at least $50^{\\circ}$ and gain margin of $10\\ \\mathrm{dB}$ or more. From the Bode diagram of Figure 8–14, we notice that the gain crossover frequency is approximately $\\omega=1.8$ rad 'sec. Let us assume the gain crossover frequency of the compensated system to be somewhere between $\\omega=1$ and $\\omega\\,=\\,10$ rad 'sec. Noting that  \n\n$$\nG_{c}(s)\\,=\\frac{4(a s\\,+\\,1)(b s\\,+\\,1)}{s}\n$$  \n\nwe choose $a\\,=\\,5$ .Then, $(a s\\,+\\,1)$ will contribute up to $90^{\\circ}$ phase lead in the highfrequency region. MATLAB Program 8–3 produces the Bode diagram of  \n\n$$\n\\frac{4(5s\\,+\\,1)}{s(s^{2}\\,+\\,1)}\n$$  \n\nThe resulting Bode diagram is shown in Figure 8–15.  \n\n![](images/2ea780c80bf95dcb13bef19bc497b24daeb0eb6ebbfd2ecae41a06a330ec3e29.jpg)  \n\n![](images/0401585761a9edfb062926dba0100e91d2dd47bd42429597b3c369d43535e493.jpg)  \nFigure 8–15 Bode diagram of $G(s)\\,=\\,4(5s\\,+\\,1)/$ $\\left[s(s^{2}+1)\\right]$ .  \n\nBased on the Bode diagram of Figure 8–15, we choose the value of $^b$ .The term $(b s+1)$ needs to give the phase margin of at least $50^{\\circ}$ . By simple MATLAB trials, we find $b=0.25$ to give the phase margin of at least $50^{\\circ}$ and gain margin of $+\\infty$ dB.Therefore, by choosing $b=0.25$ ,we have  \n\n$$\nG_{c}(s)\\,=\\frac{4(5s\\,+\\,1)(0.25s\\,+\\,1)}{s}\n$$  \n\nand the open-loop transfer function of the designed system becomes  \n\n$$\n{\\begin{array}{r l}{\\operatorname{Open-loop\\transfer\\function}={\\frac{4(5s\\ +1)(0.25s\\ +1)}{s}}{\\frac{1}{s^{2}\\ +1}}}\\\\ {={\\frac{5s^{2}\\ +21s\\ +4}{s^{3}\\ +s}}}\\end{array}}\n$$  \n\nMATLAB Program 8–4 produces the Bode diagram of the open-loop transfer function. The resulting Bode diagram is shown in Figure 8–16. From it we see that the static velocity error constant is $4~\\mathrm{sec}^{-1}$ , the phase margin is $55^{\\circ}$ , and the gain margin is $+\\infty$ dB.  \n\n![](images/892a9bdf4f998f312c87ef03aa8c610953aab0169ded1c4c9b17c50ee55f98c8.jpg)  \n\n![](images/b1392f9f4b715f17ddb718e820aea450e84b53d87b4ccf0f02de2c94dd822afe.jpg)  \nFigure 8–16 $4(5s\\,+\\,1)(0.25s\\,+\\,1)/$ Bode diagram of D$\\left[s(s^{2}+1)\\right]$ .  \n\nTherefore, the designed system satisfies all the requirements.Thus, the designed system is acceptable. (Note that there exist infinitely many systems that satisfy all the requirements.The present system is just one of them.)  \n\nNext, we shall obtain the unit-step response and the unit-ramp response of the designed system.The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{5s^{2}\\,+\\,21s\\,+\\,4}{s^{3}\\,+\\,5s^{2}\\,+\\,22s\\,+\\,4}\n$$  \n\nNote that the closed-loop zeros are located at  \n\n$$\ns\\,=\\,-4,\\qquad s\\,=\\,-0.2\n$$  \n\nThe closed-loop poles are located at  \n\n$$\n\\begin{array}{l}{{s=-2.4052\\,+\\,j3.9119}}\\\\ {{\\,s=-2.4052\\,-\\,j3.9119}}\\\\ {{\\,s=-0.1897}}\\end{array}\n$$  \n\nNotice that the complex-conjugate closed-loop poles have the damping ratio of 0.5237.   \nMATLAB Program 8–5 produces the unit-step response and the unit-ramp response.  \n\n![](images/2d96392a5a402313c1dc861729113c724cd134d2dcde163f626fc3c3d4743105.jpg)  \n\n![](images/8aeaafa83500fb9c39dd55a6cc5af26528e7381bafc525dc3402b9db4d65531f.jpg)  \nFigure 8–17 Unit-step response curve.  \n\nThe resulting unit-step response curve is shown in Figure 8–17 and the unitramp response curve in Figure 8–18. Notice that the closed-loop pole at $s=-0.1897$ and the zero at $s\\,=\\,-0.2$ produce a long tail of small amplitude in the unit-step response.  \n\nFor an additional example of design of a PID controller based on the frequencyresponse approach, see Problem A–8–7 .  \n\n![](images/0302c11e960ebc33d61534f76dc33d8bb77947f822a2dbd8d928079cbcdb6375.jpg)  \nFigure 8–18 Unit-ramp input and the output curve.  \n\nIn this section we shall explore how to obtain an optimal set (or optimal sets) of parameter values of PID controllers to satisfy the transient response specifications by use of MATLAB.We shall present two examples to illustrate the approach in this section.  \n\n# EXAMPLE 8–2  \n\nConsider the PID-controlled system shown in Figure 8–19.The PID controller is given by  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\nIt is desired to find a combination of $K$ and $a$ such that the closed-loop system will have $10\\%$ (or less) maximum overshoot in the unit-step response. (We will not include any other condition in this problem.But other conditions can easily be included,such as that the settling time be less than a specified value. See, for example, Example 8–3.)  \n\nThere may be more than one set of parameters that satisfy the specifications. In this example, we shall obtain all sets of parameters that satisfy the given specifications.  \n\nTo solve this problem with MATLAB, we first specify the region to search for appropriate $K$ and a .We then write a MATLAB program that, in the unit-step response, will find a combination of $K$ and $a$ which will satisfy the criterion that the maximum overshoot is $10\\%$ or less.  \n\nNote that the gain $K$ should not be too large, so as to avoid the possibility that the system require an unnecessarily large power unit.  \n\nAssume that the region to search for $K$ and $a$ is  \n\n$$\n2\\leq K\\leq3\\qquad\\mathrm{and}\\qquad0.5\\leq a\\leq1.5\n$$  \n\nIf a solution does not exist in this region, then we need to expand it. In some problems, however, there is no solution, no matter what the search region might be.  \n\nIn the computational approach, we need to determine the step size for each of $K$ and $a$ . In the actual design process,we need to choose step sizes small enough.However,in this example,to avoid an overly large number of computations, we choose the step sizes to be reasonable—say, 0.2 for both $K$ and $a$ .  \n\nTo solve this problem it is possible to write many different MATLAB programs.We present here   \none such program,MATLAB Program 8–6.In this program,notice that we use two “for”loops.We   \nstart the program with the outer loop to vary the $\\bullet K^{\\bullet}$ values. Then we vary the “ a ” values in the   \ninner loop. We proceed by writing the MATLAB program such that the nested loops in the pro  \ngram begin with the lowest values of “ $K$ ” and “ $^a$ ” and step toward the highest. Note that, depend  \ning on the system and the ranges of search for “ $K$ ” and “ $a$ ” and the step sizes chosen, it may take   \nfrom several seconds to a few minutes for MATLAB to compute the desired sets of the values. In this program the statement  \n\n$$\n\\mathsf{s o l u t i o n(k,:)=[k(i)\\Delta~a(j)\\Delta~m]}\n$$  \n\nwill produce a table of $K,a,m$ values. (In the present system there are 15 sets of $K$ and $a$ that will exhibit $m<1.10\\AA$ —that is, the maximum overshoot is less than $10\\%$ .)  \n\nFigure 8–19 PID-controlled system.  \n\n![](images/aec954a95d61b72977e6bed16561587978d292ded97fbb98cec8890ebcef6702.jpg)  \n\nTo sort out the solution sets in the order of the magnitude of the maximum overshoot  (starting from the smallest value of $_m$ and ending at the largest value of $_m$ in the table),we use the command  \n\nsortsolution $=$ sortrows(solution,3)  \n\n![](images/dc6343bf96ce038eab9b023914f3ea65ceb6ed6bb37744250e38c133cd2ca143.jpg)  \n\nsortsolution $=$ 2.0000  0.5000  0.9002 2.2000  0.5000  0.9114 2.4000  0.5000  0.9207 2.6000  0.5000  0.9283 2.8000  0.5000  0.9348 3.0000  0.5000  0.9402 2.0000  0.7000  0.9807 2.2000  0.7000  0.9837 2.4000  0.7000  0.9859 2.6000  0.7000  0.9877 2.8000  0.7000  1.0024 3.0000  0.7000  1.0177 2.0000  0.9000  1.0614 2.2000  0.9000  1.0772 2.4000  0.9000  1.0923   \n$\\%$ Plot the response with the largest overshoot that is less than $10\\%$   \n$\\mathsf{K}=$ sortsolution(k,1)   \n$\\mathsf{K}=$ 2.4000   \na $=$ sortsolution(k,2)   \na = 0.9000   \n$\\mathrm{gc}=\\mathrm{tf}(\\mathsf{K}^{*}[1\\ \\ 2^{*}\\mathsf{a}\\ \\ \\mathsf{a}^{\\wedge}2],\\,[1\\ \\ 0]);$ ;  \n$\\mathrm{G}=\\mathrm{gc}^{*}\\mathrm{g}/(1\\,+\\,\\mathrm{gc}^{*}\\mathrm{g})$ ;  \nstep(G,t)   \ngrid $\\%$ See Figure 8–20   \n$\\%$ If you wish to plot the response with the smallest overshoot that is   \n$\\%$ greater than $0\\%$ , then enter the following values of $^{1}{\\sf K}^{1}$ and $\"\\mathrm{a}^{\\prime}$   \n$\\mathsf{K}=$ sortsolution(11,1)   \n$\\mathsf{K}=$ 2.8000   \na $=$ sortsolution(11,2)   \na = 0.7000   \n$\\mathrm{gc}=\\mathrm{tf}(\\mathsf{K}^{*}[1\\ \\ 2^{*}\\mathsf{a}\\ \\ \\mathsf{a}^{\\wedge}2],\\,[1\\ \\ 0]);$ ;  \n$\\mathrm{G}=\\mathrm{gc}^{*}\\mathrm{g}/(1\\,+\\,\\mathrm{gc}^{*}\\mathrm{g})$ ;  \nstep(G,t)   \ngrid $\\%$ See Figure 8–21  \n\n![](images/d4c6da0ac1f336128e00b951e9cff8cd3eb137b974b268009f3306a944a565c1.jpg)  \nFigure 8–20 Unit-step response of the system with $K=2.4$ and $a=0.9$ .(The maximum overshoot is $9.23\\%.$ .)  \n\nTo plot the unit-step response curve of the last set of the $K$ and $a$ values in the sorted table, we enter the commands  \n\n$$\n\\begin{array}{r}{\\mathsf{K}=\\mathsf{s o r t s o l u t i o n}\\left(\\mathsf{k},1\\right)}\\\\ {\\mathsf{a}=\\mathsf{s o r t s o l u t i o n}\\left(\\mathsf{k},2\\right)}\\end{array}\n$$  \n\nand use the step command. (The resulting unit-step response curve is shown in Figure 8–20.) To plot the unit-step response curve with the smallest overshoot that is greater than $0\\%$ found in the sorted table, enter the commands  \n\n$$\n\\begin{array}{r}{\\mathsf{K}=\\mathsf{s o r t s o l u t i o n}\\left(11,1\\right)}\\\\ {\\mathsf{a}=\\mathsf{s o r t s o l u t i o n}\\left(11,2\\right)}\\end{array}\n$$  \n\nand use the step command. (The resulting unit-step response curve is shown in Figure 8–21.)  \n\n![](images/06969de206f43cd74826ef465b3d377f3ef34e09c8545f37aa54a6970872834e.jpg)  \n\n# Figure 8–21  \n\nUnit-step response of the system with   \n$K=2.8$ and $a=0.7$ .(The maximum   \novershoot is $0.24\\%$ .)  \n\n![](images/2fdb9cdbca7e2919cb06efadf941781af28f700950e0e862072ffd857715562b.jpg)  \nFigure 8–22 Unit-step response curves of system with $K=2$ ,$a=0.9$ ;$K=2.2$ ,$a=0.9$ ;and $K=2.4$ ,$a=0.9$ .  \n\nTo plot the unit-step response curve of the system with any set shown in the sorted table, we specify the $K$ and $a$ values by entering an appropriate sortsolution command.  \n\nNote that for a specification that the maximum overshoot be between $10\\%$ and $5\\%$ ,there would be three sets of solutions:  \n\n$$\n\\begin{array}{l l}{{K=2.0000,\\;\\;\\;}}&{{a=0.9000,\\;\\;\\;\\;\\;m=1.0614}}\\\\ {{K=2.2000,\\;\\;\\;}}&{{a=0.9000,\\;\\;\\;\\;}}&{{m=1.0772}}\\\\ {{K=2.4000,\\;\\;\\;}}&{{a=0.9000,\\;\\;\\;\\;}}&{{m=1.0923}}\\end{array}\n$$  \n\n# EXAMPLE 8–3  \n\nUnit-step response curves for these three cases are shown in Figure 8–22. Notice that the system with a larger gain $K$ has a smaller rise time and larger maximum overshoot.Which one of these three systems is best depends on the system’s objective.  \n\nConsider the system shown in Figure 8–23. We want to find all combinations of $K$ and $a$ values such that the closed-loop system has a maximum overshoot of less than $15\\%$ , but more than $10\\%$ ,in the unit-step response. In addition, the settling time should be less than 3 sec. In this problem, assume that the search region is  \n\n$$\n3\\leq K\\leq5\\;\\;\\;{\\mathrm{and}}\\;\\;\\;0.1\\leq a\\leq3\n$$  \n\nDetermine the best choice of the parameters $K$ and $a$ .  \n\nFigure 8–23 PID-controlled system with a simplified PID controller.  \n\n![](images/c79a696f2795f28de8d4decdb6846a957dd117191252f019e25c7260d99097f6.jpg)  \n\nIn this problem, we choose the step sizes to be reasonable, — say 0.2 for $K$ and 0.1 for a . MATLAB Program 8–7 gives the solution to this problem. From the sortsolution table, it looks like the first row is a good choice.Figure 8–24 shows the unit step response curve for $K=3.2$ and $a\\!=\\!0.9.$ .Since this choice requires a smaller $K$ value than most other choices,we may decide that the first row is the best choice.  \n\n![](images/eecd6dea782bc5d05b161bcd08789869af76759d1cc49fd065fd63f9cef7ec26.jpg)  \n\n![](images/fce9c3511d4bdf0976b0d0fede0ee31ca6802fffdf26f17efa53c81b0feae7c6.jpg)  \n\n![](images/d2cd8b31b57ef6f756962e037ea1663625bf79587d49c45c17cbc2031accf4f4.jpg)  \nFigure 8–24 Unit-step response curve of the system with $K=3.2$ and $a=0.9$ .  \n\nConsider the basic PID control system shown in Figure 8–25(a), where the system is subjected to disturbances and noises.Figure 8–25(b) is a modified block diagram of the same system.In the basic PID control system such as the one shown in Figure 8–25(b),if the reference input is a step function, then, because of the presence of the derivative term in the control action,the manipulated variable $u(t)$ will involve an impulse function (delta function). In an actual PID controller, instead of the pure derivative term $T_{d}s$ , we employ  \n\n$$\n\\frac{T_{d}s}{1\\,+\\,\\gamma T_{d}s}\n$$  \n\nwhere the value of $\\gamma$ is somewhere around 0.1.Therefore, when the reference input is a step function,the manipulated variable $u(t)$ will not involve an impulse function,but will involve a sharp pulse function. Such a phenomenon is called set-point kick .  \n\nPI-D Control. To avoid the set-point kick phenomenon, we may wish to operate the derivative action only in the feedback path so that differentiation occurs only on the feedback signal and not on the reference signal.The control scheme arranged in this way is called the PI-D control. Figure 8–26 shows a PI-D-controlled system.  \n\nFrom Figure 8–26, it can be seen that the manipulated signal $U(s)$ is given by  \n\n$$\nU(s)\\,=\\,K_{p}\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\bigg)R(s)\\,-\\,K_{p}\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)B(s)\n$$  \n\n![](images/3d0161d15e15465282a1fb8ad57212185a7784705fea9bdb3a121e667645aa08.jpg)  \nFigure 8–25 (a) PID-controlled system; (b) equivalent block diagram.  \n\n![](images/853c5f2814823145740416fd76f35797989fc9f4e507556c1ea746f444625214.jpg)  \nFigure 8–26 PI-D-controlled system.  \n\nNotice that in the absence of the disturbances and noises, the closed-loop transfer function of the basic PID control system [shown in Figure 8–25(b)] and the PI-D control system (shown in Figure 8–26) are given, respectively, by  \n\n$$\n\\frac{Y(s)}{R(s)}=\\bigg(1+\\frac{1}{T_{i}s}+T_{d}s\\bigg)\\frac{K_{p}G_{p}(s)}{1+\\bigg(1+\\frac{1}{T_{i}s}+T_{d}s\\bigg)K_{p}G_{p}(s)}\n$$  \n\nand  \n\n$$\n\\frac{Y(s)}{R(s)}=\\left(1\\,+\\,\\frac{1}{T_{i}s}\\right)\\frac{K_{p}G_{p}(s)}{1\\,+\\,\\left(1\\,+\\,\\frac{1}{T_{i}s}+\\,T_{d}s\\right)K_{p}G_{p}(s)}\n$$  \n\nIt is important to point out that in the absence of the reference input and noises, the closed-loop transfer function between the disturbance $D(s)$ and the output $Y(s)$ in either case is the same and is given by  \n\n$$\n\\frac{Y(s)}{D(s)}=\\frac{G_{p}(s)}{1\\,+\\,K_{p}G_{p}(s)\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)}\n$$  \n\nI-PD Control. Consider the case where the reference input is a step function. Both PID control and PI-D control involve a step function in the manipulated signal. Such a step change in the manipulated signal may not be desirable in many occasions. Therefore, it may be advantageous to move the proportional action and derivative action to the feedback path so that these actions affect the feedback signal only.Figure 8–27 shows such a control scheme. It is called the I-PD control.The manipulated signal is given by  \n\n$$\nU(s)\\,=\\,K_{p}\\,\\frac{1}{T_{i}s}\\,R(s)\\,-\\,K_{p}\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)B(s)\n$$  \n\nNotice that the reference input $R(s)$ appears only in the integral control part. Thus, in I-PD control, it is imperative to have the integral control action for proper operation of the control system.  \n\n![](images/2ff7b1ca6624d1686a04fd7f08a2aeabcbec645333d25517cf2dc5cdbd3b3058.jpg)  \n\nFigure 8–27   \nI-PD-controlled system.  \n\nThe closed-loop transfer function $Y(s)/R(s)$ in the absence of the disturbance input and noise input is given by  \n\n$$\n{\\frac{Y(s)}{R(s)}}=\\,\\bigg({\\frac{1}{T_{i}s}}\\bigg)\\,{\\frac{K_{p}G_{p}(s)}{1\\,+\\,K_{p}G_{p}(s)\\bigg(1\\,+\\,{\\frac{1}{T_{i}s}}+\\,T_{d}s\\bigg)}}\n$$  \n\nIt is noted that in the absence of the reference input and noise signals,the closed-loop transfer function between the disturbance input and the output is given by  \n\n$$\n\\frac{Y(s)}{D(s)}=\\frac{G_{p}(s)}{1\\,+\\,K_{p}G_{p}(s)\\bigg(1\\,+\\,\\frac{1}{T_{i}s}\\,+\\,T_{d}s\\bigg)}\n$$  \n\nThis expression is the same as that for PID control or PI-D control.  \n\nTwo-Degrees-of-Freedom PID Control. We have shown that PI-D control is obtained by moving the derivative control action to the feedback path, and I-PD control is obtained by moving the proportional control and derivative control actions to the feedback path. Instead of moving the entire derivative control action or proportional control action to the feedback path, it is possible to move only portions of these control actions to the feedback path, retaining the remaining portions in the feedforward path. In the literature, PI-PD control has been proposed. The characteristics of this control scheme lie between PID control and I-PD control. Similarly, PID-PD control can be considered. In these control schemes, we have a controller in the feedforward path and another controller in the feedback path. Such control schemes lead us to a more general two-degrees-of-freedom control scheme.We shall discuss details of such a two-degreesof-freedom control scheme in subsequent sections of this chapter.  \n\n# 8–6 TWO-DEGREES-OF-FREEDOM CONTROL  \n\nConsider the system shown in Figure 8–28, where the system is subjected to the disturbance input $D(s)$ and noise input $N(s)$ ,in addition to the reference input $R(s)$ .$G_{c}(s)$ is the transfer function of the controller and $G_{p}(s)$ is the transfer function of the plant.We assume that $G_{p}(s)$ is fixed and unalterable.  \n\n![](images/96150599cf95a269165c0c107402c2d6f620aad566a56c041d0647be00a5e2ae.jpg)  \nFigure 8–28 One-degree-offreedom control system.  \n\nFor this system, three closed-loop transfer functions $Y(s)/R(s)\\,=\\,G_{y r}$ ,$Y(s)/D(s)\\,=\\,G_{y d}$ ,and $Y(s)/N(s)\\,=\\,G_{y n}$ may be derived.They are  \n\n$$\n\\begin{array}{l}{\\displaystyle G_{y r}=\\frac{Y(s)}{R(s)}=\\frac{G_{c}G_{p}}{1+G_{c}G_{p}}}\\\\ {\\displaystyle G_{y d}=\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+G_{c}G_{p}}}\\\\ {\\displaystyle G_{y n}=\\frac{Y(s)}{N(s)}=-\\frac{G_{c}G_{p}}{1+G_{c}G_{p}}}\\end{array}\n$$  \n\n[In deriving $Y(s)/R(s)$ ,we assumed $D(s)\\,=\\,0$ and $N(s)\\,=\\,0$ .Similar comments apply to the derivations of $Y(s)/D(s)$ and $Y(s)/N(s).]$ The degrees of freedom of the control system refers to how many of these closed-loop transfer functions are independent. In the present case, we have  \n\n$$\n\\begin{array}{l}{G_{y r}=\\frac{G_{p}-G_{y d}}{G_{p}}}\\\\ {G_{y n}=\\frac{G_{y d}-G_{p}}{G_{p}}}\\end{array}\n$$  \n\nAmong the three closed-loop transfer functions $G_{y r},\\,G_{y n}$ ,and $G_{y d}$ , if one of them is given, the remaining two are fixed.This means that the system shown in Figure 8–28 is a one-degree-of-freedom control system.  \n\nNext consider the system shown in Figure 8–29, where $G_{p}(s)$ is the transfer function of the plant. For this system, closed-loop transfer functions $G_{y r}$ ,$G_{y n}$ ,and $G_{y d}$ are given, respectively, by  \n\n$$\n\\begin{array}{l}{\\displaystyle G_{y r}=\\frac{Y(s)}{R(s)}=\\frac{G_{c1}G_{p}}{1+\\big(G_{c1}+G_{c2}\\big)G_{p}}}\\\\ {\\displaystyle G_{y d}=\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+\\big(G_{c1}+G_{c2}\\big)G_{p}}}\\\\ {\\displaystyle G_{y n}=\\frac{Y(s)}{N(s)}=-\\frac{\\big(G_{c1}+G_{c2}\\big)G_{p}}{1+\\big(G_{c1}+G_{c2}\\big)G_{p}}}\\end{array}\n$$  \n\n![](images/64d9be6ed1079887c30b5b08cebca10ef5e2570c5d3abebf949a98d162314fda.jpg)  \nFigure 8–29 Two-degrees-offreedom control system.  \n\nHence, we have  \n\n$$\n\\begin{array}{l}{G_{y r}=G_{c1}G_{y d}}\\\\ {G_{y n}=\\displaystyle\\frac{G_{y d}-G_{p}}{G_{p}}}\\end{array}\n$$  \n\nIn this case, if $G_{y d}$ is given, then $G_{y n}$ is fixed, but $G_{y r}$ is not fixed, because $G_{c1}$ is independent of $G_{y d}$ .Thus, two closed-loop transfer functions among three closed-loop transfer functions $G_{y r}$ ,$G_{y d}$ ,and $G_{y n}$ are independent.Hence,this system is a two-degreesof-freedom control system.  \n\nSimilarly, the system shown in Figure 8–30 is also a two-degrees-of-freedom control system, because for this system  \n\n$$\n\\begin{array}{l}{{G_{y r}=\\displaystyle\\frac{Y(s)}{R(s)}=\\frac{G_{c1}G_{p}}{1+G_{c1}G_{p}}+\\frac{G_{c2}G_{p}}{1+G_{c1}G_{p}}}}\\\\ {{\\displaystyle}}\\\\ {{G_{y d}=\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+G_{c1}G_{p}}}}\\\\ {{\\displaystyle}}\\\\ {{G_{y n}=\\displaystyle\\frac{Y(s)}{N(s)}=-\\frac{G_{c1}G_{p}}{1+G_{c1}G_{p}}}}\\end{array}\n$$  \n\n![](images/a7a968a62bcb5003b92076058a6eb6dbcc7837c608a9d6d5e89d91fcbc3752ba.jpg)  \nFigure 8–30 Two-degrees-offreedom control system.  \n\nHence,  \n\n$$\n\\begin{array}{l}{{G_{y r}=G_{c2}G_{y d}+\\displaystyle\\frac{G_{p}-G_{y d}}{G_{p}}}}\\\\ {{\\displaystyle G_{y n}=\\displaystyle\\frac{G_{y d}-G_{p}}{G_{p}}}}\\end{array}\n$$  \n\nClearly, if $G_{y d}$ is given, then $G_{y n}$ is fixed, but $G_{y r}$ is not fixed, because $G_{c2}$ is independent of $G_{y d}$ .  \n\nIt will be seen in Section 8–7 that, in such a two-degrees-of-freedom control system, both the closed-loop characteristics and the feedback characteristics can be adjusted independently to improve the system response performance.  \n\n# 8–7 ZERO-PLACEMENT APPROACH TO IMPROVE RESPONSE CHARACTERISTICS  \n\nWe shall show here that by use of the zero-placement approach presented later in this section, we can achieve the following:  \n\nThe responses to the ramp reference input and acceleration reference input exhibit no steady-state errors.  \n\nIn high-performance control systems it is always desired that the system output follow the changing input with minimum error. For step, ramp, and acceleration inputs, it is desired that the system output exhibit no steady-state error.  \n\nIn what follows, we shall demonstrate how to design control systems that will exhibit no steady-state errors in following ramp and acceleration inputs and at the same time force the response to the step disturbance input to approach zero quickly.  \n\nConsider the two-degrees-of-freedom control system shown in Figure 8–31. Assume that the plant transfer function $G_{p}(s)$ is a minimum-phase transfer function and is given by  \n\n$$\nG_{p}(s)\\,=\\,K\\,{\\frac{A(s)}{B(s)}}\n$$  \n\n![](images/3aaee17db5ea855aef86879b86db26aef978f345bd852986e3f8cea935cfff42.jpg)  \nFigure 8–31 Two-degrees-offreedom control system.  \n\nwhere  \n\n$$\n\\begin{array}{l}{{A(s)\\,=\\,\\bigl(s\\,+\\,z_{1}\\bigr)\\bigl(s\\,+\\,z_{2}\\bigr)\\cdots\\bigl(s\\,+\\,z_{m}\\bigr)}}\\\\ {{\\phantom{A}B(s)\\,=\\,s^{N}\\bigl(s\\,+\\,p_{N+1}\\bigr)\\bigl(s\\,+\\,p_{N+2}\\bigr)\\cdots\\bigl(s\\,+\\,p_{n}\\bigr)}}\\end{array}\n$$  \n\nwhere $N$ may be $0,1,2$ and $n\\geq m$ .Assume also that $G_{c1}$ is a PID controller followed by a filter $1/A(s)$ ,or  \n\n$$\nG_{c1}(s)\\,=\\frac{\\alpha_{1}s\\,+\\,\\beta_{1}\\,+\\,\\gamma_{1}s^{2}}{s}\\frac{1}{A(s)}\n$$  \n\nand $G_{c2}$ is a PID, PI, PD, I, D, or $\\mathrm{P}$ controller followed by a filter $1/A(s)$ .That is  \n\n$$\nG_{c2}(s)\\,={\\frac{\\alpha_{2}s\\,+\\,\\beta_{2}\\,+\\,\\gamma_{2}s^{2}}{s}}{\\frac{1}{A(s)}}\n$$  \n\nwhere some of $\\alpha_{2},\\beta_{2}$ , and $\\gamma_{2}$ may be zero.Then it is possible to write $G_{c1}+G_{c2}$ as  \n\n$$\nG_{c1}\\,+\\,G_{c2}\\,=\\frac{\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}}{s}\\frac{1}{A(s)}\n$$  \n\nwhere $\\alpha,\\beta$ , and $\\gamma$ are constants.Then  \n\n$$\n\\begin{array}{c}{{\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+\\big(G_{c1}+G_{c2}\\big)G_{p}}=\\frac{K\\,\\frac{A(s)}{B(s)}}{1+\\frac{\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}}{s}\\frac{K}{B(s)}}}}\\\\ {{=\\displaystyle\\frac{s K A(s)}{s B(s)\\,+\\,\\big(\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}\\big)K}}}\\end{array}\n$$  \n\nBecause of the presence of $s$ in the numerator, the response $y(t)$ to a step disturbance input approaches zero as $t$ approaches infinity, as shown below. Since  \n\n$$\nY(s)\\,=\\frac{s K A(s)}{s B(s)\\,+\\,\\left(\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}\\right)K}\\,D(s)\n$$  \n\nif the disturbance input is a step function of magnitude $d$ ,or  \n\n$$\nD(s)={\\frac{d}{s}}\n$$  \n\nand assuming the system is stable, then  \n\n$$\n\\begin{array}{r l}&{y(\\infty)=\\underset{s\\rightarrow0}{\\operatorname*{lim}}s\\bigg[\\frac{s K A(s)}{s B(s)\\ +\\ \\big(\\alpha s\\ +\\ \\beta+\\ \\gamma s^{2}\\big)K}\\bigg]\\frac{d}{s}}\\\\ &{\\quad\\quad=\\underset{s\\rightarrow0}{\\operatorname*{lim}}\\frac{s K A(0)d}{s B(0)\\ +\\ \\beta K}}\\\\ &{\\quad\\quad=0}\\end{array}\n$$  \n\n![](images/fbd0f4084403c83726867176a5014e67bd4d5dcb7f9e21e252d069d0e7f3defa.jpg)  \nFigure 8–32 Typical response curve to a step disturbance input.  \n\nThe response $y(t)$ to a step disturbance input will have the general form shown in   \nFigure 8–32. Note that $Y(s)/R(s)$ and $Y(s)/D(s)$ are given by  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{G_{c1}G_{p}}{1\\,+\\,\\bigl(G_{c1}\\,+\\,G_{c2}\\bigr)G_{p}},\\qquad\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1\\,+\\,\\bigl(G_{c1}\\,+\\,G_{c2}\\bigr)G_{p}}\n$$  \n\nNotice that the denominators of $Y(s)/R(s)$ and $Y(s)/D(s)$ are the same. Before we choose the poles of $Y(s)/R(s)$ ,we need to place the zeros of $Y(s)/R(s)$ .  \n\nZero Placement. Consider the system  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{p(s)}{s^{n+1}\\,+\\,a_{n}s^{n}\\,+\\,a_{n-1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{2}s^{2}\\,+\\,a_{1}s\\,+\\,a_{0}}}\n$$  \n\nIf we choose $p(s)$ as  \n\n$$\np(s)\\,=\\,a_{2}s^{2}\\,+\\,a_{1}s\\,+\\,a_{0}\\,=\\,a_{2}\\bigl(s\\,+\\,s_{1}\\bigr)\\bigl(s\\,+\\,s_{2}\\bigr)\n$$  \n\nthat is, choose the zeros $s=-s_{1}$ and $s=-s_{2}$ such that, together with $a_{2}$ , the numerator polynomial $p(s)$ is equal to the sum of the last three terms of the denominator polynomial—then the system will exhibit no steady-state errors in response to the step input, ramp input, and acceleration input.  \n\nRequirement Placed on System Response Characteristics. Suppose that it is desired that the maximum overshoot in the response to the unit-step reference input be between arbitrarily selected upper and lower limits—for example,  \n\n$$\n2\\%<\\mathrm{maximum~overshoot}<10\\%\n$$  \n\nwhere we choose the lower limit to be slightly above zero to avoid having overdamped systems.The smaller the upper limit, the harder it is to determine the coefficient $a$ ’s. In some cases, no combination of the $a$ ’s may exist to satisfy the specification, so we must allow a higher upper limit for the maximum overshoot.We use MATLAB to search at least one set of the $a$ ’s to satisfy the specification.As a practical computational matter, instead of searching for the $a$ ’s, we try to obtain acceptable closed-loop poles by searching a reasonable region in the left-half $s$ plane for each closed-loop pole. Once we determine all closed-loop poles,then all coefficients $a_{n},a_{n-1},\\ldots,a_{1},a_{0}$ will be determined.  \n\n# Determination of $G_{c2}$ .Now that the coefficients of the transfer function $Y(s)/R(s)$ are all known and $Y(s)/R(s)$ is given by  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{a_{2}s^{2}+a_{1}s\\,+\\,a_{0}}{s^{n+1}\\,+\\,a_{n}s^{n}\\,+\\,a_{n-1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{2}s^{2}\\,+\\,a_{1}s\\,+\\,a_{0}}}\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\frac{Y(s)}{R(s)}=G_{c1}\\frac{Y(s)}{D(s)}}}\\\\ &{=\\frac{G_{c1}s K A(s)}{s B(s)\\,+\\,\\left(\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}\\right)K}}\\\\ &{=\\frac{G_{c1}s K A(s)}{s^{n+1}\\,+\\,a_{n}s^{n}\\,+\\,\\,a_{n-1}s^{n-1}\\,+\\,\\cdots\\,+\\,\\,a_{2}s^{2}\\,+\\,\\,a_{1}s\\,+\\,a_{0}}}\\end{array}\n$$  \n\nSince $G_{c1}$ is a PID controller and is given by  \n\n$$\nG_{c1}=\\frac{\\alpha_{1}s\\,+\\,\\beta_{1}\\,+\\,\\gamma_{1}s^{2}}{s}\\frac{1}{A(s)}\n$$  \n\n$Y(s)/R(s)$ can be written as  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{K{\\bigl(}\\alpha_{1}s\\,+\\,\\beta_{1}\\,+\\,\\gamma_{1}s^{2}{\\bigr)}}{s^{n+1}\\,+\\,a_{n}s^{n}\\,+\\,a_{n-1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{2}s^{2}\\,+\\,a_{1}s\\,+\\,a_{0}}}\n$$  \n\nTherefore, we choose  \n\n$$\nK{\\gamma}_{1}={{a}_{2}},\\qquad K{{\\alpha}_{1}}={{a}_{1}},\\qquad K{{\\beta}_{1}}={{a}_{0}}\n$$  \n\nso that  \n\n$$\nG_{c1}={\\frac{a_{1}s\\,+\\,a_{0}\\,+\\,a_{2}s^{2}}{K s}}{\\frac{1}{A(s)}}\n$$  \n\nThe response of this system to the unit-step reference input can be made to exhibit the maximum overshoot between the chosen upper and lower limits, such as  \n\n$$\n2\\%<\\mathrm{maximum~overshoot}<10\\%\n$$  \n\nThe response of the system to the ramp reference input or acceleration reference input can be made to exhibit no steady-state error.The characteristic of the system of Equation (8–4) is that it generally exhibits a short settling time. If we wish to further shorten the settling time, then we need to allow a larger maximum overshoot—for example,  \n\n$$\n2\\%<\\mathrm{maximum~overshoot}<20\\%\n$$  \n\nThe controller $G_{c2}$ can now be determined from Equations (8–3) and (8–5). Since  \n\n$$\nG_{c1}\\,+\\,G_{c2}\\,=\\frac{\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}}{s}\\frac{1}{A(s)}\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}&{G_{c2}=\\bigg[\\frac{\\alpha s\\,+\\,\\beta\\,+\\,\\gamma s^{2}}{s}-\\frac{a_{1}s\\,+\\,a_{0}\\,+\\,a_{2}s^{2}}{K s}\\bigg]\\frac{1}{A(s)}}\\\\ &{\\quad\\quad=\\frac{\\big(K\\alpha\\,-\\,a_{1}\\big)s\\,+\\,\\big(K\\beta\\,-\\,a_{0}\\big)\\,+\\,\\big(K\\gamma\\,-\\,a_{2}\\big)s^{2}}{K s}\\frac{1}{A(s)}}\\end{array}\n$$  \n\nThe two controllers $G_{c1}$ and $G_{c2}$ can be determined from Equations (8–5) and (8–6).  \n\n# EXAMPLE 8–4  \n\nConsider the two-degrees-of-freedom control system shown in Figure 8–33. The plant transfer function $G_{p}(s)$ is given by  \n\n$$\nG_{p}(s)=\\frac{10}{s(s+1)}\n$$  \n\nDesign controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that the maximum overshoot in the response to the unit-step reference input be less than $19\\%$ , but more than $2\\%$ , and the settling time be less than 1 sec. It is desired that the steady-state errors in following the ramp reference input and acceleration reference input be zero.The response to the unit-step disturbance input should have a small amplitude and settle to zero quickly.  \n\nTo design suitable controllers $G_{c1}(s)$ and $G_{c2}(s)$ ,first note that  \n\n$$\n\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1\\,+\\,G_{p}\\bigl(G_{c1}\\,+\\,G_{c2}\\bigr)}\n$$  \n\nTo simplify the notation, let us define  \n\n$$\nG_{c}=G_{c1}+G_{c2}\n$$  \n\nThen  \n\n$$\n\\begin{array}{c}{{\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1\\,+\\,G_{p}G_{c}}=\\frac{\\frac{10}{s(s\\,+\\,1)}}{1\\,+\\,\\frac{10}{s(s\\,+\\,1)}G_{c}}}}\\\\ {{=\\displaystyle\\frac{10}{s(s\\,+\\,1)\\,+\\,10G_{c}}}}\\end{array}\n$$  \n\n![](images/485bf16855ba7977fa9f706605b303406418e2117b04cb5276752e57729eef84.jpg)  \nFigure 8–33 Two-degreesof-freedom control system.  \n\nSecond, note that  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{G_{p}G_{c1}}{1\\ +\\ G_{p}G_{c}}}={\\frac{10G_{c1}}{s(s\\ +\\ 1)\\ +\\ 10G_{c}}}\n$$  \n\nNotice that the characteristic equation for $Y(s)/D(s)$ and the one for $Y(s)/R(s)$ are identical.  \n\nWe may be tempted to choose a zero of $G_{c}(s)$ at $s=-1$ to cancel a pole at $s=-1$ of the plant $G_{p}(s)$ .However, the canceled pole $s=-1$ becomes a closed-loop pole of the entire system, as seen below. If we define $G_{c}(s)$ as a PID controller such that  \n\n$$\nG_{c}(s)={\\frac{K(s\\,+\\,1)(s\\,+\\,\\beta)}{s}}\n$$  \n\nthen  \n\n$$\n\\begin{array}{c}{\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{10}{s(s+1)+\\frac{10K(s+1)(s+\\beta)}{s}}}\\\\ {\\displaystyle=\\frac{10s}{(s+1)\\big[s^{2}+10K(s+\\beta)\\big]}}\\end{array}\n$$  \n\nThe closed-loop pole at $s=-1$ is a slow-response pole, and if this closed-loop pole is included in the system, the settling time will not be less than 1 sec.Therefore, we should not choose $G_{c}(s)$ as given by Equation (8–7).  \n\nThe design of controllers $G_{c1}(s)$ and $G_{c2}(s)$ consists of two steps.  \n\nDesign Step 1: We design $G_{c}(s)$ to satisfy the requirements on the response to the stepdisturbance input $D(s)$ .In this design stage, we assume that the reference input is zero.  \n\nSuppose that we assume that $G_{c}(s)$ is a PID controller of the form  \n\n$$\nG_{c}(s)\\,=\\frac{K(s\\,+\\,\\alpha)(s\\,+\\,\\beta)}{s}\n$$  \n\nThen the closed-loop transfer function $Y(s)/D(s)$ becomes  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{10}{s(s+1)+10G_{c}}}\\\\ {\\displaystyle=\\frac{10}{s(s+1)+\\frac{10K(s+\\alpha)(s+\\beta)}{s}}}\\\\ {\\displaystyle=\\frac{10s}{s^{2}(s+1)+10K(s+\\alpha)(s+\\beta)}}\\end{array}\n$$  \n\nNote that the presence of “ $\"s'$ in the numerator of $Y(s)/D(s)$ assures that the steady-state response to the step disturbance input is zero.  \n\nLet us assume that the desired dominant closed-loop poles are complex conjugates and are given by  \n\n$$\ns=-a\\,\\pm\\,j b\n$$  \n\nand the remaining closed-loop pole is real and is located at  \n\n$$\ns=-c\n$$  \n\nNote that in this problem there are three requirements. The first requirement is that the response to the step disturbance input damp out quickly.The second requirement is that the maximum overshoot in the response to the unit-step reference input be between $19\\%$ and $2\\%$ and the settling time be less than 1 sec. The third requirement is that the steady-state errors in the responses to both the ramp and acceleration reference inputs be zero.  \n\nA set (or sets) of reasonable values of $a,\\,b$ ,and $c$ must be searched using a computational approach.To satisfy the first requirement, we choose the search region for $a,b$ ,and $c$ to be  \n\n$$\n2\\leq a\\leq6,\\;\\;\\;\\;\\;\\;2\\leq b\\leq6,\\;\\;\\;\\;\\;\\;6\\leq c\\leq12\n$$  \n\nThis region is shown in Figure 8–34. If the dominant closed-loop poles $s=-a\\,\\pm\\,j b$ are located anywhere in the shaded region,the response to a step disturbance input will damp out quickly.(The first requirement will be met.)  \n\nNotice that the denominator of $Y(s)/D(s)$ can be written as  \n\n$$\n\\begin{array}{r l}&{s^{2}(s+1)\\,+\\,10K(s\\,+\\,\\alpha)(s\\,+\\,\\beta)}\\\\ &{=s^{3}\\,+\\,(1\\,+\\,10K)s^{2}\\,+\\,10K(\\alpha\\,+\\,\\beta)s\\,+\\,10K\\alpha\\beta}\\\\ &{=\\,(s\\,+\\,a\\,+\\,j b)(s\\,+\\,a\\,-\\,j b)(s\\,+\\,c)}\\\\ &{=s^{3}\\,+\\,(2a\\,+\\,c)s^{2}\\,+\\,\\bigl(a^{2}\\,+\\,b^{2}\\,+\\,2a c\\bigr)s\\,+\\,\\bigl(a^{2}\\,+\\,b^{2}\\bigr)c}\\end{array}\n$$  \n\n![](images/5eeeb70e0d28919e1ddb390c1883ca5d7fef932204635125afa3251603b5abed.jpg)  \nFigure 8–34 Search regions for $a,b$ ,and $c$ .  \n\nSince the denominators of $Y(s)/D(s)$ and $Y(s)/R(s)$ are the same, the denominator of $Y(s)/D(s)$ determines also the response characteristics for the reference input.To satisfy the third requirement, we refer to the zero-placement method and choose the closed-loop transfer function $Y(s)/R(s)$ to be of the following form:  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{(2a\\,+\\,c)s^{2}\\,+\\,(a^{2}\\,+\\,b^{2}\\,+\\,2a c)s\\,+\\,(a^{2}\\,+\\,b^{2})c}{s^{3}\\,+\\,(2a\\,+\\,c)s^{2}\\,+\\,(a^{2}\\,+\\,b^{2}\\,+\\,2a c)s\\,+\\,(a^{2}\\,+\\,b^{2})c}}\n$$  \n\nin which case the third requirement is automatically satisfied.  \n\nOur problem then becomes a search of a set or sets of desired closed-loop poles in terms of $a,b.$ ,and $c$ in the specified region, such that the system will satisfy the requirement on the response to the unit-step reference input that the maximum overshoot be between $19\\%$ and $2\\%$ and the settling time be less than 1 sec. (If an acceptable set cannot be found in the search region, we need to widen the region.)  \n\nIn the computational search, we need to assume a reasonable step size. In this problem, we assume it to be 0.2.  \n\nMATLAB Program 8–8 produces a table of sets of acceptable values of $a,b$ ,and $c$ .Using this program, we find that the requirement on the response to the unit-step reference input is met by any of the 23 sets shown in the table in MATLAB Program 8–8. Note that the last row in the table corresponds to the last search point.This point does not satisfy the requirement and thus it should simply be ignored. (In the program written, the last search point produces the last row in the table whether or not it satisfies the requirement.)  \n\n![](images/d4838dedffbdfe042f87ce43c1bb6fa609e43c587a851ff65fc5e8ceda0cdd65.jpg)  \n\n(continues on next page )  \n\n![](images/a456ab4493f36782c5a93c5948f08fc7adb311166f5eb09d4d4fa030ee4d6626.jpg)  \n\nAs noted above, 23 sets of variables $a,\\,b$ ,and $c$ satisfy the requirement. Unit-step response curves of the system with any of the 23 sets are about the same.The unit-step response curve with  \n\n$$\na\\,=\\,4.2,\\qquad b\\,=\\,2,\\qquad c\\,=\\,12\n$$  \n\nis shown in Figure 8–35(a). The maximum overshoot is $18.96\\%$ and the settling time is 0.85 sec. Using these values of $a,b$ ,and $c$ ,the desired closed-loop poles are located at  \n\n$$\ns\\,=\\,-4.2\\,\\pm\\,j2,\\qquad s\\,=\\,-12\n$$  \n\nUsing these closed-loop poles, the denominator of $Y(s)/D(s)$ becomes  \n\n$$\ns^{2}(s\\,+\\,1)\\,+\\,10K(s\\,+\\,\\alpha)(s\\,+\\,\\beta)\\,=\\,(s\\,+\\,4.2\\,+\\,j2)(s\\,+\\,4.2\\,-\\,j2)(s\\,+\\,12)\n$$  \n\nor  \n\n$$\ns^{3}\\,+\\,(1\\,+\\,10K)s^{2}\\,+\\,10K(\\alpha\\,+\\,\\beta)s\\,+\\,10K\\alpha\\beta\\,=\\,s^{3}\\,+\\,20.4s^{2}\\,+\\,122.44s\\,+\\,259.68\n$$  \n\n![](images/ef827629ac45671f14a79fb4955eafedd7c96d8359c4429376586e88d5b8c182.jpg)  \nResponse to Unit-Step Disturbance Input  \n\n# Figure 8–35  \n\n(a) Response to unitstep reference input $(a=4.2,b=2,c=12)$ ;(b) response to unit-step disturbance input $(a=4.2,b=2,c=12)$ ).  \n\n![](images/f3d3517f7ff054853174867bee62e9e4364b9532da9b7eae8385ac34951a4d27.jpg)  \n\nBy equating the coefficients of equal powers of $s$ on both sides of this last equation, we obtain  \n\n$$\n\\begin{array}{c}{{1+10K=20.4}}\\\\ {{10K(\\alpha+\\beta)=122.44}}\\\\ {{10K\\alpha\\beta=259.68}}\\end{array}\n$$  \n\nHence  \n\n$$\nK=1.94,\\;\\;\\;\\;\\;\\;\\alpha+\\beta=\\frac{122.44}{19.4},\\;\\;\\;\\;\\;\\;\\alpha\\beta=\\frac{259.68}{19.4}\n$$  \n\nThen $G_{c}(s)$ can be written as  \n\n$$\n\\begin{array}{r l}&{G_{c}(s)=K\\,{\\frac{(s\\,+\\,\\alpha)(s\\,+\\,\\beta)}{s}}}\\\\ &{\\qquad={\\frac{K\\left[s^{2}\\,+\\,(\\alpha\\,+\\,\\beta)s\\,+\\,\\alpha\\beta\\right]}{s}}}\\\\ &{\\qquad={\\frac{1.94s^{2}\\,+\\,12.244s\\,+\\,25.968}{s}}}\\end{array}\n$$  \n\nThe closed-loop transfer function $Y(s)/D(s)$ becomes  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{Y(s)}{D(s)}}={\\frac{10}{s(s+1)+10G_{c}}}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\qquad={\\frac{10}{s(s+1)+10\\,{\\frac{1.94s^{2}+12.244s+25.968}{s}}}}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$  \n\nUsing this expression, the response $y(t)$ to a unit-step disturbance input can be obtained as shown in Figure 8–35(b).  \n\nFigure 8–36(a) shows the response of the system to the unit-step reference input when $a,b$ ,and $c$ are chosen as  \n\n$$\na\\,=\\,3.2,\\qquad b\\,=\\,2,\\qquad c\\,=\\,12\n$$  \n\nFigure 8–36(b) shows the response of this system when it is subjected to a unit-step disturbance input. Comparing Figures 8–35(a) and Figure 8–36(a), we find that they are about the same. However, comparing Figures 8–35(b) and 8–36(b), we find the former to be a little bit better than the latter. Comparing the responses of systems with each set in the table, we conclude the first set of values $(a=4.2,b=2,c=12)$ )to be one of the best.Therefore, as the solution to this problem, we choose  \n\n$$\na\\,=\\,4.2,\\qquad b\\,=\\,2,\\qquad c\\,=\\,12\n$$  \n\nDesign Step 2: Next, we determine $G_{c1}$ . Since $Y(s)/R(s)$ can be given by  \n\n$$\n{\\begin{array}{l}{{\\frac{Y(s)}{R(s)}}={\\cfrac{G_{p}G_{c1}}{1\\,+\\,G_{p}G_{c}}}}\\\\ {\\,}\\\\ {={\\cfrac{\\frac{10}{s(s\\,+\\,1)}\\,G_{c1}}{1\\,+\\,{\\cfrac{10}{s(s\\,+\\,1)}}{\\cfrac{1.94s^{2}\\,+\\,12.244s\\,+\\,25.968}{s}}}}}\\\\ {\\,}\\\\ {={\\cfrac{10s G_{c1}}{s^{3\\,+\\,20.4s^{2}\\,+\\,122.44s\\,+\\,259.68}}}}\\end{array}}\n$$  \n\n![](images/039754eba8f299bfa0ee6e1bde0ddedcad7e93942801c7f9bb80f3086f6955b8.jpg)  \n\n# Figure 8–36  \n\n(a) Response to unit-step reference input   \n$(a=3.2,b=2,c=12)$ ); (b) response to unit-step disturbance input   \n$(a=3.2,b=2,c=12)$ ).  \n\n![](images/908c3f70e2095f143fd66b31e6c1f953d3cb3e9004c93cd551633b39f329189f.jpg)  \n\nour problem becomes that of designing $G_{c1}(s)$ to satisfy the requirements on the responses to the step, ramp, and acceleration inputs.  \n\nSince the numerator involves “ $s^{*}$ ,$G_{c1}(s)$ must include an integrator to cancel this “ $\"s\"$ .[Although we want “ s” in the numerator of the closed-loop transfer function $Y(s)/D(s)$ to obtain zero steady-state error to the step disturbance input, we do not want to have “ $s^{*}$ in the numerator of the closed-loop transfer function $Y(s)/R(s).]$ To eliminate the offset in the response to the step reference input and eliminate the steady-state errors in following the ramp reference input and acceleration reference input, the numerator of $Y(s)/R(s)$ must be equal to the last three terms of the denominator, as mentioned earlier.That is,  \n\n$$\n10s G_{c1}(s)\\,=\\,20.4s^{2}\\,+\\,122.44s\\,+\\,259.68\n$$  \n\nor  \n\n$$\nG_{c1}(s)\\,=\\,2.04s\\,+\\,12.244\\,+\\,{\\frac{25.968}{s}}\n$$  \n\nThus, $G_{c1}(s)$ is a PID controller. Since $G_{c}(s)$ is given as  \n\n$$\nG_{c}(s)\\,=\\,G_{c1}(s)\\,+\\,G_{c2}(s)\\,=\\frac{1.94s^{2}\\,+\\,12.244s\\,+\\,25.968}{s}\n$$  \n\nwe obtain  \n\n$$\n{\\begin{array}{r l}&{G_{c2}(s)\\,=\\,G_{c}(s)\\,-\\,G_{c1}(s)}\\\\ &{}\\\\ &{\\qquad=\\,\\left(1.94s\\,+\\,12.244+{\\frac{25.968}{s}}\\right)\\,-\\,\\left(2.04s\\,+\\,12.244+{\\frac{25.968}{s}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\,}\\\\ &{\\qquad=\\,-0.1s}\\end{array}}\n$$  \n\nThus, $G_{c2}(s)$ is a derivative controller. A block diagram of the designed system is shown in Figure 8–37.  \n\nThe closed-loop transfer function $Y(s)/R(s)$ now becomes  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{20.4s^{2}\\,+\\,122.44s\\,+\\,259.68}{s^{3}\\,+\\,20.4s^{2}\\,+\\,122.44s\\,+\\,259.68}\n$$  \n\nFigure 8–37 Block diagram of the designed system.  \n\n![](images/15d02a35c11ae5f3be3bb9c696f7b1435f71b13027d6fbf5a7033178bdcb3567.jpg)  \n\n![](images/5f8d207b5b9ed558286c51362344ec7f48d70fb0f0acf50616ea8953ca15b306.jpg)  \nFigure 8–38 (a) Response to unitramp reference input; (b) response to unit-acceleration reference input.  \n\n# EXAMPLE 8–5  \n\nThe response to the unit-ramp reference input and that to the unit-acceleration reference input are shown in Figures 8–38(a) and (b), respectively.The steady-state errors in following the ramp input and acceleration input are zero. Thus, all the requirements of the problem are satisfied. Hence, the designed controllers $G_{c1}(s)$ and $G_{c2}(s)$ are acceptable.  \n\nConsider the control system shown in Figure 8–39. This is a two-degrees-of-freedom system.In the design problem considered here, we assume that the noise input $N(s)$ is zero. Assume that the plant transfer function $G_{p}(s)$ is given by  \n\n$$\nG_{p}(s)={\\frac{5}{(s\\,+\\,1)(s\\,+\\,5)}}\n$$  \n\n![](images/21afe5fcdd807c832e4c8561aaf9a7b07be319bc2ca7badf69d90a657366dfc3.jpg)  \nFigure 8–39 Two-degrees-offreedom control system.  \n\nAssume also that the controller $G_{c1}(s)$ is of PID type.That is,  \n\n$$\nG_{c1}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}+\\,T_{d}s\\bigg)\n$$  \n\nThe controller $G_{c2}(s)$ is of $\\mathbf{P}$ or PD type. [If $G_{c2}(s)$ involves integral control action, then this will introduce a ramp component in the input signal, which is not desirable.Therefore, $G_{c2}(s)$ should not include the integral control action.] Thus, we assume that  \n\n$$\nG_{c2}(s)\\,=\\,\\hat{K}_{p}\\!\\big(1\\,+\\,\\hat{T}_{d}s\\big)\n$$  \n\nwhere $\\hat{T}_{d}$ may be zero.  \n\nLet us design controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that the responses to the step-disturbance input and the step-reference input are of “desirable characteristics” in the sense that  \n\n1. The response to the step-disturbance input will have a small peak and eventually approach zero. (That is, there will be no steady-state error.)   \n2. The response to the step reference input will exhibit less than $25\\%$ overshoot with a settling time less than 2 sec.The steady-state errors to the ramp reference input and acceleration reference input should be zero.  \n\n![](images/c034dad81b2e2f776d3fe128db927032748e863059ebea1268d1278201ba8509.jpg)  \nFigure 8–40 Control system.  \n\nThe design of this two-degrees-of-freedom control system may be carried out by following the steps 1 and 2 below.  \n\n1. Determine $G_{c1}(s)$ so that the response to the step-disturbance input is of desirable characteristics. 2. Design $G_{c2}(s)$ so that the responses to the reference inputs are of desirable characteristics without changing the response to the step disturbance considered in step 1 .  \n\nDesign of $G_{c1}(s)$ :First, note that we assumed the noise input $N(s)$ to be zero.To obtain the response to the step-disturbance input, we assume that the reference input is zero. Then the block diagram which relates $Y(s)$ and $D(s)$ can be drawn as shown in Figure 8–40. The transfer function $Y(s)/D(s)$ is given by  \n\n$$\n\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+G_{c1}G_{p}}\n$$  \n\nwhere  \n\n$$\nG_{c1}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}+\\,T_{d}s\\bigg)\n$$  \n\nThis controller involves one pole at the origin and two zeros. If we assume that the two zeros are located at the same place (a double zero), then $G_{c1}(s)$ can be written as  \n\n$$\nG_{c1}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\nThen the characteristic equation for the system becomes  \n\n$$\n1\\,+\\,G_{c1}(s)G_{p}(s)\\,=\\,1\\,+\\,{\\frac{K(s\\,+\\,a)^{2}}{s}}{\\frac{5}{(s\\,+\\,1)(s\\,+\\,5)}}\\,=\\,0\n$$  \n\nor  \n\n$$\ns(s\\,+\\,1)(s\\,+\\,5)\\,+\\,5K(s\\,+\\,a)^{2}=0\n$$  \n\nwhich can be rewritten as  \n\n$$\ns^{3}\\,+\\,(6\\,+\\,5K)s^{2}\\,+\\,(5\\,+\\,10K a)s\\,+\\,5K a^{2}=0\n$$  \n\nIf we place the double zero between $s=-3$ and $s=-6$ ,then the root-locus plot of $G_{c1}(s)G_{p}(s)$ may look like the one shown in Figure 8–41. The speed of response should be fast, but not faster than necessary, because faster response generally implies larger or more expensive components. Therefore, we may choose the dominant closed-loop poles at  \n\n$$\ns=-3\\,\\pm\\,j2\n$$  \n\n(Note that this choice is not unique.There are infinitely many possible closed-loop poles that we may choose from.)  \n\nSince the system is of third order, there are three closed-loop poles. The third one is located on the negative real axis to the left of point $s\\,=-5$ .  \n\nLet us substitute $s\\,=\\,-3\\,+\\,j2$ into Equation (8–8).  \n\n$$\n(-3\\,+\\,j2)^{3}\\,+\\,(6\\,+\\,5K)(-3\\,+\\,j2)^{2}\\,+\\,(5\\,+\\,10K a)(-3\\,+\\,j2)\\,+\\,5K a^{2}=0\n$$  \n\n![](images/8c7232a82cff2148a5d17d309fa5d6e0b1c80703294b0963f67d88235978ca68.jpg)  \nFigure 8–41 $5K(s\\,+\\,_{a})^{2}\\bar{/}[s(s\\,+\\,1)$ Root-locus plots of D$\\left(s\\,+\\,5\\right)]$ when a=3, a=4, a=4.5, and a=6.  \n\nwhich can be simplified to  \n\n$$\n24\\,+\\,25K\\,-\\,30K a\\,+\\,5K a^{2}\\,+\\,j(-16\\,-\\,60K\\,+\\,20K a)\\,=\\,0\n$$  \n\nBy equating the real part and imaginary part to zero, respectively, we obtain  \n\n$$\n\\begin{array}{r}{24\\mathrm{~+~}25K\\mathrm{~-~}30K a\\mathrm{~+~}5K a^{2}\\mathrm{~=~}0}\\\\ {\\mathrm{~-}16\\mathrm{~-~}60K\\mathrm{~+~}20K a\\mathrm{~=~}0}\\end{array}\n$$  \n\nFrom Equation (8–10), we have  \n\n$$\nK=\\frac{4}{5a-15}\n$$  \n\nSubstituting Equation (8–11) into Equation (8–9), we get  \n\n$$\na^{2}=13\n$$  \n\nor $a=3.6056$ or $-3.6056$ .Notice that the values of $K$ become  \n\n$$\n\\begin{array}{l l}{{K=1.3210~~~~~~~~~~~}}&{{\\mathrm{for}\\;a=3.6056}}\\\\ {{{}}}&{{{}}}\\\\ {{K=-0.1211}}&{{\\mathrm{for}\\;a=-3.6056}}\\end{array}\n$$  \n\nSince $G_{c1}(s)$ is in the feedforward path, the gain $K$ should be positive. Hence, we choose  \n\n$$\nK=1.3210,\\;\\;\\;\\;\\;\\;a=3.6056\n$$  \n\nThen $G_{c1}(s)$ can be given by  \n\n$$\n\\begin{array}{r l}{\\phantom{\\frac{1}{1}}G_{c1}(s)\\,=\\,K\\,{\\frac{(s~+~a)^{2}}{s}}}\\\\ {\\phantom{\\frac{1}{1}}}&{\\,=\\,1.3210{\\frac{(s~+~3.6056)^{2}}{s}}}\\\\ &{\\,=\\,{\\frac{1.3210s^{2}\\,+\\,9.5260s\\,+\\,17.1735}{s}}}\\end{array}\n$$  \n\nTo determine $K_{p},T_{i}$ ,and $T_{d}$ ,we proceed as follows:  \n\n$$\n\\begin{array}{r l}{G_{c1}(s)=\\frac{1.3210\\left(s^{2}\\,+\\,7.2112s\\,+\\,13\\right)}{s}}\\\\ {~~}&{{}}\\\\ {=9.5260\\bigg(1\\,+\\,\\frac{1}{0.5547s}+\\,0.1387s\\bigg)}\\end{array}\n$$  \n\nThus,  \n\n$$\nK_{p}=9.5260,\\qquad T_{i}=0.5547,\\qquad T_{d}=0.1387\n$$  \n\nTo check the response to a unit-step disturbance input, we obtain the closed-loop transfer function $Y(s)/D(s)$ .  \n\n$$\n\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{G_{p}}{1+G_{c1}G_{p}}}\\\\ {=\\frac{5s}{s(s+1)(s+5)+5K(s+a)^{2}}}\\\\ {=\\frac{5s}{s^{3}+12.605s^{2}+52.63s+85.8673}\n$$  \n\nThe response to the unit-step disturbance input is shown in Figure 8–42. The response curve seems good and acceptable. Note that the closed-loop poles are located at $s\\,=\\,-3\\,\\pm\\,j2$ and $s=-6.6051$ .The complex-conjugate closed-loop poles act as dominant closed-loop poles.  \n\nDesign of $G_{c2}(s)$ :We now design $G_{c2}(s)$ to obtain the desired responses to the reference inputs. The closed-loop transfer function $Y(s)/R(s)$ can be given by  \n\n$$\n\\begin{array}{r l}&{\\frac{Y(s)}{R(s)}=\\frac{\\big(G_{\\mathrm{cl}}+G_{\\mathrm{cl}}\\big)G_{p}}{1+\\ G_{\\mathrm{cl}}G_{p}}}\\\\ &{\\qquad=\\frac{\\left[\\frac{1.321s^{2}+9.526s+17.1735}{s}+\\hat{K}_{p}(1+\\hat{T}_{d}s)\\right]\\frac{5}{\\left(s+1\\right)\\left(s+5\\right)}}{s}}\\\\ &{\\qquad\\qquad+\\frac{1.321s^{2}+9.526s+17.1735}{s}\\frac{5}{\\left(s+1\\right)\\left(s+5\\right)}}\\\\ &{\\qquad=\\frac{\\big(6.6051+5\\hat{K}_{p}\\hat{T}_{d}\\big)s^{2}+\\big(47.63+5\\hat{K}_{p}\\big)s+85.8673}{s^{3}+12.6051s^{2}+52.63s+85.8673}}\\end{array}\n$$  \n\nZero placement. We place two zeros together with the dc gain constant such that the numerator is the same as the sum of the last three terms of the denominator.That is,  \n\n$$\n(6.6051\\,+\\,5\\hat{K}_{p}\\hat{T}_{d})s^{2}\\,+\\,(47.63\\,+\\,5\\hat{K}_{p})s\\,+\\,85.8673\\,=\\,12.6051s^{2}\\,+\\,52.63s\\,+\\,85.8673\n$$  \n\nBy equating the coefficients of $s^{2}$ terms and $s$ terms on both sides of this last equation,  \n\n$$\n\\begin{array}{r}{6.6051\\,+\\,5\\hat{K}_{p}\\hat{T}_{d}=12.6051}\\\\ {47.63\\,+\\,5\\hat{K}_{p}=52.63}\\end{array}\n$$  \n\nfrom which we get  \n\nTherefore,  \n\n$$\n\\hat{K}_{p}\\,=\\,1,\\qquad\\hat{T}_{d}\\,=\\,1.2\n$$  \n\n$$\nG_{c2}(s)\\,=\\,1\\,+\\,1.2s\n$$  \n\n![](images/8c8fd79545d70d2d8cdef8f09023b6487d0d4efdefad3dd470c4675bc0278623.jpg)  \nFigure 8–42 Response to unitstep disturbance input.  \n\nWith this controller $G_{c2}(s)$ ,the closed-loop transfer function $Y(s)/R(s)$ becomes  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{12.6051s^{2}+\\,52.63s\\,+\\,85.8673}{s^{3}+\\,12.6051s^{2}\\,+\\,52.63s\\,+\\,85.8673}\n$$  \n\nThe response to the unit-step reference input becomes as shown in Figure 8–43(a).  \n\n# Figure 8–43  \n\n![](images/23632474deba35b39eca8a171539c4f1b5eae0bafe070d7a4078121e874938e1.jpg)  \n(a) Response to unitstep reference input; (b) response to unitramp reference input; (c) response to unit-acceleration reference input.  \n\n![](images/64ebe6eb893764624f63fb53c8955c91353f10d079537e1d77f0534684ad0eeb.jpg)  \n(c)  \n\nFigure 8–43 (continued)  \n\nThe response exhibits the maximum overshoot of $21\\%$ and the settling time is approximately 1.6 sec. Figures 8–43(b) and (c) show the ramp response and acceleration response. The steadystate errors in both responses are zero.The response to the step disturbance was satisfactory.Thus, the designed controllers $G_{c1}(s)$ and $G_{c2}(s)$ given by Equations (8–12) and (8–13), respectively, are satisfactory.  \n\nIf the response characteristics to the unit-step reference input are not satisfactory, we need to change the location of the dominant closed-loop poles and repeat the design process.The dominant closed-loop poles should lie in a certain region in the left-half $s$ plane (such as $2\\le a\\le6$ ,$2\\leq b\\leq6,6\\leq c\\leq12)$ . If the computational search is desired, write a computer program (similar to MATLAB Program 8–8) and execute the search process.Then a desired set or sets of values of $a,\\,b$ ,and $c$ may be found such that the system response to the unit-step reference input satisfies all requirements on maximum overshoot and settling time.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–8–1. Describe briefly the dynamic characteristics of the PI controller, PD controller, and PID controller.  \n\nSolution. The PI controller is characterized by the transfer function  \n\n$$\nG_{c}(s)\\,=\\,K_{p}\\bigg(1\\,+\\frac{1}{T_{i}s}\\bigg)\n$$  \n\nThe PI controller is a lag compensator. It possesses a zero at $s=-1/T_{i}$ and a pole at $s\\,=\\,0.$ .Thus, the characteristic of the PI controller is infinite gain at zero frequency. This improves the steady-state characteristics. However, inclusion of the PI control action in the system increases the type number of the compensated system by 1, and this causes the compensated system to be less stable or even makes the system unstable.Therefore, the values of $K_{p}$ and $T_{i}$ must be chosen carefully to ensure a proper transient response. By properly designing the PI controller, it is possible to make the transient response to a step input exhibit relatively small or no overshoot.The speed of response, however, becomes much slower. This is because the PI controller, being a low-pass filter, attenuates the high-frequency components of the signal.  \n\nThe PD controller is a simplified version of the lead compensator.The PD controller has the transfer function $G_{c}(s)$ ,where  \n\n$$\nG_{c}(s)\\,=\\,K_{p}(1\\,+\\,T_{d}s)\n$$  \n\nThe value of $K_{p}$ is usually determined to satisfy the steady-state requirement. The corner frequency $1/T_{d}$ is chosen such that the phase lead occurs in the neighborhood of the gain crossover frequency. Although the phase margin can be increased, the magnitude of the compensator continues to increase for the frequency region $1/T_{d}<\\omega$ .(Thus, the PD controller is a high-pass filter.) Such a continued increase of the magnitude is undesirable, since it amplifies high-frequency noises that may be present in the system. Lead compensation can provide a sufficient phase lead, while the increase of the magnitude for the high-frequency region is very much smaller than that for PD control. Therefore, lead compensation is preferred over PD control.  \n\nBecause the transfer function of the PD controller involves one zero, but no pole, it is not possible to electrically realize it by passive $R L C$ elements only. Realization of the PD controller using op amps, resistors, and capacitors is possible, but because the PD controller is a high-pass filter, as mentioned earlier, the differentiation process involved may cause serious noise problems in some cases.There is,however,no problem if the PD controller is realized by use of the hydraulic or pneumatic elements.  \n\nThe PD control, as in the case of the lead compensator, improves the transient-response characteristics, improves system stability, and increases the system bandwidth, which implies fast rise time.  \n\nThe PID controller is a combination of the PI and PD controllers.It is a lag–lead compensator. Note that the PI control action and PD control action occur in different frequency regions. The PI control action occurs at the low-frequency region and PD control action occurs at the highfrequency region.The PID control may be used when the system requires improvements in both transient and steady-state performances.  \n\nShow that the transfer function $U(s)/E(s)$ of the PID controller shown in Figure 8–44 is  \n\n$$\n\\frac{U(s)}{E(s)}=\\,K_{0}\\,\\frac{T_{1}\\,+\\,T_{2}}{T_{1}}\\left[1\\,+\\,\\frac{1}{\\left(T_{1}\\,+\\,T_{2}\\right)\\!s}+\\frac{T_{1}T_{2}s}{T_{1}\\,+\\,T_{2}}\\right]\n$$  \n\nAssume that the gain $K$ is very large compared with unity, or $K\\gg1$ .  \n\n![](images/00738beb23fc1b6c6e1fc414f98728bc5f933e5d323dff1acbaf11c0443408dd.jpg)  \n\nFigure 8–44 PID controller.  \n\nSolution  \n\n$$\n\\begin{array}{r l}{\\frac{\\dot{v}_{1}}{v_{1}}=\\frac{K}{1+K\\left(\\displaystyle\\frac{1}{K_{1}S_{1}}\\frac{1}{1+T_{2}S_{2}}\\right)}}&{{}}\\\\ {+\\frac{K}{K\\left(\\displaystyle\\frac{1}{K_{1}}\\frac{T_{2}S_{1}}{1+T_{2}S_{1}}\\frac{1}{1+T_{2}S_{2}}\\right)}}&{{}}\\\\ {=\\frac{K_{6}\\left(1+T_{3}\\right)\\left(1+T_{3}\\right)}{T_{1}s}}&{{}}\\\\ {=K_{6}\\left(1+\\displaystyle\\frac{1}{T_{1}s}\\right)\\left(1+T_{2}s\\right)}&{{}}\\\\ {=K_{6}\\left(1+\\displaystyle\\frac{1}{T_{2}s}+T_{3}+\\displaystyle\\frac{T_{2}}{T_{1}}\\right)}&{{}}\\\\ {=K_{8}\\frac{T_{1}+T_{2}}{T_{1}}\\left[1+\\displaystyle\\frac{1}{(T_{1}+T_{2})s}+\\frac{T_{1}T_{3}}{T_{1}+T_{2}}\\right]}&{{}}\\end{array}\n$$  \n\nA–8–3. Consider the electronic circuit involving two operational amplifiers shown in Figure 8–45. This is a modified PID controller in that the transfer function involves an integrator and a first-order lag term. Obtain the transfer function of this PID controller.  \n\nSolution. Since  \n\n$$\nZ_{1}={\\frac{1}{{\\frac{1}{R_{1}}}+C_{1}s}}+R_{3}={\\frac{R_{1}+R_{3}+R_{1}R_{3}C_{1}s}{1+R_{1}C_{1}s}}\n$$  \n\nand  \n\n$$\nZ_{2}=R_{2}+\\frac{1}{C_{2}s}\n$$  \n\nwe have  \n\n$$\n\\frac{E(s)}{E_{i}(s)}=-\\,\\frac{Z_{2}}{Z_{1}}=-\\,\\frac{\\big(R_{2}C_{2}s\\,+\\,1\\big)\\big(R_{1}C_{1}s\\,+\\,1\\big)}{C_{2}s\\big(R_{1}\\,+\\,R_{3}\\,+\\,R_{1}R_{3}C_{1}s\\big)}\n$$  \n\nAlso,  \n\n![](images/fada7d54fc37f60bd9864fbe0e2a7a91bd8059671c675000962f113eaf4a7e59.jpg)  \nFigure 8–45 Modified PID controller.  \n\n![](images/d42ba1d9b3b2079f095e8fabb97d743363c65ca3379d6f4347bb297ca41278dc.jpg)  \nFigure 8–46 Approximate differentiator.  \n\nConsequently,  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{E_{o}(s)}{E_{i}(s)}=\\frac{E_{o}(s)}{E(s)}\\frac{E(s)}{E_{i}(s)}=\\frac{R_{s}}{R_{4}(R_{1}+R_{3})C_{2}}\\frac{\\big(R_{1}C_{1}s\\,+1\\big)\\big(R_{2}C_{2}s\\,+1\\big)}{s\\Big(\\frac{R_{1}R_{3}}{R_{1}+R_{3}}C_{1}s\\,+1\\Big)}}\\\\ {\\displaystyle=\\frac{R_{s}R_{2}}{R_{4}R_{3}}\\frac{\\left(s+\\frac{1}{R_{1}C_{1}}\\right)\\left(s+\\frac{1}{R_{2}C_{2}}\\right)}{s\\left(s+\\frac{R_{1}+R_{3}}{R_{1}R_{3}C_{1}}\\right)}}\\end{array}\n$$  \n\nNotice that $R_{1}C_{1}$ and $R_{2}C_{2}$ determine the locations of the zeros of the controller, while $R_{1},R_{3}$ ,and $C_{1}$ affect the location of the pole on the negative real axis. ${R}_{5}/{R}_{4}$ adjusts the gain of the controller.  \n\nA–8–4. In practice, it is impossible to realize the true differentiator. Hence, we always have to approximate the true differentiator $T_{d}s$ by something like  \n\n$$\n\\frac{T_{d}s}{1\\,+\\,\\gamma T_{d}s}\n$$  \n\nOne way to realize such an approximate differentiator is to utilize an integrator in the feedback path. Show that the closed-loop transfer function of the system shown in Figure 8–46 is given by the preceding expression. (In the commercially available differentiator, the value of $\\gamma$ may be set as 0.1.)  \n\nSolution. The closed-loop transfer function of the system shown in Figure 8–46 is  \n\n$$\n{\\frac{C(s)}{R(s)}}={\\frac{\\frac{1}{\\gamma}}{1+{\\frac{1}{\\gamma T_{d}s}}}}={\\frac{T_{d}s}{1\\,+\\,{\\gamma T_{d}s}}}\n$$  \n\nNote that such a differentiator with first-order delay reduces the bandwidth of the closed-loop control system and reduces the detrimental effect of noise signals.  \n\nA–8–5. Consider the system shown in Figure 8–47. This is a PID control of a second-order plant $G(s)$ .Assume that disturbances $D(s)$ enter the system as shown in the diagram. It is assumed that the reference input $R(s)$ is normally held constant, and the response characteristics to disturbances are a very important consideration in this system.  \n\nFigure 8–47 PID-controlled system.  \n\n![](images/0dee9f7ac888bd6305b3db12822c769c396419fb3c5787ce1f67402a6ca2c355.jpg)  \n\nDesign a control system such that the response to any step disturbance will be damped out quickly (in 2 to 3 sec in terms of the $2\\%$ settling time). Choose the configuration of the closed-loop poles such that there is a pair of dominant closed-loop poles. Then obtain the response to the unit-step disturbance input.Also, obtain the response to the unit-step reference input.  \n\nSolution. The PID controller has the transfer function  \n\n$$\nG_{c}(s)\\,=\\frac{K(a s\\,+\\,1)(b s\\,+\\,1)}{s}\n$$  \n\nFor the disturbance input in the absence of the reference input, the closed-loop transfer function becomes  \n\n$$\n\\begin{array}{c}{\\displaystyle\\frac{C_{d}(s)}{D(s)}=\\frac{s}{s\\big(s^{2}+3.6s+9\\big)+K(a s+1)(b s+1)}}\\\\ {\\displaystyle=\\frac{s}{s^{3}+(3.6+K a b)s^{2}+(9+K a+K b)s+K}}\\end{array}\n$$  \n\nThe specification requires that the response to the unit-step disturbance be such that the settling time be 2 to 3 sec and the system have a reasonable damping.We may interpret the specification as $\\zeta=0.5$ and $\\omega_{n}=4$ rad 'sec for the dominant closed-loop poles.We may choose the third pole at $s=-10$ so that the effect of this real pole on the response is small. Then the desired characteristic equation can be written as  \n\n$$\n(s~+~10){\\left({{s^{2}}+2\\times0.5\\times4s+4^{2}}\\right)}=(s~+~10){\\left({{s^{2}}+4s+16}\\right)}=s^{3}+~14s^{2}+56s~+~16s^{3}+~257s^{2}\n$$  \n\nThe characteristic equation for the system given by Equation (8–14) is  \n\n$$\ns^{3}\\,+\\,(3.6\\,+\\,K a b)s^{2}\\,+\\,(9\\,+\\,K a\\,+\\,K b)s\\,+\\,K\\,=\\,0\n$$  \n\nHence, we require  \n\n$$\n\\begin{array}{c}{3.6\\,+\\,K a b\\,=\\,14}\\\\ {9\\,+\\,K a\\,+\\,K b\\,=\\,56}\\\\ {K=\\,160}\\end{array}\n$$  \n\nwhich yields  \n\n$$\na b=0.065,~~~~~a+b=0.29375\n$$  \n\nThe PID controller now becomes  \n\n$$\n\\begin{array}{r l}{G_{c}(s)=\\frac{K\\left[a b s^{2}\\,+\\,\\left(a\\,+\\,b\\right)s\\,+\\,1\\right]}{s}}\\\\ {~~}&{=\\frac{160\\left(0.065s^{2}\\,+\\,0.29375s\\,+\\,1\\right)}{s}}\\\\ {~~}&{=\\frac{10.4\\left(s^{2}\\,+\\,4.5192s\\,+\\,15.385\\right)}{s}}\\end{array}\n$$  \n\nWith this PID controller, the response to the disturbance is given by  \n\n$$\n\\begin{array}{c}{{C_{d}(s)\\,=\\displaystyle\\frac{s}{s^{3}\\,+\\,14s^{2}\\,+\\,56s\\,+\\,160}\\,D(s)}}\\\\ {{=\\displaystyle\\frac{s}{(s\\,+\\,10)\\big(s^{2}\\,+\\,4s\\,+\\,16\\big)}\\,D(s)}}\\end{array}\n$$  \n\nClearly, for a unit-step disturbance input, the steady-state output is zero, since  \n\n$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}c_{d}(t)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}s C_{d}(s)\\,=\\,\\operatorname*{lim}_{s\\rightarrow0}\\frac{s^{2}}{(s\\,+\\,10)\\bigl(s^{2}\\,+\\,4s\\,+\\,16\\bigr)}\\frac{1}{s}=0\n$$  \n\nThe response to a unit-step disturbance input can be obtained easily with MATLAB. MATLAB Program 8–9 produces a response curve as shown in Figure 8–48(a). From the response curve, we see that the settling time is approximately 2.7 sec.The response damps out quickly.Therefore, the system designed here is acceptable.  \n\n![](images/674fed3037596a494220d1dbc7e2e02cb6ccd3a3925486980e28b2e4513acd50.jpg)  \n\nFor the reference input $r(t)$ ,the closed-loop transfer function is  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{C_{r}(s)}{R(s)}=\\frac{10.4\\big(s^{2}\\,+\\,4.5192s\\,+\\,15.385\\big)}{s^{3}\\,+\\,14s^{2}\\,+\\,56s\\,+\\,160}}}\\\\ {\\,}\\\\ {\\,=\\displaystyle{\\frac{10.4s^{2}\\,+\\,47s\\,+\\,160}{s^{3}\\,+\\,14s^{2}\\,+\\,56s\\,+\\,160}}}\\end{array}\n$$  \n\nThe response to a unit-step reference input can also be obtained by use of MATLAB Program 8–9. The resulting response curve is shown in Figure 8–48(b).The response curve shows that the maximum overshoot is $7.3\\%$ and the settling time is 1.2 sec.The system has quite acceptable response characteristics.  \n\n![](images/c5e6d1da460871a3ccfefd438beb840ae38b61a8fe0026a81573b3359cb969c9.jpg)  \nResponse to Unit-Step Reference Input  \n\n![](images/333a2afdf6bf22a367f2e88129e094d18b67cd260f5f4b365c01dfe85464af9b.jpg)  \nFigure 8–48 (a) Response to unit-step disturbance input; (b) response to unit-step reference input.  \n\nA–8–6. Consider the system shown in Figure 8–49. It is desired to design a PID controller $G_{c}(s)$ such that the dominant closed-loop poles are located at $s=-1\\,\\\\Bar{\\pm}\\,j\\sqrt{3}$ .For the PID controller, choose $a=1$ and then determine the values of $K$ and $^b$ .Sketch the root-locus diagram for the designed system.  \n\nSolution. Since  \n\n$$\nG_{c}(s)G(s)\\,=\\,K\\,\\frac{(s\\,+\\,1)(s\\,+\\,b)}{s}\\frac{1}{s^{2}\\,+\\,1}\n$$  \n\n# Figure 8–49 PID-controlled system.  \n\n![](images/71c7dcf02e47e994d74dd34bf5f7f09c46cc8b95a0886a08540b83aaedc16248.jpg)  \n\nthe sum of the angles at $s=-1\\,+\\,j{\\sqrt{3}}$ ,one of the desired closed-loop poles, from the zero at $s=-1$ and poles at $s\\,=\\,0,s\\,=\\,j$ ,and $s=-j$ is  \n\n$$\n90^{\\circ}\\mathrm{~-~}143.794^{\\circ}\\mathrm{~-~}120^{\\circ}\\mathrm{~-~}110.104^{\\circ}\\mathrm{~=~}-283.898^{\\circ}\n$$  \n\nHence the zero at $s=-b$ must contribute $103.898^{\\circ}$ .This requires that the zero be located at  \n\n$$\nb=0.5714\n$$  \n\nThe gain constant $K$ can be determined from the magnitude condition.  \n\n$$\n\\left|K\\,{\\frac{(s\\,+\\,1)(s\\,+\\,0.5714)}{s}}{\\frac{1}{s^{2}\\,+\\,1}}\\right|_{s=-1+j\\sqrt3}=1\n$$  \n\nor  \n\n$$\nK\\,=\\,2.3333\n$$  \n\nThen the compensator can be written as follows:  \n\n$$\nG_{c}(s)\\,=\\,2.3333\\,{\\frac{(s\\,+\\,1)(s\\,+\\,0.5714)}{s}}\n$$  \n\nThe open-loop transfer function becomes  \n\n$$\nG_{c}(s)G(s)\\,=\\frac{2.3333(s\\,+\\,1)(s\\,+\\,0.5714)}{s}\\frac{1}{s^{2}\\,+\\,1}\n$$  \n\nFrom this equation a root-locus plot for the compensated system can be drawn. Figure 8–50 is a root-locus plot.  \n\n![](images/def262e9252b6358137c567ffd9b507219227854e022cefff5d207c67cae5d7a.jpg)  \nFigure 8–50 Root-locus plot of the compensated system.   \nReal Axis  \n\n![](images/d2518579ce7273c2b3b3ff13400154e37037d0e00abd805eead86f7ee6ce25e5.jpg)  \nFigure 8–51 Unit-step response of the compensated system.  \n\nThe closed-loop transfer function is given by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{2.3333(s+1)(s+0.5714)}{s^{3}+s+2.3333(s+1)(s+0.5714)}\n$$  \n\nThe closed-loop poles are located at $s=-1\\,\\pm\\,j{\\sqrt{3}}$ and $s\\,=\\,-0.3333.$ A unit-step response curve is shown in Figure 8–51. The closed-loop pole at $s\\,=\\,-0.3333$ and a zero at $s\\,=\\,-0.5714$ produce a long tail of small amplitude.  \n\nA–8–7. Consider the system shown in Figure 8–52. Design a compensator such that the static velocity error constant is $4\\;\\mathrm{sec}^{-1}$ , phase margin is $50^{\\circ}$ , and gain margin is $10\\,\\mathrm{dB}$ or more. Plot unit-step and unit-ramp response curves of the compensated system with MATLAB.Also, draw a Nyquist plot of the compensated system with MATLAB. Using the Nyquist stability criterion, verify that the designed system is stable.  \n\nSolution. Since the plant does not have an integrator, it is necessary to have an integrator in the compensator. Let us choose the compensator to be  \n\n$$\nG_{c}(s)\\,=\\,\\frac K{s}\\,\\hat{G}_{c}(s),\\quad\\operatorname*{lim}_{s\\to0}\\hat{G}_{c}(s)\\,=\\,1\n$$  \n\nwhere $\\hat{G}_{c}(s)$ is to be determined later. Since the static velocity error constant is specified as $4\\;\\mathrm{sec}^{-1}$ , we have  \n\n$$\nK_{v}=\\operatorname*{lim}_{s\\rightarrow0}s G_{c}(s){\\frac{s\\,+\\,0.1}{s^{2}\\,+\\,1}}=\\operatorname*{lim}_{s\\rightarrow0}s\\,{\\frac{K}{s}}\\,\\hat{G}_{c}(s){\\frac{s\\,+\\,0.1}{s^{2}\\,+\\,1}}=0.1K\\,=\\,4\n$$  \n\nFigure 8–52 Control system.  \n\n![](images/65983b33580c33ca936d2fae1986a8997b14bf64b64966603372eaa55bcb12c5.jpg)  \nChapter 8 /PID Controllers and Modified PID Controllers  \n\nThus, $K=40,$ . Hence  \n\n$$\nG_{c}(s)=\\frac{40}{s}\\,\\hat{G}_{c}(s)\n$$  \n\nNext, we plot a Bode diagram of  \n\n$$\nG(s)=\\frac{40(s+0.1)}{s(s^{2}+1)}\n$$  \n\nMATLAB Program 8–10 produces a Bode diagram of $G(s)$ as shown in Figure 8–53.  \n\n![](images/ffbb51527ca23388730aae9388c2621256f136fc1a9764a07f8cf7ad799ae388.jpg)  \n\nWe need the phase margin of $50^{\\circ}$ and gain margin of $10\\;\\mathrm{dB}$ or more. Let us choose $\\hat{G}_{c}(s)$ to be  \n\n$$\n\\hat{G}_{c}(s)\\,=\\,a s\\,+\\,1\\;\\;\\;\\;(a>0)\n$$  \n\nThen $G_{c}(s)$ will contribute up to $90^{\\circ}$ phase lead in the high-frequency region.By simple MATLAB trials, we find that $a\\,=\\,0.1526$ gives the phase margin of $50^{\\circ}$ and gain margin of $+\\infty$ dB.  \n\n![](images/dedfa5c740f5bb4aa7d76a835b0bfd021543b71c8562101ec81f71c8322b45e5.jpg)  \nFigure 8–53 Bode diagram of $G(s)=$ $40(s\\,+\\,0.1)/[s(s^{2}\\,+\\,1)].$  \n\nSee MATLAB Program 8–11 and the resulting Bode diagram shown in Figure 8–54. From this Bode diagram we see that the static velocity error constant is $4\\,\\mathrm{sec}^{-1}$ , phase margin is $50^{\\circ}$ and gain margin is +q dB.Therefore, the designed system satisfies all the requirements.  \n\n![](images/e5c29ea2b9eba99954a5b1225638d2ffc67c9e15dc9429d352789a65c4c57c16.jpg)  \n\nThe designed compensator has the following transfer function:  \n\n$$\nG_{c}(s)\\,=\\frac{40}{s}\\,\\hat{G}_{c}(s)\\,=\\frac{40(0.1526s\\,+\\,1)}{s}\n$$  \n\n![](images/1d82a257422791c85ae9f4b41fa1677f161530a2e3ef4bcce1f118eaced5e311.jpg)  \nFigure 8–54 Bode diagram of $G(s)\\,=\\,40(s\\,+\\,0.1)$ $(0.1526s\\,+\\,1)/$ $\\left[s(s^{2}+1)\\right]$ .  \n\nThe open-loop transfer function of the designed system is  \n\n$$\n{\\begin{array}{r l}&{{\\mathrm{Open-loop~transfer~function}}={\\frac{40\\left(0.1526s\\;+\\;1\\right)}{s}}{\\frac{s\\;+\\;0.1}{s^{2}\\;+\\;1}}}\\\\ &{={\\frac{6.104s^{2}\\;+\\;40.6104s\\;+\\;4}{s(s^{2}\\;+\\;1)}}.}\\end{array}}\n$$  \n\nWe shall next check the unit-step response and the unit-ramp response of the designed system. The closed-loop transfer function is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{6.104s^{2}\\,+\\,40.6104s\\,+\\,4}{s^{3}\\,+\\,6.104s^{2}\\,+\\,41.6104s\\,+\\,4}\n$$  \n\nThe closed-loop poles are located at  \n\n$$\n\\begin{array}{l}{{s\\,=\\,-3.0032\\,+\\,j5.6573}}\\\\ {{s\\,=\\,-3.0032\\,-\\,j5.6573}}\\\\ {{s\\,=\\,-0.0975}}\\end{array}\n$$  \n\nMATLAB Program 8–12 will produce the unit-step response curve of the designed system.The resulting unit-step response curve is shown in Figure 8–55. Notice that the closed-loop pole at $s=-0.0975$ and the plant zero at $s=-0.1$ produce a long tail of small amplitude.  \n\n![](images/2a6c35a24b116234fb0b4d3e8682e15aa2d759ff8e401ceb511db51dffe63c1c.jpg)  \n\n![](images/e6b8f343bb01930d2043757e9fa5c67a523d318ddfc066bda1ff0179847baf8d.jpg)  \nFigure 8–55 Unit-step response of $C(s)/R(s)\\,=\\,(6.104s^{2}\\,+$ $40.6104s\\,+\\,4)/(s^{3}\\,+$ $6.104s^{2}\\,+\\,41.6104s\\,+\\,4)$ .  \n\n![](images/a4541eeab0eca0078bf4fdc7f98d9dd5389567f08505db98acc0d5e4a7a55482.jpg)  \nFigure 8–56 Unit-ramp response of $C(s)/R(s)=$ $^{\\left(6.104s^{2}\\right.+\\left.40.6104s\\right.+}$ $4)/(s^{3}\\,+\\,6.104s^{2}\\,+$ $41.6104s\\,+\\,4)$ .  \n\nMATLAB Program 8–13 produces the unit-ramp response curve of the designed system. The resulting response curve is shown in Figure 8–56.  \n\n![](images/7b040f5520cc7f533ec771cfd4c9774a4cc0332a90763a28f7f2e51b2a3171df.jpg)  \n\nNyquist Plot. Earlier we found that the three closed-loop poles of the designed system are all in the left-half $s$ plane. Hence the designed system is stable. The purpose of plotting Nyquist diagram here is not to test the stability of the system, but to enhance our understanding of Nyquist stability analysis. For a complicated system, Nyquist plot will look complicated enough that it is not easy to count the number of encirclements of the $-1+j0$ point.  \n\n![](images/35d21f0ea50857b8405e5bb2ad258e4606ae5ef22ce857dc68b2abd944982631.jpg)  \nFigure 8–57 (a) Modified Nyquist path in the $s$ plane; (b) Nyquist path in the $s_{1}$ plane.  \n\nBecause the designed system involves three open-loop poles on the $j w$ axis, the Nyquist diagram will look quite complicated as we will see in what follows: Define the open-loop transfer function of the designed system as $G(s)$ .Then  \n\n$$\nG(s)=G_{c}(s)\\frac{s+0.1}{s^{2}+1}=\\frac{6.104s^{2}+40.6104s\\,+\\,4}{s(s^{2}+1)}\n$$  \n\nLet us choose a modified Nyquist path in the $s$ plane as shown in Figure 8–57(a). The modified path encloses three open-loop poles $(s\\,=\\,0,\\,s\\,=\\,j1,\\,s\\,=\\,-j1)$ . Now define $s_{1}=s+\\sigma_{0}$ . Then, the Nyquist path in the $s_{1}$ plane becomes as shown in Figure 8–57(b). In the $s_{1}$ plane, the openloop transfer function has three poles in the right-half $s_{1}$ plane.  \n\nLet us choose $\\sigma_{0}=0.01$ Since $s\\,=\\,s_{1}\\,-\\,\\sigma_{0}$ ,we have  \n\n$$\nG(s)=G(s_{1}-0.01)\n$$  \n\nOpen-loop transfer function in the $s_{1}$ plane  \n\n$$\n\\begin{array}{l}{{={\\frac{6.104(s_{1}^{2}\\,-\\,0.02s_{1}\\,+\\,0.0001)\\,+\\,40.6104(s_{1}\\,-\\,0.01)\\,+\\,4}{(s_{1}\\,-\\,0.01)(s_{1}^{2}\\,-\\,0.02s_{1}\\,+\\,1.0001)}}}}\\\\ {{\\qquad\\qquad\\qquad(s_{1}\\,-\\,0.01)(s_{1}^{2}\\,-\\,0.02s_{1}\\,+\\,1.0001)}}\\\\ {{={\\frac{6.104s_{1}^{2}\\,+\\,40.48832s_{1}\\,+\\,3.5945064}{s_{1}^{3}\\,-\\,0.03s_{1}^{2}\\,+\\,1.0003s_{1}\\,-\\,0.010001}}}}\\end{array}\n$$  \n\nA MATLAB program to obtain the Nyquist plot is shown in MATLAB Program 8–14. The resulting Nyquist plot is shown in Figure 8–58.  \n\n![](images/8ee2287f8d8361d9cef6a510f6837f0b59f2886b22636832f760f64af6631ddd.jpg)  \n\n![](images/a808d6f7ec3e00b21f7a0e5efbdb6a3b56b8f078f6429b32a6fc6090adc017a3.jpg)  \nFigure 8–58 Nyquist plot.  \n\nFigure 8–59   \nRedrawn Nyquist plot.  \n\nUsing the Nyquist plot obtained here, it is not easy to determine the encirclements of the $-1+j0$ point by the Nyquist locus. Therefore, we need to redraw this Nyquist plot qualitatively to show the details near the $-1+j0$ point. Such a redrawn Nyquist diagram is shown in Figure 8–59.  \n\nFrom this diagram we find that the $-1+j0$ point is encircled counterclockwise three times. Hence, $N=-3$ . Since the open-loop transfer function has three poles in the right-half $s_{1}$ plane, we have $P=3.$ .Then, we have $Z=N\\,+\\,P=0.$ This means that there are no closed-loop poles in the right-half $s_{1}$ plane.The system is therefore stable.  \n\nA–8–8. Show that the I-PD-controlled system shown in Figure 8–60(a) is equivalent to the PID-controlled system with input filter shown in Figure 8–60(b).  \n\n![](images/a31153345fe96a68840b94e932ada14b316f3ae20db482c261e8483ea7f5cdef.jpg)  \nFigure 8–60 (a) I-PD-controlled system; (b) PID-controlled system with input filter.  \n\nSolution. The closed-loop transfer function $C(s)/R(s)$ of the I-PD-controlled system is  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{\\displaystyle{\\frac{K_{p}}{T_{i}s}}G_{p}(s)}{\\displaystyle1+\\,K_{p}\\bigg(1+\\frac{1}{T_{i}s}+T_{d}s\\bigg)G_{p}(s)}\n$$  \n\nThe closed-loop transfer function $C(s)/R(s)$ of the PID-controlled system with input filter shown in Figure 8–60(b) is  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{C(s)}{R(s)}=\\frac{1}{1\\ +\\ T_{i}s\\ +\\ T_{i}T_{d}s^{2}}\\frac{K_{p}\\bigg(1\\ +\\frac{1}{T_{i}s}\\ +\\ T_{d}s\\bigg)G_{p}(s)}{1\\ +\\ K_{p}\\bigg(1\\ +\\frac{1}{T_{i}s}\\ +\\ T_{d}s\\bigg)G_{p}(s)}}\\\\ {\\displaystyle=\\frac{\\frac{K_{p}}{T_{i}s}G_{p}(s)}{1\\ +\\ K_{p}\\bigg(1\\ +\\frac{1}{T_{i}s}\\ +\\ T_{d}s\\bigg)G_{p}(s)}}\\end{array}\n$$  \n\nThe closed-loop transfer functions of both systems are the same.Thus,the two systems are equivalent.  \n\nA–8–9. The basic idea of the I-PD control is to avoid large control signals (which will cause a saturation phenomenon) within the system. By bringing the proportional and derivative control actions to the feedback path, it is possible to choose larger values for $K_{p}$ and $T_{d}$ than those possible by the PID control scheme.  \n\nCompare,qualitatively,the responses of the PID-controlled system and I-PD-controlled system to the disturbance input and to the reference input.  \n\nSolution. Consider first the response of the I-PD-controlled system to the disturbance input. Since, in the I-PD control of a plant, it is possible to select larger values for $K_{p}$ and $T_{d}$ than those of the PID-controlled case, the I-PD-controlled system will attenuate the effect of disturbance faster than the PID-controlled case.  \n\nNext, consider the response of the I-PD-controlled system to a reference input. Since the I-PD-controlled system is equivalent to the PID-controlled system with input filter (refer to Problem A–8–8 ),the PID-controlled system will have faster responses than the corresponding I-PD-controlled system, provided a saturation phenomenon does not occur in the PID-controlled system.  \n\nA–8–10. In some cases it is desirable to provide an input filter as shown in Figure 8–61(a). Notice that the input filter $G_{f}(s)$ is outside the loop. Therefore, it does not affect the stability of the closedloop portion of the system.An advantage of having the input filter is that the zeros of the closed-loop transfer function can be modified (canceled or replaced by other zeros) so that the closedloop response is acceptable.  \n\nShow that the configuration in Figure 8–61(a) can be modified to that shown in Figure 8–61(b), where $G_{d}(s)\\,=\\,\\bigl[G_{f}(s)\\,\\stackrel{\\leftarrow}{-}\\,1\\bigr]G_{c}(s).$ .The compensation structure shown in Figure 8–61(b) is sometimes called command compensation.  \n\nSolution. For the system of Figure 8–61(a), we have  \n\n$$\n\\frac{C(s)}{R(s)}=G_{f}(s)\\,\\frac{G_{c}(s)G_{p}(s)}{1\\,+\\,G_{c}(s)G_{p}(s)}\n$$  \n\nFor the system of Figure 8–61(b), we have  \n\n$$\n\\begin{array}{l}{{U(s)=G_{d}(s)R(s)\\,+\\,G_{c}(s)E(s)}}\\\\ {{E(s)=R(s)\\,-\\,C(s)}}\\\\ {{C(s)=G_{p}(s)U(s)}}\\end{array}\n$$  \n\nThus  \n\nor  \n\n$$\n\\begin{array}{c}{{C(s)\\,=\\,G_{p}(s)\\bigl\\{G_{d}(s)R(s)\\,+\\,G_{c}(s)\\bigl[R(s)\\,-\\,C(s)\\bigr]\\bigr\\}}}\\\\ {{{}}}\\\\ {{{\\displaystyle\\frac{C(s)}{R(s)}=\\frac{\\bigl[G_{d}(s)\\,+\\,G_{c}(s)\\bigr]G_{p}(s)}{1\\,+\\,G_{c}(s)G_{p}(s)}}}}\\end{array}\n$$  \n\nBy substituting $G_{d}(s)\\,=\\,\\bigl[G_{f}(s)\\,-\\,1\\bigr]G_{c}(s)$ into Equation (8–16), we obtain  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{C(s)}{R(s)}=\\frac{\\big[G_{f}(s)G_{c}(s)\\,-\\,G_{c}(s)\\,+\\,G_{c}(s)\\big]G_{p}(s)}{1\\,+\\,G_{c}(s)G_{p}(s)}}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,G_{f}(s)\\,\\frac{G_{c}(s)G_{p}(s)}{1\\,+\\,G_{c}(s)G_{p}(s)}}\\end{array}\n$$  \n\n![](images/2d125bad5b28985070cd58966cfa17ac2f783803eec82fa93385f54c23893f0d.jpg)  \nFigure 8–61 (a) Block diagram of control system with input filter; (b) modified block diagram.  \n\nwhich is the same as Equation (8–15). Hence, we have shown that the systems shown in Figures 8–61(a) and (b) are equivalent.  \n\nIt is noted that the system shown in Figure 8–61(b) has a feedforward controller $G_{d}(s)$ .In such a case, $G_{d}(s)$ does not affect the stability of the closed-loop portion of the system.  \n\nA–8–11. A closed-loop system has the characteristic that the closed-loop transfer function is nearly equal to the inverse of the feedback transfer function whenever the open-loop gain is much greater than unity.  \n\nThe open-loop characteristic may be modified by adding an internal feedback loop with a characteristic equal to the inverse of the desired open-loop characteristic. Suppose that a unity-feedback system has the open-loop transfer function  \n\n$$\nG(s)\\,=\\,\\frac{K}{(T_{1}s\\,+\\,1)(T_{2}s\\,+\\,1)}\n$$  \n\nDetermine the transfer function $H(s)$ of the element in the internal feedback loop so that the inner loop becomes ineffective at both low and high frequencies.  \n\nSolution. Figure 8–62(a) shows the original system. Figure 8–62(b) shows the addition of the internal feedback loop around $G(s)$ .Since  \n\n$$\n{\\frac{C(s)}{E(s)}}={\\frac{G(s)}{1\\,+\\,G(s)H(s)}}={\\frac{1}{H(s)}}{\\frac{G(s)H(s)}{1\\,+\\,G(s)H(s)}}\n$$  \n\nif the gain around the inner loop is large compared with unity, then $G(s)H(s)/[1\\,+\\,G(s)H(s)]$ CDis approximately equal to unity,and the transfer function $C(s)/E(s)$ is approximately equal to $1/H(s)$ .On the other hand, if the gain $\\left|G(s)H(s)\\right|$ is much less than unity, the inner loop becomes ineffective and $C(s)/E(s)$ becomes approximately equal to $G(s)$ .  \n\nTo make the inner loop ineffective at both the low- and high-frequency ranges, we require that  \n\n$$\n|G(j\\omega)H(j\\omega)|\\,\\ll\\,1,\\;\\;\\;\\mathrm{for}\\;\\omega\\,\\ll\\,1\\;\\mathrm{and}\\;\\omega\\,\\gg\\,1\n$$  \n\nSince, in this problem,  \n\n$$\nG(j\\omega)\\,=\\frac{K}{(1\\,+\\,j\\omega T_{1})(1\\,+\\,j\\omega T_{2})}\n$$  \n\nFigure 8–62 (a) Control system; (b) addition of the internal feedback loop to modify the closed-loop characteristic.  \n\n![](images/9b1b1c7c95d24c1cd5f275757f97ae8ec4a5ed3ec01aa48094310fd92957e0f4.jpg)  \n\nthe requirement can be satisfied if $H(s)$ is chosen to be  \n\n$$\nH(s)=k s\n$$  \n\nbecause  \n\n$$\n\\operatorname*{lim}_{\\omega\\to0}G(j\\omega)H(j\\omega)\\,=\\,\\operatorname*{lim}_{\\omega\\to0}\\frac{K k j\\omega}{\\big(1\\,+\\,j\\omega T_{1}\\big)\\big(1\\,+\\,j\\omega T_{2}\\big)}=0\n$$  \n\nThus, with $H(s)\\,=\\,k s$ (velocity feedback), the inner loop becomes ineffective at both the lowand high-frequency regions. It becomes effective only in the intermediate-frequency region.  \n\nA–8–12. Consider the control system shown in Figure 8–63. This is the same system as that considered in Example 8–1.In that example we designed a PID controller $G_{c}(s)$ ,starting with the second method of the Ziegler–Nichols tuning rule. Here we design a PID controller using the computational approach with MATLAB.We shall determine the values of $K$ and $a$ of the PID controller  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\nsuch that the unit-step response will exhibit the maximum overshoot between $10\\%$ and $2\\%$ ($\\left[1.02\\right.\\leq$ maximum output $\\leq1.10$ ) and the settling time will be less than 3 sec.The search region is  \n\n$$\n2\\leq K\\leq50,\\;\\;\\;\\;\\;\\;0.05\\leq a\\leq2\n$$  \n\nLet us choose the step size for $K$ to be 1 and that for $a$ to be 0.05.  \n\nWrite a MATLAB program to find the first set of variables $K$ and $a$ that will satisfy the given specifications.Also, write a MATLAB program to find all possible sets of variables $K$ and $a$ that will satisfy the given specifications. Plot the unit-step response curves of the designed system with the chosen sets of variables $K$ and $a$ .  \n\nSolution. The transfer function of the plant is  \n\n$$\nG_{p}(s)={\\frac{1}{s^{3}+6s^{2}+5s}}\n$$  \n\nThe closed-loop transfer function $C(s)/R(s)$ is given by  \n\n$$\n\\frac{C(s)}{R(s)}=\\frac{K s^{2}+2K a s\\,+\\,K a^{2}}{s^{4}\\,+\\,6s^{3}\\,+\\,(5\\,+\\,K)s^{2}\\,+\\,2K a s\\,+\\,K a^{2}}\n$$  \n\nA possible MATLAB program that will produce the first set of variables $K$ and $a$ that will satisfy the given specifications is given in MATLAB Program 8–15. In this program we  \n\n![](images/44de69bb454f34021adf1ec529c550d69929d3e81fe7ef0d42691ab3ee1ccbcf.jpg)  \n\nFigure 8–63 Control system.  \n\nuse two ‘for’ loops. The specification for the settling time is interpreted by the following four lines:  \n\n$$\n\\begin{array}{r l}&{\\mathsf{s}=501;\\mathsf{w h i l e}\\,\\mathsf{y}(s)>0.98\\;\\mathsf{a n d}\\;\\mathsf{y}(s)<1.02;}\\\\ &{\\mathsf{s}=\\mathsf{s}-1\\,;\\mathsf{e n d};}\\\\ &{\\mathsf{t s}=(\\mathsf{s}\\,-\\,1)\\,^{\\ast}\\,0.01}\\\\ &{\\mathsf{t s}<3.0}\\end{array}\n$$  \n\nNote that for $t\\,=\\,0\\colon0.01\\colon5$ , we have 501 computing time points. $s\\,=\\,501$ corresponds to the last computing time point.  \n\nThe solution obtained by this program is  \n\n$$\nK\\,=\\,32,\\qquad a\\,=\\,0.2\n$$  \n\nwith the maximum overshoot equal to $9.69\\%$ and the settling time equal to 2.64 sec.The resulting unit-step response curve is shown in Figure 8–64.  \n\n![](images/b017cb5b930becf672cd2781a9ec0d32f20610f32cbd7660218362ab1e80fbf9.jpg)  \n\nFigure 8–64   \nUnit-step response curve.  \n\n![](images/af359d1c929008f937aa46867fc5d432ced247f5f746f88498d10341ba70a1be.jpg)  \n\nNext, we shall consider the case where we want to find all sets of variables that will satisfy the given specifications. A possible MATLAB program for this purpose is given in MATLAB Program 8–16. Note that in the table shown in the program, the last row of the table $\\left(\\mathbf{k},:\\right)$ or the first row of the sorttable should be ignored. (These are the last $K$ and $a$ values for searching purposes.)  \n\n![](images/f787073c1aebc63f37114cd29b46b47f7810c0ff82a0f7a2e3bc88efbab2cc41.jpg)  \n(continues on next page )  \n\n![](images/8df50701ff26a2802082bfc97a9271eafc51ecc09db5e9e4905325bf6bddb581.jpg)  \n\n![](images/2ca45b9a6ca6b9db755f4634301485a1495581111e49d18bac957d586b2d5dcd.jpg)  \n\n![](images/3cecb8de32041345c9ec7869193f56e3e00ad229a6cd036f17ce1adaffc7ff82.jpg)  \nFigure 8–65 Unit-step response curves.  \n\nFrom the sorttable, it seems that  \n\n$K=29$ ,$a=0.25$ (max overshoot $=9.52\\%$ , settling time $=1.78$ sec)  \n\nand  \n\n# A–8–13.  \n\nare two of the best choices.The unit-step response curves for these two cases are shown in Figure 8–65. From these curves,we might conclude that the best choice depends on the system objective.If a small maximum overshoot is desired, $K=27,a=0.2$ will be the best choice.If the shorter settling time is more important than a small maximum overshoot, then $K=29$ ,$a=0.25$ will be the best choice.  \n\nConsider the two-degrees-of-freedom control system shown in Figure 8–66. The plant $G_{p}(s)$ is given by  \n\n$$\nG_{p}(s)=\\frac{100}{s(s+1)}\n$$  \n\nAssuming that the noise input $N(s)$ is zero, design controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that the designed system satisfies the following:  \n\n1. The response to the step disturbance input has a small amplitude and settles to zero quickly (on the order of 1 sec to 2 sec).  \n\n![](images/304922b8115410e34a79d5082aed7b04e04fdc085559a37d0256fa3d62697df9.jpg)  \nFigure 8–66 Two-degrees-offreedom control system.  \n\n2. The response to the unit-step reference input has a maximum overshoot of $25\\%$ or less, and the settling time is 1 sec or less.   \n3. The steady-state errors in following ramp reference input and acceleration reference input are zero.  \n\nSolution. The closed-loop transfer functions for the disturbance input and reference input are given, respectively, by  \n\n$$\n\\begin{array}{l}{{\\displaystyle\\frac{Y(s)}{D(s)}=\\frac{G_{p}(s)}{1+G_{c1}(s)G_{p}(s)}}}\\\\ {{\\displaystyle\\frac{Y(s)}{R(s)}=\\frac{\\left[G_{c1}(s)\\,+\\,G_{c2}(s)\\right]G_{p}(s)}{1+\\,G_{c1}(s)G_{p}(s)}}}\\end{array}\n$$  \n\nLet us assume that $G_{c1}(s)$ is a PID controller and has the following form:  \n\n$$\nG_{c1}(s)\\,=\\frac{K(s\\,+\\,a)^{2}}{s}\n$$  \n\nThe characteristic equation for the system is  \n\n$$\n1\\,+\\,G_{c1}(s)G_{p}(s)\\,=\\,1\\,+\\,\\frac{K(s\\,+\\,a)^{2}}{s}\\frac{100}{s(s\\,+\\,1)}\n$$  \n\nNotice that the open-loop poles are located at $s\\,=\\,0$ (a double pole) and $s=-1$ .The zeros are located at $s=-a$ (a double zero).  \n\nIn what follows, we shall use the root-locus approach to determine the values of $a$ and $K$ . Let us choose the dominant closed-loop poles at $s=-5\\,\\pm\\,j5$ .Then,the angle deficiency at the desired closed-loop pole at $s=-5\\,+\\,j5$ is  \n\n$$\n-135^{\\circ}\\mathrm{~-~}135^{\\circ}\\mathrm{~-~}128.66^{\\circ}\\mathrm{~+~}180^{\\circ}\\mathrm{~=~}-218.66^{\\circ}\n$$  \n\nThe double zero at $s=-a$ must contribute $218.66^{\\circ}$ . (Each zero must contribute $109.33^{\\circ}.$ .) By a simple calculation, we find  \n\n$$\na=-3.2460\n$$  \n\nThe controller $G_{c1}(s)$ is then determined as  \n\n$$\nG_{c1}(s)\\,=\\,\\frac{K(s\\,+\\,3.2460)^{2}}{s}\n$$  \n\nThe constant $K$ must be determined by use of the magnitude condition.This condition is  \n\n$$\n|G_{c1}(s)G_{p}(s)|_{s=-5+j5}=1\n$$  \n\nSince  \n\n$$\nG_{c1}(s)G_{p}(s)\\,=\\frac{K(s\\,+\\,3.2460)^{2}}{s}\\frac{100}{s(s\\,+\\,1)}\n$$  \n\nwe obtain  \n\n$$\nK=\\left|\\frac{s^{2}(s+1)}{100(s+3.2460)^{2}}\\right|_{s=-5+j5}\n$$  \n\nThe controller $G_{c1}(s)$ thus becomes  \n\n$$\n\\begin{array}{r l}{G_{c1}(s)=\\frac{0.11403(s~+~3.2460)^{2}}{s}}\\\\ {=\\frac{0.11403s^{2}~+~0.74028s~+~1.20148}{s}}\\\\ {=0.74028+\\frac{1.20148}{s}+0.11403s}\\end{array}\n$$  \n\nThen, the closed-loop transfer function $Y(s)/D(s)$ is obtained as follows:  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{Y(s)}{D(s)}=\\frac{G_{p}(s)}{1+G_{c1}(s)G_{p}(s)}}}\\\\ &{}&\\\\ &{}&{=\\frac{\\frac{100}{s(s+1)}}{1+\\frac{0.11403(s+3.2460)^{2}}{s}\\frac{100}{s(s+1)}}}\\\\ &{}&\\\\ &{}&{=\\frac{100s}{s^{3}+12.403s^{2}+74.028s+120.148}}\\end{array}\n$$  \n\nThe response curve when $D(s)$ is a unit-step disturbance is shown in Figure 8–67.  \n\n![](images/f65f56eb2def57224fd58b5856d0d7ee2c9457ac57f1e13946396946867e1292.jpg)  \nFigure 8–67 Response to unitstep disturbance input.  \n\nNext, we consider the responses to reference inputs. The closed-loop transfer function $Y(s)/R(s)$ is  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{\\big[G_{c1}(s)\\,+\\,G_{c2}(s)\\big]G_{p}(s)}{1\\,+\\,G_{c1}(s)G_{p}(s)}\n$$  \n\nLet us define  \n\nThen  \n\n$$\n\\begin{array}{c}{{G_{c1}(s)\\,+\\,G_{c2}(s)\\,=\\,G_{c}(s)}}\\\\ {{{}}}\\\\ {{{\\displaystyle\\frac{Y(s)}{R(s)}=\\frac{G_{c}(s)G_{p}(s)}{1\\,+\\,G_{c1}(s)G_{p}(s)}}}}\\\\ {{{{}}}}\\\\ {{{=\\,\\frac{100s G_{c}(s)}{s^{3}\\,+\\,12.403s^{2}\\,+\\,74.028s\\,+\\,120.148}}}}\\end{array}\n$$  \n\nTo satisfy the requirements on the responses to the ramp reference input and acceleration reference input, we use the zero-placement approach. That is, we choose the numerator of $Y(s)/R(s)$ to be the sum of the last three terms of the denominator, or  \n\n$$\n100s G_{c}(s)\\,=\\,12.403s^{2}\\,+\\,74.028s\\,+\\,120.148\n$$  \n\nfrom which we get  \n\n$$\nG_{c}(s)=\\frac{0.12403s^{2}+0.74028s\\,+\\,1.20148}{s}\n$$  \n\nHence, the closed-loop transfer function $Y(s)/R(s)$ becomes as  \n\n$$\n{\\frac{Y(s)}{R(s)}}={\\frac{12.403s^{2}\\,+\\,74.028s\\,+\\,120.148}{s^{3}\\,+\\,12.403s^{2}\\,+\\,74.028s\\,+\\,120.148}}\n$$  \n\nThe response curves to the unit-step reference input, unit-ramp reference input, and unitacceleration reference input are shown in Figures 8–68(a), (b), and (c), respectively.The maximum  \n\n![](images/4e0d4fc8d18fba79627941c06a2e92a2b3eed7fed57f8bf7916a4e77cb68e3d8.jpg)  \nResponse to Unit-Step Reference Input   \nFigure 8–68 (a) Response to unitstep reference input; (b) response to unitramp reference input; (c) response to unit-acceleration reference input.   \n(a)  \n\n![](images/ac19db06553ee29c29c2606ddb7f0d6039cede11abbb9afd75a79b5dff58ec78.jpg)  \n\n![](images/f8d4f3fb6d20fb399d94cf67ad1c68defe9a0a0d3b4b4d3f237dc78f4ccd4314.jpg)  \nFigure 8–68 (continued)   \n(c)  \n\novershoot in the unit-step response is approximately $25\\%$ and the settling time is approximately $1.2\\;\\mathrm{sec}$ . The steady-state errors in the ramp response and acceleration response are zero. Therefore, the designed controller $G_{c}(s)$ given by Equation (8–18) is satisfactory.  \n\nFinally, we determine $G_{c2}(s)$ .Noting that  \n\n$$\nG_{c2}(s)\\,=\\,G_{c}(s)\\,-\\,G_{c1}(s)\n$$  \n\n![](images/5776eacdb2dc8fe71e852867d11e8d9b4b242efee92fcbb33967fbc7cba99b23.jpg)  \nFigure 8–69 Block diagram of the designed system.  \n\nand from Equation (8–17)  \n\n$$\nG_{c1}(s)\\,=\\,0.7403\\,+\\,\\frac{1.20148}{s}\\,+\\,0.11403s\n$$  \n\nwe obtain  \n\n$$\n\\begin{array}{l}{{G_{c2}(s)\\,=\\,\\Bigg(0.7403\\,+\\frac{1.20148}{s}+0.12403s\\Bigg)}}\\\\ {{\\,}}\\\\ {{\\,-\\,\\Bigg(0.7403\\,+\\frac{1.20148}{s}+\\,0.11403s\\Bigg)}}\\\\ {{\\,}}\\\\ {{\\,=\\,0.01s}}\\end{array}\n$$  \n\nEquations (8–17) and (8–19) give the transfer functions of the controllers $G_{c1}(s)$ and $G_{c2}(s)$ ,respectively.The block diagram of the designed system is shown in Figure 8–69.  \n\nNote that if the maximum overshoot were much higher than $25\\%$ and/or the settling time were much larger than 1.2 sec, then we might assume a search region (such as $3\\le a\\le6$ ,$3\\le b\\le6$ ,and $6\\le c\\le12$ ) and use the computational method presented in Example 8–4 to find a set or sets of variables that would give the desired response to the unit-step reference input.  \n\n# PROBLEMS  \n\nB–8–1. Consider the electronic PID controller shown in Figure 8–70. Determine the values of $R_{1}$ $\\mathfrak{C}_{1},\\,R_{2},\\,R_{3},\\,R_{4},\\,C_{1}$ ,and $C_{2}$ of the controller such that the transfer function $G_{c}(s)\\,=\\,E_{o}(s)/E_{i}(s)$ is  \n\n$$\n\\begin{array}{c}{\\displaystyle{G_{c}(s)=39.42\\bigg(1+\\frac{1}{3.077s}+0.7692s\\bigg)}}\\\\ {\\displaystyle{=30.3215\\,\\frac{(s\\,+\\,0.65)^{2}}{s}}}\\end{array}\n$$  \n\n![](images/eb325f0f8536cca8d408dbe738bcac3f875a78214cbc1c908e4da0b1e9ceb7f9.jpg)  \n\nFigure 8–70 Electronic PID controller.  \n\nB–8–2. Consider the system shown in Figure 8–71. Assume that disturbances $D(s)$ enter the system as shown in the diagram. Determine parameters $K$ ,$a$ ,and $^b$ such that the response to the unit-step disturbance input and the response to the unit-step reference input satisfy the following specifications: The response to the step disturbance input should attenuate rapidly with no steady-state error, and the response to the step reference input exhibits a maximum overshoot of $20\\%$ or less and a settling time of 2 sec.  \n\nB–8–3. Show that the PID-controlled system shown in Figure 8–72(a) is equivalent to the I-PD-controlled system with feedforward control shown in Figure 8–72(b).  \n\nB–8–4. Consider the systems shown in Figures 8–73(a) and (b). The system shown in Figure 8–73(a) is the system designed in Example 8–1. The response to the unit-step reference input in the absence of the disturbance input is shown in Figure 8–10. The system shown in Figure 8–73(b) is the I-PD-controlled system using the same $K_{p},T_{i}$ ,and $T_{d}$ as the system shown in Figure 8–73(a).  \n\n![](images/59966a84d966b0955397c8fa04b882c9e261557bf92c3643774cec018b288137.jpg)  \nFigure 8–71 Control system.  \n\n![](images/588eaae5ce43dad2bfba54acf7704a371e315266c587dbf82bf0e0b5cc0b4665.jpg)  \n\nFigure 8–72   \n(a) PID-controlled system; (b) I-PD-controlled system with feedforward control.  \n\nObtain the response of the I-PD-controlled system to the unit-step reference input with MATLAB. Compare the unit-step response curves of the two systems.  \n\nB–8–5. Referring to Problem B–8–4, obtain the response of the PID-controlled system shown in Figure 8–73(a) to the unit-step disturbance input.  \n\nShow that for the disturbance input, the responses of the PID-controlled system shown in Figure 8–73(a) and of the I-PD-controlled system shown in Figure 8–73(b) are exactly the same.[When considering $D(s)$ to be the input,assume that the reference input $R(s)$ is zero, and vice versa.] Also, compare the closed-loop transfer function $C(s)/R(s)$ of both systems.  \n\nB–8–6. Consider the system shown in Figure 8–74.This system is subjected to three input signals: the reference input, disturbance input, and noise input. Show that the characteristic equation of this system is the same regardless of which input signal is chosen as input.  \n\n![](images/f70ce5581d9a860007c6881f414660c8d9781698c4ffbcdb88f7823e83f5ca2e.jpg)  \n\nFigure 8–73 (a) PID-controlled system; (b) I-PD-controlled system.  \n\n![](images/47145ea1ea41e9d85d48c23b462758f564684e81567d4bc7942f8e33d62467e7.jpg)  \n\nFigure 8–74 Control system.  \n\nB–8–7. Consider the system shown in Figure 8–75. Obtain the closed-loop transfer function $C(s)/R(s)$ for the reference input and the closed-loop transfer function $C(s)/D(s)$ for the disturbance input. When considering $R(s)$ as the input, assume that $D(s)$ is zero, and vice versa.  \n\nB–8–8. Consider the system shown in Figure 8–76(a), where $K$ is an adjustable gain and $G(s)$ and $H(s)$ are fixed components. The closed-loop transfer function for the disturbance is  \n\n$$\n{\\frac{C(s)}{D(s)}}={\\frac{1}{1\\,+\\,K G(s)H(s)}}\n$$  \n\nTo minimize the effect of disturbances, the adjustable gain $K$ should be chosen as large as possible.  \n\nIs this true for the system in Figure 8–76(b), too?  \n\n![](images/1f157e66fe0a286ef84d0d7c8528452444c510e41083ae17ee2d85c92aedabdb.jpg)  \n\nFigure 8–75 Control system.  \n\n![](images/a951508f518f378ee50c99b55bb527fd1950ea336ee7d4a98f40bc0d4b11ee63.jpg)  \n\nFigure 8–76   \n(a) Control system with disturbance entering in the feedforward path; (b) control system with disturbance entering in the feedback path.  \n\nB–8–9. Show that the control systems shown in Figures 8–77(a), (b), and (c) are two-degrees-of-freedom systems. In the diagrams, $G_{c1}$ and $G_{c2}$ are controllers and $G_{p}$ is the plant.  \n\nB–8–10. Show that the control system shown in Figure 8–78 is a three-degrees-of freedom system. The transfer functions $G_{c1}$ ,$G_{c2}$ ,and $G_{c3}$ are controllers.The plant consists of transfer functions $G_{1}$ and $G_{2}$ .  \n\n![](images/f7b72f72813a1bb8b4ac2cc81713d9acc97783817210026a974dd5db95eae33f.jpg)  \nFigure 8–77 (a), (b), (c) Two degrees-of-freedom systems.  \n\n![](images/1bdec1d010a20538ba223af0cc0873827d494b18a1695c80fb3ccf7b17a05656.jpg)  \n\nFigure 8–78 Three-degrees-offreedom system.  \n\nB–8–11. Consider the control system shown in Figure 8–79. Assume that the PID controller is given by  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\nIt is desired that the unit-step response of the system exhibit the maximum overshoot of less than $10\\%$ , but more than $2\\%$ (to avoid an almost overdamped system), and the settling time be less than 2 sec.  \n\nUsing the computational approach presented in Section 8–4, write a MATLAB program to determine the values of $K$ and $a$ that will satisfy the given specifications. Choose the search region to be  \n\n$$\n1\\leq K\\leq4,\\ \\ \\ \\ \\ 0.4\\leq a\\leq4\n$$  \n\nChoose the step size for $K$ and $a$ to be 0.05. Write the program such that the nested loops start with the highest values of $K$ and $a$ and step toward the lowest.  \n\nUsing the first-found solution, plot the unit-step response curve.  \n\nB–8–12. Consider the same control system as treated in Problem B–8–11 (Figure 8–79).The PID controller is given by  \n\n$$\nG_{c}(s)\\,=\\,K\\,\\frac{(s\\,+\\,a)^{2}}{s}\n$$  \n\nIt is desired to determine the values of $K$ and $a$ such that the unit-step response of the system exhibits the maximum  \n\novershoot of less than $8\\%$ ,but more than $3\\%$ ,and the settling time is less than 2 sec. Choose the search region to be  \n\n$$\n2\\leq K\\leq4,\\ \\ \\ \\ \\ 0.5\\leq a\\leq3\n$$  \n\nChoose the step size for $K$ and $a$ to be 0.05.  \n\nFirst, write a MATLAB program such that the nested loops in the program start with the highest values of $K$ and $a$ and step toward the lowest and the computation stops when a successful set of $K$ and $a$ is found for the first time.  \n\nNext, write a MATLAB program that will find all possible sets of $K$ and $a$ that will satisfy the given specifications.  \n\nAmong multiple sets of $K$ and $a$ that satisfy the given specifications,determine the best choice.Then,plot the unitstep response curves of the system with the best choice of $K$ and $a$ .  \n\nB–8–13. Consider the two-degrees-of-freedom control system shown in Figure 8–80.The plant $G_{p}(s)$ is given by  \n\n$$\nG_{p}(s)\\,=\\frac{3(s\\,+\\,5)}{s(s\\,+\\,1)\\bigl(s^{2}\\,+\\,4s\\,+\\,13\\bigr)}\n$$  \n\nDesign controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that the response to the unit-step disturbance input should have small amplitude and settle to zero quickly (in approximately 2 sec).The response to the unit-step reference input should be such that the maximum overshoot is $25\\%$ (or less) and the settling time is 2 sec.Also, the steady-state errors in the response to the ramp and acceleration reference inputs should be zero.  \n\n![](images/20df40761d80c2366ff4e002e28005cd2b5180efb788db22365759c78951500c.jpg)  \nFigure 8–79 Control system.  \n\n![](images/76e2e7b6398669803b50f64fd403bba585b5adaacc9c0c56a26d717f9e769c1b.jpg)  \n\nFigure 8–80 Two-degrees-of-freedom control system.  \n\nB–8–14. Consider the system shown in Figure 8–81. The plant $G_{p}(s)$ is given by  \n\n$$\nG_{p}(s)\\,=\\frac{2(s\\,+\\,1)}{s(s\\,+\\,3)(s\\,+\\,5)}\n$$  \n\nDetermine the controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that, for the step disturbance input, the response shows a small amplitude and approaches zero quickly (in a matter of 1 to 2 sec). For the response to the unit-step reference input, it is desired that the maximum overshoot be $20\\%$ or less and the settling time 1 sec or less. For the ramp reference input and acceleration reference input, the steady-state errors should be zero.  \n\nB–8–15. Consider the two-degrees-of-freedom control system shown in Figure 8–82. Design controllers $G_{c1}(s)$ and $G_{c2}(s)$ such that the response to the step disturbance input shows a small amplitude and settles to zero quickly (in 1 to 2 sec) and the response to the step reference input exhibits $25\\%$ or less maximum overshoot and the settling time is less than 1 sec.The steady-state error in following the ramp reference input or acceleration reference input should be zero.  \n\n![](images/f476da24bdb2b4921542570e72d5bbb9fbbbc0eec62c2379def19d861b2fac0e.jpg)  \n\nFigure 8–81 Two-degrees-of-freedom control system.  \n\n![](images/2f50862ced0d4fb43b1173c578984b206169d186eb1e6c6d57cc862f145b2778.jpg)  \n\nFigure 8–82 Two-degrees-of-freedom control system.  \n\n# Control Systems Analysis in State Space  \n\n# 9–1 INTRODUCTION\\*  \n\nA modern complex system may have many inputs and many outputs, and these may be interrelated in a complicated manner.To analyze such a system, it is essential to reduce the complexity of the mathematical expressions,as well as to resort to computers for most of the tedious computations necessary in the analysis.The state-space approach to system analysis is best suited from this viewpoint.  \n\nWhile conventional control theory is based on the input–output relationship,or transfer function, modern control theory is based on the description of system equations in terms of $n$ first-order differential equations, which may be combined into a first-order vector-matrix differential equation.The use of vector-matrix notation greatly simplifies the mathematical representation of systems of equations.The increase in the number of state variables, the number of inputs, or the number of outputs does not increase the complexity of the equations.In fact,the analysis of complicated multiple-input,multipleoutput systems can be carried out by procedures that are only slightly more complicated than those required for the analysis of systems of first-order scalar differential equations.  \n\nThis chapter and the next deal with the state-space analysis and design of control systems. Basic materials of state-space analysis, including the state-space representation of systems, controllability, and observability are presented in this chapter. Useful design methods based on state-feedback control are given in Chapter 10.  \n\nOutline of the Chapter. Section 9–1 has presented an introduction to state-space analysis of control systems. Section 9–2 deals with the state-space representation of transfer-function systems. Here we present various canonical forms of state-space equations. Section 9–3 discusses the transformation of system models (such as from transferfunction to state-space models, and vice versa) with MATLAB. Section 9–4 presents the solution of time-invariant state equations. Section 9–5 gives some useful results in vector-matrix analysis that are necessary in studying the state-space analysis of control systems. Section 9–6 discusses the controllability of control systems and Section 9–7 treats the observability of control systems.  \n\n# 9–2 STATE-SPACE REPRESENTATIONS OF TRANSFER-FUNCTION SYSTEMS  \n\nMany techniques are available for obtaining state-space representations of transfer-function systems. In Chapter 2 we presented a few such methods. This section presents state-space representations in the controllable, observable, diagonal, or Jordan canonical forms. (Methods for obtaining such state-space representations from transfer functions are discussed in detail in Problems A–9–1 through A–9–4 .)  \n\nState-Space Representations in Canonical Forms. Consider a system defined by  \n\n$$\n{\\overset{(n)}{y}}\\,+\\,a_{1}^{\\,\\,(n-1)}\\,+\\,\\cdots\\,+\\,a_{n-1}{\\dot{y}}\\,+\\,a_{n}y\\,=\\,b_{0}^{\\,\\,(n)}\\,+\\,b_{1}^{\\,\\,(n-1)}\\,+\\,\\cdots\\,+\\,b_{n-1}{\\dot{u}}\\,+\\,b_{n}u\n$$  \n\nwhere $u$ is the input and $y$ is the output.This equation can also be written as  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nIn what follows we shall present state-space representations of the system defined by Equation (9–1) or (9–2) in controllable canonical form, observable canonical form, and diagonal (or Jordan) canonical form.  \n\nControllable Canonical Form. The following state-space representation is called a controllable canonical form:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n-1}}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {0}\\\\ {1}\\end{array}\\right]}u_{1}+...}\n$$  \n\n$$\ny=\\left[b_{n}-a_{n}b_{0}\\;\\left\\{\\begin{array}{c}{{b_{n-1}-a_{n-1}b_{0}\\;\\left\\}}\\\\ {{{}b_{n}-a_{n-1}b_{0}\\;\\left\\}}\\end{array}\\right.\\right.\\cdots\\;\\left\\{\\begin{array}{c}{{b_{1}-a_{1}}}\\\\ {{x_{2}}}\\\\ {{\\ddots}}\\\\ {{{}b_{n}-a_{1}b_{0}}}\\end{array}\\right.\\right]\\;+\\;b_{0}u\n$$  \n\nThe controllable canonical form is important in discussing the pole-placement approach to control systems design.  \n\nObservable Canonical Form. The following state-space representation is called an observable canonical form:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{0}&{\\cdots}&{0}&{-a_{n}}\\\\ {1}&{0}&{\\cdots}&{0}&{-a_{n-1}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{\\cdots}&{1}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{c}{b_{n}-a_{n}b_{0}}\\\\ {b_{n-1}-a_{n-1}b_{0}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {b_{1}-a_{1}b_{0}}\\end{array}\\right]}u\n$$  \n\n$$\n\\begin{array}{r}{y=[0\\quad0\\quad\\cdots\\quad0\\quad1]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right]+b_{0}u}\\end{array}\n$$  \n\nNote that the $n\\times n$ state matrix of the state equation given by Equation (9–5) is the transpose of that of the state equation defined by Equation (9–3).  \n\nDiagonal Canonical Form. Consider the transfer-function system defined by Equation  (9–2). Here we consider the case where the denominator polynomial involves only distinct roots. For the distinct-roots case, Equation (9–2) can be written as  \n\n$$\n{\\begin{array}{r l}&{{\\cfrac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,=\\,b_{0}+{\\frac{c_{1}}{s\\,+\\,p_{1}}}+{\\frac{c_{2}}{s\\,+\\,p_{2}}}+\\cdots+{\\frac{c_{n}}{s\\,+\\,p_{n}}}}\\end{array}}\n$$  \n\nThe diagonal canonical form of the state-space representation of this system is given by  \n\n$$\ny={\\left[\\begin{array}{l l l l l}{c_{1}}&{c_{2}}&{\\cdots}&{c_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}{\\sqrt{2}}}\\\\ {x_{2}}\\\\ {\\cdots}\\\\ {\\cdots}\\\\ {\\cdots}\\\\ {x_{n}}\\end{array}\\right]}+b_{0}u\n$$  \n\nJordan Canonical Form. Next we shall consider the case where the denominator polynomial of Equation (9–2) involves multiple roots. For this case, the preceding diagonal canonical form must be modified into the Jordan canonical form. Suppose, for example, that the $p_{i}$ ’s are different from one another, except that the first three $p_{i}$ ’s are equal, or $p_{1}=p_{2}=p_{3}$ .Then the factored form of $Y(s)/U(s)$ becomes  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}+b_{1}s^{n-1}+\\dots+b_{n-1}s+b_{n}}{{(s\\,+\\,p_{1})}^{3}(s\\,+\\,p_{4})(s\\,+\\,p_{5})\\dots(s\\,+\\,p_{n})}}\n$$  \n\nThe partial-fraction expansion of this last equation becomes  \n\n$$\n{\\frac{Y(s)}{U(s)}}=b_{0}+{\\frac{c_{1}}{{\\bigl(}s+p_{1}{\\bigr)}^{3}}}+{\\frac{c_{2}}{{\\bigl(}s+p_{1}{\\bigr)}^{2}}}+{\\frac{c_{3}}{s+p_{1}}}+{\\frac{c_{4}}{s+p_{4}}}+\\cdots+{\\frac{c_{n}}{s+p_{n}}}\n$$  \n\nA state-space representation of this system in the Jordan canonical form is given by  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\\\ {{\\dot{x}}_{4}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\dot{x}_{n}}\\end{array}\\right]}\\,=\\,{\\left[\\begin{array}{l l l}{-p_{1}}&{1}&{0}\\\\ {0}&{-p_{1}}&{1}\\\\ {0}&{0}&{-p_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{0}&{\\cdots}&{0}\\\\ {\\vdots}&&{\\vdots}\\\\ {0}&{\\cdots}&{0}\\\\ {0}&{\\cdots}&{0}\\\\ {\\cdot}&&{\\cdot}\\\\ {\\cdot}&&{\\cdot}\\\\ {\\cdot}&&{\\cdot}\\\\ {0}&{\\cdots}&&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\bar{x}_{n}}\\end{array}\\right]}\\,+\\,{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\\\ {1}\\\\ {1}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {1}\\end{array}\\right]}u,\n$$  \n\n$$\ny={\\left[\\begin{array}{l l l l l}{c_{1}}&{c_{2}}&{\\cdots}&{c_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}{\\sqrt{\\left[\\begin{array}{l}{\\ddots}\\\\ {x_{2}}\\\\ {\\cdots}\\\\ {\\ddots}\\\\ {\\ddots}\\\\ {\\ddots}\\\\ {x_{n}}\\end{array}\\right]}}}\\\\ {x_{n}}\\end{array}\\right]}+b_{0}u\n$$  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{s\\,+\\,3}{s^{2}\\,+\\,3s\\,+\\,2}}\n$$  \n\nObtain state-space representations in the controllable canonical form, observable canonical form, and diagonal canonical form.  \n\nControllable Canonical Form:  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}(t)\\right]=\\left[\\begin{array}{c c}{0}&{1}\\\\ {-2}&{-3}\\end{array}\\right]\\!\\!\\left[\\begin{array}{c}{x_{1}(t)}\\\\ {x_{2}(t)}\\end{array}\\right]+\\left[\\!\\!\\begin{array}{c}{0}\\\\ {1}\\end{array}\\!\\!\\right]u(t)}\\\\ {y(t)=[3}&{1]\\!\\!\\left[\\begin{array}{c}{x_{1}(t)}\\\\ {x_{2}(t)}\\end{array}\\!\\!\\right]}\\end{array}\n$$  \n\nObservable Canonical Form:  \n\n$$\n\\begin{array}{r l}{\\bigg[\\dot{x}_{1}(t)\\bigg]=\\bigg[0}&{-2\\bigg]\\bigg[\\sum_{x_{1}(t)}\\bigg]+\\bigg[3\\bigg]u(t)}\\\\ {\\dot{x}_{2}(t)\\bigg]=\\bigg[1}&{-3\\bigg]\\bigg[\\sum_{x_{2}(t)}\\biggr]+\\bigg[1\\bigg]u(t)}\\\\ {y(t)=[0}&{1]\\bigg[\\sum_{x_{1}(t)}\\biggr]}\\end{array}\n$$  \n\nDiagonal Canonical Form:  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}(t)}\\\\ {\\dot{x}_{2}(t)\\biggr]=\\left[\\begin{array}{l l}{-1}&{0}\\\\ {0}&{-2}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{x_{1}(t)}\\\\ {x_{2}(t)\\biggr]+\\left[\\begin{array}{l}{1}\\\\ {1}\\end{array}\\right]\\!\\!u(t)}\\\\ &{\\qquad y(t)=[2\\!\\!}&{-1]\\!\\!\\left[\\begin{array}{l}{x_{1}(t)}\\\\ {x_{2}(t)}\\end{array}\\right]}\\end{array}\n$$  \n\nEigenvalues of an $\\textbf{\\em n}\\times\\textbf{\\em n}$ Matrix A. The eigenvalues of an $n\\times n$ matrix A are the roots of the characteristic equation  \n\n$$\n|\\lambda\\mathbf{I}-\\mathbf{A}|=0\n$$  \n\nThe eigenvalues are also called the characteristic roots.  \n\nConsider, for example, the following matrix A :  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{\\ 0}&{\\ 1}&{\\ 0}\\\\ {\\ 0}&{\\ 0}&{\\ 1}\\\\ {\\ -6}&{-11}&{\\ -6}\\end{array}\\right]}\n$$  \n\nThe characteristic equation is  \n\n$$\n{\\begin{array}{r l}{|\\lambda\\mathbf{I}-\\mathbf{A}|={\\left|\\begin{array}{l l l}{\\lambda}&{-1}&{\\ 0}\\\\ {0}&{\\lambda}&{\\ -1}\\\\ {6}&{11}&{\\lambda+6}\\end{array}\\right|}}\\\\ {=\\lambda^{3}+6\\lambda^{2}+11\\lambda+6}\\\\ {=(\\lambda+1)(\\lambda+2)(\\lambda+3)=0}\\end{array}}\n$$  \n\nThe eigenvalues of $\\mathbf{A}$ are the roots of the characteristic equation, or $-1,-2$ ,and $^{-3}$ .  \n\nDiagonalization of $\\textbf{\\em n}\\times\\textbf{\\em n}$ Matrix. Note that if an $n\\times n$ matrix $\\mathbf{A}$ with distinct eigenvalues is given by  \n\n$$\n\\mathbf{A}=\\left[\\begin{array}{c c c c c c}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {.}&{.}&{.}&{}&{.}\\\\ {.}&{.}&{.}&{}&{.}\\\\ {.}&{.}&{.}&{.}&{.}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]\n$$  \n\nthe transformation $\\mathbf{x}=\\mathbf{P}\\mathbf{z}$ , where  \n\n$$\n\\mathbf{P}=\\left[\\begin{array}{c c c c}{1}&{1}&{\\cdots}&{1}\\\\ {\\lambda_{1}}&{\\lambda_{2}}&{\\cdots}&{\\lambda_{n}}\\\\ {\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\cdots}&{\\lambda_{n}^{2}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\lambda_{1}^{n-1}}&{\\lambda_{2}^{n-1}}&{\\cdots}&{\\lambda_{n}^{n-1}}\\end{array}\\right]\n$$  \n\nwill transform $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ into the diagonal matrix, or  \n\n$$\n\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\left[\\begin{array}{l l l l l l}{\\lambda_{1}}&{}&{}&{}&{}&{0}\\\\ {}&{\\lambda_{2}}&{}&{}&{}&{}\\\\ {}&{}&{\\cdot}&{}&{}&{}\\\\ {}&{}&{}&{\\cdot}&{}&{}\\\\ {}&{}&{}&{}&{\\cdot}&{}\\\\ {0}&{}&{}&{}&{}&{\\lambda_{n}}\\end{array}\\right]\n$$  \n\nIf the matrix A defined by Equation (9–12) involves multiple eigenvalues, then diagonalization is impossible. For example, if the $3\\times3$ matrix $\\mathbf{A}$ , where  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nhas the eigenvalues $\\lambda_{1},\\lambda_{1},\\lambda_{3}$ ,then the transformation $\\mathbf{x}=\\mathbf{S}\\mathbf{z}$ , where  \n\n$$\n\\mathbf{S}={\\left[\\begin{array}{l l l}{1}&{0}&{1}\\\\ {\\lambda_{1}}&{1}&{\\lambda_{3}}\\\\ {\\lambda_{1}^{2}}&{2\\lambda_{1}}&{\\lambda_{3}^{2}}\\end{array}\\right]}\n$$  \n\nwill yield  \n\nThis is in the Jordan canonical form.  \n\n$$\n\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}=\\left[\\begin{array}{l l l}{\\lambda_{1}}&{1}&{0}\\\\ {0}&{\\lambda_{1}}&{0}\\\\ {0}&{0}&{\\lambda_{3}}\\end{array}\\right]\n$$  \n\nEXAMPLE 9–2 Consider the following state-space representation of a system.  \n\n$$\n{\\begin{array}{r}{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {6}\\end{array}\\right]}u}\\\\ {y={\\left[1\\quad0\\quad0\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nEquations (9–13) and (9–14) can be put in a standard form as  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {6}\\end{array}\\right]},\\qquad\\mathbf{C}=[1\\quad0\\quad0]\n$$  \n\nThe eigenvalues of matrix A are  \n\n$$\n\\lambda_{1}=-1,\\qquad\\lambda_{2}=-2,\\qquad\\lambda_{3}=-3\n$$  \n\nThus, three eigenvalues are distinct. If we define a set of new state variables $z_{1},z_{2}$ ,and $z_{3}$ by the transformation  \n\n$$\n{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\;\\;1}&{\\;\\;1}&{\\;\\;1}\\\\ {-1}&{-2}&{-3}\\\\ {\\;\\;1}&{\\;\\;4}&{\\;\\;9}\\end{array}\\right]}{\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}\n$$  \n\nor  \n\n$$\n\\mathbf{x}=\\mathbf{P}\\mathbf{z}\n$$  \n\nwhere  \n\n$$\n\\mathbf{P}={\\left[\\begin{array}{l l l}{1}&{1}&{1}\\\\ {\\lambda_{1}}&{\\lambda_{2}}&{\\lambda_{3}}\\\\ {\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\lambda_{3}^{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{1}&{1}&{1}\\\\ {-1}&{-2}&{-3}\\\\ {1}&{4}&{9}\\end{array}\\right]}\n$$  \n\nthen, by substituting Equation (9–17) into Equation (9–15), we obtain  \n\n$$\n\\mathbf{P}\\dot{\\mathbf{z}}\\,=\\,\\mathbf{A}\\mathbf{P}\\mathbf{z}\\,+\\,\\mathbf{B}u\n$$  \n\nBy premultiplying both sides of this last equation by $\\mathbf{P}^{-1}$ ,we get  \n\n$$\n\\dot{\\mathbf{z}}=\\mathbf{P}^{\\scriptscriptstyle{-1}}\\mathbf{A}\\mathbf{P}\\mathbf{z}\\,+\\,\\mathbf{P}^{\\scriptscriptstyle{-1}}\\mathbf{B}u\n$$  \n\nor  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{z}_{1}}\\\\ {\\dot{z}_{2}}\\\\ {\\dot{z}_{3}}\\end{array}\\right]=\\left[\\begin{array}{r r r}{3}&{2.5}&{0.5}\\\\ {-3}&{-4}&{-1}\\\\ {1}&{1.5}&{0.5}\\end{array}\\right]\\left[\\begin{array}{r r r}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]\\left[\\begin{array}{r r r}{1}&{1}&{1}\\\\ {-1}&{-2}&{-3}\\\\ {1}&{4}&{9}\\end{array}\\right]\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}\\\\ &{+\\left[\\begin{array}{r r r}{3}&{2.5}&{0.5}\\\\ {-3}&{-4}&{-1}\\\\ {1}&{1.5}&{0.5}\\end{array}\\right]\\left[\\begin{array}{r}{0}\\\\ {0}\\\\ {6}\\end{array}\\right]u}\\end{array}\n$$  \n\nSimplifying gives  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{z}}_{1}}\\\\ {{\\dot{z}}_{2}}\\\\ {{\\dot{z}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-1}&{0}&{0}\\\\ {0}&{-2}&{0}\\\\ {0}&{0}&{-3}\\end{array}\\right]}{\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{3}\\\\ {-6}\\\\ {3}\\end{array}\\right]}u\n$$  \n\nEquation (9–20) is also a state equation that describes the same system as defined by Equation (9–13).  \n\nThe output equation, Equation (9–16), is modified to  \n\n$$\ny=\\mathbf{CPz}\n$$  \n\nor  \n\n$$\n{\\begin{array}{r l}{y=[1}&{0}&{0]{\\left[\\begin{array}{l l l}{1}&{1}&{1}\\\\ {-1}&{-2}&{-3}\\\\ {1}&{4}&{9}\\end{array}\\right]}{\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}}\\\\ {=[1}&{1}&{1]{\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nNotice that the transformation matrix P, defined by Equation (9–18), modifies the coefficient matrix of $\\mathbf{Z}$ into the diagonal matrix.As is clearly seen from Equation (9–20), the three scalar state equations are uncoupled. Notice also that the diagonal elements of the matrix $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ in Equation (9–19) are identical with the three eigenvalues of A . It is very important to note that the eigenvalues of A and those of $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ are identical.We shall prove this for a general case in what follows.  \n\nInvariance of Eigenvalues. To prove the invariance of the eigenvalues under a linear transformation, we must show that the characteristic polynomials $|\\lambda\\mathbf{I}-\\mathbf{A}|$ and $\\left|\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\right|$ are identical.  \n\nSince the determinant of a product is the product of the determinants, we obtain  \n\n$$\n{\\begin{array}{r l}{\\left|\\lambda\\mathbf{I}\\,-\\,\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\right|=\\left|\\lambda\\mathbf{P}^{-1}\\mathbf{P}\\,-\\,\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\right|}&{}\\\\ {\\,=\\,\\left|\\mathbf{P}^{-1}(\\lambda\\mathbf{I}\\,-\\,\\mathbf{A})\\mathbf{P}\\right|}\\\\ {\\,=\\,\\left|\\mathbf{P}^{-1}\\right|\\left|\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\right|\\mathbf{P}\\right|}\\\\ {\\,=\\,\\left|\\mathbf{P}^{-1}\\right|\\left|\\mathbf{P}\\right|\\left|\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\right|}\\end{array}}\n$$  \n\nNot at the product of the determinants $\\left|\\mathbf{P}^{-1}\\right|$ and $\\left|\\mathbf{P}\\right|$ is the determinant of the product $\\left|\\mathbf{P}^{-1}\\mathbf{P}\\right|$ , we obtain  \n\n$$\n\\begin{array}{r l}{\\left|\\lambda\\mathbf{I}\\right.-\\left.\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\right|=\\left.\\left|\\mathbf{P}^{-1}\\mathbf{P}\\right|\\right|\\lambda\\mathbf{I}\\right.-\\left.\\mathbf{A}\\right|}\\\\ &{\\ }\\\\ &{=\\left.\\left|\\lambda\\mathbf{I}\\right.-\\left.\\mathbf{A}\\right|}\\end{array}\n$$  \n\nThus, we have proved that the eigenvalues of $\\mathbf{A}$ are invariant under a linear transformation.  \n\nNonuniqueness of a Set of State Variables. It has been stated that a set of state variables is not unique for a given system.Suppose that $x_{1},x_{2},\\ldots,x_{n}$ are a set of state variables.  \n\nThen we may take as another set of state variables any set of functions  \n\n$$\n\\begin{array}{c}{\\hat{x}_{1}=\\,X_{1}\\!\\left(x_{1},x_{2},\\ldots,x_{n}\\right)}\\\\ {\\hat{x}_{2}=\\,X_{2}\\!\\left(x_{1},x_{2},\\ldots,x_{n}\\right)}\\\\ {\\quad\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\end{array}\n$$  \n\n$$\n{\\hat{x}}_{n}\\,=\\,X_{n}{\\bigl(}x_{1},\\,x_{2},\\ldots,\\,x_{n}{\\bigr)}\n$$  \n\nprovided that, for every set of values ${\\hat{x}}_{1},{\\hat{x}}_{2},\\ldots,{\\hat{x}}_{n}$ ,there corresponds a unique set of values $x_{1},x_{2},\\ldots,x_{n}$ ,and vice versa.Thus, if $\\mathbf{X}$ is a state vector, then $\\hat{\\bf x}$ ,where  \n\n$$\n\\hat{\\mathbf{x}}\\,=\\,\\mathbf{P}\\mathbf{x}\n$$  \n\nis also a state vector,provided the matrix $\\mathbf{P}$ is nonsingular.Different state vectors convey the same information about the system behavior.  \n\n# 9–3 TRANSFORMATION OF SYSTEM MODELS WITH MATLAB  \n\nIn this section we shall consider the transformation of the system model from transfer function to state space, and vice versa. We shall begin our discussion with the transformation from transfer function to state space.  \n\nLet us write the closed-loop transfer function as  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{\\mathrm{numerator~polynomial~in~}}{\\mathrm{denominator~polynomial~in~}}}s={\\frac{\\mathrm{num}}{\\mathrm{den}}}\n$$  \n\nOnce we have this transfer-function expression, the MATLAB command  \n\n$$\n[\\mathsf{A},\\,\\mathsf{B},\\,\\mathsf{C},\\,\\mathsf{D}]=\\mathsf{t f}2\\mathsf{s s}(\\mathsf{n u m},\\mathsf{d e n})\n$$  \n\nwill give a state-space representation. It is important to note that the state-space representation for any system is not unique.There are many (indeed, infinitely many) statespace representations for the same system.The MATLAB command gives one possible such state-space representation.  \n\nState-Space Formulation of Transfer-Function Systems. Consider the transfer-function system  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{10s\\,+\\,10}{s^{3}\\,+\\,6s^{2}\\,+\\,5s\\,+\\,10}\n$$  \n\nThere are many (again, infinitely many) possible state-space representations for this system. One possible state-space representation is  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l l}{\\;\\;\\;0}&{1}&{\\;\\;0}\\\\ {\\;\\;\\;0}&{0}&{1}\\\\ {-10}&{-5}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\;\\;\\;0}\\\\ {\\;\\;10}\\\\ {-50}\\end{array}\\right]}u}\\\\ {y=[1}&{0}&{0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+[0]u}\\end{array}}\n$$  \n\nAnother possible state-space representation (among infinitely many alternatives) is  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-6}&{-5}&{-10}\\\\ {1}&{0}&{0}\\\\ {0}&{1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}u}\\\\ &{\\qquad y=\\left[0\\quad10\\quad10\\right]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+\\left[0\\right]u}\\end{array}}\n$$  \n\nMATLAB transforms the transfer function given by Equation (9–22) into the state-space representation given by Equations (9–23) and (9–24).For the example system considered here, MATLAB Program 9–1 will produce matrices $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ , and $D$ .  \n\n![](images/d19c732c1ab7b7e15ed17cfdd76d5f3424dffc1eb75260df72dc70d28a142bdb.jpg)  \n\nTransformation from State Space to Transfer Function. To obtain the transfer function from state-space equations, use the following command:  \n\niu must be specified for systems with more than one input. For example, if the system has three inputs $\\left(u1,u2,u3\\right)$ ,then iu must be either 1, 2, or 3, where 1 implies u1, 2 implies $u2$ ,and 3 implies u3.  \n\nIf the system has only one input, then either  \n\n$$\n[\\mathrm{num},\\mathrm{den}]=\\mathrm{ss}2\\mathrm{tf}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D})\n$$  \n\nor  \n\n$$\n[\\mathsf{n u m},\\mathsf{d e n}]=\\mathsf{s s}2\\mathsf{t f}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},1)\n$$  \n\nmay be used. (See Example 9–3 and MATLAB Program 9–2.)  \n\nFor the case where the system has multiple inputs and multiple outputs, see Example 9–4.  \n\nEXAMPLE 9–3 Obtain the transfer function of the system defined by the following state-space equations:  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nMATLAB Program 9–2 will produce the transfer function for the given system. The transfer function obtained is given by  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{25.04s\\,+\\,5.008}{s^{3}\\,+\\,5.0325s^{2}\\,+\\,25.1026s\\,+\\,5.008}\n$$  \n\n![](images/19f955d50caf07821409dda018ea803d517099f67f24c360b1f6300515fcafd9.jpg)  \n\nConsider a system with multiple inputs and multiple outputs.When the system has more than one output, the command  \n\n$$\n[\\mathsf{N U M},\\mathsf{d e n}]=\\mathsf{s s2t f}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{i}\\mathsf{u})\n$$  \n\nproduces transfer functions for all outputs to each input. (The numerator coefficients are returned to matrix NUM with as many rows as there are outputs.)  \n\nConsider the system defined by  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}{\\biggr]}}={\\left[\\begin{array}{l l}{\\;\\;\\,0}&{1{\\biggr]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{0{\\biggr]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThis system involves two inputs and two outputs.Four transfer functions are involved: $Y_{1}(s)/U_{1}(s)$ ,$Y_{2}(s)/U_{1}(s),Y_{1}(s)/U_{2}(s)$ ,and $Y_{2}(s)/U_{2}(s)$ .(When considering input $u_{1}$ ,we assume that input $u_{2}$ is zero and vice versa.) See the output of MATLAB Program 9–3.  \n\n![](images/8a64415bb55603ca7be7a0702af6a70a9a1d1239770a9132e48ed54589c0bfa2.jpg)  \n\nThis is the MATLAB representation of the following four transfer functions:  \n\n$$\n\\begin{array}{l c l}{{\\displaystyle{\\frac{Y_{1}(s)}{U_{1}(s)}=\\frac{s\\,+\\,4}{s^{2}\\,+\\,4s\\,+\\,25},}}}&{{\\displaystyle{\\frac{Y_{2}(s)}{U_{1}(s)}=\\frac{-25}{s^{2}\\,+\\,4s\\,+\\,25}}}}\\\\ {{\\displaystyle{\\frac{Y_{1}(s)}{U_{2}(s)}=\\frac{s\\,+\\,5}{s^{2}\\,+\\,4s\\,+\\,25},}}}&{{\\displaystyle{\\frac{Y_{2}(s)}{U_{2}(s)}=\\frac{s\\,-\\,25}{s^{2}\\,+\\,4s\\,+\\,25}}}}\\end{array}\n$$  \n\nIn this section,we shall obtain the general solution of the linear time-invariant state equation.We shall first consider the homogeneous case and then the nonhomogeneous case.  \n\nSolution of Homogeneous State Equations. Before we solve vector-matrix differential equations, let us review the solution of the scalar differential equation  \n\n$$\n\\dot{x}\\,=\\,a x\n$$  \n\nIn solving this equation, we may assume a solution $x(t)$ of the form  \n\n$$\nx(t)\\,=\\,b_{0}\\,+\\,b_{1}t\\,+\\,b_{2}t^{2}\\,+\\,\\cdots\\,+\\,b_{k}t^{k}\\,+\\,\\cdots\n$$  \n\nBy substituting this assumed solution into Equation (9–25), we obtain  \n\n$$\n{\\begin{array}{r l}&{b_{1}\\,+\\,2b_{2}t\\,+\\,3b_{3}t^{2}\\,+\\,\\cdots\\,+\\,k b_{k}t^{k-1}\\,+\\,\\cdots}\\\\ &{\\,=\\,a\\bigl(b_{0}\\,+\\,b_{1}t\\,+\\,b_{2}t^{2}\\,+\\,\\cdots\\,+\\,b_{k}t^{k}\\,+\\,\\cdots\\bigr)}\\end{array}}\n$$  \n\nIf the assumed solution is to be the true solution, Equation (9–27) must hold for any $t$ .Hence, equating the coefficients of the equal powers of $t$ ,we obtain  \n\n$$\n\\begin{array}{l}{{\\displaystyle b_{1}=\\,a b_{0}}}\\\\ {{\\displaystyle b_{2}=\\frac{1}{2}\\,a b_{1}=\\frac{1}{2}\\,a^{2}b_{0}}}\\\\ {{\\displaystyle b_{3}=\\frac{1}{3}\\,a b_{2}=\\frac{1}{3\\times2}\\,a^{3}b_{0}}}\\\\ {{\\displaystyle\\;.}}\\end{array}\n$$  \n\n$$\nb_{k}=\\frac{1}{k!}\\,a^{k}b_{0}\n$$  \n\nThe value of $b_{0}$ is determined by substituting $t\\,=\\,0$ into Equation (9–26), or  \n\n$$\nx(0)=b_{0}\n$$  \n\nHence, the solution $x(t)$ can be written as  \n\n$$\n{\\begin{array}{l}{x(t)\\,=\\,\\biggl(1\\,+\\,a t\\,+\\,{\\frac{1}{2!}}\\,a^{2}t^{2}\\,+\\,\\cdots\\,+\\,{\\frac{1}{k!}}\\,a^{k}t^{k}\\,+\\,\\cdots\\biggr)x(0)}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,e^{a t}x(0)}\\end{array}}\n$$  \n\nWe shall now solve the vector-matrix differential equation  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nwhere $\\mathbf{x}=n$ -vector  \n\n$\\mathbf{A}=n\\times n$ constant matrix  \n\nBy analogy with the scalar case, we assume that the solution is in the form of a vector power series in $t$ ,or  \n\n$$\n\\mathbf{x}(t)\\,=\\,\\mathbf{b}_{0}\\,+\\,\\mathbf{b}_{1}t\\,+\\,\\mathbf{b}_{2}t^{2}\\,+\\,\\cdots\\,+\\,\\mathbf{b}_{k}t^{k}\\,+\\,\\cdots\n$$  \n\nBy substituting this assumed solution into Equation (9–28), we obtain  \n\n$$\n\\begin{array}{r l}&{\\mathbf{b}_{1}\\,+\\,2\\,\\mathbf{b}_{2}t\\,+\\,3\\mathbf{b}_{3}t^{2}\\,+\\,\\cdots\\,+\\,k\\,\\mathbf{b}_{k}t^{k-1}\\,+\\,\\cdots}\\\\ &{\\,\\,\\,=\\,\\mathbf{A}\\big(\\mathbf{b}_{0}\\,+\\,\\mathbf{b}_{1}t\\,+\\,\\mathbf{b}_{2}t^{2}\\,+\\,\\cdots\\,+\\,\\mathbf{b}_{k}t^{k}\\,+\\,\\cdots\\,\\big)}\\end{array}\n$$  \n\nIf the assumed solution is to be the true solution, Equation (9–30) must hold for all $t$ .Thus, by equating the coefficients of like powers of $t$ on both sides of Equation (9–30),we obtain  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\bf b}_{1}={\\bf A}{\\bf b}_{0}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle{\\bf b}_{2}=\\frac{1}{2}{\\bf A}{\\bf b}_{1}=\\frac{1}{2}\\,{\\bf A}^{2}{\\bf b}_{0}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle{\\bf b}_{3}=\\frac{1}{3}\\,{\\bf A}{\\bf b}_{2}=\\frac{1}{3\\times2}\\,{\\bf A}^{3}{\\bf b}_{0}}\\ ~}\\\\ {~}\\\\ {{\\displaystyle~.}}\\end{array}\n$$  \n\n$$\n\\mathbf{b}_{k}=\\frac{1}{k!}\\,\\mathbf{A}^{k}\\mathbf{b}_{0}\n$$  \n\nBy substituting $t\\,=\\,0$ into Equation (9–29), we obtain  \n\n$$\n\\mathbf{x}(0)\\,=\\,\\mathbf{b}_{0}\n$$  \n\nThus, the solution ${\\bf x}(t)$ can be written as  \n\n$$\n\\mathbf{x}(t)\\,=\\,\\left(\\mathbf{I}\\,+\\,\\mathbf{A}t\\,+\\,{\\frac{1}{2!}}\\,\\mathbf{A}^{2}t^{2}\\,+\\,\\cdots\\,+\\,{\\frac{1}{k!}}\\,\\mathbf{A}^{k}t^{k}\\,+\\,\\cdots\\right)\\mathbf{x}(0)\n$$  \n\nThe expression in the parentheses on the right-hand side of this last equation is an $n\\times n$ matrix. Because of its similarity to the infinite power series for a scalar exponential, we call it the matrix exponential and write  \n\n$$\n{\\bf\\cal I}+{\\bf\\cal A}t\\,+\\frac1{2!}\\,{\\bf\\cal A}^{2}t^{2}\\,+\\,\\cdots\\,+\\,\\frac1{k!}\\,{\\bf\\cal A}^{k}t^{k}\\,+\\,\\cdots=\\,e^{{\\bf\\cal A}t}\n$$  \n\nIn terms of the matrix exponential, the solution of Equation (9–28) can be written as  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nSince the matrix exponential is very important in the state-space analysis of linear systems, we shall next examine its properties.  \n\nMatrix Exponential. It can be proved that the matrix exponential of an $n\\times n$ matrix A ,  \n\n$$\ne^{\\mathbf{A}t}\\,=\\,\\sum_{k=0}^{\\infty}\\,{\\frac{\\mathbf{A}^{k}t^{k}}{k!}}\n$$  \n\nconverges absolutely for all finite $t$ .(Hence, computer calculations for evaluating the elements of $e^{\\mathbf{A}t}$ by using the series expansion can be easily carried out.)  \n\nBecause of the convergence of the infinite series $\\begin{array}{r}{\\sum_{k=0}^{\\infty}\\mathbf{A}^{k}t^{k}/k!}\\end{array}$ the series can be differentiated term by term to give  \n\n$$\n{\\begin{array}{r l}&{{\\cfrac{d}{d t}}\\,e^{\\mathbf{A}t}=\\mathbf{A}+\\mathbf{A}^{2}t+{\\cfrac{\\mathbf{A}^{3}t^{2}}{2!}}+\\dots+{\\cfrac{\\mathbf{A}^{k}t^{k-1}}{(k-1)!}}+\\dots}\\\\ &{\\qquad=\\mathbf{A}{\\bigg[}\\mathbf{I}+\\mathbf{A}t+{\\cfrac{\\mathbf{A}^{2}t^{2}}{2!}}+\\dots+{\\cfrac{\\mathbf{A}^{k-1}t^{k-1}}{(k-1)!}}+\\dots{\\bigg]}=\\mathbf{A}e^{\\mathbf{A}t}}\\\\ &{\\qquad=\\left[\\mathbf{I}+\\mathbf{A}t+{\\cfrac{\\mathbf{A}^{2}t^{2}}{2!}}+\\dots+{\\cfrac{\\mathbf{A}^{k-1}t^{k-1}}{(k-1)!}}+\\dots{\\bigg]}\\mathbf{A}=e^{\\mathbf{A}t}\\mathbf{A}}\\end{array}}\n$$  \n\nThe matrix exponential has the property that  \n\n$$\ne^{\\mathbf{A}(t+s)}\\,=\\,e^{\\mathbf{A}t}e^{\\mathbf{A}s}\n$$  \n\nThis can be proved as follows:  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{e^{\\mathbf{A}t}e^{\\mathbf{A}s}=\\left(\\sum_{k=0}^{\\infty}\\frac{\\mathbf{A}^{k}t^{k}}{k!}\\right)\\left(\\sum_{k=0}^{\\infty}\\frac{\\mathbf{A}^{k}s^{k}}{k!}\\right)}}\\\\ &{}&{=\\sum_{k=0}^{\\infty}\\mathbf{A}^{k}\\Bigg(\\sum_{i=0}^{\\infty}\\frac{t^{i}s^{k-i}}{i!\\left(k-i\\right)!}\\Bigg)}\\\\ &{}&{=\\sum_{k=0}^{\\infty}\\mathbf{A}^{k}\\frac{\\left(t+s\\right)^{k}}{k!}}\\\\ &{}&{=e^{\\mathbf{A}(t+s)}}\\end{array}\n$$  \n\nIn particular, if $s=-t$ ,then  \n\n$$\ne^{\\mathbf{A}t}e^{-\\mathbf{A}t}=e^{-\\mathbf{A}t}e^{\\mathbf{A}t}=e^{\\mathbf{A}(t-t)}=\\mathbf{I}\n$$  \n\nThus, the inverse of $e^{\\mathbf{A}t}$ is $e^{-\\mathbf{A}t}$ .Since the inverse of $e^{\\mathbf{A}t}$ always exists, $e^{\\mathbf{A}t}$ is nonsingular. It is very important to remember that  \n\n$$\n{\\begin{array}{r l r}&{e^{(\\mathbf{A}+\\mathbf{B})t}=e^{\\mathbf{A}t}e^{\\mathbf{B}t},}&&{{\\mathrm{if~}}\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}}\\\\ &{e^{(\\mathbf{A}+\\mathbf{B})t}\\neq e^{\\mathbf{A}t}e^{\\mathbf{B}t},}&&{{\\mathrm{if~}}\\mathbf{A}\\mathbf{B}\\neq\\mathbf{B}\\mathbf{A}}\\end{array}}\n$$  \n\nTo prove this, note that  \n\n$$\n\\begin{array}{r l}&{e^{({\\bf A}+{\\bf B})t}={\\bf I}+\\left({\\bf A}+{\\bf B}\\right)t+\\frac{\\left({\\bf A}+{\\bf B}\\right)^{2}}{2!}t^{2}+\\frac{\\left({\\bf A}+{\\bf B}\\right)^{3}}{3!}t^{3}+\\cdots}\\\\ &{\\quad e^{{\\bf A}t}e^{{\\bf B}t}=\\Bigg({\\bf I}+{\\bf A}t+\\frac{{\\bf A}^{2}t^{2}}{2!}+\\frac{{\\bf A}^{3}t^{3}}{3!}+\\cdots\\Bigg)\\Bigg({\\bf I}+{\\bf B}t+\\frac{{\\bf B}^{2}t^{2}}{2!}+\\frac{{\\bf B}^{3}t^{3}}{3!}+\\cdots\\Bigg)}\\\\ &{\\quad\\quad\\quad={\\bf I}+\\left({\\bf A}+{\\bf B}\\right)t+\\frac{{\\bf A}^{2}t^{2}}{2!}+{\\bf A}{\\bf B}t^{2}+\\frac{{\\bf B}^{2}t^{2}}{2!}+\\frac{{\\bf A}^{3}t^{3}}{3!}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{{\\bf A}^{2}{\\bf B}t^{3}}{2!}+\\frac{{\\bf A}{\\bf B}^{2}t^{3}}{2!}+\\frac{{\\bf B}^{3}t^{3}}{3!}+\\cdots.}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\begin{array}{r l}&{e^{(\\mathbf{A}+\\mathbf{B})t}\\,-\\,e^{\\mathbf{A}t}e^{\\mathbf{B}t}=\\frac{\\mathbf{B}\\mathbf{A}\\,-\\,\\mathbf{A}\\mathbf{B}}{2!}\\,t^{2}}\\\\ &{\\,\\,\\,\\,\\,+\\,\\frac{\\mathbf{B}\\mathbf{A}^{2}\\,+\\,\\mathbf{A}\\mathbf{B}\\mathbf{A}\\,+\\,\\mathbf{B}^{2}\\mathbf{A}\\,+\\,\\mathbf{B}\\mathbf{A}\\mathbf{B}\\,-\\,2\\mathbf{A}^{2}\\mathbf{B}\\,-\\,2\\mathbf{A}\\mathbf{B}^{2}}{3!}\\,t^{3}\\,+\\,\\cdots}\\end{array}\n$$  \n\nThe difference between $e^{(\\mathbf{A}+\\mathbf{B})t}$ and $e^{\\mathbf{A}t}e^{\\mathbf{B}t}$ vanishes if $\\mathbf{A}$ and $\\mathbf{B}$ commute.  \n\nLaplace Transform Approach to the Solution of Homogeneous State Equations. Let us first consider the scalar case:  \n\n$$\n\\dot{x}\\,=\\,a x\n$$  \n\nTaking the Laplace transform of Equation (9–32), we obtain  \n\n$$\ns X(s)\\,-\\,x(0)\\,=\\,a X(s)\n$$  \n\nwhere $X(s)\\,=\\,{\\mathcal{L}}[x]$ .Solving Equation (9–33) for $X(s)$ gives  \n\n$$\nX(s)\\,=\\,\\frac{x(0)}{s\\,-\\,a}=\\,(s\\,-\\,a)^{-1}x(0)\n$$  \n\nThe inverse Laplace transform of this last equation gives the solution  \n\n$$\nx(t)\\,=\\,e^{a t}x(0)\n$$  \n\nThe foregoing approach to the solution of the homogeneous scalar differential equation can be extended to the homogeneous state equation:  \n\n$$\n\\dot{\\mathbf{x}}(t)\\,=\\,{\\bf A}\\mathbf{x}(t)\n$$  \n\nTaking the Laplace transform of both sides of Equation (9–34), we obtain  \n\n$$\ns\\mathbf{X}(s)\\,-\\,\\mathbf{x}(0)\\,=\\,\\mathbf{A}\\mathbf{X}(s)\n$$  \n\nwhere $\\mathbf{X}(s)\\,=\\,{\\mathcal{L}}[\\mathbf{x}]$ .Hence,  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})\\mathbf{X}(s)=\\mathbf{x}(0)\n$$  \n\nPremultiplying both sides of this last equation by $(s\\mathbf{I}-\\mathbf{A})^{-1}$ ,we obtain  \n\n$$\n{\\bf X}(s)\\,=\\,(s{\\bf I}\\,-\\,{\\bf A})^{-1}{\\bf x}(0)\n$$  \n\nThe inverse Laplace transform of $\\mathbf{X}(s)$ gives the solution ${\\bf x}(t)$ .Thus,  \n\n$$\n\\mathbf{x}(t)\\,=\\,\\mathcal{L}^{-1}\\bigl[(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\bigr]\\mathbf{x}(0)\n$$  \n\nNote that  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})^{-1}={\\frac{\\mathbf{I}}{s}}+{\\frac{\\mathbf{A}}{s^{2}}}+{\\frac{\\mathbf{A}^{2}}{s^{3}}}+\\cdots\n$$  \n\nHence, the inverse Laplace transform of $(s\\mathbf{I}-\\mathbf{A})^{-1}$ gives  \n\n$$\n\\mathcal{L}^{-1}\\big[(s\\mathbf{I}-\\mathbf{A})^{-1}\\big]=\\mathbf{I}+\\mathbf{A}t+\\frac{\\mathbf{A}^{2}t^{2}}{2!}+\\frac{\\mathbf{A}^{3}t^{3}}{3!}+\\cdots=e^{\\mathbf{A}t}\n$$  \n\n(The inverse Laplace transform of a matrix is the matrix consisting of the inverse Laplace transforms of all elements.) From Equations (9–35) and (9–36), the solution of Equation (9–34) is obtained as  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nThe importance of Equation (9–36) lies in the fact that it provides a convenient means for finding the closed solution for the matrix exponential.  \n\nState-Transition Matrix. We can write the solution of the homogeneous state equation  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nas  \n\n$$\n\\mathbf{x}(t)\\,=\\,\\Phi(t)\\mathbf{x}(0)\n$$  \n\nwhere $\\Phi(t)$ is an $n\\times n$ matrix and is the unique solution of  \n\n$$\n\\dot{\\Phi}(t)\\,=\\,{\\bf A}\\Phi(t),\\;\\;\\;\\;\\;\\;\\;\\Phi(0)\\,=\\,{\\bf I}\n$$  \n\nTo verify this, note that  \n\n$$\n{\\bf x}(0)={\\bf\\Phi}\\Phi(0){\\bf x}(0)={\\bf x}(0)\n$$  \n\nand  \n\n$$\n\\dot{\\mathbf{x}}(t)\\,=\\,\\dot{\\Phi}(t)\\mathbf{x}(0)\\,=\\,\\mathbf{A}\\Phi(t)\\mathbf{x}(0)\\,=\\,\\mathbf{A}\\mathbf{x}(t)\n$$  \n\nWe thus confirm that Equation (9–38) is the solution of Equation (9–37). From Equations (9–31), (9–35), and (9–38), we obtain  \n\n$$\n\\Phi(t)\\,=\\,e^{\\mathbf{A}t}\\,=\\,\\mathcal{L}^{-1}\\bigl[(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\bigr]\n$$  \n\nNote that  \n\n$$\n\\Phi^{-1}(t)\\,=\\,e^{-\\mathbf{A}t}\\,=\\,\\Phi(-t)\n$$  \n\nFrom Equation (9–38), we see that the solution of Equation (9–37) is simply a transformation of the initial condition. Hence, the unique matrix $\\Phi(t)$ is called the statetransition matrix.The state-transition matrix contains all the information about the free motions of the system defined by Equation (9–37).  \n\nIf the eigenvalues $\\lambda_{1},\\,\\lambda_{2},\\ldots,\\,\\lambda_{n}$ of the matrix $\\mathbf{A}$ are distinct, than $\\Phi(t)$ will contain the $n$ exponentials  \n\n$$\ne^{\\lambda_{1}t},e^{\\lambda_{2}t},\\dots,e^{\\lambda_{n}t}\n$$  \n\nIn particular, if the matrix $\\mathbf{A}$ is diagonal, then  \n\n$$\n\\Phi(t)=e^{\\mathbf{A}t}={\\left[\\begin{array}{l l l l l l}{e^{\\lambda_{1}t}}&&&&&{0}\\\\ &{e^{\\lambda_{2}t}}&&&&\\\\ &&{\\cdot}&&&\\\\ &&&{\\cdot}&&\\\\ &&&&{\\cdot}&\\\\ {0}&&&&&{e^{\\lambda_{n}t}}\\end{array}\\right]}\n$$  \n\nIf there is a multiplicity in the eigenvalues—for example, if the eigenvalues of $\\mathbf{A}$ are  \n\n$$\n\\lambda_{1},\\lambda_{1},\\lambda_{1},\\lambda_{4},\\lambda_{5},\\ldots,\\lambda_{n},\n$$  \n\nthen $\\Phi(t)$ will contain, in addition to the exponentials $e^{\\lambda_{1}t},e^{\\lambda_{4}t},e^{\\lambda_{5}t},\\ldots,e^{\\lambda_{n}t}$ ,terms like $t e^{\\lambda_{1}t}$ and $t^{2}e^{\\lambda_{1}t}$ .  \n\nProperties of State-Transition Matrices. We shall now summarize the important properties of the state-transition matrix $\\Phi(t)$ For the time-invariant system  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nfor which  \n\n$$\n\\Phi(t)\\,=\\,e^{{\\bf A}t}\n$$  \n\nwe have the following:  \n\n1. $\\Phi(0)\\,=\\,e^{\\mathbf{A}0}=\\mathbf{I}$   \n3. $\\begin{array}{r l}&{\\Phi(t)\\,=\\,e^{\\mathbf{A}t}=\\left(e^{-\\mathbf{A}t}\\right)^{-1}\\,=\\,\\left[\\Phi(-t)\\right]^{-1}\\mathrm{or}\\;\\Phi^{-1}(t)\\,=\\,\\Phi(-t)}\\\\ &{\\Phi(t_{1}\\,+\\,t_{2})=\\,e^{\\mathbf{A}(t_{1}+t_{2})}=\\,e^{\\mathbf{A}t_{1}}e^{\\mathbf{A}t_{2}}=\\Phi(t_{1})\\Phi(t_{2})=\\,\\Phi(t_{2})\\Phi(t_{1})}\\end{array}$ CD  \n4. $\\bigl[\\Phi(t)\\bigr]^{n}=\\Phi(n t)$   \n5. $\\Phi(t_{2}-\\,t_{1})\\Phi(t_{1}\\,-\\,t_{0})\\,=\\,\\Phi(t_{2}\\,-\\,t_{0})\\,=\\,\\Phi(t_{1}\\,-\\,t_{0})\\Phi(t_{2}\\,-\\,t_{1})$  \n\nEXAMPLE 9–5 Obtain the state-transition matrix $\\Phi(t)$ of the following system:  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {-2}&{-3}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nObtain also the inverse of the state-transition matrix, $\\Phi^{-1}(t)$ .For this system,  \n\n$$\n{\\bf A}=\\left[\\begin{array}{c r c}{{0}}&{{1}}\\\\ {{-2}}&{{-3\\sum}}\\end{array}\\right]\n$$  \n\nThe state-transition matrix $\\Phi(t)$ is given by  \n\n$$\n\\Phi(t)\\,=\\,e^{\\mathbf{A}t}\\,=\\,\\mathcal{L}^{-1}\\bigl[(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\bigr]\n$$  \n\nSince  \n\n$$\ns\\mathbf{I}\\,-\\,\\mathbf{A}={\\left[\\begin{array}{l l}{s}&{0}\\\\ {0}&{s}\\end{array}\\right]}\\,-\\,{\\left[\\begin{array}{l l}{0}&{1}\\\\ {-2}&{-3}\\end{array}\\right]}={\\left[\\begin{array}{l l}{s}&{-1}\\\\ {2}&{s+3}\\end{array}\\right]}\n$$  \n\nthe inverse of $(s\\mathbf{I}\\mathrm{~-~}\\mathbf{A})$ is given by  \n\n$$\n{\\begin{array}{r l}{(s\\mathbf{I}-\\mathbf{A})^{-1}={\\cfrac{1}{(s+1)(s+2)}}{\\left[\\begin{array}{l l}{s+3}&{1}\\\\ {-2}&{s}\\end{array}\\right]}}\\\\ &{={\\left[\\begin{array}{l l}{{\\cfrac{s+3}{(s+1)(s+2)}}}&{{\\cfrac{1}{(s+1)(s+2)}}}\\\\ {{\\cfrac{-2}{(s+1)(s+2)}}}&{{\\cfrac{s}{(s+1)(s+2)}}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nHence,  \n\n$$\n\\begin{array}{l}{{\\Phi(t)\\,=\\,e^{\\mathbf{A}t}\\,=\\,{\\mathcal{L}}^{-1}\\bigl[(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\bigr]}}\\\\ {{\\,=\\,\\biggl[\\,\\begin{array}{l}{{2e^{-t}\\,-\\,e^{-2t}}}&{{e^{-t}\\,-\\,e^{-2t}}}\\\\ {{-2e^{-t}\\,+\\,2e^{-2t}}}&{{-e^{-t}\\,+\\,2e^{-2t}}}\\end{array}\\biggr]}}\\end{array}\n$$  \n\nNoting that $\\Phi^{-1}(t)\\,=\\,\\Phi(-t)$ ,we obtain the inverse of the state-transition matrix as follows:  \n\n$$\n\\Phi^{-1}(t)\\,=\\,e^{-\\mathbf{A}t}\\,=\\left[{\\begin{array}{c c}{2e^{t}\\,-\\,e^{2t}}&{e^{t}\\,-\\,e^{2t}}\\\\ {-2e^{t}\\,+\\,2e^{2t}}&{-e^{t}\\,+\\,2e^{2t}}\\end{array}}\\right]\n$$  \n\nSolution of Nonhomogeneous State Equations. We shall begin by considering the scalar case  \n\n$$\n\\dot{x}\\,=\\,a x\\,+\\,b u\n$$  \n\nLet us rewrite Equation (9–39) as  \n\n$$\n\\Dot{x}\\,-\\,a x\\,=\\,b u\n$$  \n\nMultiplying both sides of this equation by $e^{-a t}$ ,we obtain  \n\n$$\ne^{-a t}\\bigl[\\dot{x}(t)\\,-\\,a x(t)\\bigr]\\,=\\frac{d}{d t}\\bigl[e^{-a t}x(t)\\bigr]\\,=\\,e^{-a t}b u(t)\n$$  \n\nIntegrating this equation between 0 and $t$ gives  \n\n$$\ne^{-a t}x(t)\\,-\\,x(0)\\,=\\,\\int_{0}^{t}\\!e^{-a\\tau}b u(\\tau)\\,d\\tau\n$$  \n\nor  \n\n$$\nx(t)\\,=\\,e^{a t}x(0)\\,+\\,e^{a t}\\int_{0}^{t}e^{-a\\tau}b u(\\tau)\\,d\\tau\n$$  \n\nThe first term on the right-hand side is the response to the initial condition and the second term is the response to the input $u(t)$ .  \n\nLet us now consider the nonhomogeneous state equation described by  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\nwhere $\\mathbf{x}=n$ -vector  \n\n${\\bf u}=r$ -vector $\\mathbf{A}=n\\times n$ constant matrix $\\mathbb{B}\\,=\\,n\\,\\times\\,r$ constant matrix  \n\nBy writing Equation (9–40) as  \n\n$$\n{\\dot{\\mathbf{x}}}(t)\\,-\\,\\mathbf{A}\\mathbf{x}(t)\\,=\\,\\mathbf{B}\\mathbf{u}(t)\n$$  \n\nand premultiplying both sides of this equation by $e^{-\\mathbf{A}t}$ ,we obtain  \n\n$$\ne^{{\\bf-A}t}\\big[\\dot{\\bf x}(t)\\,-\\,{\\bf A}{\\bf x}(t)\\big]\\,=\\frac{d}{d t}\\big[e^{{\\bf-A}t}{\\bf x}(t)\\big]\\,=\\,e^{{\\bf-A}t}{\\bf B}{\\bf u}(t)\n$$  \n\nChapter 9 /Control Systems Analysis in State Space  \n\nIntegrating the preceding equation between 0 and $t$ gives  \n\n$$\ne^{-\\mathbf{A}t}\\mathbf{x}(t)\\,-\\,\\mathbf{x}(0)\\,=\\,\\int_{0}^{t}\\!e^{-\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nor  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nEquation (9–41) can also be written as  \n\n$$\n\\mathbf{x}(t)\\,=\\,\\Phi(t)\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}\\!\\Phi(t\\,-\\,\\tau)\\,\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nwhere $\\Phi(t)\\,=\\,e^{\\mathbf{A}t}$ .Equation (9–41) or (9–42) is the solution of Equation (9–40). The solution ${\\bf x}(t)$ is clearly the sum of a term consisting of the transition of the initial state and a term arising from the input vector.  \n\nLaplace Transform Approach to the Solution of Nonhomogeneous State Equations. The solution of the nonhomogeneous state equation  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\ncan also be obtained by the Laplace transform approach.The Laplace transform of this last equation yields  \n\n$$\ns\\mathbf{X}(s)\\,-\\,\\mathbf{x}(0)\\,=\\,\\mathbf{A}\\mathbf{X}(s)\\,+\\,\\mathbf{B}\\mathbf{U}(s)\n$$  \n\nor  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})\\mathbf{X}(s)=\\mathbf{x}(0)\\,+\\,\\mathbf{B}\\mathbf{U}(s)\n$$  \n\nPremultiplying both sides of this last equation by $(s\\mathbf{I}-\\mathbf{A})^{-1}$ ,we obtain  \n\n$$\n\\mathbf{X}(s)\\,=\\,(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{x}(0)\\,+\\,(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\\mathbf{U}(s)\n$$  \n\nUsing the relationship given by Equation (9–36) gives  \n\n$$\n\\mathbf{X}(s)\\,=\\,{\\mathcal{L}}{\\big[}e^{\\mathbf{A}t}{\\big]}\\mathbf{x}(0)\\,+\\,{\\mathcal{L}}{\\big[}e^{\\mathbf{A}t}{\\big]}\\mathbf{B}\\mathbf{U}(s)\n$$  \n\nThe inverse Laplace transform of this last equation can be obtained by use of the convolution integral as follows:  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nSolution in Terms of $\\mathbf{x}(t_{0})$ .Thus far we have assumed the initial time to be zero. If, however, the initial time is given by $t_{0}$ instead of 0, then the solution to Equation (9–40) must be modified to  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}(t-t_{0})}\\mathbf{x}\\big(t_{0}\\big)\\,+\\,\\int_{t_{0}}^{t}\\!e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nEXAMPLE 9–6 Obtain the time response of the following system:  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\left[\\begin{array}{l l}{\\;\\;0}&{\\;\\;1}\\\\ {\\!-2}&{-3}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\binom{0}{1}}u\n$$  \n\nwhere $u(t)$ is the unit-step function occurring at $t\\,=\\,0$ ,or  \n\n$$\nu(t)\\,=\\,1(t)\n$$  \n\nFor this system,  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{\\,\\,0}&{\\,\\,\\,1}\\\\ {-2}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nThe state-transition matrix $\\Phi(t)\\,=\\,e^{\\mathbf{A}t}$ was obtained in Example 9–5 as  \n\n$$\n\\Phi(t)\\,=\\,e^{\\mathbf{A}t}\\,=\\left[\\begin{array}{c c}{{2e^{-t}\\,-\\,e^{-2t}}}&{{e^{-t}\\,-\\,e^{-2t}}}\\\\ {{-2e^{-t}\\,+\\,2e^{-2t}}}&{{-e^{-t}\\,+\\,2e^{-2t}}}\\end{array}\\right]\n$$  \n\nThe response to the unit-step input is then obtained as  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}\\!\\left[\\!\\!\\begin{array}{c c}{2e^{-(t-\\tau)}-\\,e^{-2(t-\\tau)}}&{e^{-(t-\\tau)}-\\,e^{-2(t-\\tau)}}\\\\ {-2e^{-(t-\\tau)}+\\,2e^{-2(t-\\tau)}}&{-e^{-(t-\\tau)}+\\,2e^{-2(t-\\tau)}}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\!0\\,\\right]\\!\\!\\left[1\\right]d\\tau\n$$  \n\nor  \n\n$$\n{\\left[\\begin{array}{l}{x_{1}(t)}\\\\ {x_{2}(t)}\\end{array}\\right]}={\\left[\\begin{array}{l l}{2e^{-t}-e^{-2t}}&{e^{-t}-e^{-2t}}\\\\ {-2e^{-t}+2e^{-2t}}&{-e^{-t}+2e^{-2t}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {x_{2}(0)}\\end{array}\\right]}+{\\left[\\begin{array}{l}{{\\frac{1}{2}}-e^{-t}+{\\frac{1}{2}}e^{-2t}}\\\\ {e^{-t}-e^{-2t}}\\end{array}\\right]}\n$$  \n\nIf the initial state is zero, or $\\mathbf{x}(0)=\\mathbf{0}$ , then ${\\bf x}(t)$ can be simplified to  \n\n$$\n\\left[{\\bf\\Psi}_{x_{1}(t)}\\right]=\\left[\\begin{array}{c}{{1}}\\\\ {{2}}\\\\ {{\\epsilon^{-t}-e^{-2t}}}\\end{array}\\right]\n$$  \n\n# 9–5 SOME USEFUL RESULTS IN VECTOR-MATRIX ANALYSIS  \n\nIn this section we present some useful results in vector-matrix analysis that we use in Section 9–6. Specifically, we present the Cayley–Hamilton theorem, the minimal polynomial,Sylvester’s interpolation method for calculating $e^{\\mathbf{A}t}$ ,and the linear independence of vectors.  \n\nCayley–Hamilton Theorem. The Cayley–Hamilton theorem is very useful in proving theorems involving matrix equations or solving problems involving matrix equations.  \n\nConsider an $n\\times n$ matrix A and its characteristic equation:  \n\n$$\n\\vert\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\vert\\,=\\,\\lambda^{n}\\,+\\,a_{1}\\lambda^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}\\lambda\\,+\\,a_{n}=0\n$$  \n\nThe Cayley–Hamilton theorem states that the matrix A satisfies its own characteristic equation, or that  \n\n$$\n\\mathbf{A}^{n}\\,+\\,a_{1}\\,\\mathbf{A}^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}\\,\\mathbf{A}\\,+\\,a_{n}\\,\\mathbf{I}\\,=\\,\\mathbf{0}\n$$  \n\nTo prove this theorem, note that adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ is a polynomial in $\\lambda$ of degree $n\\,-\\,1$ .That is,  \n\n$$\n\\operatorname{adj}(\\lambda\\mathbf{I}\\,-\\,\\mathbf{A})=\\mathbf{B}_{1}\\lambda^{n-1}\\,+\\,\\mathbf{B}_{2}\\lambda^{n-2}\\,+\\,\\cdots\\,+\\,\\mathbf{B}_{n-1}\\lambda\\,+\\,\\mathbf{B}_{n}\n$$  \n\nwhere $\\mathbf{B}_{1}=\\mathbf{I},$ .Since  \n\n$$\n(\\lambda\\mathbf{I}-\\mathbf{A})\\;\\mathrm{adj}(\\lambda\\mathbf{I}-\\mathbf{A})=[\\mathrm{adj}(\\lambda\\mathbf{I}-\\mathbf{A})](\\lambda\\mathbf{I}-\\mathbf{A})=|\\lambda\\mathbf{I}-\\mathbf{A}|\\mathbf{I}\n$$  \n\nwe obtain  \n\n$$\n{\\begin{array}{r l}&{|\\lambda\\mathbf{I}-\\mathbf{A}|\\mathbf{I}=\\mathbf{I}\\lambda^{n}+\\,a_{1}\\mathbf{I}\\lambda^{n-1}+\\dots+\\,a_{n-1}\\mathbf{I}\\lambda\\,+\\,a_{n}\\mathbf{I}}\\\\ &{\\qquad\\qquad=\\,(\\lambda\\mathbf{I}-\\mathbf{A}){\\big(}\\mathbf{B}_{1}\\lambda^{n-1}+\\,\\mathbf{B}_{2}\\lambda^{n-2}+\\dots+\\,\\mathbf{B}_{n-1}\\lambda\\,+\\,\\mathbf{B}_{n}{\\big)}}\\\\ &{\\qquad\\qquad=\\,{\\big(}\\mathbf{B}_{1}\\lambda^{n-1}+\\,\\mathbf{B}_{2}\\lambda^{n-2}+\\dots+\\,\\mathbf{B}_{n-1}\\lambda\\,+\\,\\mathbf{B}_{n}{\\big)}(\\lambda\\mathbf{I}-\\,\\mathbf{A})}\\end{array}}\n$$  \n\nFrom this equation, we see that $\\mathbf{A}$ and $\\mathbf{B}_{i}\\left(i=1,2,\\ldots,n\\right)$ commute. Hence, the product of $(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A})$ and adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ becomes zero if either of these is zero. If $\\mathbf{A}$ is substituted for $\\lambda$ in this last equation, then clearly $\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}$ becomes zero. Hence, we obtain  \n\n$$\n\\mathbf{A}^{n}\\,+\\,a_{1}\\,\\mathbf{A}^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}\\,\\mathbf{A}\\,+\\,a_{n}\\,\\mathbf{I}\\,=\\,\\mathbf{0}\n$$  \n\nThis proves the Cayley–Hamilton theorem, or Equation (9–44).  \n\nMinimal Polynomial. Referring to the Cayley–Hamilton theorem, every $n\\times n$ matrix A satisfies its own characteristic equation. The characteristic equation is not, however, necessarily the scalar equation of least degree that A satisfies.The least-degree polynomial having $\\mathbf{A}$ as a root is called the minimal polynomial . That is, the minimal polynomial of an $n\\times n$ matrix $\\mathbf{A}$ is defined as the polynomial $\\phi(\\lambda)$ of least degree,  \n\n$$\n\\phi(\\lambda)\\,=\\,\\lambda^{m}\\,+\\,a_{1}\\lambda^{m-1}\\,+\\,\\cdots\\,+\\,a_{m-1}\\lambda\\,+\\,a_{m},\\qquad m\\,\\le\\,n\n$$  \n\nsuch that $\\phi(\\mathbf{A})=\\mathbf{0}$ ,or  \n\n$$\n\\phi(\\mathbf{A})=\\mathbf{A}^{m}+a_{1}\\mathbf{A}^{m-1}+\\cdots+\\,a_{m-1}\\mathbf{A}\\,+\\,a_{m}\\mathbf{I}=\\mathbf{0}\n$$  \n\nThe minimal polynomial plays an important role in the computation of polynomials in an $n\\times n$ matrix.  \n\nLet us suppose that $d(\\lambda)$ ,a polynomial in $\\lambda$ , is the greatest common divisor of all the elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ .We can show that if the coefficient of the highest-degree term in $\\lambda$ of $d(\\lambda)$ is chosen as 1, then the minimal polynomial $\\phi(\\lambda)$ is given by  \n\n$$\n\\phi(\\lambda)=\\frac{|\\lambda\\mathbf{I}-\\mathbf{A}|}{d(\\lambda)}\n$$  \n\n[See Problem $\\mathbf{A-9-8}$ for the derivation of Equation (9–45).]  \n\nIt is noted that the minimal polynomial $\\phi(\\lambda)$ of an $n\\times n$ matrix A can be determined by the following procedure:  \n\n1. Form adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ and write the elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ as factored polynomials in $\\lambda$ .  \n2. Determine $d(\\lambda)$ as the greatest common divisor of all the elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ .Choose the coefficient of the highest-degree term in $\\lambda$ of $d(\\lambda)$ to be 1.If there is no common divisor, $d(\\lambda)=1$ .  \n3. The minimal polynomial $\\phi(\\lambda)$ is then given as $|\\lambda\\mathbf{I}-\\mathbf{A}|$ divided by $d(\\lambda)$ .  \n\nMatrix Exponential $e^{\\mathbf{A}t}$ .In solving control engineering problems, it often becomes necessary to compute $e^{\\mathbf{A}t}$ .If matrix $\\mathbf{A}$ is given with all elements in numerical values, MATLAB provides a simple way to compute $e^{\\mathbf{A}T}$ , where $T$ is a constant.  \n\nAside from computational methods, several analytical methods are available for the computation of $e^{\\mathbf{A}t}$ .We shall present three methods here.  \n\nComputation of $e^{\\mathbf{A}t}.$ : Method 1. If matrix A can be transformed into a diagonal form, then $e^{\\mathbf{A}t}$ can be given by  \n\n$$\ne^{\\mathbf{A}t}=\\mathbf{P}e^{\\mathbf{D}t}\\mathbf{P}^{-1}=\\mathbf{P}\\left[\\begin{array}{c c c c c c}{e^{\\lambda_{1}t}}&&&&&{0}\\\\ &{e^{\\lambda_{2}t}}&&&&\\\\ &&{\\cdot}&&&\\\\ &&&{\\cdot}&&\\\\ &&&&{\\cdot}&\\\\ {0}&&&&&{e^{\\lambda_{n}t}}\\end{array}\\right]\\mathbf{P}^{-1}\n$$  \n\nwhere $\\mathbf{P}$ is a diagonalizing matrix for A . [For the derivation of Equation (9–46), see Problem A–9–11 .]  \n\nIf matrix A can be transformed into a Jordan canonical form,then $e^{\\mathbf{A}t}$ can be given by  \n\n$$\ne^{\\mathbf{A}t}\\,=\\,\\mathbf{S}e^{\\mathbf{J}t}\\mathbf{S}^{-1}\n$$  \n\nwhere Sis a transformation matrix that transforms matrix $\\mathbf{A}$ into a Jordan canonical form J.  \n\nAs an example, consider the following matrix A:  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {1}&{-3}&{3}\\end{array}\\right]}\n$$  \n\nThe characteristic equation is  \n\n$$\n\\vert\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\vert\\,=\\,\\lambda^{3}\\,-\\,3\\lambda^{2}\\,+\\,3\\lambda\\,-\\,1\\,=\\,(\\lambda\\,-\\,1)^{3}\\,=\\,0\n$$  \n\nThus,matrix A has a multiple eigenvalue of order 3 at $\\lambda=1.$ It can be shown that matrix A has a multiple eigenvector of order 3.The transformation matrix that will transform matrix A into a Jordan canonical form can be given by  \n\n$$\n\\mathbf{S}={\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {1}&{1}&{0}\\\\ {1}&{2}&{1}\\end{array}\\right]}\n$$  \n\nThe inverse of matrix Sis  \n\n$$\n\\mathbf{S}^{-1}=\\left[{\\begin{array}{r r r}{1}&{0}&{0}\\\\ {-1}&{1}&{0}\\\\ {1}&{-2}&{1}\\end{array}}\\right]\n$$  \n\nThen it can be seen that  \n\n$$\n{\\begin{array}{r l}{\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}={\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {-1}&{1}&{0}\\\\ {1}&{-2}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {1}&{-3}&{3}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {1}&{1}&{0}\\\\ {1}&{2}&{1}\\end{array}\\right]}}\\\\ {={\\left[\\begin{array}{l l l}{1}&{1}&{0}\\\\ {0}&{1}&{1}\\\\ {0}&{0}&{1}\\end{array}\\right]}=\\mathbf{J}}\\end{array}}\n$$  \n\nNoting that  \n\n$$\ne^{\\mathbf{J}t}={\\left[\\begin{array}{l l l}{e^{t}}&{t e^{t}}&{{\\frac{1}{2}}t^{2}e^{t}}\\\\ {0}&{e^{t}}&{t e^{t}}\\\\ {0}&{0}&{e^{t}}\\end{array}\\right]}\n$$  \n\nwe find  \n\n$$\n\\begin{array}{r l}&{e^{\\mathbf{A}t}=\\mathbf{S}e^{\\mathbf{J}t}\\mathbf{S}^{-1}}\\\\ &{\\qquad={\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {1}&{1}&{0}\\\\ {1}&{2}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{e^{t}}&{t e^{t}}&{\\frac{1}{2}t^{2}e^{t}}\\\\ {0}&{e^{t}}&{t e^{t}}\\\\ {0}&{0}&{e^{t}}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {-1}&{1}&{0}\\\\ {1}&{-2}&{1}\\end{array}\\right]}}\\\\ &{\\qquad={\\left[\\begin{array}{l l l}{e^{t}-t e^{t}+\\frac{1}{2}t^{2}e^{t}}&{t e^{t}-t^{2}e^{t}}&{\\frac{1}{2}t^{2}e^{t}}\\\\ {\\;}&{\\frac{1}{2}t^{2}e^{t}}&{e^{t}-t e^{t}-t^{2}e^{t}}&{t e^{t}+\\frac{1}{2}t^{2}e^{t}}\\\\ {t e^{t}+\\frac{1}{2}t^{2}e^{t}}&{-3t e^{t}-t^{2}e^{t}}&{e^{t}+2t e^{t}+\\frac{1}{2}t^{2}e^{t}}\\end{array}\\right]}}\\end{array}\n$$  \n\nComputation of $e^{\\mathbf{A}t}$ : Method 2. The second method of computing $e^{\\mathbf{A}t}$ uses the Laplace transform approach. Referring to Equation (9–36), $e^{\\mathbf{A}t}$ can be given as follows:  \n\n$$\ne^{\\mathbf{A}t}\\,=\\,\\mathcal{L}^{-1}\\bigl[\\bigl(s\\mathbf{I}\\,-\\,\\mathbf{A}\\bigr)^{-1}\\bigr]\n$$  \n\nThus, to obtain $e^{\\mathbf{A}t}$ ,first invert the matrix $(s\\mathbf{I}\\mathrm{~-~}\\mathbf{A})$ .This results in a matrix whose elements are rational functions of $s$ .Then take the inverse Laplace transform of each element of the matrix.  \n\nEXAMPLE 9–7 Consider the following matrix A:  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{-2}\\end{array}\\right]}\n$$  \n\nCompute $e^{\\mathbf{A}t}$ by use of the two analytical methods presented previously.  \n\nMethod 1. The eigenvalues of $\\mathbf{A}$ are 0 and $-2\\left(\\lambda_{1}=0,\\lambda_{2}=-2\\right)$ .A necessary transformation matrix $\\mathbf{P}$ may be obtained as  \n\n$$\n\\mathbf{P}={\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{-2}\\end{array}\\right]}\n$$  \n\nThen, from Equation (9–46), $e^{\\mathbf{A}t}$ is obtained as follows:  \n\n$$\ne^{\\mathbf{A}t}={\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l l}{e^{0}}&{0}\\\\ {0}&{e^{-2t}}\\end{array}\\right]}{\\left[\\begin{array}{l l}{1}&{{\\frac{1}{2}}}\\\\ {0}&{-{\\frac{1}{2}}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{{\\frac{1}{2}}{\\big(}1\\,-\\,e^{-2t}{\\big)}}\\\\ {0}&{e^{-2t}}\\end{array}\\right]}\n$$  \n\nMethod 2. Since  \n\n$$\ns\\mathbf{I}-\\mathbf{A}={\\left[\\begin{array}{l l}{s}&{0}\\\\ {0}&{s}\\end{array}\\right]}-{\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{-2}\\end{array}\\right]}={\\left[\\begin{array}{l l}{s}&{-1}\\\\ {0}&{s+2}\\end{array}\\right]}\n$$  \n\nwe obtain  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})^{-1}={\\left[\\begin{array}{l l}{{\\cfrac{1}{s}}}&{{\\cfrac{1}{s(s+2)}}}\\\\ {0}&{{\\cfrac{1}{s+2}}}\\end{array}\\right]}\n$$  \n\n$$\ne^{\\mathbf{A}t}={\\mathcal{L}}^{\\mathrm{1}}{\\bigl[}(s\\mathbf{I}-\\mathbf{A})^{-1}{\\bigr]}={\\left[\\begin{array}{l l}{1}&{{\\frac{1}{2}}{\\bigl(}1\\,-\\,e^{-2t}{\\bigr)}}\\\\ {0}&{e^{-2t}}\\end{array}\\right]}\n$$  \n\nComputation of $e^{\\mathbf{A}t}$ : Method 3. The third method is based on Sylvester’s interpolation method.(For Sylvester’s interpolation formula,see Problem A–9–12 .) We shall first consider the case where the roots of the minimal polynomial $\\phi(\\lambda)$ of $\\mathbf{A}$ are distinct. Then we shall deal with the case of multiple roots.  \n\nCase 1: Minimal Polynomial of A Involves Only Distinct Roots. We shall assume that the degree of the minimal polynomial of $\\mathbf{A}$ is m. By using Sylvester’s interpolation formula, it can be shown that $e^{\\mathbf{\\bar{A}}t}$ can be obtained by solving the following determinant equation:  \n\n$$\n\\begin{array}{r l r l r}{{1}}&{\\lambda_{1}}&{\\lambda_{1}^{2}}&{\\cdots}&{\\lambda_{1}^{m-1}}&{e^{\\lambda_{1}t}}\\\\ {1}&{\\lambda_{2}}&{\\lambda_{2}^{2}}&{\\cdots}&{\\lambda_{2}^{m-1}}&{e^{\\lambda_{2}t}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}&{\\cdot}\\\\ {1}&{\\lambda_{m}}&{\\lambda_{m}^{2}}&{\\cdots}&{\\lambda_{m}^{m-1}}&{e^{\\lambda_{m}t}}\\\\ {\\mathbf{I}}&{\\mathbf{A}}&{\\mathbf{A}^{2}}&{\\cdots}&{\\mathbf{A}^{m-1}}&{e^{\\mathbf{A}t}}\\end{array}\n$$  \n\nBy solving Equation (9–47) for $e^{\\mathbf{A}t},\\,e^{\\mathbf{A}t}$ can be obtained in terms of the $\\mathbf{A}^{k}$ $(k=0,1$ ,$2,\\ldots,m\\,-\\,1)$ and the $e^{\\lambda_{i}t}\\,(i\\,=\\,1,2,3,\\ldots,m)$ . [Equation (9–47) may be expanded, for example, about the last column.]  \n\nNotice that solving Equation (9–47) for $e^{\\mathbf{A}t}$ is the same as writing  \n\n$$\ne^{\\mathbf{A}t}=\\alpha_{0}(t)\\mathbf{I}+\\alpha_{1}(t)\\mathbf{A}+\\alpha_{2}(t)\\mathbf{A}^{2}+\\cdots+\\alpha_{m-1}(t)\\mathbf{A}^{m-1}\n$$  \n\nand determining the $x_{k}(t)\\ (k=0,1,2,\\ldots,m\\,-\\,1)$ by solving the following set of $_m$ equations for the $\\alpha_{k}(t)$ :  \n\n$$\n\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{m}\\,+\\,\\alpha_{2}(t)\\lambda_{m}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}(t)\\lambda_{m}^{m-1}\\,=\\,e^{\\lambda_{m}t}\n$$  \n\nIf $\\mathbf{A}$ is an $n\\times n$ matrix and has distinct eigenvalues, then the number of $\\alpha_{k}(t)$ ’sto be determined is $m\\,=\\,n$ .If $\\mathbf{A}$ involves multiple eigenvalues, but its minimal polynomial has only simple roots, however, then the number $_m$ of $\\alpha_{k}(t)$ ’sto be determined is less than $n$ .  \n\nCase 2: Minimal Polynomial of A Involves Multiple Roots. As an example, consider Bthe case where the minimal polynomial of $\\mathbf{A}$ involves three equal roots $\\hat{(\\lambda_{1}^{\\circ}}=\\lambda_{2}=\\lambda_{3})$ and has other roots $\\left(\\lambda_{4},\\bar{\\lambda_{5}},\\dots,\\lambda_{m}\\right)$ that are all distinct. By applying Sylvester’s interpolation formula, it can be shown that $e^{\\mathbf{A}t}$ can be obtained from the following determinant equation:  \n\n$$\n\\begin{array}{c c c c c c c}{{0}}&{{0}}&{{1}}&{{3\\lambda_{1}}}&{{\\cdots}}&{{\\frac{(m-1)(m-2)}{2}\\,\\lambda_{1}^{m-3}}}&{{\\frac{t^{2}}{2}\\,e^{\\lambda_{1}t}}}\\\\ {{0}}&{{1}}&{{2\\lambda_{1}}}&{{3\\lambda_{1}^{2}}}&{{\\cdots}}&{{(m-1)\\lambda_{1}^{m-2}}}&{{t e^{\\lambda_{1}t}}}\\\\ {{1}}&{{\\lambda_{1}}}&{{\\lambda_{1}^{2}}}&{{\\lambda_{1}^{3}}}&{{\\cdots}}&{{\\lambda_{1}^{m-1}}}&{{e^{\\lambda_{1}t}}}\\\\ {{1}}&{{\\lambda_{4}}}&{{\\lambda_{2}^{4}}}&{{\\lambda_{3}^{3}}}&{{\\cdots}}&{{\\lambda_{m-1}^{m-1}}}&{{e^{\\lambda_{4}t}}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdots}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdots}}&{{\\cdot}}&{{\\cdot}}\\\\ {{1}}&{{\\lambda_{m}}}&{{\\lambda_{m}^{2}}}&{{\\lambda_{m}^{3}}}&{{\\cdots}}&{{\\lambda_{m-1}^{m-1}}}&{{e^{\\lambda_{m}t}}}\\\\ {{{\\bf1}}}&{{{\\bf A}}}&{{{\\bf A}^{2}}}&{{\\bf A}^{3}}&{{\\cdots}}&{{\\bf A}^{m-1}}&{{e^{\\lambda_{1}t}}}\\end{array}\\right]={\\bf0}\n$$  \n\nEquation (9–49) can be solved for $e^{\\mathbf{A}t}$ by expanding it about the last column. It is noted that, just as in case 1, solving Equation (9–49) for $e^{\\mathbf{A}t}$ is the same as writing  \n\n$$\ne^{\\mathbf{A}t}=\\alpha_{0}(t)\\mathbf{I}+\\alpha_{1}(t)\\mathbf{A}+\\alpha_{2}(t)\\mathbf{A}^{2}+\\cdots+\\,\\alpha_{m-1}(t)\\,\\mathbf{A}^{m-1}\n$$  \n\nand determining the $\\alpha_{k}(t)$ ’s $(k=0,1,2,\\ldots,m\\,-\\,1)$ from  \n\n$$\n\\begin{array}{c}{{\\alpha_{2}(t)\\,+\\,3\\alpha_{3}(t)\\lambda_{1}\\,+\\,\\cdots\\,+\\,\\displaystyle{\\frac{(m-1)(m-2)}{2}}\\,\\alpha_{m-1}(t)\\lambda_{1}^{m-3}\\,=\\,\\displaystyle{\\frac{t^{2}}{2}}\\,e^{\\lambda_{1}t}}}\\\\ {{\\alpha_{1}(t)\\,+\\,2\\alpha_{2}(t)\\lambda_{1}\\,+\\,3\\alpha_{3}(t)\\lambda_{1}^{2}\\,+\\,\\cdots\\,+\\,(m-1)\\alpha_{m-1}(t)\\lambda_{1}^{m-2}\\,=\\,t e^{\\lambda_{1}t}}}\\\\ {{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{1}\\,+\\,\\alpha_{2}(t)\\lambda_{1}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}(t)\\lambda_{1}^{m-1}\\,=\\,e^{\\lambda_{1}t}}}\\\\ {{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{4}\\,+\\,\\alpha_{2}(t)\\lambda_{4}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}(t)\\lambda_{4}^{m-1}\\,=\\,e^{\\lambda_{4}t}}}\\\\ {{\\,\\cdot\\,\\,}}\\\\ {{\\cdot\\,\\,\\,}}\\\\ {{\\cdot\\,\\,\\,}}\\\\ {{\\langle{\\partial_{\\lambda}(t)}\\,\\bar{\\mathrm{\\boldmath{~x~}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}}\\\\ {{\\,\\cdot\\,\\cdot\\,\\cdot\\,\\bar{\\mathrm{\\boldmath~x~}}\\!\\!\\!\\!\\!\\!\\!}}\\\\ {{\\,\\cdot\\,\\cdot\\,\\cdot\\,\\bar{\\mathrm{\\boldmath~x~}}\\!\\!\\!\\!\\!\\!\\!}}\\\\ {{\\,\\cdot\\,\\cdot\\,\\cdot\\,\\cdot\\,\\bar{\\mathrm{\\boldmath~x~}}\\!\\!\\!\\!\\!\\!}}\\end{array}\n$$  \n\n$$\n\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{m}\\,+\\,\\alpha_{2}(t)\\lambda_{m}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}(t)\\lambda_{m}^{m-1}\\,=\\,e^{\\lambda_{m}t}\n$$  \n\nThe extension to other cases where, for example, there are two or more sets of multiple roots will be apparent.Note that if the minimal polynomial of A is not found,it is possible to substitute the characteristic polynomial for the minimal polynomial.The number of computations may, of course, be increased.  \n\n# EXAMPLE 9–8 Consider the matrix  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{-2}\\end{array}\\right]}\n$$  \n\nCompute $e^{\\mathbf{A}t}$ using Sylvester’s interpolation formula. From Equation (9–47), we get  \n\n$$\n\\left|\\begin{array}{l l l}{1}&{\\lambda_{1}}&{e^{\\lambda_{1}t}}\\\\ {1}&{\\lambda_{2}}&{e^{\\lambda_{2}t}}\\\\ {\\mathbf{I}}&{\\mathbf{A}}&{e^{\\mathbf{A}t}}\\end{array}\\right|=\\mathbf{0}\n$$  \n\nSubstituting 0 for $\\lambda_{1}$ and $^{-2}$ for $\\lambda_{2}$ in this last equation, we obtain  \n\n$$\n\\left|{\\begin{array}{c c c}{1}&{0}&{1}\\\\ {1}&{-2}&{e^{-2t}}\\\\ {\\mathbf{I}}&{\\mathbf{A}}&{e^{\\mathbf{A}t}}\\end{array}}\\right|=\\mathbf{0}\n$$  \n\nExpanding the determinant, we obtain  \n\n$$\n-2e^{\\mathbf{A}t}\\,+\\,\\mathbf{A}\\,+\\,2\\mathbf{I}\\,-\\,\\mathbf{A}e^{-2t}\\,=\\,\\mathbf{0}\n$$  \n\nor  \n\n$$\n{\\begin{array}{r l}&{e^{\\mathbf{A}t}={\\frac{1}{2}}{\\left(\\mathbf{A}\\right.}+2\\mathbf{I}-\\mathbf{A}e^{-2t})}\\\\ &{\\qquad={\\cfrac{1}{2}}{\\left\\{\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{-2}\\end{array}\\right]\\right.}+\\left[\\begin{array}{l l}{2}&{0}\\\\ {0}&{2}\\end{array}\\right]}-{\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{-2}\\end{array}\\right]}e^{-2t}{\\right\\}}}\\\\ &{\\qquad={\\left[\\begin{array}{l l}{1}&{{\\frac{1}{2}}{\\left(1\\right.-\\left.e^{-2t}\\right)}}\\\\ {0}&{e^{-2t}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nAn alternative approach is to use Equation (9–48).We first determine $\\alpha_{0}(t)$ and $\\alpha_{1}(t)$ from  \n\n$$\n\\begin{array}{r}{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{1}=\\,e^{\\lambda_{1}t}}\\\\ {\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{2}=\\,e^{\\lambda_{2}t}}\\end{array}\n$$  \n\nSince $\\lambda_{1}=0$ and $\\lambda_{2}=-2$ ,the last two equations become  \n\n$$\n\\begin{array}{c}{\\alpha_{0}(t)\\,=\\,1}\\\\ {\\alpha_{0}(t)\\,-\\,2\\alpha_{1}(t)\\,=\\,e^{-2t}}\\end{array}\n$$  \n\nSolving for $\\alpha_{0}(t)$ and $\\alpha_{1}(t)$ gives  \n\n$$\n\\alpha_{0}(t)\\,=\\,1,\\qquad\\alpha_{1}(t)\\,=\\frac{1}{2}\\,\\bigl(1\\,-\\,e^{-2t}\\bigr)\n$$  \n\nThen $e^{\\mathbf{A}t}$ can be written as  \n\n$$\ne^{\\mathbf{A}t}=\\alpha_{0}(t)\\mathbf{I}+\\alpha_{1}(t)\\mathbf{A}=\\mathbf{I}+{\\frac{1}{2}}\\bigl(1-e^{-2t}\\bigr)\\mathbf{A}={\\left[\\begin{array}{l l}{1}&{{\\frac{1}{2}}\\bigl(1-e^{-2t}\\bigr)}\\\\ {0}&{e^{-2t}}\\end{array}\\right]}\n$$  \n\nLinear Independence of Vectors. The vectors $\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}$ are said to be linearly independent if  \n\n$$\nc_{1}\\mathbf{x}_{1}+c_{2}\\mathbf{x}_{2}+\\cdots+c_{n}\\mathbf{x}_{n}=\\mathbf{0}\n$$  \n\nwhere $c_{1},c_{2},\\ldots,c_{n}$ are constants, implies that  \n\n$$\nc_{1}=c_{2}=\\cdots=c_{n}=0\n$$  \n\nConversely, the vectors $\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}$ are said to be linearly dependent if and only if $\\mathbf{X}_{i}$ can be expressed as a linear combination of $\\mathbf{x}_{j}\\left(j=1,2,\\ldots,n;j\\neq i\\right)$ ,or  \n\n$$\n\\mathbf{x}_{i}=\\sum_{\\stackrel{j=1}{j\\neq i}}^{n}c_{j}\\mathbf{x}_{j}\n$$  \n\nfor some set of constants $c_{j}$ .This means that if $\\mathbf{X}_{i}$ can be expressed as a linear combination of the other vectors in the set,it is linearly dependent on them or it is not an independent member of the set.  \n\nEXAMPLE 9–9 The vectors  \n\n$$\n\\mathbf{x}_{1}={\\left[\\begin{array}{l}{1}\\\\ {2}\\\\ {3}\\end{array}\\right]},\\qquad\\mathbf{x}_{2}={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{x}_{3}={\\left[\\begin{array}{l}{2}\\\\ {2}\\\\ {4}\\end{array}\\right]}\n$$  \n\nare linearly dependent since  \n\n$$\n\\mathbf{x}_{1}+\\mathbf{\\deltax}_{2}-\\mathbf{\\deltax}_{3}=\\mathbf{0}\n$$  \n\nThe vectors  \n\n$$\n\\mathbf{y}_{1}={\\left[\\begin{array}{l}{1}\\\\ {2}\\\\ {3}\\end{array}\\right]},\\qquad\\mathbf{y}_{2}={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{y}_{3}={\\left[\\begin{array}{l}{2}\\\\ {2}\\\\ {2}\\end{array}\\right]}\n$$  \n\nare linearly independent since  \n\n$$\nc_{1}\\mathbf{y}_{1}\\,+\\,c_{2}\\mathbf{y}_{2}\\,+\\,c_{3}\\mathbf{y}_{3}\\,=\\,\\mathbf{0}\n$$  \n\nimplies that  \n\n$$\nc_{1}=c_{2}=c_{3}=0\n$$  \n\nNote that if an $n\\times n$ matrix is nonsingular (that is, the matrix is of rank $n$ or the determinant is nonzero) then $n$ column (or row) vectors are linearly independent.If the $n\\times n$ matrix is singular (that is, the rank of the matrix is less than $n$ or the determinant is zero), then $n$ column (or row) vectors are linearly dependent.To demonstrate this, notice that  \n\n$$\n{\\begin{array}{r l}&{\\left[\\mathbf{x}_{1}\\ \\mid\\ \\mathbf{x}_{2}\\ \\mid\\ \\mathbf{x}_{3}\\right]={\\left[\\begin{array}{l l l}{1}&{1}&{2}\\\\ {2}&{0}&{2}\\\\ {3}&{1}&{4}\\end{array}\\right]}={\\mathrm{singular}}}\\\\ &{\\left[\\mathbf{y}_{1}\\ \\mid\\ \\mathbf{y}_{2}\\ \\mid\\ \\mathbf{y}_{3}\\right]={\\left[\\begin{array}{l l l}{1}&{1}&{2}\\\\ {2}&{0}&{2}\\\\ {3}&{1}&{2}\\end{array}\\right]}={\\mathrm{nonsingular}}}\\end{array}}\n$$  \n\n# 9–6 CONTROLLABILITY  \n\nControllability and Observability. A system is said to be controllable at time $t_{0}$ if it is possible by means of an unconstrained control vector to transfer the system from any initial state ${\\bf x}(t_{0})$ to any other state in a finite interval of time.  \n\nA system is said to be observable at time $t_{0}$ if,with the system in state ${\\bf x}(t_{0})$ ,it is possible to determine this state from the observation of the output over a finite time interval.  \n\nThe concepts of controllability and observability were introduced by Kalman.They play an important role in the design of control systems in state space. In fact, the conditions of controllability and observability may govern the existence of a complete solution to the control system design problem. The solution to this problem may not exist if the system considered is not controllable. Although most physical systems are controllable and observable, corresponding mathematical models may not possess the property of controllability and observability.Then it is necessary to know the conditions under which a system is controllable and observable.This section deals with controllability and the next section discusses observability.  \n\nIn what follows, we shall first derive the condition for complete state controllability. Then we derive alternative forms of the condition for complete state controllability followed by discussions of complete output controllability.Finally,we present the concept of stabilizability.  \n\nComplete State Controllability of Continuous-Time Systems. Consider the continuous-time system.  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere $\\mathbf{X}=$ state vector (n -vector )  \n\n$u=$ control signal (scalar )  \n$\\mathbf{A}=n\\times n$ matrix   \nB=n\\* 1 matrix  \n\nThe system described by Equation (9–51) is said to be state controllable at $t\\,=\\,t_{0}$ if it is possible to construct an unconstrained control signal that will transfer an initial state to any final state in a finite time interval $t_{0}\\le t\\le t_{1}$ .If every state is controllable, then the system is said to be completely state controllable.  \n\nWe shall now derive the condition for complete state controllability.Without loss of generality, we can assume that the final state is the origin of the state space and that the initial time is zero, or $t_{0}=0$ .  \n\nThe solution of Equation (9–51) is  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)+\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}u(\\tau)\\,d\\tau\n$$  \n\nApplying the definition of complete state controllability just given, we have  \n\n$$\n\\mathbf{x}(t_{1})\\,=\\,\\pmb{0}\\,=\\,e^{\\mathbf{A}t_{1}}\\mathbf{x}(0)+\\int_{0}^{t_{1}}\\!e^{\\mathbf{A}(t_{1}-\\tau)}\\mathbf{B}u(\\tau)\\,d\\tau\n$$  \n\nor  \n\n$$\n\\mathbf{x}(0)=-\\int_{0}^{t_{1}}\\!e^{-\\mathbf{A}\\tau}\\mathbf{B}u(\\tau)\\,d\\tau\n$$  \n\nReferring to Equation (9–48) or (9–50), $e^{-\\mathbf{A}\\tau}$ can be written  \n\n$$\ne^{-\\mathbf{A}\\tau}=\\sum_{k=0}^{n-1}\\alpha_{k}(\\tau)\\mathbf{A}^{k}\n$$  \n\nSubstituting Equation (9–53) into Equation (9–52) gives  \n\n$$\n\\mathbf{x}(0)=-\\sum_{k=0}^{n-1}\\mathbf{A}^{k}\\mathbf{B}\\int_{0}^{t_{1}}\\!\\!\\alpha_{k}(\\tau)u(\\tau)\\,d\\tau\n$$  \n\nLet us put  \n\n$$\n\\int_{0}^{t_{1}}\\!\\alpha_{k}(\\tau)u(\\tau)\\,d\\tau=\\beta_{k}\n$$  \n\nThen Equation (9–54) becomes  \n\n$$\n\\begin{array}{r l r}{\\mathbf{x}(0)}&{=-\\overset{n-1}{\\underset{k=0}{\\sum}}\\mathbf{A}^{k}\\mathbf{B}\\beta_{k}}&\\\\ &{=-\\bigl[\\mathbf{B}\\mid\\mathbf{A}\\mathbf{B}\\;\\bigr\\}\\;\\cdots\\;\\bigr\\downarrow\\;\\mathbf{A}^{n-1}\\mathbf{B}\\bigr]\\!\\stackrel{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\frac{\\beta_{0}}{\\beta_{1}}}{\\biggl\\downarrow}}&\\\\ &{=-\\bigl[\\mathbf{B}\\;\\;\\bigr\\downarrow\\;\\mathbf{A}\\mathbf{B}\\;\\bigr\\downarrow\\;\\cdots\\;\\bigr\\downarrow\\;\\mathbf{A}^{n-1}\\mathbf{B}\\bigr]\\!\\stackrel{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\;\\;\\;\\;\\;\\;}{\\longrightarrow}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\;\\;\\;\\;\\;}\\\\ &{\\quad\\cdots}&\\end{array}\n$$  \n\nIf the system is completely state controllable, then, given any initial state ${\\bf x}(0)$ ,Equation (9–55) must be satisfied.This requires that the rank of the $n\\times n$ matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nbe $n$ .  \n\nFrom this analysis,we can state the condition for complete state controllability as follows: The system given by Equation (9–51) is completely state controllable if and only if the vectors B,AB ,p,$\\mathbf{\\bar{A}}^{n-\\bar{1}}\\mathbf{B}$ are linearly independent, or the $n\\times n$ matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nis of rank $n$ .  \n\nThe result just obtained can be extended to the case where the control vector $\\mathbf{u}$ is $r$ -dimensional. If the system is described by  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\nwhere $\\mathbf{u}$ is an $r$ -vector, then it can be proved that the condition for complete state controllability is that the $n\\times n r$ matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nbe of rank $n$ ,or contain $n$ linearly independent column vectors.The matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nis commonly called the controllability matrix.  \n\nEXAMPLE 9–10 Consider the system given by  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{1}{0}}-1{\\binom{1}{L}}{\\binom{x_{1}}{x_{2}}}+{\\binom{1}{0}}u\n$$  \n\nSince  \n\n$$\n\\left[\\mathbf{B}\\;\\;\\{\\mathrm{\\bf~A}\\mathbf{B}\\}\\right]=\\left[{\\begin{array}{r r}{1}&{1}\\\\ {0}&{0}\\end{array}}\\right]=\\,\\mathrm{singular}\n$$  \n\nthe system is not completely state controllable.  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{\\dot{1}}{2}}-1{\\binom{\\biggr[}{\\underline{{x}}}{\\underline{{1}}}{\\biggr]}}+{\\binom{0}{1}}[u]\n$$  \n\nFor this case,  \n\n$$\n\\left[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\right]=\\left[{\\begin{array}{r r}{0}&{1}\\\\ {1}&{-1}\\end{array}}\\right]={\\mathrm{nonsingular}}\n$$  \n\nThe system is therefore completely state controllable.  \n\nAlternative Form of the Condition for Complete State Controllability. Consider the system defined by  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ($n$ -vector )  \n\n${\\textbf{u}}=$ control vector ($r$ -vector )  \nA =n\\* nmatrix   \nB=n\\* rmatrix  \n\nIf the eigenvectors of $\\mathbf{A}$ are distinct, then it is possible to find a transformation matrix $\\mathbf{P}$ such that  \n\n$$\n\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}={\\left[\\begin{array}{l l l l l l}{\\lambda_{1}}&{}&{}&{}&{}&{0}\\\\ {}&{\\lambda_{2}}&{}&{}&{}&{}\\\\ {}&{}&{\\cdot}&{}&{}&{}\\\\ {}&{}&{}&{\\cdot}&{}&{}\\\\ {}&{}&{}&{}&{\\cdot}&{}\\\\ {0}&{}&{}&{}&{}&{\\lambda_{n}}\\end{array}\\right]}\n$$  \n\nNote that if the eigenvalues of A are distinct,then the eigenvectors of A are distinct;however, the converse is not true. For example, an $n\\times n$ real symmetric matrix having multiple eigenvalues has $n$ distinct eigenvectors. Note also that each column of the $\\mathbf{P}$ matrix is an eigenvector of $\\mathbf{A}$ associated with $\\lambda_{i}\\,(i\\,=\\,1,2,\\ldots,n)$ .  \n\nLet us define  \n\n$$\n\\mathbf{x}=\\mathbf{P}\\mathbf{z}\n$$  \n\nSubstituting Equation (9–57) into Equation (9–56), we obtain  \n\n$$\n\\dot{\\mathbf{z}}\\,=\\,\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\mathbf{z}\\,+\\,\\mathbf{P}^{-1}\\mathbf{B}\\mathbf{u}\n$$  \n\nBy defining  \n\n$$\n\\mathbf{P}^{-1}\\mathbf{B}=\\mathbf{F}=\\left(f_{i j}\\right)\n$$  \n\nChapter 9 /Control Systems Analysis in State Space  \n\nwe can rewrite Equation (9–58) as  \n\n$$\n\\begin{array}{l}{{\\dot{z}}_{1}=\\lambda_{1}z_{1}\\,+\\,f_{11}u_{1}\\,+\\,f_{12}u_{2}\\,+\\,\\cdots\\,+\\,f_{1r}u_{r}}\\\\ {{\\dot{z}}_{2}=\\lambda_{2}z_{2}\\,+\\,f_{21}u_{1}\\,+\\,f_{22}u_{2}\\,+\\,\\cdots\\,+\\,f_{2r}u_{r}}\\end{array}\n$$  \n\n$$\n{\\dot{z}}_{n}\\,=\\,\\lambda_{n}z_{n}\\,+\\,f_{n1}u_{1}\\,+\\,f_{n2}u_{2}\\,+\\,\\cdots\\,+\\,f_{n r}u_{r}\n$$  \n\nIf the elements of any one row of the $n\\times r$ matrix $\\mathbf{F}$ are all zero, then the corresponding state variable cannot be controlled by any of the $u_{i}$ .Hence, the condition of complete state controllability is that if the eigenvectors of $\\mathbf{A}$ are distinct, then the system is completely state controllable if and only if no row of $\\mathbf{P}^{-1}\\mathbf{B}$ has all zero elements. It is important to note that, to apply this condition for complete state controllability, we must put the matrix $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ in Equation (9–58) in diagonal form.  \n\nIf the A matrix in Equation (9–56) does not possess distinct eigenvectors, then diagonalization is impossible.In such a case,we may transform A into a Jordan canonical form. If, for example, A has eigenvalues $\\lambda_{1},\\lambda_{1},\\lambda_{1},\\lambda_{4},\\lambda_{4},\\lambda_{6},\\ldots,\\lambda_{n}$ and has $n\\,-\\,3$ distinct eigenvectors, then the Jordan canonical form of $\\mathbf{A}$ is  \n\n![](images/257b4b02b4b19762961ff71b8770ca231a7d43c71d6b73e0915abdcbd541b659.jpg)  \n\nThe square submatrices on the main diagonal are called Jordan blocks. Suppose that we can find a transformation matrix Ssuch that  \n\n$$\n\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}=\\mathbf{J}\n$$  \n\nIf we define a new state vector $\\mathbf{z}$ by  \n\n$$\n\\mathbf{x}=\\mathbf{S}\\mathbf{z}\n$$  \n\nthen substitution of Equation (9–59) into Equation (9–56) yields  \n\n$$\n\\begin{array}{c}{{\\dot{\\mathbf{z}}=\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}\\mathbf{z}\\,+\\,\\mathbf{S}^{-1}\\mathbf{B}\\mathbf{u}}}\\\\ {{=\\mathbf{J}\\mathbf{z}\\,+\\,\\mathbf{S}^{-1}\\mathbf{B}\\mathbf{u}}}\\end{array}\n$$  \n\nThe condition for complete state controllability of the system of Equation (9–56) may then be stated as follows: The system is completely state controllable if and only if (1)  \n\nno two Jordan blocks in $\\mathbf{J}$ of Equation (9–60) are associated with the same eigenvalues, (2) the elements of any row of $\\bar{\\mathbf{S}}^{-1}\\mathbf{B}$ that correspond to the last row of each Jordan block are not all zero, and (3) the elements of each row of $\\mathbf{S}^{-1}\\mathbf{B}$ that correspond to distinct eigenvalues are not all zero.  \n\nEXAMPLE 9–12 The following systems are completely state controllable:  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\left[\\begin{array}{l l}{-1}&{\\;\\;0}\\\\ {\\;\\;0}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\binom{2}{5}}u\n$$  \n\n$$\n\\left[\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]=\\left[\\begin{array}{r r r}{-1}&{1}&{0}\\\\ {0}&{-1}&{0}\\\\ {0}&{0}&{-2}\\end{array}\\right]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{c}{0}\\\\ {4}\\\\ {3}\\end{array}\\right]u\n$$  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\\\ {\\dot{x}_{4}}\\\\ {\\dot{x}_{5}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{-2}&{1}&{0\\left\\{\\right\\}}&{0}\\\\ {0}&{-2}&{1}&{\\left\\}\\\\ {0}&{0}&{-2}&{\\left\\}}\\\\ {\\dots}&{0}&{-3}&{1}\\\\ {0}&{\\left\\}&{\\left\\}&{0}&{-5}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\\\ {x_{5}}\\end{array}\\right]}+\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\\\ {3}&{0}\\\\ {0}&{0}\\\\ {2}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\\\ {u_{3}}\\end{array}\\right]}\n$$  \n\nThe following systems are not completely state controllable:  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{-1}{0}}{\\binom{0}{-2}}{\\left[\\!\\!{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}}\\right]}+{\\binom{2}{0}}u\n$$  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-1}&{\\;\\;\\;1}&{\\;\\;0}\\\\ {\\;\\;0}&{-1}&{\\;\\;\\;0}\\\\ {\\;\\;0}&{\\;\\;\\;0}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{4}&{2}\\\\ {0}&{0}\\\\ {3}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}\n$$  \n\nCondition for Complete State Controllability in the sPlane. The condition for complete state controllability can be stated in terms of transfer functions or transfer matrices.  \n\nIt can be proved that a necessary and sufficient condition for complete state controllability is that no cancellation occur in the transfer function or transfer matrix. If cancellation occurs, the system cannot be controlled in the direction of the canceled mode.  \n\nEXAMPLE 9–13 Consider the following transfer function:  \n\n$$\n{\\frac{X(s)}{U(s)}}={\\frac{s\\,+\\,2.5}{(s\\,+\\,2.5)(s\\,-\\,1)}}\n$$  \n\nClearly, cancellation of the factor $(s+2.5)$ occurs in the numerator and denominator of this transfer function. (Thus one degree of freedom is lost.) Because of this cancellation, this system is not completely state controllable.  \n\nThe same conclusion can be obtained by writing this transfer function in the form of a state equation.A state-space representation is  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {2.5}&{-1.5}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {1}\\end{array}\\right]}u\n$$  \n\nSince  \n\n$$\n\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\right]=\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{1}\\end{array}\\right]\n$$  \n\nthe rank of the matrix $\\left[\\mathbf{B}\\right.\\vdots\\left.\\mathbf{A}\\mathbf{B}\\right]$ is 1.Therefore, we arrive at the same conclusion:The system is not completely state controllable.  \n\nOutput Controllability. In the practical design of a control system, we may want to control the output rather than the state of the system. Complete state controllability is neither necessary nor sufficient for controlling the output of the system. For this reason, it is desirable to define separately complete output controllability.  \n\nConsider the system described by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\n$$\n\\begin{array}{r l}&{\\mathbf x=\\mathrm{state\\;vector}\\;(n\\mathrm{\\-vector})}\\\\ &{\\mathbf u=\\mathrm{control}\\;\\mathrm{vector}\\;(r\\mathrm{\\-vector})}\\\\ &{\\mathbf y=\\mathrm{output\\;vector}\\;(m\\mathrm{\\-vector})}\\\\ &{\\mathbf A=n\\times n\\;\\mathrm{matrix}}\\\\ &{\\mathbf B=n\\times r\\;\\mathrm{matrix}}\\\\ &{\\mathbf C=m\\times n\\;\\mathrm{matrix}}\\\\ &{\\mathbf D=m\\times r\\;\\mathrm{matrix}}\\end{array}\n$$  \n\nThe system described by Equations (9–61) and (9–62) is said to be completely output controllable if it is possible to construct an unconstrained control vector ${\\bf\\delta u}(t)$ that will y given initial output $\\mathbf{y}(t_{0})$ to any final output $\\mathbf{y}(t_{1})$ in a finite time interval $t_{0}\\le t\\le t_{1}$ .  \n\nIt can be proved that the condition for complete output controllability is as follows: The system described by Equations (9–61) and (9–62) is completely output controllable if and only if the $m\\,\\times\\,(n\\,+\\,1)r$ matrix  \n\n$$\n\\left[\\mathbf{CB}\\mathbf{\\begin{array}{l l l l l l l}{\\vdots}&{\\mathbf{CAB}}&{\\vdots}&{\\mathbf{CA^{2}B}}&{\\vdots}&{\\cdots}&{\\vdots}&{\\mathbf{CA^{n-1}B}}&{\\vdots}&{\\mathbf{D}}\\end{array}}\\right]\n$$  \n\nis of rank m. (For a proof, see Problem A–9–16 .) Note that the presence of the Du term in Equation (9–62) always helps to establish output controllability.  \n\nUncontrollable System. An uncontrollable system has a subsystem that is physically disconnected from the input.  \n\nStabilizability. For a partially controllable system, if the uncontrollable modes are stable and the unstable modes are controllable, the system is said to be stabilizable. For example, the system defined by  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{-1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]}u\n$$  \n\nis not state controllable.The stable mode that corresponds to the eigenvalue of $-1$ is not controllable.The unstable mode that corresponds to the eigenvalue of 1 is controllable. Such a system can be made stable by the use of a suitable feedback.Thus this system is stabilizable.  \n\n# 9–7 OBSERVABILITY  \n\nIn this section we discuss the observability of linear systems. Consider the unforced system described by the following equations:  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ($n$ -vector )  \n\n$\\mathbf{y}=$ output vector (m -vector )  \n$\\mathbf{A}=n\\times n$ matrix   \nC=m\\* nmatrix  \n\nThe system is said to be mpletely observable if ever ${\\bf x}\\!\\left(t_{0}\\right)$ can be determined from the observation of $\\mathbf{y}(t)$ over a finite time interval, $t_{0}\\le t\\le t_{1}$ .The system is, therefore, completely observable if every transition of the state eventually affects every element of the output vector.The concept of observability is useful in solving the problem of reconstructing unmeasurable state variables from measurable variables in the minimum possible length of time. In this section we treat only linear, time-invariant systems. Therefore, without loss of generality, we can assume that $t_{0}=0$ .  \n\nThe concept of observability is very important because, in practice, the difficulty encountered with state feedback control is that some of the state variables are not accessible for direct measurement, with the result that it becomes necessary to estimate the unmeasurable state variables in order to construct the control signals. It will be shown in Section 10–5 that such estimates of state variables are possible if and only if the system is completely observable.  \n\nIn discussing observability conditions, we consider the unforced system as given by Equations (9–63) and (9–64).The reason for this is as follows: If the system is described by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}+\\mathbf{D}\\mathbf{u}}\\end{array}\n$$  \n\nthen  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)+\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nand $\\mathbf{y}(t)$ is  \n\n$$\n\\mathbf{y}(t)=\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\mathbf{C}\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\\,+\\,\\mathbf{D}\\mathbf{u}\n$$  \n\nSince the matrices $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ , and $\\mathbf{D}$ are known and ${\\bf\\delta u}(t)$ is also known, the last two terms on the right-hand side of this last equation are known quantities. Therefore, they may be subtracted from the observed value of ${\\bf y}(t)$ .Hence, for investigating a necessary and sufficient condition for complete observability,it suffices to consider the system described by Equations (9–63) and (9–64).  \n\nComplete Observability of Continuous-Time Systems. Consider the system described by Equations (9–63) and (9–64).The output vector $\\mathbf{y}(t)$ is  \n\n$$\n\\mathbf{y}(t)\\,=\\,\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nReferring to Equation (9–48) or (9–50), we have  \n\n$$\ne^{\\mathbf{A}t}=\\sum_{k=0}^{n-1}\\alpha_{k}(t)\\,\\mathbf{A}^{k}\n$$  \n\nwhere $n$ is the degree of the characteristic polynomial. [Note that Equations (9–48) and (9–50) with $_m$ replaced by $n$ can be derived using the characteristic polynomial.]  \n\nHence, we obtain  \n\n$$\n{\\mathbf{y}}(t)\\,=\\,\\sum_{k=0}^{n-1}\\alpha_{k}(t){\\mathbf{C}}{\\mathbf{A}}^{k}{\\mathbf{x}}(0)\n$$  \n\nor  \n\n$$\n\\mathbf{y}(t)\\,=\\,\\alpha_{0}(t)\\mathbf{C}\\mathbf{x}(0)\\,+\\,\\alpha_{1}(t)\\mathbf{C}\\mathbf{A}\\mathbf{x}(0)\\,+\\,\\cdots+\\,\\alpha_{n-1}(t)\\,\\mathbf{C}\\mathbf{A}^{n-1}\\mathbf{x}(0)\n$$  \n\nIf the system is completely observable, then, given the output $\\mathbf{y}(t)$ over a time interval $0\\le t\\le t_{1},{\\bf x}(0)$ is uniquely determined from Equation (9–65). It can be shown that this requires the rank of the $n m\\times n$ matrix  \n\n$$\n\\begin{array}{r}{\\overbrace{\\phantom{\\left(\\begin{array}{c}{\\nabla_{\\mathrm{e}}\\kappa}\\\\ {\\cdot}\\\\ {\\nabla_{\\mathrm{e}}\\kappa}\\end{array}\\right)}^{\\mathbf{C}}}^{\\mathbf{C}}}\\\\ {\\cdot\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ {\\cdot\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ {\\cdot\\--\\cdots\\cdots}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}}\\end{array}\n$$  \n\nto be $n$ .(See Problem A–9–19 for the derivation of this condition.)  \n\nFrom this analysis, we can state the condition for complete observability as follows: The system described by Equations (9–63) and (9–64) is completely observable if and only if the $n\\times n m$ matrix  \n\n$$\n[\\mathbf{C}^{*}\\mid\\mathbf{\\nabla}\\mathbf{A}^{*}\\mathbf{C}^{*}\\mid\\mathbf{\\nabla}\\cdots\\mid\\mathbf{\\nabla}(\\mathbf{A}^{*})^{n-1}\\mathbf{C}^{*}]\n$$  \n\nis of rank $n$ or has $n$ linearly independent column vectors. This matrix is called the observability matrix.  \n\nEXAMPLE 9–14 Consider the system described by  \n\n$$\n\\begin{array}{r l}&{\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{c c}{1}&{1}\\\\ {-2}&{-1}\\end{array}\\right]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\left[\\!\\!\\begin{array}{c}{0}\\\\ {1}\\end{array}\\!\\!\\right]\\!\\!u}\\\\ &{\\qquad y=[1}&{0]\\!\\!\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}\\end{array}\n$$  \n\nIs this system controllable and observable? Since the rank of the matrix  \n\n$$\n\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\right]=\\left[\\begin{array}{c c}{0}&{1}\\\\ {1}&{-1\\underline{{\\mathbf{\\sigma}}}}\\end{array}\\right]\n$$  \n\nis 2, the system is completely state controllable.  \n\nFor output controllability, let us find the rank of the matrix $\\left[\\mathbf{CB}\\right.\\mathrm{~};\\mathrm{~}\\left.\\mathbf{CAB}\\right]$ .Since  \n\n$$\n\\left[\\mathbf{CB}\\ :\\ \\mathbf{CAB}\\right]=\\left[0\\ \\ 1\\right]\n$$  \n\nthe rank of this matrix is 1. Hence, the system is completely output controllable. To test the observability condition, examine the rank of $\\left[\\mathbf{C}^{*}\\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\right]$ .Since  \n\n$$\n\\left[\\mathbf{C}^{*}\\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\right]={\\left[\\begin{array}{l l}{1}&{1}\\\\ {0}&{1}\\end{array}\\right]}\n$$  \n\nthe rank of $\\left[\\mathbf{C}^{*}\\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\right]$ is 2. Hence, the system is completely observable.  \n\nConditions for Complete Observability in the s Plane. The conditions for complete observability can also be stated in terms of transfer functions or transfer matrices. The necessary and sufficient conditions for complete observability is that no cancellation occur in the transfer function or transfer matrix. If cancellation occurs, the canceled mode cannot be observed in the output.  \n\n# EXAMPLE 9–15  \n\nShow that the following system is not completely observable:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{x}={\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]},\\qquad\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[4}&{5}&{1]\n$$  \n\nNote that the control function $u$ does not affect the complete observability of the system.To examine complete observability, we may simply set $u\\,=\\,0$ .For this system, we have  \n\n$$\n[\\mathbf{C}^{*}\\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{2}\\mathbf{C}^{*}]=\\left[\\begin{array}{r r r}{4}&{-6}&{6}\\\\ {5}&{-7}&{5}\\\\ {1}&{-1}&{-1}\\end{array}\\right]\n$$  \n\nNote that  \n\n$$\n\\left|{\\begin{array}{r r r}{4}&{-6}&{6}\\\\ {5}&{-7}&{5}\\\\ {1}&{-1}&{-1}\\end{array}}\\right|=0\n$$  \n\nHence, the rank of the matrix $\\left[\\mathbf{C}^{*}\\;\\vdots\\;\\mathbf{A}^{*}\\mathbf{C}^{*}\\;\\vdots\\;\\mathbf{\\Gamma}(\\mathbf{A}^{*})^{2}\\mathbf{C}^{*}\\right]$ is less than 3.Therefore, the system is not completely observable.  \n\nIn fact, in this system, cancellation occurs in the transfer function of the system.The transfer function between $X_{1}(s)$ and $U(s)$ is  \n\n$$\n\\frac{X_{1}(s)}{U(s)}=\\frac{1}{(s\\,+\\,1)(s\\,+\\,2)(s\\,+\\,3)}\n$$  \n\nand the transfer function between $Y(s)$ and $X_{1}(s)$ is  \n\n$$\n{\\frac{Y(s)}{X_{1}(s)}}=\\,(s\\,+\\,1)(s\\,+\\,4)\n$$  \n\nTherefore, the transfer function between the output $Y(s)$ and the input $U(s)$ is  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{(s\\,+\\,1)(s\\,+\\,4)}{(s\\,+\\,1)(s\\,+\\,2)(s\\,+\\,3)}}\n$$  \n\nClearly, the two factors $(s+1)$ cancel each other.This means that there are nonzero initial states ${\\bf x}(0)$ ,which cannot be determined from the measurement of $y(t)$ .  \n\nComments. The transfer function has no cancellation if and only if the system is completely state controllable and completely observable.This means that the canceled transfer function does not carry along all the information characterizing the dynamic system.  \n\nAlternative Form of the Condition for Complete Observability. Consider the system described by Equations (9–63) and (9–64), rewritten  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nSuppose that the transformation matrix $\\mathbf{P}$ transforms $\\mathbf{A}$ into a diagonal matrix, or  \n\n$$\n\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}\n$$  \n\nwhere $\\mathbf{D}$ is a diagonal matrix. Let us define  \n\n$$\n\\mathbf{x}=\\mathbf{P}\\mathbf{z}\n$$  \n\nThen Equations (9–66) and (9–67) can be written  \n\n$$\n\\begin{array}{l}{{\\dot{\\mathbf{z}}=\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\mathbf{z}=\\mathbf{D}\\mathbf{z}}}\\\\ {{\\mathbf{y}=\\mathbf{C}\\mathbf{P}\\mathbf{z}}}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\mathbf{y}(t)=\\mathbf{C}\\mathbf{P}e^{\\mathbf{D}t}\\mathbf{z}(0)\n$$  \n\n$$\n\\mathbf{y}(t)\\,=\\,\\mathbf{CP}\\left[\\begin{array}{c c c c c c}{e^{\\lambda_{1}t}}&&&&&{0}\\\\ &{e^{\\lambda_{2}t}}&&&&\\\\ &&{\\cdot}&&&\\\\ &&&{\\cdot}&&\\\\ &&&&{\\cdot}&\\\\ {0}&&&&&{e^{\\lambda_{n}t}}\\end{array}\\right]\\mathbf{z}(0)\\,=\\,\\mathbf{CP}\\left[\\begin{array}{c}{e^{\\lambda_{1}t}z_{1}(0)}\\\\ {e^{\\lambda_{2}t}z_{2}(0)}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {e^{\\lambda_{n}t}z_{n}(0)}\\end{array}\\right]\n$$  \n\nThe system is completely observable if none of the columns of the $m\\times n$ matrix CP consists of all zero elements.This is because, if the i th column of CP consists of all zero elements, then the state variable $z_{i}(0)$ will not appear in the output equation and therefore cannot be determined from observation of $\\mathbf{y}(t)$ .Thus, ${\\bf x}(0)$ ,which is related to ${\\bf z}(0)$ by the nonsingular matrix $\\mathbf{P}$ ,cannot be determined.(Remember that this test applies only if the matrix $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$ is in diagonal form.)  \n\nIf the matrix A cannot be transformed into a diagonal matrix,then by use of a suitable transformation matrix S, we can transform A into a Jordan canonical form, or  \n\n$$\n\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}=\\mathbf{J}\n$$  \n\nwhere $\\mathbf{J}$ is in the Jordan canonical form. Let us define  \n\n$$\n\\mathbf{x}=\\mathbf{S}\\mathbf{z}\n$$  \n\nThen Equations (9–66) and (9–67) can be written  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\dot{\\bf z}}\\,=\\,{\\bf S}^{-1}{\\bf A}{\\bf S}{\\bf z}}={\\bf J}{\\bf z}}\\\\ {{\\displaystyle{\\bf y}\\,=\\,{\\bf C}{\\bf S}{\\bf z}}}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\mathbf{y}(t)=\\mathbf{C}\\mathbf{S}e^{\\mathbf{J}t}\\mathbf{z}(0)\n$$  \n\nThe system is completely observable if (1) no two Jordan blocks in Jare associated with the same eigenvalues, (2) no columns of CS that correspond to the first row of each Jordan block consist of zero elements, and (3) no columns of CS that correspond to distinct eigenvalues consist of zero elements.  \n\nTo clarify condition (2), in Example 9–16 we have encircled by dashed lines the columns of CS that correspond to the first row of each Jordan block.  \n\nEXAMPLE 9–16 The following systems are completely observable.  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l}{-1}&{0}\\\\ {0}&{-2}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right],\\qquad y=[1}&{3]\\Bigg[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\\\ &{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l}{2}&{1}&{0}\\\\ {0}&{2}&{1}\\\\ {0}&{0}&{2}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right],\\qquad\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l}{[3]!}&{0}\\\\ {\\vdots}&{0}\\\\ {\\mu\\vdots}&{0}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\\\ &{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l}{2}&{1}&{0}\\\\ {0}&{2}&{1}\\end{array}\\right]}\\\\ &{\\dot{x}_{3}}\\\\ &{\\dot{x}_{4}}\\end{array}\n$$  \n\n![](images/42955aeea9866d7e9fe32c755dcbc45e01e018b806d9f97d27aeceedc52796fb.jpg)  \n\nPrinciple of Duality. We shall now discuss the relationship between controllability and observability.We shall introduce the principle of duality, due to Kalman, to clarify apparent analogies between controllability and observability.  \n\nConsider the system $S_{1}$ described by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}}\\\\ {\\mathbf{y}\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{x}=\\operatorname{state~vector~}(n{\\mathrm{-vector}})}\\\\ &{\\mathbf{u}=\\operatorname{control~vector~}(r{\\mathrm{-vector}})}\\\\ &{\\mathbf{y}=\\operatorname{output~vector~}(m{\\mathrm{-vector}})}\\\\ &{\\mathbf{A}=n\\times n\\operatorname{matrix}}\\\\ &{\\mathbf{B}=n\\times r\\operatorname{matrix}}\\\\ &{\\mathbf{C}=m\\times n\\operatorname{matrix}}\\end{array}}\n$$  \n\nand the dual system $S_{2}$ defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}=\\mathbf{A}^{*}\\mathbf{z}\\mathbf{\\Sigma}+\\mathbf{C}^{*}\\mathbf{v}}\\\\ {\\mathbf{n}=\\mathbf{B}^{*}\\mathbf{z}}\\end{array}\n$$  \n\nwhere ${\\textbf{Z}}=$ state vector ($n$ -vector )  \n\n$\\textbf{v}=$ control vector (m -vector )$\\mathbf{n}=$ output vector ($r$ -vector )$\\mathbf{A}^{*}=$ conjugate transpose of A $\\mathbf{B}^{*}=$ conjugate transpose of $\\mathbf{B}$ $\\mathbf{C}^{*}=$ conjugate transpose of C  \n\nThe principle of duality states that the system $S_{1}$ is completely state controllable (observable) if and only if system $S_{2}$ is completely observable (state controllable).  \n\nTo verify this principle, let us write down the necessary and sufficient conditions for complete state controllability and complete observability for systems $S_{1}$ and $S_{2}$ .  \n\nFor system $S_{1}$ :  \n\n1. A necessary and sufficient condition for complete state controllability is that the rank of the $n\\times n r$ matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nbe $n$ .  \n\n2. A necessary and sufficient condition for complete observability is that the rank of the $n\\times n m$ matrix  \n\n$$\n\\left[\\mathbf{C}^{*}\\ \\ \\right\\vert\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\nbe $n$ .  \n\nFor system $S_{2}$ :  \n\n1. A necessary and sufficient condition for complete state controllability is that the rank of the $n\\times n m$ matrix  \n\n$$\n\\left[\\mathbf{C}^{*}\\ \\ \\right\\vert\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\nbe $n$ .  \n\n2. A necessary and sufficient condition for complete observability is that the rank of the $n\\times n r$ matrix  \n\n$$\n[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ ;\\ \\mathbf{A}^{n-1}\\mathbf{B}]\n$$  \n\nbe $n$ .  \n\nBy comparing these conditions, the truth of this principle is apparent. By use of this principle, the observability of a given system can be checked by testing the state controllability of its dual.  \n\nDetectability. For a partially observable system, if the unobservable modes are stable and the observable modes are unstable, the system is said to be detectable. Note that the concept of detectability is dual to the concept of stabilizability.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–9–1. Consider the transfer function system defined by Equation (9–2), rewritten  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nDerive the following controllable canonical form of the state-space representation for this transfer-function system:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n-1}}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {0}\\\\ {1}\\end{array}\\right]}u\n$$  \n\n$$\ny\\,=\\,{\\bigl[}\\,b_{n}\\,-\\,a_{n}b_{0}\\,\\,{\\bigr\\}\\,\\,b_{n-1}\\,-\\,a_{n-1}b_{0}\\,\\,{\\bigr\\}\\,\\,\\cdots\\,{\\bigr\\downarrow}\\,\\,b_{1}\\,-\\,a_{1}b_{0}{\\bigr]}\\,{\\begin{array}{l}{{\\,x_{1}}}\\\\ {{x_{2}}}\\\\ {{\\,\\cdot}}\\\\ {{\\,\\cdot}}\\\\ {{\\,\\cdot}}\\\\ {{x_{n}}}\\end{array}}\\,{\\Biggr]}\\,+\\,b_{0}u\n$$  \n\nSolution. Equation (9–68) can be written as  \n\n$$\n{\\frac{Y(s)}{U(s)}}=b_{0}+{\\frac{(b_{1}-a_{1}b_{0})s^{n-1}+\\dots+(b_{n-1}-\\,a_{n-1}b_{0})s\\,+\\,{\\bigl(}b_{n}-\\,a_{n}b_{0}{\\bigr)}}{s^{n}+a_{1}s^{n-1}+\\dots+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nwhich can be modified to  \n\n$$\nY(s)\\,=\\,b_{0}U(s)\\,+\\,{\\hat{Y}}(s)\n$$  \n\nwhere  \n\n$$\n{\\hat{Y}}(s)={\\frac{(b_{1}-a_{1}b_{0})s^{n-1}+\\dots+(b_{n-1}-a_{n-1}b_{0})s+(b_{n}-a_{n}b_{0})}{s^{n}+a_{1}s^{n-1}+\\dots+a_{n-1}s\\,+\\,a_{n}}}U(s)\n$$  \n\nLet us rewrite this last equation in the following form:  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{\\hat{Y}(s)}{\\bigl(b_{1}\\,-\\,a_{1}b_{0}\\bigr)s^{n-1}\\,+\\,\\cdots\\,+\\,\\bigl(b_{n-1}\\,-\\,a_{n-1}b_{0}\\bigr)s\\,+\\,\\bigl(b_{n}\\,-\\,a_{n}b_{0}\\bigr)}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{}=\\frac{U(s)}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}=Q(s)}}\\end{array}\n$$  \n\nFrom this last equation, the following two equations may be obtained:  \n\n$$\n\\begin{array}{r l}&{s^{n}{\\mathcal Q}(s)=-a_{1}s^{n-1}{\\mathcal Q}(s)\\,-\\cdots-\\,a_{n-1}s{\\mathcal Q}(s)\\,-\\,a_{n}{\\mathcal Q}(s)\\,+\\,U(s)}\\\\ &{\\quad\\hat{Y}(s)=\\big(b_{1}\\,-\\,a_{1}b_{0}\\big)s^{n-1}{\\mathcal Q}(s)\\,+\\,\\cdots\\,+\\,\\big(b_{n-1}\\,-\\,a_{n-1}b_{0}\\big)s{\\mathcal Q}(s)}\\\\ &{\\quad\\qquad\\quad+\\,\\big(b_{n}\\,-\\,a_{n}b_{0}\\big){\\mathcal Q}(s)}\\end{array}\n$$  \n\nNow define state variables as follows:  \n\n$$\n\\begin{array}{l}{{X_{1}(s)=Q(s)}}\\\\ {{{\\cal X}_{2}(s)\\,=\\,s Q(s)}}\\\\ {{~~}}\\\\ {{~~\\cdot}}\\\\ {{~~}}\\end{array}\n$$  \n\n$$\n\\begin{array}{r}{X_{n-1}(s)\\,=\\,s^{n-2}Q(s)}\\\\ {\\qquad X_{n}(s)\\,=\\,s^{n-1}Q(s)}\\end{array}\n$$  \n\nThen, clearly,  \n\n$$\n{\\begin{array}{r}{s X_{1}(s)\\,=\\,X_{2}(s)}\\\\ {s X_{2}(s)\\,=\\,X_{3}(s)}\\\\ {~\\cdot~}\\\\ {~\\cdot~}\\\\ {~\\cdot~}\\\\ {s X_{n-1}(s)\\,=\\,X_{n}(s)}\\end{array}}\n$$  \n\nwhich may be rewritten as  \n\n$$\n\\begin{array}{r}{\\dot{x}_{1}=x_{2}}\\\\ {\\dot{x}_{2}=x_{3}}\\\\ {\\dot{\\mathbf{\\eta}}}\\\\ {\\dot{\\mathbf{\\eta}}}\\\\ {\\dot{\\mathbf{\\eta}}}\\\\ {\\dot{x}_{n-1}=x_{n}}\\end{array}\n$$  \n\nNoting that $s^{n}Q(s)\\,=\\,s X_{n}(s)$ ,we can rewrite Equation (9–72) as  \n\n$$\ns X_{n}(s)\\,=-a_{1}X_{n}(s)\\,-\\,\\cdots\\,-\\,a_{n-1}X_{2}(s)\\,-\\,a_{n}X_{1}(s)\\,+\\,U(s)\n$$  \n\nor  \n\n$$\n{\\dot{x}}_{n}=-a_{n}x_{1}\\,-\\,a_{n-1}x_{2}\\,-\\,\\cdots\\,-\\,a_{1}x_{n}\\,+\\,u\n$$  \n\n![](images/a488f80019310330ebbda3e583c22d88dc47bf51a855113aeac392a3eb94572c.jpg)  \nFigure 9–1 Block diagram representation of the system defined by Equations (9–69) and (9–70) (controllable canonical form).  \n\nAlso, from Equations (9–71) and (9–73), we obtain  \n\n$$\n\\begin{array}{r l}&{Y(s)=b_{0}U(s)\\,+\\,\\bigl(b_{1}\\,-\\,a_{1}b_{0}\\bigr)s^{n-1}\\mathcal{Q}(s)\\,+\\,\\cdots+\\,\\bigl(b_{n-1}\\,-\\,a_{n-1}b_{0}\\bigr)s\\mathcal{Q}(s)}\\\\ &{\\quad\\,+\\,\\bigl(b_{n}\\,-\\,a_{n}b_{0}\\bigr)\\mathcal{Q}(s)}\\\\ &{\\quad\\,=\\,b_{0}U(s)\\,+\\,\\bigl(b_{1}\\,-\\,a_{1}b_{0}\\bigr)X_{n}(s)\\,+\\,\\cdots\\,+\\,\\bigl(b_{n-1}\\,-\\,a_{n-1}b_{0}\\bigr)X_{2}(s)}\\\\ &{\\quad\\,+\\,\\bigl(b_{n}\\,-\\,a_{n}b_{0}\\bigr)X_{1}(s)}\\end{array}\n$$  \n\nThe inverse Laplace transform of this output equation becomes  \n\n$$\ny\\,=\\,{\\bigl(}b_{n}\\,-\\,a_{n}b_{0}{\\bigr)}x_{1}\\,+\\,{\\bigl(}b_{n-1}\\,-\\,a_{n-1}b_{0}{\\bigr)}x_{2}\\,+\\,\\cdots\\,+\\,{\\bigl(}b_{1}\\,-\\,a_{1}b_{0}{\\bigr)}x_{n}\\,+\\,b_{0}u\n$$  \n\nCombining Equations (9–74) and (9–75) into one vector–matrix differential equation, we obtain Equation (9–69). Equation (9–76) can be rewritten as given by Equation (9–70). Equations (9–69) and (9–70) are said to be in the controllable canonical form. Figure 9–1 shows the block diagram representation of the system defined by Equations (9–69) and (9–70).  \n\nChapter 9 /Control Systems Analysis in State Space A–9–2. Consider the following transfer-function system:  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}}}\n$$  \n\nDerive the following observable canonical form of the state-space representation for this transferfunction system:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{0}&{\\cdots}&{0}&{-a_{n}}\\\\ {1}&{0}&{\\cdots}&{0}&{-a_{n-1}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{\\cdots}&{1}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{b_{n}-a_{n}b_{0}}\\\\ {b_{n-1}-a_{n-1}b_{0}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {b_{1}-a_{1}b_{0}}\\end{array}\\right]}u\n$$  \n\n$$\n\\begin{array}{r}{y=[0\\quad0\\quad\\cdots\\quad0\\quad1]\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {\\ddots}\\\\ {\\ddots}\\\\ {\\ddots}\\\\ {x_{n-1}}\\\\ {x_{n}}\\end{array}\\right]+b_{0}u}\\end{array}\n$$  \n\nSolution. Equation (9–77) can be modified into the following form:  \n\n$$\n\\begin{array}{l}{{s^{n}\\big[Y(s)\\,-\\,b_{0}U(s)\\big]\\,+\\,s^{n-1}\\big[a_{1}Y(s)\\,-\\,b_{1}U(s)\\big]\\,+\\,\\cdots\\,}}\\\\ {{\\,+\\,s\\big[a_{n-1}Y(s)\\,-\\,b_{n-1}U(s)\\big]\\,+\\,a_{n}Y(s)\\,-\\,b_{n}U(s)\\,=\\,0}}\\end{array}\n$$  \n\nBy dividing the entire equation by $s^{n}$ and rearranging, we obtain  \n\n$$\n\\begin{array}{l}{{Y(s)\\,=\\,b_{0}U(s)\\,+\\frac{1}{s}\\bigl[b_{1}U(s)\\,-\\,a_{1}Y(s)\\bigr]\\,+\\cdots}}\\\\ {{\\mathrm{}}}\\\\ {{+\\,\\frac{1}{s^{n-1}}\\bigl[b_{n-1}U(s)\\,-\\,a_{n-1}Y(s)\\bigr]\\,+\\,\\frac{1}{s^{n}}\\bigl[b_{n}U(s)\\,-\\,a_{n}Y(s)\\bigr]}}\\end{array}\n$$  \n\nNow define state variables as follows:  \n\n$$\n\\begin{array}{c c c}{{\\displaystyle X_{n}(s)=\\frac{1}{s}\\big[b_{1}U(s)\\,-\\,a_{1}Y(s)\\,+\\,X_{n-1}(s)\\big]}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle X_{n-1}(s)=\\frac{1}{s}\\big[b_{2}U(s)\\,-\\,a_{2}Y(s)\\,+\\,X_{n-2}(s)\\big]}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle\\qquad\\cdot}}\\\\ {{\\therefore}}\\\\ {{\\therefore}}\\end{array}\n$$  \n\n$$\n\\begin{array}{l}{{\\displaystyle X_{2}(s)\\,=\\frac1s\\left[b_{n-1}U(s)\\,-\\,a_{n-1}Y(s)\\,+\\,X_{1}(s)\\right]}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle X_{1}(s)\\,=\\frac1s\\left[b_{n}U(s)\\,-\\,a_{n}Y(s)\\right]}}\\end{array}\n$$  \n\nThen Equation (9–80) can be written as  \n\n$$\nY(s)\\,=\\,b_{0}U(s)\\,+\\,X_{n}(s)\n$$  \n\nBy substituting Equation (9–82) into Equation (9–81) and multiplying both sides of the equations by $s$ , we obtain  \n\n$$\n\\begin{array}{r l r}&{}&{s X_{n}(s)\\,=\\,X_{n-1}(s)\\,-\\,a_{1}X_{n}(s)\\,+\\,\\bigl(b_{1}\\,-\\,a_{1}b_{0}\\bigr)U(s)}\\\\ &{}&\\\\ &{}&{s X_{n-1}(s)\\,=\\,X_{n-2}(s)\\,-\\,a_{2}X_{n}(s)\\,+\\,\\bigl(b_{2}\\,-\\,a_{2}b_{0}\\bigr)U(s)}\\\\ &{}&\\\\ &{}&{\\,.}\\\\ &{}&{\\,.}\\end{array}\n$$  \n\n$$\n\\begin{array}{r l}&{s X_{2}(s)\\,=\\,X_{1}(s)\\,-\\,a_{n-1}X_{n}(s)\\,+\\,\\bigl(b_{n-1}\\,-\\,a_{n-1}b_{0}\\bigr)U(s)}\\\\ &{}\\\\ &{s X_{1}(s)\\,=\\,-a_{n}X_{n}(s)\\,+\\,\\bigl(b_{n}\\,-\\,a_{n}b_{0}\\bigr)U(s)}\\end{array}\n$$  \n\nTaking the inverse Laplace transforms of the preceding $n$ equations and writing them in the reverse order, we get  \n\n$$\n\\begin{array}{r l}&{\\dot{x}_{1}=-a_{n}x_{n}+\\big(b_{n}-a_{n}b_{0}\\big)u}\\\\ &{\\dot{x}_{2}=x_{1}-a_{n-1}x_{n}+\\big(b_{n-1}-a_{n-1}b_{0}\\big)u}\\\\ &{\\qquad\\mathrm{.}}\\end{array}\n$$  \n\n$$\n\\begin{array}{r}{\\dot{x}_{n-1}=x_{n-2}-a_{2}x_{n}+(b_{2}-a_{2}b_{0})u}\\\\ {\\dot{x}_{n}=x_{n-1}-a_{1}x_{n}+(b_{1}-a_{1}b_{0})u}\\end{array}\n$$  \n\nAlso, the inverse Laplace transform of Equation (9–82) gives  \n\n$$\ny\\,=\\,x_{n}\\,+\\,b_{0}u\n$$  \n\nRewriting the state and output equations in the standard vector-matrix forms gives Equations (9–78) and (9–79). Figure 9–2 shows a block diagram representation of the system defined by Equations (9–78) and (9–79).  \n\n![](images/fc157ec7908ba75a36bdbc2964669f187be04d8e254b961a1d6f6e5e7ffbcd4a.jpg)  \nFigure 9–2 Block diagram representation of the system defined by Equations (9–78) and (9–79) (observable canonical form).  \n\nChapter 9 /Control Systems Analysis in State Space  \n\nA–9–3. Consider the transfer-function system defined by  \n\n$$\n{\\begin{array}{l}{{\\cfrac{Y(s)}{U(s)}}={\\cfrac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{\\left(s\\,+\\,p_{1}\\right)\\left(s\\,+\\,p_{2}\\right)\\cdots\\left(s\\,+\\,p_{n}\\right)}}}\\\\ {\\qquad=\\,b_{0}+{\\cfrac{c_{1}}{s\\,+\\,p_{1}}}+{\\cfrac{c_{2}}{s\\,+\\,p_{2}}}+\\,\\cdots\\,+{\\cfrac{c_{n}}{s\\,+\\,p_{n}}}}\\end{array}}\n$$  \n\nwhere $p_{i}\\neq p_{j}$ .Derive the state-space representation of this system in the following diagonal canonical form:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l l}{-p_{1}}&&&&&{0}\\\\ &{-p_{2}}&&&&\\\\ &&{\\cdot}&&&\\\\ &&&{\\cdot}&&\\\\ &&&&{\\cdot}&\\\\ {0}&&&&&{-p_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {1}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {1}\\end{array}\\right]}u\n$$  \n\n$$\ny={\\left[\\begin{array}{l l l l l}{c_{1}}&{c_{2}}&{\\cdots}&{c_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]}+b_{0}u\n$$  \n\nSolution. Equation (9–83) may be written as  \n\n$$\nY(s)=b_{0}U(s)\\,+\\frac{c_{1}}{s\\,+\\,p_{1}}U(s)\\,+\\frac{c_{2}}{s\\,+\\,p_{2}}U(s)\\,+\\,\\cdots+\\frac{c_{n}}{s\\,+\\,p_{n}}U(s)\n$$  \n\nDefine the state variables as follows:  \n\n$$\n\\begin{array}{l c r}{{X_{1}(s)=\\displaystyle\\frac{1}{s+\\ p_{1}}U(s)}}\\\\ {{\\ }}\\\\ {{X_{2}(s)=\\displaystyle\\frac{1}{s+\\ p_{2}}U(s)}}\\\\ {{\\ }}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\end{array}\n$$  \n\n$$\nX_{n}(s)={\\frac{1}{s\\,+\\,p_{n}}}\\,U(s)\n$$  \n\nwhich may be rewritten as  \n\n$$\n\\begin{array}{l}{{s X_{1}(s)\\,=\\,-p_{1}X_{1}(s)\\,+\\,U(s)}}\\\\ {{s X_{2}(s)\\,=\\,-p_{2}X_{2}(s)\\,+\\,U(s)}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}}\\\\ {{s X_{n}(s)\\,=\\,-p_{n}X_{n}(s)\\,+\\,U(s)}}\\end{array}\n$$  \n\nThe inverse Laplace transforms of these equations give  \n\n$$\n\\begin{array}{r c l}{\\dot{x}_{1}=-p_{1}x_{1}+u}\\\\ {\\dot{x}_{2}=-p_{2}x_{2}+u}\\\\ {\\dot{}}&{}&\\\\ {\\cdot}&{}\\\\ {\\cdot}&{}&\\\\ {\\dot{x}_{n}=-p_{n}x_{n}+u}\\end{array}\n$$  \n\nThese $n$ equations make up a state equation.  \n\nIn terms of the state variables $X_{1}(s),X_{2}(s),\\ldots,X_{n}(s)$ , Equation (9–86) can be written as  \n\n$$\nY(s)\\,=\\,b_{0}U(s)\\,+\\,c_{1}X_{1}(s)\\,+\\,c_{2}X_{2}(s)\\,+\\,\\cdots\\,+\\,c_{n}X_{n}(s)\n$$  \n\nThe inverse Laplace transform of this last equation is  \n\n$$\ny\\,=\\,c_{1}\\,x_{1}\\,+\\,c_{2}x_{2}\\,+\\,\\cdots\\,+\\,c_{n}x_{n}\\,+\\,b_{0}u\n$$  \n\nwhich is the output equation.  \n\nEquation (9–87) can be put in the vector-matrix equation as given by Equation (9–84). Equation (9–88) can be put in the form of Equation (9–85).  \n\nFigure 9–3 shows a block diagram representation of the system defined by Equations (9–84) and (9–85).  \n\nIt is noted that if we choose the state variables as  \n\n$$\n\\begin{array}{l}{{\\displaystyle\\hat{X}_{1}(s)\\,=\\frac{c_{1}}{s\\,+\\,p_{1}}\\,U(s)}}\\\\ {{\\displaystyle\\hat{X}_{2}(s)\\,=\\frac{c_{2}}{s\\,+\\,p_{2}}\\,U(s)}}\\end{array}\n$$  \n\n$$\n{\\hat{X}}_{n}(s)\\,=\\,{\\frac{c_{n}}{s\\,+\\,p_{n}}}\\,U(s)\n$$  \n\n![](images/cf4335fe5b38dffb799e6781f2a8b3de95b75f2545d91b80680054318debafca.jpg)  \nFigure 9–3 Block diagram representation of the system defined by Equations (9–84) and (9–85) (diagonal canonical form).  \n\nthen we get a slightly different state-space representation.This choice of state variables gives  \n\n$$\n\\begin{array}{c}{{s\\hat{X}_{1}(s)\\,=\\,-p_{1}\\hat{X}_{1}(s)\\,+\\,c_{1}U(s)}}\\\\ {{\\,}}\\\\ {{s\\hat{X}_{2}(s)\\,=\\,-p_{2}\\hat{X}_{2}(s)\\,+\\,c_{2}U(s)}}\\\\ {{\\,}}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\\\ {{\\cdot}}\\end{array}\n$$  \n\n$$\ns\\hat{X}_{n}(s)\\,=\\,-p_{n}\\hat{X}_{n}(s)\\,+\\,c_{n}U(s)\n$$  \n\nfrom which we obtain  \n\n$$\n\\begin{array}{r}{\\dot{\\hat{x}}_{1}=-p_{1}\\hat{x}_{1}+c_{1}u}\\\\ {\\dot{\\hat{x}}_{2}=-p_{2}\\hat{x}_{2}+c_{2}u}\\\\ {\\cdot\\ \\ \\ }\\\\ {\\cdot\\ \\ \\ }\\\\ {\\cdot\\ \\ \\ }\\\\ {\\dot{\\hat{x}}_{n}=-p_{n}\\hat{x}_{n}+c_{n}u}\\end{array}\n$$  \n\nReferring to Equation (9–86), the output equation becomes  \n\n$$\nY(s)\\,=\\,b_{0}U(s)\\,+\\,{\\hat{X}}_{1}(s)\\,+\\,{\\hat{X}}_{2}(s)\\,+\\,\\cdots\\,+\\,{\\hat{X}}_{n}(s)\n$$  \n\nfrom which we get  \n\n$$\ny\\,=\\,{\\hat{x}}_{1}\\,+\\,{\\hat{x}}_{2}\\,+\\,\\cdots\\,+\\,{\\hat{x}}_{n}\\,+\\,b_{0}u\n$$  \n\nEquations (9–89) and (9–90) give the following state-space representation for the system:  \n\n$$\n\\left[\\begin{array}{c}{\\dot{\\hat{x}}_{1}}\\\\ {\\dot{\\hat{x}}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{x}_{n}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c c}{-p_{1}}&&&&&{0}\\\\ &{-p_{2}}&&&&\\\\ &&{\\cdot}&&&\\\\ &&&{\\cdot}&&&\\\\ &&&&&{\\cdot}&\\\\ {0}&&&&&&{-p_{n}}\\end{array}\\right]\\left[\\begin{array}{c}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{x}_{n}}\\\\ {\\hat{x}_{n}}\\end{array}\\right]+\\left[\\begin{array}{c}{c_{1}}\\\\ {c_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{x}_{n}}\\end{array}\\right]u_{n}\n$$  \n\n$$\ny=[1\\quad1\\quad\\cdots\\quad1]\\left[\\begin{array}{c}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{x}_{n}}\\end{array}\\right]+b_{0}u\n$$  \n\nA–9–4. Consider the system defined by  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{b_{0}s^{n}\\,+\\,b_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,b_{n-1}s\\,+\\,b_{n}}{{(s\\,+\\,p_{1})}^{3}(s\\,+\\,p_{4}){(s\\,+\\,p_{5})}\\cdots{(s\\,+\\,p_{n})}}}\n$$  \n\nwhere the system involves a triple pole at $s=-p_{1}$ . (We assume that, except for the first three $p_{i}$ ’s being equal, the $p_{i}$ ’s are different from one another.) Obtain the Jordan canonical form of the state-space representation for this system.  \n\n# Solution. The partial-fraction expansion of Equation (9–91) becomes  \n\n$$\n{\\frac{Y(s)}{U(s)}}=b_{0}+{\\frac{c_{1}}{{\\bigl(}s+p_{1}{\\bigr)}^{3}}}+{\\frac{c_{2}}{{\\bigl(}s+p_{1}{\\bigr)}^{2}}}+{\\frac{c_{3}}{s+p_{1}}}+{\\frac{c_{4}}{s+p_{4}}}+\\cdots+{\\frac{c_{n}}{s+p_{n}}}\n$$  \n\nwhich may be written as  \n\n$$\n\\begin{array}{c}{{Y(s)\\,=\\,b_{0}U(s)\\,+\\displaystyle\\frac{c_{1}}{\\big(s\\,+\\,p_{1}\\big)^{3}}U(s)\\,+\\displaystyle\\frac{c_{2}}{\\big(s\\,+\\,p_{1}\\big)^{2}}U(s)}}\\\\ {{+\\displaystyle\\frac{c_{3}}{s\\,+\\,p_{1}}U(s)\\,+\\,\\displaystyle\\frac{c_{4}}{s\\,+\\,p_{4}}U(s)\\,+\\,\\cdots+\\,\\displaystyle\\frac{c_{n}}{s\\,+\\,p_{n}}U(s)}}\\end{array}\n$$  \n\nDefine  \n\n$$\n\\begin{array}{l}{{\\displaystyle X_{1}(s)=\\frac{1}{(s+p_{1})^{3}}U(s)}}\\\\ {~~}\\\\ {{\\displaystyle X_{2}(s)=\\frac{1}{(s+p_{1})^{2}}U(s)}}\\\\ {~~}\\\\ {{\\displaystyle X_{3}(s)=\\frac{1}{s+p_{1}}U(s)}}\\\\ {~~}\\\\ {{\\displaystyle X_{4}(s)=\\frac{1}{s+p_{4}}U(s)}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~.}}\\end{array}\n$$  \n\n$$\nX_{n}(s)={\\frac{1}{s\\,+\\,p_{n}}}\\,U(s)\n$$  \n\nNotice that the following relationships exist among $X_{1}(s),X_{2}(s)$ , and $X_{3}(s)$ :  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\frac{X_{1}(s)}{X_{2}(s)}}=\\frac{1}{s\\,\\,+\\,\\,p_{1}}}}\\\\ {{\\displaystyle{\\frac{X_{2}(s)}{X_{3}(s)}}=\\frac{1}{s\\,\\,+\\,\\,p_{1}}}}\\end{array}\n$$  \n\nThen, from the preceding definition of the state variables and the preceding relationships, we obtain  \n\n$$\n\\begin{array}{l}{{s X_{1}(s)=-p_{1}X_{1}(s)\\,+\\,X_{2}(s)}}\\\\ {{s X_{2}(s)=-p_{1}X_{2}(s)\\,+\\,X_{3}(s)}}\\\\ {{s X_{3}(s)=-p_{1}X_{3}(s)\\,+\\,U(s)}}\\\\ {{s X_{4}(s)=-p_{4}X_{4}(s)\\,+\\,U(s)}}\\\\ {{~}}\\\\ {{~.}}\\end{array}\n$$  \n\n$$\ns X_{n}(s)\\,=\\,-p_{n}X_{n}(s)\\,+\\,U(s)\n$$  \n\nThe inverse Laplace transforms of the preceding $n$ equations give  \n\n$$\n\\begin{array}{l c r}{{\\dot{x}_{1}=-p_{1}x_{1}\\,+\\,x_{2}}}\\\\ {{\\dot{x}_{2}=-p_{1}x_{2}\\,+\\,x_{3}}}\\\\ {{\\dot{x}_{3}=-p_{1}x_{3}\\,+\\,u}}\\\\ {{\\dot{x}_{4}=-p_{4}x_{4}\\,+\\,u}}\\end{array}\n$$  \n\n$$\n\\dot{x}_{n}=-p_{n}x_{n}\\,+\\,u\n$$  \n\nThe output equation, Equation (9–92), can be rewritten as  \n\n$$\nY(s)=b_{0}U(s)\\,+\\,c_{1}X_{1}(s)\\,+\\,c_{2}X_{2}(s)\\,+\\,c_{3}X_{3}(s)\\,+\\,c_{4}X_{4}(s)\\,+\\,\\cdots\\,+\\,c_{n}X_{n}(s)\n$$  \n\nThe inverse Laplace transform of this output equation is  \n\n$$\ny\\,=\\,c_{1}x_{1}\\,+\\,c_{2}x_{2}\\,+\\,c_{3}x_{3}\\,+\\,c_{4}x_{4}\\,+\\,\\cdots\\,+\\,c_{n}x_{n}\\,+\\,b_{0}u\n$$  \n\nThus, the state-space representation of the system for the case when the denominator polynomial involves a triple root $-p_{1}$ can be given as follows:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\\\ {{\\dot{x}}_{4}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {{\\dot{x}}_{n}}\\end{array}\\right]}\\,=\\,{\\left[\\begin{array}{l l l}{-p_{1}}&{1}&{0}\\\\ {0}&{-p_{1}}&{1}\\\\ {0}&{0}&{-p_{1}}\\end{array}\\right]}\\,{\\left[\\begin{array}{l l l l}{0}&{\\cdots}&{0}\\\\ {\\cdots}&&{0}\\\\ {0}&{\\cdots}&{0}\\\\ {0}&{\\cdots}&{0}\\end{array}\\right]}\\,{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\end{array}\\right]}\\,+\\,{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\\\ {1}\\\\ {\\cdots}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {1}\\end{array}\\right]}\\,u,\n$$  \n\n$$\ny={\\left[\\begin{array}{l l l l l}{c_{1}}&{c_{2}}&{\\cdots}&{c_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdots}\\\\ {\\cdots}\\\\ {\\cdots}\\\\ {x_{n}}\\end{array}\\right]}+b_{0}u\n$$  \n\nThe state-space representation in the form given by Equations (9–93) and (9–94) is said to be in the Jordan canonical form. Figure 9–4 shows a block diagram representation of the system given by Equations (9–93) and (9–94).  \n\nA–9–5. Consider the transfer-function system  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{25.04s\\,+\\,5.008}{s^{3}\\,+\\,5.03247s^{2}\\,+\\,25.1026s\\,+\\,5.008}\n$$  \n\nObtain a state-space representation of this system with MATLAB.  \n\n![](images/54297f97771df1fd0aa39bf121b8ad5808e78aaf5c7ae5b5bea3a6081fd4d2f5.jpg)  \nFigure 9–4 Block diagram representation of the system defined by Equations (9–93) and (9–94) (Jordan canonical form).  \n\nSolution. MATLAB command  \n\n$$\n[\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D}]=\\mathsf{t f}2\\mathsf{s s}(\\mathsf{n u m},\\mathsf{d e n})\n$$  \n\nwill produce a state-space representation for the system. See MATLAB Program 9–4.  \n\n![](images/b8eeae7a645a3f51677ee960c0b2442901f8ee5fc7570552ab1cd7af420086b4.jpg)  \n\nThis is the MATLAB representation of the following state-space equations:  \n\n$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c c}{-5.0325}&{-25.1026}&{-5.008}\\\\ {1}&{0}&{0}\\\\ {0}&{1}&{0}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\!\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{c}{1}\\\\ {0}\\\\ {0}\\end{array}\\!\\!\\right]u}\\end{array}\n$$  \n\n$$\ny=[0\\ \\ 25.04\\ \\ 5.008]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+[0]u\n$$  \n\nA–9–6. Consider the system defined by  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}\\mathbf{u}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ($n$ -vector )  \n\n$\\mathbf{u}=$ control vector ($r$ -vector )$\\mathbf{A}=n\\times n$ constant matrix $\\mathbf{B}={\\boldsymbol{n}}\\times{\\boldsymbol{r}}$ constant matrix  \n\nObtain the response of the system to each of the following inputs:  \n\n(a ) The $r$ components of $\\mathbf{u}$ are impulse functions of various magnitudes.   \n(b) The $r$ components of $\\mathbf{u}$ are step functions of various magnitudes.   \n(c) The $r$ components of $\\mathbf{u}$ are ramp functions of various magnitudes.  \n\n# Solution.  \n\n(a )Impulse response: Referring to Equation (9–43), the solution to the given state equation is  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}(t-t_{0})}\\mathbf{x}\\big(t_{0}\\big)\\,+\\,\\int_{t_{0}}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nSubstituting $t_{0}=0-$ into this solution, we obtain  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0-)\\,+\\,\\int_{0-}^{t}\\!e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau\n$$  \n\nLet us write the impulse input ${\\bf\\delta u}(t)$ as  \n\n$$\n\\mathbf{u}(t)\\,=\\,\\delta(t)\\,\\mathbf{w}\n$$  \n\nwhere wis a vector whose components are the magnitudes of $r$ impulse functions applied at $t\\,=\\,0,$ .The solution of the state equation when the impulse input $\\delta(t)\\,\\mathbf{w}$ is given at $t\\,=\\,0$ is  \n\n$$\n\\begin{array}{l}{\\displaystyle\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0\\mathrm{-})\\,+\\,\\int_{0-}^{t}\\!e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\delta(\\tau)\\,\\mathbf{w}\\,d\\tau}\\\\ {\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0\\mathrm{-})\\,+\\,e^{\\mathbf{A}t}\\mathbf{B}\\mathbf{w}}\\end{array}\n$$  \n\n(b)Step response: Let us write the step input ${\\bf\\delta u}(t)$ as  \n\n$$\n{\\mathbf{u}}(t)={\\mathbf{k}}\n$$  \n\nwhere $\\mathbf{k}$ is a vector whose components are the magnitudes of $r$ step functions applied at $t\\,=\\,0.$ The solution to the step input at $t\\,=\\,0$ is given by  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}e^{\\mathbf{A}(t-\\tau)}\\mathbf{B}\\mathbf{k}\\,d\\tau}\\\\ &{\\qquad=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,e^{\\mathbf{A}t}{\\Bigg[}\\int_{0}^{t}{\\bigg(}\\mathbf{I}-\\,\\mathbf{A}\\tau\\,+\\,{\\frac{\\mathbf{A}^{2}\\tau^{2}}{2!}}-\\cdots{\\bigg)}\\,d\\tau{\\Bigg]}\\mathbf{B}\\mathbf{k}}\\\\ &{\\qquad=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,e^{\\mathbf{A}t}{\\Bigg(}\\mathbf{I}t\\,-\\,{\\frac{\\mathbf{A}t^{2}}{2!}}+{\\frac{\\mathbf{A}^{2}t^{3}}{3!}}-\\cdots{\\Bigg)}\\mathbf{B}\\mathbf{k}}\\end{array}}\n$$  \n\nIf $\\mathbf{A}$ is nonsingular, then this last equation can be simplified to give  \n\n$$\n\\begin{array}{r l}&{\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,e^{\\mathbf{A}t}[-(\\mathbf{A}^{-1})(e^{-\\mathbf{A}t}\\,-\\,\\mathbf{I})]\\mathbf{B}\\mathbf{k}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\mathbf{A}^{-1}(e^{\\mathbf{A}t}\\,-\\,\\mathbf{I})\\mathbf{B}\\mathbf{k}}\\end{array}\n$$  \n\n(c)Ramp response: Let us write the ramp input ${\\bf\\delta u}(t)$ as  \n\n$$\n\\mathbf{u}(t)\\,=\\,t\\,\\mathbf{v}\n$$  \n\nwhere $\\mathbf{v}$ is a vector whose components are magnitudes of ramp functions applied at $t\\,=\\,0.$ The solution to the ramp input $t\\mathbf{v}$ given at $t\\,=\\,0$ is  \n\n$$\n{\\bf x}(t)\\,=\\,e^{{\\bf A}t}{\\bf x}(0)\\,+\\,\\int_{0}^{t}e^{{\\bf A}(t-\\tau)}{\\bf B}\\tau{\\bf v}\\,d\\tau\\,\\,\\,}\\\\ {\\,=\\,e^{{\\bf A}t}{\\bf x}(0)\\,+\\,e^{{\\bf A}t}\\!\\int_{0}^{t}e^{-{\\bf A}\\tau}\\tau\\,d\\tau{\\bf B}{\\bf v}\\,}\\\\ {\\,=\\,e^{{\\bf A}t}{\\bf x}(0)\\,+\\,e^{{\\bf A}t}\\!\\left(\\frac{\\cal I}{2}\\,t^{2}\\,-\\,\\frac{2\\,{\\bf A}}{3!}\\,t^{3}\\,+\\,\\frac{3\\,{\\bf A}^{2}}{4!}\\,t^{4}\\,-\\,\\frac{4\\,{\\bf A}^{3}}{5!}\\,t^{5}\\,+\\,\\cdots\\right){\\bf B}{\\bf v}\\,.\\,\\,\\,}\\end{array}\n$$  \n\nIf $\\mathbf{A}$ is nonsingular, then this last equation can be simplified to give  \n\n$$\n\\begin{array}{r l}&{\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\bigl(\\mathbf{A}^{-2}\\bigr)\\bigl(e^{\\mathbf{A}t}\\,-\\,\\mathbf{I}\\,-\\,\\mathbf{A}t\\bigr)\\mathbf{B}\\mathbf{v}}\\\\ &{\\qquad=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\bigl[\\mathbf{A}^{-2}\\bigl(e^{\\mathbf{A}t}\\,-\\,\\mathbf{I}\\bigr)\\,-\\,\\mathbf{A}^{-1}t\\bigr]\\mathbf{B}\\mathbf{v}}\\end{array}\n$$  \n\nA–9–7. Obtain the response $y(t)$ of the following system:  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{1}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{-1}&{-0.5}\\\\ {1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0.5}\\\\ {0}\\end{array}\\right]}u,\\qquad{\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {x_{2}(0)}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\end{array}\\right]}}\\\\ &{\\qquad y={\\left[\\begin{array}{l l}{1}&{0}\\\\ {1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nwhere $u(t)$ is the unit-step input occurring at $t\\,=\\,0$ , or  \n\n$$\nu(t)\\,=\\,1(t)\n$$  \n\nSolution. For this system  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{-1}&{-0.5}\\\\ {1}&{\\;\\;\\;0}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0.5}\\\\ {0}\\end{array}\\right]}\n$$  \n\nThe state transition matrix $\\Phi(t)\\,=\\,e^{\\mathbf{A}t}$ can be obtained as follows:  \n\n$$\n\\Phi(t)\\,=\\,e^{\\mathbf{A}t}\\,=\\,\\mathcal{L}^{-1}\\bigl[(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\bigr]\n$$  \n\nSince  \n\n$$\n(s\\mathbf{I}-\\mathbf{A})^{-1}={\\left[\\begin{array}{l l}{s\\neq1}&{0.5}\\\\ {-1}&{s}\\end{array}\\right]}^{-1}={\\frac{1}{s^{2}+~s\\,+\\,0.5}}{\\left[\\begin{array}{l l}{s}&{-0.5}\\\\ {1}&{s+1}\\end{array}\\right]}\n$$  \n\n$$\n=\\left[\\begin{array}{c c c}{{\\displaystyle\\frac{s\\,+\\,0.5\\,-\\,0.5}{(s\\,+\\,0.5)^{2}\\,+\\,0.5^{2}}}}&{{\\displaystyle\\frac{-0.5}{(s\\,+\\,0.5)^{2}\\,+\\,0.5^{2}}}}\\\\ {{\\displaystyle\\frac{1}{(s\\,+\\,0.5)^{2}\\,+\\,0.5^{2}}}}&{{\\displaystyle\\frac{s\\,+\\,0.5\\,+\\,0.5}{(s\\,+\\,0.5)^{2}\\,+\\,0.5^{2}}}}\\end{array}\\right]\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}{\\Phi(t)\\,=\\,e^{\\mathbf{A}t}=\\,\\mathcal{L}^{-1}\\bigl[(s\\mathbf{I}-\\mathbf{A})^{-1}\\bigr]}&{}\\\\ {=\\left[\\begin{array}{c c}{e^{-0.5t}(\\cos{0.5t}\\,-\\,\\sin{0.5t})}&{-e^{-0.5t}\\sin{0.5t}}\\\\ {2\\,e^{-0.5t}\\sin{0.5t}}&{e^{-0.5t}(\\cos{0.5t}\\,+\\,\\sin{0.5t})}\\end{array}\\right]}\\end{array}\n$$  \n\nSince $\\mathbf{x}(0)=\\mathbf{0}$ and $k=1$ ,referring to Equation (9–96), we have  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\\,+\\,\\mathbf{A}^{-1}\\!\\left(e^{\\mathbf{A}t}\\,-\\,\\mathbf{I}\\right)\\mathbf{B}k}\\\\ &{\\qquad=\\,\\mathbf{A}^{-1}\\!\\left(e^{\\mathbf{A}t}\\,-\\,\\mathbf{I}\\right)\\mathbf{B}}\\\\ &{\\qquad=\\,{\\left[\\begin{array}{l l}{0}&{1}\\\\ {-2}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l}{0.5e^{-0.5t}(\\cos{0.5t}\\,-\\,\\sin{0.5t})\\,-\\,0.5{\\sqrt{2}}}\\\\ {\\qquad\\qquad\\qquad e^{-0.5t}\\sin{0.5t}}\\end{array}\\right]}}\\\\ &{\\qquad=\\,{\\left[\\begin{array}{l}{e^{-0.5t}\\sin{0.5t}}\\\\ {-e^{-0.5t}(\\cos{0.5t}\\,+\\,\\sin{0.5t})\\,+\\,1}\\end{array}\\right]}}\\end{array}}\n$$  \n\nHence, the output $y(t)$ can be given by  \n\n$$\ny(t)\\,=\\,[1\\,\\mathrm{~\\boldmath~\\Gamma~}_{0}]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\,=\\,x_{1}\\,=\\,e^{-0.5t}\\sin0.5t\n$$  \n\nA–9–8. The Cayley–Hamilton theorem states that every $n\\times n$ matrix $\\mathbf{A}$ satisfies its own characteristic equation. The characteristic equation is not, however, necessarily the scalar equation of least degree that $\\mathbf{A}$ satisfies. The least-degree polynomial having A as a root is called the minimal polynomial . That is, the minimal polynomial of an $n\\times n$ matrix $\\mathbf{A}$ is defined as the polynomial $\\phi(\\lambda)$ of least degree,  \n\n$$\n\\phi(\\lambda)\\,=\\,\\lambda^{m}\\,+\\,a_{1}\\lambda^{m-1}\\,+\\,\\cdots\\,+\\,a_{m-1}\\lambda\\,+\\,a_{m},\\qquad m\\le n\n$$  \n\nsuch that $\\phi(\\mathbf{A})\\,=\\,\\mathbf{0}$ ,or  \n\n$$\n\\phi(\\mathbf{A})=\\mathbf{A}^{m}+a_{1}\\mathbf{A}^{m-1}+\\cdots+a_{m-1}\\mathbf{A}+a_{m}\\mathbf{I}=\\mathbf{0}\n$$  \n\nThe minimal polynomial plays an important role in the computation of polynomials in an $n\\times n$ matrix.  \n\nLet us suppose that $d(\\lambda)$ , a polynomial in $\\lambda$ , is the greatest common divisor of all the elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ . Show that, if the coefficient of the highest-degree term in $\\lambda$ of $d(\\lambda)$ is chosen as 1, then the minimal polynomial $\\phi(\\lambda)$ is given by  \n\n$$\n\\phi(\\lambda)=\\left|\\frac{\\lambda\\mathbf{I}-\\mathbf{A}}{d(\\lambda)}\\right|\n$$  \n\nSolution. By assumption,the greatest common divisor of the matrix adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ is $d(\\lambda)$ .Therefore,  \n\n$$\n\\mathbf{adj}(\\lambda\\mathbf{I}-\\mathbf{A})=d(\\lambda)\\mathbf{B}(\\lambda)\n$$  \n\nwhere the greatest common divisor of the $n^{2}$ elements (which are functions of $\\lambda$ ) of $\\mathbf{B}(\\lambda)$ is unity. Since  \n\n$$\n(\\lambda\\mathbf{I}-\\mathbf{A})\\,\\mathrm{adj}(\\lambda\\mathbf{I}-\\mathbf{A})=|\\lambda\\mathbf{I}-\\mathbf{A}|\\mathbf{I}\n$$  \n\nwe obtain  \n\n$$\nd(\\lambda)(\\lambda\\mathbf{I}-\\mathbf{A})\\mathbf{B}(\\lambda)\\,=\\,|\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}|\\mathbf{I}\n$$  \n\nfrom which we find that $|\\lambda\\mathbf{I}-\\mathbf{A}|$ is divisible by $d(\\lambda)$ . Let us put  \n\n$$\n|\\lambda\\mathbf{I}\\rrangle-\\,\\mathbf{A}|\\Tilde{\\mathit{\\Omega}}=d(\\lambda)\\psi(\\lambda)\n$$  \n\nBecause the coefficient of the highest-degree term in $\\lambda$ of $d(\\lambda)$ has been chosen to be 1, the coefficient of the highest-degree term in $\\lambda$ of $\\psi(\\lambda)$ is also 1. From Equations (9–98) and (9–99), we have  \n\n$$\n\\left(\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\right)\\mathbf{B}(\\lambda)\\,=\\,\\psi(\\lambda)\\mathbf{I}\n$$  \n\nHence,  \n\n$$\n\\psi(\\mathbf{A})=\\mathbf{0}\n$$  \n\nNote that $\\psi(\\lambda)$ can be written as  \n\n$$\n\\psi(\\lambda)\\,=\\,g(\\lambda)\\phi(\\lambda)\\,+\\,\\alpha(\\lambda)\n$$  \n\nwhere $\\alpha(\\lambda)$ is of lower degree than $\\phi(\\lambda)$ . Since $\\psi(\\mathbf{A})=\\mathbf{0}$ and $\\phi(\\mathbf{A})=\\mathbf{0}$ , we must have $\\alpha(\\mathbf{A})=\\mathbf{0}$ .Also, since $\\phi(\\lambda)$ is the minimal polynomial, $\\alpha(\\lambda)$ must be identically zero, or  \n\n$$\n\\psi(\\lambda)\\,=\\,g(\\lambda)\\phi(\\lambda)\n$$  \n\nNote that because $\\phi(\\mathbf{A})=\\mathbf{0}$ , we can write  \n\n$$\n\\phi(\\lambda)\\mathbf{I}=\\,(\\lambda\\mathbf{I}\\,-\\,\\mathbf{A})\\mathbf{C}(\\lambda)\n$$  \n\nHence,  \n\n$$\n\\psi(\\lambda)\\mathbf{I}=g(\\lambda)\\phi(\\lambda)\\mathbf{I}=g(\\lambda)(\\lambda\\mathbf{I}-\\mathbf{A})\\mathbf{C}(\\lambda)\n$$  \n\nNoting that $\\left(\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\right)\\mathbf{B}(\\lambda)\\,=\\,\\psi(\\lambda)\\mathbf{I}$ , we obtain  \n\n$$\n\\mathbf{B}(\\lambda)\\,=\\,g(\\lambda)\\mathbf{C}(\\lambda)\n$$  \n\nSince the greatest common divisor of the $n^{2}$ elements of ${\\bf{B}}(\\lambda)$ is unity, we have  \n\n$$\ng(\\lambda)=1\n$$  \n\nTherefore,  \n\n$$\n\\psi(\\lambda)\\,=\\,\\phi(\\lambda)\n$$  \n\nThen, from this last equation and Equation (9–99), we obtain  \n\n$$\n\\phi(\\lambda)=\\frac{|\\lambda\\mathbf{I}-\\mathbf{A}|}{d(\\lambda)}\n$$  \n\nA–9–9. If an $n\\times n$ matrix A has $n$ distinct eigenvalues, then the minimal polynomial of $\\mathbf{A}$ is identical to the characteristic polynomial.Also, if the multiple eigenvalues of A are linked in a Jordan chain, the minimal polynomial and the characteristic polynomial are identical. If, however, the multiple eigenvalues of $\\mathbf{A}$ are not linked in a Jordan chain, the minimal polynomial is of lower degree than the characteristic polynomial.  \n\nUsing the following matrices $\\mathbf{A}$ and $\\mathbf{B}$ as examples, verify the foregoing statements about the minimal polynomial when multiple eigenvalues are involved:  \n\n$$\n{\\bf A}={\\left[\\begin{array}{l l l}{2}&{1}&{4}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]},\\qquad{\\bf B}={\\left[\\begin{array}{l l l}{2}&{0}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}\n$$  \n\nSolution. First, consider the matrix A .The characteristic polynomial is given by  \n\n$$\n|\\lambda\\mathbf{I}-\\mathbf{A}|={\\left|\\begin{array}{l l l}{\\lambda\\,-\\,2}&{\\;\\,-1}&{\\;\\,-4}\\\\ {0}&{\\lambda\\,-\\,2}&{\\;\\,0}\\\\ {0}&{\\;\\,-3}&{\\;\\lambda\\,-1}\\end{array}\\right|}=(\\lambda\\,-\\,2)^{2}(\\lambda\\,-\\,1)\n$$  \n\nThus, the eigenvalues of $\\mathbf{A}$ are 2, 2, and 1. It can be shown that the Jordan canonical form of $\\mathbf{A}$ is  \n\n$$\n\\left[\\begin{array}{l l l}{2}&{1}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\n$$  \n\nand the multiple eigenvalues are linked in the Jordan chain as shown. To determine the minimal polynomial, let us first obtain adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ . It is given by  \n\n$$\n\\mathrm{adj}(\\lambda\\mathbf{I}-\\mathbf{A})={\\left[\\begin{array}{c c c}{(\\lambda-2)(\\lambda-1)}&{(\\lambda+11)}&{4(\\lambda-2)}\\\\ {0}&{(\\lambda-2)(\\lambda-1)}&{0}\\\\ {0}&{3(\\lambda-2)}&{(\\lambda-2)^{2}}\\end{array}\\right]}\n$$  \n\nNotice that there is no common divisor of all the elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ .Hence, $d(\\lambda)=1$ .Thus, the minimal polynomial $\\phi(\\lambda)$ is identical to the characteristic polynomial, or  \n\n$$\n\\begin{array}{c}{\\phi(\\lambda)=|\\lambda\\mathbf{I}-\\mathbf{A}|=(\\lambda-2)^{2}(\\lambda-1)}\\\\ {=\\lambda^{3}-5\\lambda^{2}+8\\lambda-4}\\end{array}\n$$  \n\nA simple calculation proves that  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{A}^{3}-5\\mathbf{A}^{2}+8\\mathbf{A}-4\\mathbf{I}}\\\\ &{={\\left[\\begin{array}{l l l}{8}&{72}&{28}\\\\ {0}&{8}&{0}\\\\ {0}&{21}&{1}\\end{array}\\right]}-{\\5}{\\left[\\begin{array}{l l l}{4}&{16}&{12}\\\\ {0}&{4}&{0}\\\\ {0}&{9}&{1}\\end{array}\\right]}+8{\\left[\\begin{array}{l l l}{2}&{1}&{4}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}-4{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]}}\\\\ &{={\\left[\\begin{array}{l l l}{0}&{0}&{0}\\\\ {0}&{0}&{0}\\\\ {0}&{0}&{0}\\end{array}\\right]}=\\mathbf{0}}\\end{array}}\n$$  \n\nbut  \n\n$$\n={\\left[\\begin{array}{l l l}{4}&{16}&{12}\\\\ {0}&{4}&{0}\\\\ {0}&{9}&{1}\\end{array}\\right]}-3{\\left[\\begin{array}{l l l}{2}&{1}&{4}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}+2{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]}\n$$  \n\n$$\n{\\mathbf{\\alpha}}={\\left[\\begin{array}{l l l}{0}&{13}&{0}\\\\ {0}&{0}&{0}\\\\ {0}&{0}&{0}\\end{array}\\right]}\\neq\\mathbf{0}\n$$  \n\nThus, we have shown that the minimal polynomial and the characteristic polynomial of this matrix A are the same.  \n\nNext, consider the matrix B.The characteristic polynomial is given by  \n\n$$\n|\\lambda\\mathbf{I}-\\mathbf{B}|={\\left|\\begin{array}{l l l}{\\lambda\\,-\\,2}&{\\,\\,\\,0}&{\\,\\,\\,0}\\\\ {\\,\\,0}&{\\,\\,\\,\\lambda\\,-\\,2}&{\\,\\,\\,0}\\\\ {\\,\\,0}&{\\,\\,\\,\\,-3}&{\\,\\,\\lambda\\,-\\,1}\\end{array}\\right|}=(\\lambda\\,-\\,2)^{2}(\\lambda\\,-\\,1)\n$$  \n\nA simple computation reveals that matrix $\\mathbf{B}$ has three eigenvectors, and the Jordan canonical form of $\\mathbf{B}$ is given by  \n\n$$\n\\left[\\begin{array}{l l l}{2}&{0}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\n$$  \n\nThus, the multiple eigenvalues are not linked.To obtain the minimal polynomial, we first compute adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{B}\\right)$ :  \n\n$$\n\\mathrm{adj}(\\lambda\\mathbf{I}-\\mathbf{B})=\\left[\\begin{array}{c c c}{\\left(\\lambda-2\\right)(\\lambda-1)}&{0}&{0}\\\\ {0}&{(\\lambda-2)(\\lambda-1)}&{0}\\\\ {0}&{3(\\lambda-2)}&{(\\lambda-2)^{2}}\\end{array}\\right]\n$$  \n\nfrom which it is evident that  \n\n$$\nd(\\boldsymbol{\\lambda})\\,=\\,\\boldsymbol{\\lambda}\\,-\\,2\n$$  \n\nHence,  \n\n$$\n\\phi(\\lambda)\\,={\\frac{|\\lambda\\mathbf{I}\\,-\\,\\mathbf{B}|}{d(\\lambda)}}={\\frac{(\\lambda\\,-\\,2)^{2}(\\lambda\\,-\\,1)}{\\lambda\\,-\\,2}}=\\lambda^{2}\\,-\\,3\\lambda\\,+\\,2\n$$  \n\nAs a check, let us compute $\\phi({\\bf B})$ :  \n\n$$\n=\\mathbf{B}^{2}-3\\mathbf{B}+2\\mathbf{I}={\\left[\\begin{array}{l l l}{4}&{0}&{0}\\\\ {0}&{4}&{0}\\\\ {0}&{9}&{1}\\end{array}\\right]}-3{\\left[\\begin{array}{l l l}{2}&{0}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}+2{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{0}&{0}\\\\ {0}&{0}&{0}\\\\ {0}&{0}&{0}\\end{array}\\right]}=0\n$$  \n\nFor the given matrix B, the degree of the minimal polynomial is lower by 1 than that of the characteristic polynomial.As shown here, if the multiple eigenvalues of an $n\\times n$ matrix are not linked in a Jordan chain, the minimal polynomial is of lower degree than the characteristic polynomial.  \n\nA–9–10. Show that by use of the minimal polynomial, the inverse of a nonsingular matrix A can be expressed as a polynomial in $\\mathbf{A}$ with scalar coefficients as follows:  \n\n$$\n\\mathbf{A}^{-1}=-\\,{\\frac{1}{a_{m}}}\\left(\\mathbf{A}^{m-1}\\,+\\,a_{1}\\,\\mathbf{A}^{m-2}\\,+\\,\\cdots\\,+\\,a_{m-2}\\,\\mathbf{A}\\,+\\,a_{m-1}\\mathbf{I}\\right)\n$$  \n\nwhere $a_{1},a_{2},\\ldots,a_{m}$ are coefficients of the minimal polynomial  \n\n$$\n\\phi(\\lambda)=\\lambda^{m}\\,+\\,a_{1}\\lambda^{m-1}\\,+\\,\\cdots\\,+\\,a_{m-1}\\lambda\\,+\\,a_{m}\n$$  \n\nThen obtain the inverse of the following matrix A :  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{1}&{2}&{0}\\\\ {3}&{-1}&{-2}\\\\ {1}&{0}&{-3}\\end{array}\\right]}\n$$  \n\nSolution. For a nonsingular matrix $\\mathbf{A}$ , its minimal polynomial $\\phi(\\mathbf{A})$ can be written as  \n\n$$\n\\phi(\\mathbf{A})=\\mathbf{A}^{m}+a_{1}\\mathbf{A}^{m-1}+\\cdots+a_{m-1}\\mathbf{A}+a_{m}\\mathbf{I}=\\mathbf{0}\n$$  \n\nwhere $a_{m}\\neq0.$ . Hence,  \n\n$$\n\\mathbf{I}=-\\,{\\frac{1}{a_{m}}}\\bigl(\\mathbf{A}^{m}\\,+\\,a_{1}\\mathbf{A}^{m-1}\\,+\\,\\cdots\\,+\\,a_{m-2}\\mathbf{A}^{2}\\,+\\,a_{m-1}\\mathbf{A}\\bigr)\n$$  \n\nPremultiplying by $\\mathbf{A}^{-1}$ , we obtain  \n\n$$\n\\mathbf{A}^{-1}=-\\,{\\frac{1}{a_{m}}}\\left(\\mathbf{A}^{m-1}\\,+\\,a_{1}\\,\\mathbf{A}^{m-2}\\,+\\,\\cdots\\,+\\,a_{m-2}\\,\\mathbf{A}\\,+\\,a_{m-1}\\mathbf{I}\\right)\n$$  \n\nwhich is Equation (9–100).  \n\nFor the given matrix $\\mathbf{A}$ , adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ can be given as  \n\n$$\n\\operatorname{adj}(\\operatorname{AI}-\\operatorname{A})={\\left[\\begin{array}{c c c}{\\lambda^{2}+4\\lambda+3}&{2\\lambda+6}&{-4}\\\\ {3\\lambda+7}&{\\lambda^{2}+2\\lambda-3}&{-2\\lambda+2}\\\\ {\\lambda+1}&{2}&{\\lambda^{2}-7}\\end{array}\\right]}\n$$  \n\nClearly, there is no common divisor $d(\\lambda)$ of all elements of adj $\\left(\\lambda\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\right)$ . Hence, $d(\\lambda)=1$ .Consequently, the minimal polynomial $\\phi(\\lambda)$ is given by  \n\n$$\n\\phi(\\boldsymbol{\\lambda})\\,=\\frac{|\\boldsymbol{\\lambda}\\mathbf{I}\\,-\\,\\mathbf{A}|}{d(\\boldsymbol{\\lambda})}=\\,|\\boldsymbol{\\lambda}\\mathbf{I}\\,-\\,\\mathbf{A}|\n$$  \n\nThus, the minimal polynomial $\\phi(\\lambda)$ is the same as the characteristic polynomial. Since the characteristic polynomial is  \n\n$$\n\\left|\\lambda\\mathbf{I}\\,-\\,\\mathbf{A}\\right|\\,=\\,\\lambda^{3}\\,+\\,3\\lambda^{2}\\,-\\,7\\lambda\\,-\\,17\n$$  \n\nwe obtain  \n\n$$\n\\phi(\\lambda)\\,=\\,\\lambda^{3}\\,+\\,3\\lambda^{2}\\,-\\,7\\lambda\\,-\\,17\n$$  \n\nBy identifying the coefficients $a_{i}$ of the minimal polynomial (which is the same as the characteristic polynomial in this case), we have  \n\n$$\na_{1}=3,\\qquad a_{2}=-7,\\qquad a_{3}=-17\n$$  \n\nThe inverse of $\\mathbf{A}$ can then be obtained from Equation (9–100) as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathbf{A}^{-1}=-\\frac{1}{a_{3}}\\left(\\mathbf{A}^{2}+a_{1}\\mathbf{A}+a_{2}\\mathbf{I}\\right)=\\frac{1}{17}\\left(\\mathbf{A}^{2}+3\\mathbf{A}-7\\mathbf{I}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =\\frac{1}{17}\\left\\{\\left\\{\\begin{array}{l l l}{7}&{0}&{-4}\\\\ {-2}&{7}&{8}\\\\ {-2}&{2}&{9}\\end{array}\\right\\}+3\\left[\\begin{array}{l l l}{1}&{2}&{0}\\\\ {3}&{-1}&{-2}\\\\ {1}&{0}&{-3}\\end{array}\\right]-7\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ =\\frac{1}{17}\\left[\\begin{array}{l l l}{3}&{6}&{-4}\\\\ {7}&{-3}&{2}\\\\ {1}&{2}&{-7}\\end{array}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ =\\left[\\begin{array}{l l l}{3}&{\\frac{6}{17}}&{-\\frac{4}{17}}\\\\ {\\frac{7}{19}}&{-\\frac{13}{19}}&{\\frac{1}{17}}\\\\ {\\frac{1}{17}}&{\\frac{1}{19}}&{-\\frac{1}{17}}\\end{array}\\right]}\\end{array}\n$$  \n\nA–9–11. Show that if matrix A can be diagonalized, then  \n\n$$\ne^{\\mathbf{A}t}=\\mathbf{P}e^{\\mathbf{D}t}\\mathbf{P}^{-1}\n$$  \n\nwhere $\\mathbf{P}$ is a diagonalizing transformation matrix that transforms $\\mathbf{A}$ into a diagonal matrix, or $\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}$ , where $\\mathbf{D}$ is a diagonal matrix.  \n\nShow also that if matrix A can be transformed into a Jordan canonical form, then  \n\n$$\ne^{\\mathbf{A}t}=\\mathbf{S}e^{\\mathbf{J}t}\\mathbf{S}^{-1}\n$$  \n\nwhere $\\mathbf{S}$ is a transformation matrix that transforms A into a Jordan canonical form J,or $\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}=\\mathbf{J}$ .  \n\nSolution. Consider the state equation  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nIf a square matrix can be diagonalized, then a diagonalizing matrix (transformation matrix) exists and it can be obtained by a standard method. Let $\\mathbf{P}$ be a diagonalizing matrix for A . Let us define  \n\n$$\n\\mathbf{x}=\\mathbf{P}\\hat{\\mathbf{x}}\n$$  \n\nThen  \n\n$$\n\\hat{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\hat{\\mathbf{x}}\\,=\\,\\mathbf{D}\\hat{\\mathbf{x}}\n$$  \n\nwhere $\\mathbf{D}$ is a diagonal matrix.The solution of this last equation is  \n\n$$\n\\hat{\\mathbf{x}}(t)\\,=\\,e^{\\mathbf{D}t}\\hat{\\mathbf{x}}(0)\n$$  \n\nHence,  \n\n$$\n{\\bf x}(t)\\,=\\,{\\bf P}\\hat{\\bf x}(t)\\,=\\,{\\bf P}e^{\\bf D}\\imath\\,{\\bf P}^{-1}{\\bf x}(0)\n$$  \n\nNoting that ${\\bf x}(t)$ can also be given by the equation  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nwe obtain $e^{\\mathbf{A}t}=\\mathbf{P}e^{\\mathbf{D}t}\\mathbf{P}^{-1}$ ,or  \n\n$$\ne^{\\mathbf{A}t}=\\mathbf{P}e^{\\mathbf{D}t}\\mathbf{P}^{-1}=\\mathbf{P}\\left[\\begin{array}{l l l l l l}{e^{\\lambda_{1}t}}&&&&&{0}\\\\ &{e^{\\lambda_{2}t}}&&&\\\\ &&{\\cdot}&&\\\\ &&&{\\cdot}&\\\\ &&&&{\\cdot}&\\\\ {0}&&&&&{e^{\\lambda_{n}t}}\\end{array}\\right]\\mathbf{P}^{-1}\n$$  \n\nNext, we shall consider the case where matrix A may be transformed into a Jordan canonical form. Consider again the state equation  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nFirst obtain a transformation matrix Sthat will transform matrix $\\mathbf{A}$ into a Jordan canonical form so that  \n\n$$\n\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}=\\mathbf{J}\n$$  \n\nwhere $\\mathbf{J}$ is a matrix in a Jordan canonical form. Now define  \n\n$$\n\\mathbf{x}=\\mathbf{S}\\hat{\\mathbf{x}}\n$$  \n\nThen  \n\n$$\n\\dot{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}\\hat{\\mathbf{x}}\\,=\\,\\mathbf{J}\\hat{\\mathbf{x}}\n$$  \n\nThe solution of this last equation is  \n\n$$\n\\hat{\\mathbf{x}}(t)\\,=\\,e^{\\mathbf{J}t}\\hat{\\mathbf{x}}(0)\n$$  \n\nHence,  \n\n$$\n{\\bf x}(t)\\,=\\,{\\bf S}\\hat{\\bf x}(t)\\,=\\,{\\bf S}e^{\\bf J}{\\bf S}^{-1}{\\bf x}(0)\n$$  \n\nSince the solution ${\\bf x}(t)$ can also be given by the equation  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nwe obtain  \n\n$$\ne^{\\mathbf{A}t}=\\mathbf{S}e^{\\mathbf{J}t}\\mathbf{S}^{-1}\n$$  \n\nNote that $e^{\\mathbf{J}t}$ is a triangular matrix [which means that the elements below (or above, as the case may be) the principal diagonal line are zeros] whose elements are $e^{\\lambda t},t e^{\\lambda t},{\\frac{1}{2}}t^{2}e^{\\lambda t}$ , and so forth. For example, if matrix $\\mathbf{J}$ has the following Jordan canonical form:  \n\n$$\n{\\bf J}=\\left[\\begin{array}{c c c}{{\\lambda_{1}}}&{{1}}&{{0}}\\\\ {{0}}&{{\\lambda_{1}}}&{{1}}\\\\ {{0}}&{{0}}&{{\\lambda_{1}}}\\end{array}\\right]\n$$  \n\nthen  \n\n$$\ne^{\\mathbf{J}t}={\\left[\\begin{array}{l l l}{e^{\\lambda_{1}t}}&{t e^{\\lambda_{1}t}}&{{\\frac{1}{2}}t^{2}e^{\\lambda_{1}t}}\\\\ {0}&{e^{\\lambda_{1}t}}&{t e^{\\lambda_{1}t}}\\\\ {0}&{0}&{e^{\\lambda_{1}t}}\\end{array}\\right]}\n$$  \n\nSimilarly, if  \n\n$$\n\\mathbf{J}={\\frac{\\left[\\begin{array}{l l l}{\\lambda_{1}}&{1}&{0}\\\\ {0}&{\\lambda_{1}}&{1}\\\\ {...}&{0}&{...}\\\\ {...}&{...}&{...}\\\\ {.}&{\\qquad\\qquad\\;\\;\\vdots}&{...}\\end{array}\\right]}{\\left[\\begin{array}{l l}{0}&{\\lambda_{4}}\\\\ {..}&{0}\\end{array}\\right]}}_{\\left\\{\\begin{array}{l}{1}\\\\ {0}\\\\ {1}\\end{array}\\right\\}}}\\end{array}}\n$$  \n\nthen  \n\n$$\ne^{\\boldsymbol{\\imath}t}=\\left[\\begin{array}{c c c c c}{e^{\\lambda_{1}t}}&{t e^{\\lambda_{1}t}}&{\\frac{1}{2}t^{2}e^{\\lambda_{1}t}}&{\\vdots}&{0}\\\\ {0}&{e^{\\lambda_{1}t}}&{t e^{\\lambda_{1}t}}&{\\vdots}&{0}\\\\ {....}&{0}&{....}&{....}&{...}\\\\ &&&{\\vdots}&{e^{\\lambda_{1}t}}&{t e^{\\lambda_{4}t}}\\\\ &&&{\\vdots}&{0}&{...}\\\\ {0}&&&&{\\vdots}&{e^{\\lambda_{4}t}}&{0}\\\\ {0}&&&&&{\\vdots}&{e^{\\lambda_{7}t}}\\end{array}\\right]\n$$  \n\nA–9–12. Consider the following polynomial in $\\lambda$ of degree $m\\textrm{--}1$ , where we assume $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{m}$ to be distinct:  \n\n$$\np_{k}(\\lambda)=\\frac{(\\lambda-\\lambda_{1})\\cdots(\\lambda-\\lambda_{k-1})(\\lambda-\\lambda_{k+1})\\cdots(\\lambda-\\lambda_{m})}{\\bigl(\\lambda_{k}-\\,\\lambda_{1}\\bigr)\\cdots\\bigl(\\lambda_{k}\\,-\\,\\lambda_{k-1}\\bigr)\\bigl(\\lambda_{k}\\,-\\,\\lambda_{k+1}\\bigr)\\cdots\\bigl(\\lambda_{k}\\,-\\,\\lambda_{m}\\bigr)}\n$$  \n\nwhere $k=1,2,\\dotsc,m.$ . Notice that  \n\n$$\np_{k}{\\big(}\\lambda_{i}{\\big)}={\\binom{1,}{0,}}\\qquad{\\mathrm{if~}}i=k\n$$  \n\nThen the polynomial $f(\\lambda)$ of degree $m\\,-\\,1$ ,  \n\n$$\n\\begin{array}{r l r}{f(\\lambda)\\lefteqn{}f(\\lambda)=}&{\\sum_{k=1}^{m}f\\big(\\lambda_{k}\\big)p_{k}(\\lambda)}\\\\ &{}&{=\\,\\sum_{k=1}^{m}f\\big(\\lambda_{k}\\big)\\,{\\displaystyle\\frac{\\big(\\lambda\\,-\\,\\lambda_{1}\\big)\\cdots\\big(\\lambda\\,-\\,\\lambda_{k-1}\\big)\\big(\\lambda\\,-\\,\\lambda_{k+1}\\big)\\cdots\\big(\\lambda\\,-\\,\\lambda_{m}\\big)}{\\big(\\lambda_{k}\\,-\\,\\lambda_{1}\\big)\\cdots\\big(\\lambda_{k}\\,-\\,\\lambda_{k-1}\\big)\\big(\\lambda_{k}\\,-\\,\\lambda_{k+1}\\big)\\cdots\\big(\\lambda_{k}\\,-\\,\\lambda_{m}\\big)}}}\\end{array}\n$$  \n\ninterpolation formula takes A B$f{\\bigl(}\\lambda_{1}{\\bigr)},\\;\\;f{\\bigl(}\\lambda_{2}{\\bigr)}$ A BA s$f\\!\\left(\\lambda_{k}\\right)$ .The polynomial B,f$\\bar{f}\\bigl(\\lambda_{m}\\bigr)$ A e poin hat is, the polyno $f(\\lambda)$ $\\lambda_{k}$ of degree . This last $m\\,-\\,1$ is determined from o$f(\\lambda)$ commonly cal passes through mdindependent L$_m$ points range’s $f{\\big(}\\lambda_{1}{\\big)},f{\\big(}\\lambda_{2}{\\big)},\\ldots,f{\\big(}\\lambda_{m}{\\big)}$ . Since $f(\\lambda)$ is a polynomial of degree m-1 , it is uniquely determined. Any other representations of the polynomial of degree $m\\textrm{--}1$ can be reduced to the Lagrange polynomial $f(\\lambda)$ .  \n\nAssuming that the eigenvalues of an $n\\times n$ matrix $\\mathbf{A}$ are distinct, substitute $\\mathbf{A}$ for $\\lambda$ in the polynomial $p_{k}(\\lambda)$ .Then we get  \n\n$$\np_{k}(\\mathbf{A})\\,={\\frac{(\\mathbf{A}\\,-\\,\\lambda_{1}\\,\\mathbf{I})\\cdots\\bigl(\\mathbf{A}\\,-\\,\\lambda_{k-1}\\,\\mathbf{I}\\bigr)\\bigl(\\mathbf{A}\\,-\\,\\lambda_{k+1}\\,\\mathbf{I}\\bigr)\\cdots\\bigl(\\mathbf{A}\\,-\\,\\lambda_{m}\\,\\mathbf{I}\\bigr)}{{\\bigl(}\\lambda_{k}\\,-\\,\\lambda_{1}{\\bigr)}\\cdots{\\bigl(}\\lambda_{k}\\,-\\,\\lambda_{k-1}{\\bigr)}{\\bigl(}\\lambda_{k}\\,-\\,\\lambda_{k+1}{\\bigr)}\\cdots{\\bigl(}\\lambda_{k}\\,-\\,\\lambda_{m}{\\bigr)}}}\n$$  \n\nNotice that $p_{k}(\\mathbf{A})$ is a polynomial in $\\mathbf{A}$ of degree m-1 . Notice also that  \n\n$$\np_{k}{\\big(}\\lambda_{i}\\mathbf{I}{\\big)}\\,=\\,{\\Bigg\\{}{\\mathbf{I}},\\qquad{\\mathrm{if~}}i{\\mathbf{\\Gamma}}=k\n$$  \n\nNow define  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{f(\\mathbf{A})\\ =\\ \\sum_{k=1}^{m}f\\big(\\lambda_{k}\\big)p_{k}(\\mathbf{A})}}\\\\ &{}&{=\\ \\displaystyle\\sum_{k=1}^{m}f\\big(\\lambda_{k}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\cdots\\big(\\mathbf{A}\\,-\\,\\lambda_{k-1}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{k+1}\\mathbf{I}\\big)\\cdots\\big(\\mathbf{A}\\,-\\,\\lambda_{m}\\mathbf{I}\\big)}{\\big(\\lambda_{k}\\,-\\,\\lambda_{1}\\big)\\cdots\\big(\\lambda_{k}\\,-\\,\\lambda_{k-1}\\big)\\big(\\lambda_{k}\\,-\\,\\lambda_{k+1}\\big)\\cdots\\big(\\lambda_{k}\\,-\\,\\lambda_{m}\\big)}}\\end{array}\n$$  \n\nEquation (9–102) is known as Sylvester’s interpolation formula. Equation (9–102) is equivalent to the following equation:  \n\n$$\n{\\begin{array}{c c c c c}{{1}}&{{1}}&{{\\cdots}}&{{1}}&{{{\\bf I}}}\\\\ {{\\lambda_{1}}}&{{\\lambda_{2}}}&{{\\cdots}}&{{\\lambda_{m}}}&{{{\\bf A}}}\\\\ {{\\lambda_{1}^{2}}}&{{\\lambda_{2}^{2}}}&{{\\cdots}}&{{\\lambda_{m}^{2}}}&{{{\\bf A}^{2}}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\lambda_{1}^{m-1}}}&{{\\lambda_{2}^{m-1}}}&{{\\cdots}}&{{\\lambda_{m}^{m-1}}}&{{{\\bf A}^{m-1}}}\\\\ {{f\\left(\\lambda_{1}\\right)}}&{{f\\left(\\lambda_{2}\\right)}}&{{\\cdots}}&{{f\\left(\\lambda_{m}\\right)}}&{{f\\left({\\bf A}\\right)}}\\end{array}}\\,\n$$  \n\nEquations (9–102) and (9–103) are frequently used for evaluating functions $f(\\mathbf{A})$ of matrix A —for example, $(\\lambda\\mathbf{I}-\\mathbf{A})^{-1},e^{\\mathbf{A}t}$ , and so forth. Note that Equation (9–103) can also be written as  \n\n$$\n\\left|\\begin{array}{c c c c c c}{{1}}&{{\\lambda_{1}}}&{{\\lambda_{1}^{2}}}&{{\\cdots}}&{{\\lambda_{1}^{m-1}}}&{{f(\\lambda_{1})}}\\\\ {{1}}&{{\\lambda_{2}}}&{{\\lambda_{2}^{2}}}&{{\\cdots}}&{{\\lambda_{2}^{m-1}}}&{{f(\\lambda_{2})}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{1}}&{{\\lambda_{m}}}&{{\\lambda_{m}^{2}}}&{{\\cdots}}&{{\\lambda_{m}^{m-1}}}&{{f(\\lambda_{m})}}\\\\ {{{\\bf1}}}&{{{\\bf A}}}&{{{\\bf A}^{2}}}&{{\\cdots}}&{{{\\bf A}^{m-1}}}&{{f({\\bf A})}}\\end{array}\\right|={\\bf0}\n$$  \n\nShow that Equations (9–102) and (9–103) are equivalent.To simplify the arguments, assume that $m=4$ .  \n\n$$\n\\Delta\\,=\\,\\left|\\begin{array}{c c c c c c}{{1}}&{{1}}&{{1}}&{{1}}&{{{\\bf I}}}\\\\ {{\\lambda_{1}}}&{{\\lambda_{2}}}&{{\\lambda_{3}}}&{{\\lambda_{4}}}&{{{\\bf A}}}\\\\ {{\\lambda_{1}^{2}}}&{{\\lambda_{2}^{2}}}&{{\\lambda_{3}^{2}}}&{{\\lambda_{4}^{2}}}&{{{\\bf A}^{2}}}\\\\ {{\\lambda_{1}^{3}}}&{{\\lambda_{2}^{3}}}&{{\\lambda_{3}^{3}}}&{{\\lambda_{4}^{3}}}&{{{\\bf A}^{3}}}\\\\ {{f\\!\\left(\\lambda_{1}\\right)}}&{{f\\!\\left(\\lambda_{2}\\right)}}&{{f\\!\\left(\\lambda_{3}\\right)}}&{{f\\!\\left(\\lambda_{4}\\right)}}&{{f\\!\\left(\\bf A\\right)}}\\end{array}\\right|\n$$  \n\n$$\n=f(\\mathbf{A}){\\left|\\begin{array}{l l l l}{1}&{1}&{1}&{1}\\\\ {\\lambda_{1}}&{\\lambda_{2}}&{\\lambda_{3}}&{\\lambda_{4}}\\\\ {\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\lambda_{3}^{2}}&{\\lambda_{4}^{2}}\\\\ {\\lambda_{1}^{3}}&{\\lambda_{2}^{3}}&{\\lambda_{3}^{3}}&{\\lambda_{4}^{3}}\\end{array}\\right|}-f(\\lambda_{4}){\\left|\\begin{array}{l l l l}{1}&{1}&{1}&{\\mathbf{I}}\\\\ {\\lambda_{1}}&{\\lambda_{2}}&{\\lambda_{3}}&{\\mathbf{A}}\\\\ {\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\lambda_{3}^{2}}&{\\mathbf{A}^{2}}\\\\ {\\lambda_{1}^{3}}&{\\lambda_{2}^{3}}&{\\lambda_{3}^{3}}&{\\mathbf{A}^{3}}\\end{array}\\right|}\n$$  \n\n$$\n{\\begin{array}{r l}&{{\\left|\\begin{array}{l l l l}{1}&{1}&{1}&{\\mathbf{I}}\\\\ {+\\;f(\\lambda_{3}){\\Biggl|}\\lambda_{1}}&{\\lambda_{2}}&{\\lambda_{4}}&{\\mathbf{A}}\\\\ {\\;\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\lambda_{4}^{2}}&{\\mathbf{A}^{2}}\\end{array}\\right|}-{\\boldsymbol{f}}\\left(\\lambda_{2}\\right){\\Biggl|}\\lambda_{1}^{1}}&{\\lambda_{3}}&{\\lambda_{4}}&{\\mathbf{A}}\\\\ &{\\;\\;\\;\\left|\\lambda_{1}^{3}}&{\\lambda_{2}^{3}}&{\\lambda_{4}^{3}}&{\\mathbf{A}^{3}\\right|}\\\\ &{\\;\\;\\;\\left|\\lambda_{1}^{3}}&{\\lambda_{2}^{3}}&{\\lambda_{4}^{3}}&{\\mathbf{A}^{3}\\right|}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{+\\;{\\footnotesize}f(\\lambda_{1}){\\Biggl|}\\lambda_{2}^{1}}&{\\lambda_{3}}&{\\lambda_{4}}&{\\mathbf{A}}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}}\n$$  \n\nSince  \n\n$$\n{\\begin{array}{l l l}{\\left|{\\begin{array}{r r r r}{1}&{1}&{1}&{1}\\\\ {\\lambda_{1}}&{\\lambda_{2}}&{\\lambda_{3}}&{\\lambda_{4}}\\\\ {\\lambda_{1}^{2}}&{\\lambda_{2}^{2}}&{\\lambda_{3}^{2}}&{\\lambda_{4}^{2}}\\end{array}}\\right|}=(\\lambda_{4}-\\,\\lambda_{3})(\\lambda_{4}-\\,\\lambda_{2})(\\lambda_{4}-\\,\\lambda_{1})(\\lambda_{3}-\\,\\lambda_{2})(\\lambda_{3}-\\,\\lambda_{1})(\\lambda_{2}-\\,\\lambda_{1})}\\\\ {|\\lambda_{1}^{3}}&{\\lambda_{2}^{3}}&{\\lambda_{3}^{3}}&{\\lambda_{4}^{3}{\\Big|}}\\end{array}}\n$$  \n\nand  \n\n$$\n{\\left|\\begin{array}{l l l l}{1}&{1}&{1}&{\\mathbf{I}}\\\\ {\\lambda_{i}}&{\\lambda_{j}}&{\\lambda_{k}}&{\\mathbf{A}}\\\\ {\\lambda_{i}^{2}}&{\\lambda_{j}^{2}}&{\\lambda_{k}^{2}}&{\\mathbf{A}^{2}}\\end{array}\\right|}=\\left(\\mathbf{A}\\,-\\,\\lambda_{k}\\mathbf{I}\\right)\\!(\\mathbf{A}\\,-\\,\\lambda_{j}\\mathbf{I})\\!(\\mathbf{A}\\,-\\,\\lambda_{i}\\mathbf{I})\\!(\\lambda_{k}\\,-\\,\\lambda_{j})\\!(\\lambda_{k}\\,-\\,\\lambda_{i})\\!(\\lambda_{j}\\,-\\,\\lambda_{i})\n$$  \n\nwe obtain  \n\n$$\n\\begin{array}{r l}&{\\Delta=f(\\mathbf{A})\\big[\\big(\\lambda_{4}-\\lambda_{3}\\big)\\big(\\lambda_{4}-\\,\\lambda_{2}\\big)\\big(\\lambda_{4}-\\,\\lambda_{1}\\big)\\big(\\lambda_{3}-\\,\\lambda_{2}\\big)\\big(\\lambda_{3}-\\,\\lambda_{1}\\big)\\big(\\lambda_{2}-\\,\\lambda_{1}\\big)\\big]}\\\\ &{\\qquad-\\,f\\big(\\lambda_{4}\\big)\\big[\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\lambda_{3}-\\,\\lambda_{2}\\big)\\big(\\lambda_{3}-\\,\\lambda_{1}\\big)\\big(\\lambda_{2}\\,-\\,\\lambda_{1}\\big)\\big]}\\\\ &{\\qquad+\\,f\\big(\\lambda_{3}\\big)\\big[\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\lambda_{4}-\\,\\lambda_{2}\\big)\\big(\\lambda_{4}-\\,\\lambda_{1}\\big)\\big(\\lambda_{2}\\,-\\,\\lambda_{1}\\big)\\big]}\\\\ &{\\qquad-\\,f\\big(\\lambda_{2}\\big)\\big[\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\lambda_{4}-\\,\\lambda_{3}\\big)\\big(\\lambda_{4}-\\,\\lambda_{1}\\big)\\big(\\lambda_{3}\\,-\\,\\lambda_{1}\\big)\\big]}\\\\ &{\\qquad+\\,f\\big(\\lambda_{1}\\big)\\big[\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\lambda_{4}-\\,\\lambda_{3}\\big)\\big(\\lambda_{4}-\\,\\lambda_{2}\\big)\\big(\\lambda_{3}\\,-\\,\\lambda_{2}\\big)\\big]}\\\\ &{\\qquad=\\,\\mathbf{0}}\\end{array}\n$$  \n\nSolving this last equation for $f(\\mathbf{A})$ , we obtain  \n\n$$\n\\begin{array}{r l}&{f(\\mathbf{A})=f\\big(\\lambda_{1}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)}{\\big(\\lambda_{1}\\,-\\,\\lambda_{2}\\big)\\big(\\lambda_{1}\\,-\\,\\lambda_{3}\\big)\\big(\\lambda_{1}\\,-\\,\\lambda_{4}\\big)}+f\\big(\\lambda_{2}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)}{\\big(\\lambda_{2}\\,-\\,\\lambda_{1}\\big)\\big(\\lambda_{2}\\,-\\,\\lambda_{3}\\big)\\big(\\lambda_{2}\\,-\\,\\lambda_{4}\\big)}}\\\\ &{\\qquad+\\,f\\big(\\lambda_{3}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{4}\\mathbf{I}\\big)}{\\big(\\lambda_{3}\\,-\\,\\lambda_{1}\\big)\\big(\\lambda_{3}\\,-\\,\\lambda_{2}\\big)\\big(\\lambda_{3}\\,-\\,\\lambda_{4}\\big)}+f\\big(\\lambda_{4}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{2}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{3}\\big)}{\\big(\\lambda_{4}\\,-\\,\\lambda_{1}\\big)\\big(\\lambda_{4}\\,-\\,\\lambda_{2}\\big)\\big(\\lambda_{4}\\,-\\,\\lambda_{3}\\big)}}\\\\ &{\\qquad=\\,\\frac{m}{k=1}f\\big(\\lambda_{k}\\big)\\frac{\\big(\\mathbf{A}\\,-\\,\\lambda_{1}\\mathbf{I}\\big)\\dotsm\\big(\\mathbf{A}\\,-\\,\\lambda_{k-1}\\mathbf{I}\\big)\\big(\\mathbf{A}\\,-\\,\\lambda_{k+1}\\mathbf{I}\\big)\\dotsm\\big(\\mathbf{A}\\,-\\,\\lambda_{m}\\mathbf{I}\\big)}{\\big(\\lambda_{k}\\,-\\,\\lambda_{1}\\big)\\dotsm\\big(\\lambda_{k}\\,-\\,\\lambda_{k-1}\\big \n$$  \n\nwhere $m\\,=\\,4.$ .Thus, we have shown the equivalence of Equations (9–102) and (9–103).Although we assumed $m=4$ , the entire argument can be extended to an arbitrary positive integer $_m$ . (For the case when the matrix A involves multiple eigenvalues, refer to Problem A–9–13 .)  \n\nA–9–13. Consider Sylvester’s interpolation formula in the form given by Equation (9–104):  \n\n$$\n\\left|\\begin{array}{c c c c c c}{{1}}&{{\\lambda_{1}}}&{{\\lambda_{1}^{2}}}&{{\\cdots}}&{{\\lambda_{1}^{m-1}}}&{{f(\\lambda_{1})}}\\\\ {{1}}&{{\\lambda_{2}}}&{{\\lambda_{2}^{2}}}&{{\\cdots}}&{{\\lambda_{2}^{m-1}}}&{{f(\\lambda_{2})}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{.}}&{{.}}&{{.}}&{{.}}&{{.}}&{{.}}\\\\ {{1}}&{{\\lambda_{m}}}&{{\\lambda_{m}^{2}}}&{{\\cdots}}&{{\\lambda_{m}^{m-1}}}&{{f(\\lambda_{m})}}\\\\ {{{\\bf1}}}&{{{\\bf A}}}&{{{\\bf A}^{2}}}&{{\\cdots}}&{{{\\bf A}^{m-1}}}&{{f({\\bf A})}}\\end{array}\\right|={\\bf0}\n$$  \n\nThis formula for the determination of $f(\\mathbf{A})$ applies to the case where the minimal polynomial of A involves only distinct roots.  \n\nSuppose that the minimal polynomial of $\\mathbf{A}$ involves multiple roots. Then the rows in the determinant that correspond to the multiple roots become identical, and therefore modification of the determinant in Equation (9–104) becomes necessary.  \n\nminimal polynomial of Modify the form of Sylvester’s interpolation formula given by Equation (9–104) when the A involves multiple roots. In deriving a modified determinant equation, Bassume that there are three equal roots $\\bar{\\left(\\!\\!\\begin{array}{l}{\\lambda_{1}}\\end{array}\\!\\!\\right)}=\\lambda_{2}=\\lambda_{3}$ in the minimal polynomial of $\\mathbf{A}$ and that there are other roots $\\left(\\lambda_{4},\\lambda_{5},\\ldots,\\lambda_{m}\\right)$ that are distinct.  \n\nSolution. Since the minimal polynomial of $\\mathbf{A}$ involves three equal roots, the minimal polynomial $\\phi(\\lambda)$ can be written as  \n\n$$\n\\begin{array}{c}{{\\phi(\\lambda)\\,=\\,\\lambda^{m}\\,+\\,a_{1}\\lambda^{m-1}\\,+\\,\\cdots\\,+\\,a_{m-1}\\lambda\\,+\\,a_{m}}}\\\\ {{{}}}\\\\ {{=\\,\\big(\\lambda\\,-\\,\\lambda_{1}\\big)^{3}\\big(\\lambda\\,-\\,\\lambda_{4}\\big)\\big(\\lambda\\,-\\,\\lambda_{5}\\big)\\cdots\\big(\\lambda\\,-\\,\\lambda_{m}\\big)}}\\end{array}\n$$  \n\nAn arbitrary function $f(\\mathbf{A})$ of an $n\\times n$ matrix A can be written as  \n\n$$\nf(\\mathbf{A})\\,=\\,g(\\mathbf{A})\\phi(\\mathbf{A})\\,+\\,\\alpha(\\mathbf{A})\n$$  \n\nwhere the minimal polynomial $\\phi(\\mathbf{A})$ is of degree $_m$ and $\\alpha(\\mathbf{A})$ is a polynomial in $\\mathbf{A}$ of degree $m\\,-\\,1$ or less. Hence we have  \n\n$$\nf(\\lambda)\\,=\\,g(\\lambda)\\phi(\\lambda)\\,+\\,\\alpha(\\lambda)\n$$  \n\nwhere $\\alpha(\\lambda)$ is a polynomial in $\\lambda$ of degree $m\\textrm{--}1$ or less, which can thus be written as  \n\n$$\n\\alpha(\\lambda)\\,=\\,\\alpha_{0}\\,+\\,\\alpha_{1}\\lambda\\,+\\,\\alpha_{2}\\lambda^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}\\lambda^{m-1}\n$$  \n\nIn the present case we have  \n\n$$\n\\begin{array}{l}{{f(\\lambda)\\,=\\,g(\\lambda)\\phi(\\lambda)\\,+\\,\\alpha(\\lambda)}}\\\\ {{\\,}}\\\\ {{\\,=\\,g(\\lambda)\\bigl[(\\lambda\\,-\\,\\lambda_{1})^{3}(\\lambda\\,-\\,\\lambda_{4})\\cdots(\\lambda\\,-\\,\\lambda_{m})\\bigr]\\,+\\,\\alpha(\\lambda)}}\\end{array}\n$$  \n\nBy substituting $\\lambda_{1},\\lambda_{4},\\ldots,\\lambda_{m}$ for $\\lambda$ in Equation (9–106), we obtain the following $m\\textrm{--}2$ equations:  \n\n$$\n\\begin{array}{r}{f\\big(\\lambda_{1}\\big)\\,=\\,\\alpha\\big(\\lambda_{1}\\big)}\\\\ {f\\big(\\lambda_{4}\\big)\\,=\\,\\alpha\\big(\\lambda_{4}\\big)}\\\\ {\\,.}\\end{array}\n$$  \n\n$$\nf(\\lambda_{m})\\,=\\,\\alpha(\\lambda_{m})\n$$  \n\nBy differentiating Equation (9–106) with respect to $\\lambda$ , we obtain  \n\n$$\n\\frac{d}{d\\lambda}\\,f(\\lambda)\\,=\\,\\bigl(\\lambda\\,-\\,\\lambda_{1}\\bigr)^{2}h(\\lambda)\\,+\\,\\frac{d}{d\\lambda}\\,\\alpha(\\lambda)\n$$  \n\nwhere  \n\n$$\n(\\lambda\\,-\\,\\lambda_{1})^{2}h(\\lambda)\\,=\\frac{d}{d\\lambda}\\left[g(\\lambda)\\bigl(\\lambda\\,-\\,\\lambda_{1}\\bigr)^{3}\\bigl(\\lambda\\,-\\,\\lambda_{4}\\bigr)\\cdots\\bigl(\\lambda\\,-\\,\\lambda_{m}\\bigr)\\right]\n$$  \n\nSubstitution of $\\lambda_{1}$ for $\\lambda$ in Equation (9–108) gives  \n\n$$\n\\frac{d}{d\\lambda}\\,f(\\lambda)\\Bigg\\vert_{\\lambda=\\lambda_{1}}=f^{\\prime}\\big(\\lambda_{1}\\big)=\\frac{d}{d\\lambda}\\,\\alpha(\\lambda)\\Bigg\\vert_{\\lambda=\\lambda_{1}}\n$$  \n\nReferring to Equation (9–105), this last equation becomes  \n\n$$\nf^{\\prime}\\!\\left(\\lambda_{1}\\right)=\\alpha_{1}+2\\alpha_{2}\\lambda_{1}+\\cdots+(m-1)\\alpha_{m-1}\\lambda_{1}^{m-2}\n$$  \n\nSimilarly, differentiating Equation (9–106) twice with respect to $\\lambda$ and substituting $\\lambda_{1}$ for $\\lambda$ , we obtain  \n\n$$\n\\frac{d^{2}}{d^{2}\\lambda}\\,f(\\lambda)\\bigg\\vert_{\\lambda=\\lambda_{1}}=f^{\\prime\\prime}\\bigl(\\lambda_{1}\\bigr)=\\frac{d^{2}}{d\\lambda^{2}}\\,\\alpha(\\lambda)\\bigg\\vert_{\\lambda=\\lambda_{1}}\n$$  \n\nThis last equation can be written as  \n\n$$\nf^{\\prime\\prime}\\!\\!\\left(\\lambda_{1}\\right)=2\\alpha_{2}+6\\alpha_{3}\\lambda_{1}+\\cdots+(m-1)(m-2)\\alpha_{m-1}\\lambda_{1}^{m-3}\n$$  \n\nRewriting Equations (9–110), (9–109), and (9–107), we get  \n\n$$\n\\begin{array}{c}{{\\alpha_{2}\\,+\\,3\\alpha_{3}\\lambda_{1}\\,+\\,\\cdots+\\displaystyle\\frac{(m\\,-\\,1)(m\\,-\\,2)}{2}\\,\\alpha_{m-1}\\lambda_{1}^{m-3}=\\displaystyle\\frac{f^{m}\\big(\\lambda_{1}\\big)}{2}}}\\\\ {{{}}}\\\\ {{\\alpha_{1}\\,+\\,2\\alpha_{2}\\lambda_{1}\\,+\\,\\cdots\\,+\\,(m\\,-\\,1)\\alpha_{m-1}\\lambda_{1}^{m-2}=f^{\\prime}\\big(\\lambda_{1}\\big)}}\\\\ {{{}}}\\\\ {{\\alpha_{0}\\,+\\,\\alpha_{1}\\lambda_{1}\\,+\\,\\alpha_{2}\\lambda_{1}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}\\lambda_{1}^{m-1}=f\\big(\\lambda_{1}\\big)}}\\\\ {{{}}}\\\\ {{\\alpha_{0}\\,+\\,\\alpha_{1}\\lambda_{4}\\,+\\,\\alpha_{2}\\lambda_{4}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}\\lambda_{4}^{m-1}=f\\big(\\lambda_{4}\\big)}}\\\\ {{{}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,.}}\\end{array}\n$$  \n\n$$\n\\alpha_{0}\\,+\\,\\alpha_{1}\\lambda_{m}\\,+\\,\\alpha_{2}\\lambda_{m}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}\\lambda_{m}^{m-1}\\,=\\,f\\bigl(\\lambda_{m}\\bigr)\n$$  \n\nThese $_m$ simultaneous equations determine the $\\alpha_{k}$ values (where $k=0,1,2,\\ldots,m\\,-\\,1)$ ). Noting that $\\phi(\\mathbf{A})\\,=\\,\\mathbf{0}$ because it is a minimal polynomial, we have $f(\\mathbf{A})$ as follows:  \n\n$$\nf(\\mathbf{A})\\,=\\,g(\\mathbf{A})\\phi(\\mathbf{A})\\,+\\,\\alpha(\\mathbf{A})\\,=\\,\\alpha(\\mathbf{A})\n$$  \n\nHence, referring to Equation (9–105), we have  \n\n$$\nf(\\mathbf{A})=\\alpha(\\mathbf{A})=\\alpha_{0}\\mathbf{I}+\\alpha_{1}\\mathbf{A}+\\alpha_{2}\\mathbf{A}^{2}+\\cdots+\\alpha_{m-1}\\mathbf{A}^{m-1}\n$$  \n\nwhere the $\\alpha_{k}$ values are gi terms of $f{\\big(}\\lambda_{1}{\\big)},f^{\\prime}{\\big(}\\lambda_{1}{\\big)},f^{\\prime\\prime}{\\big(}\\lambda_{1}{\\big)},f{\\big(}\\lambda_{4}{\\big)},f{\\big(}\\lambda_{5}{\\big)},\\ldots,f{\\big(}\\lambda_{m}{\\big)}.$ . In terms of the determinant equation, $f(\\mathbf{A})$ can be obtained by solving the following equation:  \n\n$$\n\\left|\\begin{array}{c c c c c c c c}{{0}}&{{0}}&{{1}}&{{3\\lambda_{1}}}&{{\\cdots}}&{{\\frac{\\left(m-1\\right)\\left(m-2\\right)}{2}\\lambda_{1}^{m-3}}}&{{\\frac{f^{\\prime}\\left(\\lambda_{1}\\right)}{2}}}\\\\ {{0}}&{{1}}&{{2\\lambda_{1}}}&{{3\\lambda_{1}^{2}}}&{{\\cdots}}&{{\\left(m-1\\right)\\lambda_{1}^{m-2}}}&{{f^{\\prime}\\left(\\lambda_{1}\\right)}}\\\\ {{1}}&{{\\lambda_{1}}}&{{\\lambda_{1}^{2}}}&{{\\lambda_{1}^{3}}}&{{\\cdots}}&{{\\lambda_{1}^{m-1}}}&{{f\\left(\\lambda_{1}\\right)}}\\\\ {{1}}&{{\\lambda_{4}}}&{{\\lambda_{4}^{2}}}&{{\\lambda_{4}^{3}}}&{{\\cdots}}&{{\\lambda_{4}^{m-1}}}&{{f\\left(\\lambda_{4}\\right)}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}&{{\\cdot}}\\\\ {{1}}&{{\\lambda_{m}}}&{{\\lambda_{m}^{2}}}&{{\\lambda_{m}^{3}}}&{{\\cdots}}&{{\\lambda_{m-1}^{m-1}}}&{{f\\left(\\lambda_{m}\\right)}}\\\\ {{{\\bf{I}}}}&{{{\\bf{A}}}}&{{{\\bf{A}}^{2}}}&{{{\\bf{A}}^{3}}}&{{\\cdots}}&{{{\\bf{A}}^{m-1}}}&{{f\\left({\\bf{A}}\\right)}}\\end{array}\\right|={\\bf0}\n$$  \n\nEquation (9–113) shows the desired modification in the form of the determinant. This equation gives the form of Sylvester’s interpolation formula when the minimal polynomial of $\\mathbf{A}$ involves three equal roots. (The necessary modification of the form of the determinant for other cases will be apparent.)  \n\nA–9–14. Using Sylvester’s interpolation formula, compute $e^{\\mathbf{A}t}$ , where  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{2}&{1}&{4}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}\n$$  \n\nSolution. Referring to Problem A–9–9 , the characteristic polynomial and the minimal polynomial are the same for this A .The minimal polynomial (characteristic polynomial) is given by  \n\n$$\n\\phi(\\lambda)\\,=\\,(\\lambda\\,-\\,2)^{2}(\\lambda\\,-\\,1)\n$$  \n\nNote that $\\lambda_{1}=\\,\\lambda_{2}=\\,2$ and $\\lambda_{3}=1$ . Referring to Equation (9–112) and noting that $f(\\mathbf{A})$ in this problem is $e^{\\mathbf{A}t}$ , we have  \n\n$$\ne^{\\mathbf{A}t}\\,=\\,\\alpha_{0}(t)\\,\\mathbf{I}\\,+\\,\\alpha_{1}(t)\\,\\mathbf{A}\\,+\\,\\alpha_{2}(t)\\,\\mathbf{A}^{2}\n$$  \n\nwhere $\\alpha_{0}(t),\\alpha_{1}(t)$ , and $\\alpha_{2}(t)$ are determined from the equations  \n\n$$\n\\begin{array}{r l r}&{}&{\\alpha_{1}(t)\\,+\\,2\\alpha_{2}(t)\\lambda_{1}=\\,t e^{\\lambda_{1}t}}\\\\ &{}&{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{1}+\\,\\alpha_{2}(t)\\lambda_{1}^{2}=\\,e^{\\lambda_{1}t}}\\\\ &{}&{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\lambda_{3}\\,+\\,\\alpha_{2}(t)\\lambda_{3}^{2}=\\,e^{\\lambda_{3}t}}\\end{array}\n$$  \n\nSubstituting $\\lambda_{1}=2$ , and $\\lambda_{3}=1$ into these three equations gives  \n\n$$\n\\begin{array}{c}{{\\alpha_{1}(t)\\,+\\,4\\alpha_{2}(t)\\,=\\,t e^{2\\,t}}}\\\\ {{{}}}\\\\ {{\\alpha_{0}(t)\\,+\\,2\\alpha_{1}(t)\\,+\\,4\\alpha_{2}(t)\\,=\\,e^{2\\,t}}}\\\\ {{{}}}\\\\ {{\\alpha_{0}(t)\\,+\\,\\alpha_{1}(t)\\,+\\,\\alpha_{2}(t)\\,=\\,e^{t}}}\\end{array}\n$$  \n\nSolving for $\\alpha_{0}(t),\\alpha_{1}(t)$ , and $\\alpha_{2}(t)$ , we obtain  \n\n$$\n\\begin{array}{l}{{\\alpha_{0}(t)\\,=\\,4e^{t}\\,-\\,3e^{2t}\\,+\\,2t e^{2t}}}\\\\ {{\\,}}\\\\ {{\\alpha_{1}(t)\\,=\\,-4e^{t}\\,+\\,4e^{2t}\\,-\\,3t e^{2t}}}\\\\ {{\\,}}\\\\ {{\\alpha_{2}(t)\\,=\\,e^{t}\\,-\\,e^{2t}\\,+\\,t e^{2t}}}\\end{array}\n$$  \n\nHence,  \n\n$$\ne^{\\mathbf{A}t}={\\left(4e^{t}-3e^{2t}+2t e^{2t}\\right)}{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]}+{\\left(-4e^{t}+4e^{2t}-3t e^{2t}\\right)}{\\left[\\begin{array}{l l l}{2}&{1}&{4}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}\n$$  \n\n$$\n+\\,\\left(e^{t}\\,-\\,e^{2t}\\,+\\,t e^{2t}\\right)\\left[\\begin{array}{c c c}{{4}}&{{16}}&{{12}}\\\\ {{0}}&{{4}}&{{0}}\\\\ {{0}}&{{9}}&{{1}}\\end{array}\\right]\\,\n$$  \n\n$$\n=\\left[\\begin{array}{c c c c}{{e^{2t}}}&{{12e^{t}-12e^{2t}+13t e^{2t}}}&{{-4e^{t}+4e^{2t}}}\\\\ {{0}}&{{e^{2t}}}&{{0}}\\\\ {{0}}&{{-3e^{t}+3e^{2t}}}&{{e^{t}}}\\end{array}\\right]\n$$  \n\nA–9–15. Show that the system described by  \n\nwhere $\\mathbf{X}=$  \n\nis completely output controllable if and only if the composite $m\\times n r$ matrix $\\mathbf{P}$ , where  \n\n$$\n\\mathbf{P}=\\left[\\mathbf{C}\\mathbf{B}\\mathbf{\\begin{array}{l}{\\vdots}\\\\ {\\mathbf{C}\\mathbf{A}\\mathbf{B}}\\end{array}}\\mid\\mathbf{C}\\mathbf{A}^{2}\\mathbf{B}\\mathbf{\\begin{array}{l}{\\vdots}\\\\ {\\mathbf{\\ddots}}\\end{array}}\\mid\\mathbf{C}\\mathbf{A}^{n-1}\\mathbf{B}\\right]\n$$  \n\nis of rank $_m$ . (Notice that complete state controllability is neither necessary nor sufficient for complete output controllability.)  \n\nSolution. Suppose that the system is output controllable and the output $\\mathbf{y}(t)$ starting from any ${\\bf y}(0)$ ,the initial output, can be transferred to the origin of the output space in a finite time interval $0\\leq t\\leq T.$ .That is,  \n\n$$\n\\mathbf{y}(T)=\\mathbf{C}\\mathbf{x}(T)=\\mathbf{0}\n$$  \n\nSince the solution of Equation (9–114) is  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{\\mathbf{A}t}{\\Biggl[}\\mathbf{x}(0)\\,+\\,\\int_{0}^{t}e^{-\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau{\\Biggr]}\n$$  \n\nat $t=T$ , we have  \n\n$$\n\\mathbf{x}(T)\\,=\\,e^{\\mathbf{A}T}{\\biggl[}\\mathbf{x}(0)\\,+\\,\\int_{0}^{T}\\!e^{-\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau{\\biggr]}\n$$  \n\nSubstituting Equation (9–117) into Equation (9–116), we obtain  \n\n$$\n\\begin{array}{l}{{{\\displaystyle{\\bf y}(T)\\,=\\,{\\bf C}{\\bf x}(T)}}\\ ~}\\\\ {{\\displaystyle~~~~~~=\\,{\\bf C}e^{{\\bf A}T}\\bigg[{\\bf x}(0)\\,+\\,\\int_{0}^{T}\\!e^{-{\\bf A}\\tau}{\\bf B}{\\bf u}(\\tau)\\,d\\tau\\bigg]\\,=\\,{\\bf0}}}\\end{array}\n$$  \n\nOn the other hand, ${\\mathbf y}(0)\\,=\\,{\\mathbf C}{\\mathbf x}(0).$ Notice that the complete output controllability means that the vector $\\mathbf{Cx}(0)$ spans the $_m$ -dimensional output space. Since $e^{\\mathbf{A}T}$ is nonsingular, if $\\mathbf{Cx}(0)$ spans the $m$ -dimensional output space, so does $\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)$ , and vice versa. From Equation (9–118) we obtain  \n\n$$\n\\begin{array}{r l r}&{}&{\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)\\!\\!\\!\\!\\!}&{=}&{\\!\\!\\!\\!\\!-\\mathbf{C}e^{\\mathbf{A}T}\\!\\int_{0}^{T}\\!e^{-\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(\\tau)\\,d\\tau}\\\\ &{}&\\\\ &{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$  \n\nNote that $\\begin{array}{r}{\\int_{0}^{T}e^{\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(T\\mathrm{~-~}\\tau)\\,d\\tau}\\end{array}$ can be expressed as the sum of $\\mathbf{A}^{i}\\mathbf{B}_{j}$ ; that is,  \n\n$$\n\\int_{0}^{T}\\!e^{\\mathbf{A}\\tau}\\mathbf{B}\\mathbf{u}(T\\,-\\,\\tau)\\,d\\tau=\\,\\sum_{i=0}^{p-1}\\,\\sum_{j=1}^{r}\\gamma_{i j}\\,\\mathbf{A}^{i}\\mathbf{B}_{j}\n$$  \n\nwhere  \n\n$$\n\\gamma_{i j}=\\,\\int_{0}^{T}\\!\\alpha_{i}(\\tau)u_{j}(T\\,-\\,\\tau)\\,d\\tau=\\,\\mathrm{scalar}\n$$  \n\nand $\\alpha_{i}(\\tau)$ satisfies  \n\n$$\ne^{\\mathbf{A}\\tau}=\\sum_{i=0}^{p-1}\\alpha_{i}(\\tau)\\mathbf{A}^{i}\\qquad(p;\\deg\\mathrm{{ex}\\in\\ o f\\ t h e\\ m i n i m a l\\ p o l y n o m i a l\\ o f\\ }\\mathbf{A})\n$$  \n\nand $\\mathbf{B}_{j}$ is the $j$ th column of $\\mathbf{B}$ .Therefore, we can write $\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)$ as  \n\n$$\n\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)=-\\sum_{i=0}^{p-1}\\sum_{j=1}^{r}\\gamma_{i j}\\mathbf{C}\\mathbf{A}^{i}\\mathbf{B}_{j}\n$$  \n\nFrom this last equation, we see that $\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)$ is a linear combination of $\\mathbf{CA}^{i}\\mathbf{B}_{j}\\left(i=0,1,2,\\ldots.\\right)$ ,$p\\,-\\,1;\\,j\\,=\\,1,2,\\ldots,r)$ . Note that if the rank of $\\mathbf{Q}$ , where  \n\n$$\n\\mathbf{Q}=\\left[\\mathbf{C}\\mathbf{B}\\mathbf{\\Lambda}\\colon\\mathbf{C}\\mathbf{A}\\mathbf{B}\\mathbf{\\Lambda}\\colon\\mathbf{C}\\mathbf{A}^{2}\\mathbf{B}\\mathbf{\\Lambda}\\colon\\cdots\\colon\\mathbf{C}\\mathbf{A}^{p-1}\\mathbf{B}\\right]\\mathbf{\\Lambda}\\quad\\mathbf{\\Phi}(p\\leq n)\n$$  \n\nis $_m$ , then so is the rank of $\\mathbf{P}$ , and vice versa. [This is obvious if $p\\,=n.$ . If $p<n$ , then the $\\mathbf{CA}^{h}\\mathbf{B}_{j}$ (where $p\\leq h\\leq n\\,-\\,1)$ )are linearly dependent on $\\mathbf{CB}_{j},\\mathbf{CAB}_{j},\\ldots,\\mathbf{CA}^{p\\mathrm{~-~}1}\\mathbf{B}_{j}.$ Hence, the rank of $\\mathbf{P}$ is equal to that of Q.] If the rank of $\\mathbf{P}$ is $_m$ , then $\\mathbf{C}e^{\\mathbf{A}T}\\mathbf{x}(0)$ spans the $_m$ -dimensional output space.This means that if the rank of $\\mathbf{P}$ is $_m$ , then $\\mathbf{Cx}(0)$ also spans the $_m$ -dimensional output space and the system is completely output controllable.  \n\nConversely, suppose that the system is completely output controllable, but the rank of $\\mathbf{P}$ is $k$ ,where $k<m$ . Then the set of all initial outputs that can be transferred to the origin is of $k$ -dimensional space. Hence, the dimension of this set is less than $_m$ . This contradicts the assumption that the system is completely output controllable.This completes the proof.  \n\nNote that it can be immediately proved that, in the system of Equations (9–114) and (9–115), complete state controllability on $0\\le t\\le T$ implies complete output controllability on $0\\le t\\le T$ if and only if $_m$ rows of Care linearly independent.  \n\nA–9–16. Discuss the state controllability of the following system:  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{\\mathord{\\overbrace{-3}}}{-2}}\\-\\ \\ 1\\-\\{\\Biggl[}{\\Biggl[}{\\Biggl[}{x_{1}}{\\Biggr]}+{\\Biggl[}{\\Biggl]}{u}\n$$  \n\nSolution. For this system,  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{-3}&{1}\\\\ {-2}&{1.5}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{1}\\\\ {4}\\end{array}\\right]}\n$$  \n\nSince  \n\n$$\n\\mathbf{AB}={\\left[\\begin{array}{l l}{-3}&{1}\\\\ {-2}&{1.5}\\end{array}\\right]}{\\left[\\begin{array}{l}{1}\\\\ {4}\\end{array}\\right]}={\\left[\\begin{array}{l}{1}\\\\ {4}\\end{array}\\right]}\n$$  \n\nwe see that vectors $\\mathbf{B}$ and AB are not linearly independent and the rank of the matrix $[\\mathbf{B}_{}^{\\mathrm{~\\tiny~}}\\mathbf{A}\\mathbf{B}]$ is 1. Therefore, the system is not completely state controllable. In fact, elimination of $x_{2}$ from Equation (9–119), or the following two simultaneous equations,  \n\n$$\n\\begin{array}{l}{\\dot{x}_{1}=-3x_{1}\\,+\\,x_{2}\\,+\\,u}\\\\ {\\qquad\\dot{x}_{2}=-2x_{1}\\,+\\,1.5x_{2}\\,+\\,4u}\\end{array}\n$$  \n\nyields  \n\n$$\n\\ddot{x}_{1}\\,+\\,1.5\\dot{x}_{1}\\,-\\,2.5x_{1}\\,=\\,\\dot{u}\\,+\\,2.5u\n$$  \n\nor, in the form of a transfer function,  \n\n$$\n{\\frac{X_{1}(s)}{U(s)}}={\\frac{s\\,+\\,2.5}{(s\\,+\\,2.5)(s\\,-\\,1)}}\n$$  \n\nNotice that cancellation of the factor $(s+2.5)$ occurs in the numerator and denominator of the transfer function. Because of this cancellation, this system is not completely state controllable. This is an unstable system. Remember that stability and controllability are quite different things. There are many systems that are unstable, but are completely state controllable.  \n\nA–9–17. A state-space representation of a system in the controllable canonical form is given by  \n\n$$\n\\begin{array}{r}{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}{\\ }}\\\\ {{\\dot{x}}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\;\\,0\\;\\;}&{\\;\\,1}\\\\ {\\;\\,-0.4\\;\\;-1.3{\\sqrt{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}}+{\\biggl[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}u}\\\\ {y={\\left[0.8\\;\\;\\;1\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}}\\end{array}\n$$  \n\nThe same system may be represented by the following state-space equation, which is in the observable canonical form:  \n\n$$\n\\begin{array}{r l}{\\left[\\dot{x}_{1}\\right]=\\left[0\\!\\!\\!\\!}&{{}\\!\\!\\!\\!-0.4\\right]\\!\\!\\left[\\!\\!\\!\\left[x_{1}\\right]\\!\\!\\!\\right]+\\left[\\!\\!\\!\\!\\begin{array}{c}{0.8}\\\\ {1}\\end{array}\\!\\!\\right]u}\\\\ {y=[0\\!\\!\\!\\!}&{{}\\!\\!\\!\\!1]\\!\\!\\left[\\!\\!\\!\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\!\\right]}\\end{array}\n$$  \n\nShow that the state-space representation given by Equations (9–120) and (9–121) gives a system that is state controllable,but not observable.Show,on the other hand,that the state-space representation defined by Equations (9–122) and (9–123) gives a system that is not completely state controllable, but is observable. Explain what causes the apparent difference in the controllability and observability of the same system.  \n\nSolution. Consider the system defined by Equations (9–120) and (9–121). The rank of the controllability matrix  \n\n$$\n[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}]={\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{-1.3\\rfloor}\\end{array}\\right]}\n$$  \n\nis 2. Hence, the system is completely state controllable.The rank of the observability matrix  \n\n$$\n[\\mathbf{C}^{*}\\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}]=\\left[\\begin{array}{l l}{0.8}&{-0.4}\\\\ {1}&{-0.5}\\end{array}\\right]\n$$  \n\nis 1. Hence the system is not observable.  \n\nNext consider the system defined by Equations (9–122) and (9–123). The rank of the controllability matrix  \n\n$$\n[\\mathbf{B}\\vdots\\ \\mathbf{A}\\mathbf{B}]={\\left[\\begin{array}{l l}{0.8}&{-0.4}\\\\ {1}&{-0.5}\\end{array}\\right]}\n$$  \n\nis 1. Hence, the system is not completely state controllable.The rank of the observability matrix  \n\n$$\n[\\mathbf{C}^{*}\\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}]={\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{-1.3{\\underline{{\\,}}}}\\end{array}\\right]}\n$$  \n\nis 2. Hence, the system is observable.  \n\nThe apparent difference in the controllability and observability of the same system is caused by the fact that the original system has a pole-zero cancellation in the transfer function. Referring to Equation (2–29), for $D=0$ we have  \n\n$$\nG(s)\\,=\\,{\\bf C}(s{\\bf I}\\,-\\,{\\bf A})^{-1}{\\bf B}\n$$  \n\nIf we use Equations (9–120) and (9–121), then  \n\n$$\n\\begin{array}{r l}{G(s)=[0.8}&{1]\\!\\!\\left[\\!\\!\\left[{\\begin{array}{l l l}{s}&{-1}&{\\!\\!\\!\\!\\!-1}\\\\ {0.4}&{s+1.3}\\end{array}}\\right]^{\\!\\!\\!-1}\\!\\!\\left[{\\frac{0}{1}}\\right]\\!\\!\\right]}\\\\ {={\\cfrac{1}{s^{2}+1.3s+0.4}}\\left[\\!\\!\\left[\\!\\!\\left[{\\begin{array}{l l l}{s+1.3}&{\\!\\!\\!\\!\\!-1}&{\\!\\!\\!\\!\\!1}\\\\ {-0.4}&{s}\\end{array}}\\right]\\!\\!\\right]\\!\\!\\left[\\!\\!\\!{\\begin{array}{l}{0}\\\\ {1}\\end{array}}\\!\\!\\!\\right]}\\\\ {={\\cfrac{s+\\ 0.8}{(s+0.8)(s+0.5)}}}\\end{array}\n$$  \n\n[Note that the same transfer function can be obtained by using Equations (9–122) and (9–123).] Clearly, cancellation occurs in this transfer function.  \n\nIf a pole-zero cancellation occurs in the transfer function,then the controllability and observability vary, depending on how the state variables are chosen. Remember that, to be completely state controllable and observable, the transfer function must not have any pole-zero cancellations.  \n\nA–9–18. Prove that the system defined by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}}\\\\ {\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector (n -vector )  \n\n$\\mathbf{y}=$ output vecto (m -vector )$(m\\leq n)$   \nA =n\\* nmatrix   \nC=m\\* nmatrix  \n\nis completely observable if and only if the composite $m n\\times n$ matrix $\\mathbf{P}$ , where  \n\n$$\n\\mathbf{P}=\\left[\\begin{array}{c}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}}\\end{array}\\right]\n$$  \n\nis of rank $n$ .  \n\nSolution. We shall first obtain the necessary condition. Suppose that  \n\n$$\n:\\mathbf{P}<n\n$$  \n\nThen there exists ${\\bf x}(0)$ such that  \n\n$$\n\\mathbf{P}\\mathbf{x}(0)\\,=\\,\\mathbf{0}\n$$  \n\nor  \n\n$$\n\\mathbf{P}\\mathbf{x}(0)=\\left[\\begin{array}{c}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}}\\end{array}\\right]\\mathbf{x}(0)=\\left[\\begin{array}{c}{\\mathbf{C}\\mathbf{x}(0)}\\\\ {\\mathbf{C}\\mathbf{A}\\mathbf{x}(0)}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}\\mathbf{x}(0)}\\end{array}\\right]=\\mathbf{0}\n$$  \n\nHence, we obtain, for a certain ${\\bf x}(0)$ ,  \n\n$$\n\\mathbf{CA}^{i}\\mathbf{x}(0)\\,=\\,\\mathbf{0},\\qquad\\mathrm{for}\\,i=0,1,2,\\dots,n\\,-\\,1\n$$  \n\nNotice that from Equation (9–48) or (9–50), we have  \n\n$$\ne^{\\mathbf{A}t}=\\alpha_{0}(t)\\mathbf{I}+\\alpha_{1}(t)\\mathbf{A}+\\alpha_{2}(t)\\mathbf{A}^{2}+\\cdots+\\alpha_{m-1}(t)\\mathbf{A}^{m-1}\n$$  \n\nwhere $m(m\\leq n)$ is the degree of the minimal polynomial for A .Hence,for a certain ${\\bf x}(0)$ ,we have  \n\n$$\n{\\bf C}e^{{\\bf A}t}{\\bf x}(0)={\\bf C}\\big[\\alpha_{0}(t){\\bf I}+\\,\\alpha_{1}(t){\\bf A}\\,+\\,\\alpha_{2}(t){\\bf A}^{2}\\,+\\,\\cdots\\,+\\,\\alpha_{m-1}(t)\\,{\\bf A}^{m-1}\\big]{\\bf x}(0)={\\bf0}\n$$  \n\nConsequently, for a certain ${\\bf x}(0)$ ,  \n\n$$\n\\mathbf{y}(t)\\,=\\,\\mathbf{C}\\mathbf{x}(t)\\,=\\,\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\\,=\\,\\mathbf{0}\n$$  \n\nwhich implies that, for a certain ${\\bf x}(0),{\\bf x}(0)$ cannot be determined from ${\\bf y}(t)$ . Therefore, the rank of matrix $\\mathbf{P}$ must be equal to $n$ .  \n\nNext we shall obtain the sufficient condition. Suppose that rank $\\mathbf{P}=n$ . Since  \n\n$$\n\\mathbf{y}(t)=\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nby premultiplying both sides of this last equation by $e^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}$ , we get  \n\n$$\ne^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}\\mathbf{y}(t)\\,=\\,e^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\n$$  \n\nIf we integrate this last equation from 0 to $t$ , we obtain  \n\n$$\n\\int_{0}^{t}e^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}\\mathbf{y}(t)\\,d t=\\ \\int_{0}^{t}e^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\\,d t\n$$  \n\nNotice that the left-hand side of this equation is a known quantity. Define  \n\n$$\n\\mathbf{Q}(t)\\,=\\,\\int_{0}^{t}\\!e^{\\mathbf{A}^{*}t}\\mathbf{C}^{*}\\mathbf{y}(t)\\,d t={\\mathrm{known~quantity}}\n$$  \n\nThen, from Equations (9–124) and (9–125), we have  \n\n$$\n\\mathbf{Q}(t)=\\mathbf{W}(t)\\mathbf{x}(0)\n$$  \n\nwhere  \n\n$$\n\\mathbf{W}(t)\\,=\\,\\int_{0}^{t}\\!e^{\\mathbf{A}^{*}\\tau}\\mathbf{C}^{*}\\mathbf{C}e^{\\mathbf{A}\\tau}\\,d\\tau\n$$  \n\nIt can be established that $\\mathbf{W}(t)$ is a nonsingular matrix as follows: If $\\left|\\mathbf{W}(t)\\right|$ were equal to 0, then  \n\n$$\n\\mathbf{x}^{*}\\mathbf{W}\\!\\left(t_{1}\\right)\\!\\mathbf{x}\\,=\\,\\int_{0}^{t_{1}}\\!\\|\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}\\|^{2}\\,d t=0\n$$  \n\nwhich means that  \n\n$$\n\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}=\\mathbf{0},\\qquad\\mathrm{for}\\ 0\\leq t\\leq t_{1}\n$$  \n\nwhich implies that rank $\\mathbb{P}<n$ . Therefore, $\\left|\\mathbf{W}(t)\\right|\\neq0$ , or $\\mathbf{W}(t)$ is nonsingular. Then, from Equation (9–126), we obtain  \n\n$$\n\\mathbf{x}(0)\\,=\\,\\left[\\mathbf{W}(t)\\right]^{-1}\\mathbf{Q}(t)\n$$  \n\nand ${\\bf x}(0)$ can be determined from Equation (9–127).  \n\nHence, we have proved that ${\\bf x}(0)$ can be determined from ${\\bf y}(t)$ if and only if rank $\\mathbf{P}=n.$ Note that ${\\bf x}(0)$ and $\\mathbf{y}(t)$ are related by  \n\n$$\n\\mathbf{y}(t)\\,=\\,\\mathbf{C}e^{\\mathbf{A}t}\\mathbf{x}(0)\\,=\\,\\alpha_{0}(t)\\,\\mathbf{C}\\mathbf{x}(0)\\,+\\,\\alpha_{1}(t)\\,\\mathbf{C}\\mathbf{A}\\mathbf{x}(0)\\,+\\,\\cdots+\\,\\alpha_{n-1}(t)\\,\\mathbf{C}\\mathbf{A}^{n-1}\\mathbf{x}(0)\n$$  \n\n# PROBLEMS  \n\nB–9–1. Consider the following transfer-function system:  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{s\\,+\\,6}{s^{2}\\,+\\,5s\\,+\\,6}}\n$$  \n\nObtain the state-space representation of this system in (a) controllable canonical form and (b) observable canonical form.  \n\nB–9–2. Consider the following system:  \n\n$$\n\\dddot{y}\\,+\\,6\\ddot{y}\\,+\\,11\\dot{y}\\,+\\,6y\\,=\\,6u\n$$  \n\nObtain a state-space representation of this system in a diagonal canonical form.  \n\nB–9–3. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\Dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{1}&{2}\\\\ {-4}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{1}\\\\ {2}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{1]\n$$  \n\nTransform the system equations into the controllable canonical form.  \n\nB–9–4. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\Dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{-1}&{\\;\\;0}&{\\;\\;1}\\\\ {\\;\\;1}&{-2}&{\\;\\;0}\\\\ {\\;\\;0}&{\\;\\;0}&{-3}\\end{array}\\right]},\\quad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\quad\\mathbf{C}=[1}&{\\;\\;1\\;\\;\\;0]\n$$  \n\nObtain the transfer function $Y(s)/U(s)$ .  \n\nB–9–5. Consider the following matrix A :  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l l}{0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {1}&{0}&{0}&{0}\\end{array}\\right]}\n$$  \n\nObtain the eigenvalues $\\lambda_{1},\\lambda_{2},\\lambda_{3}$ , and $\\lambda_{4}$ of the matrix A .Then obtain a transformation matrix $\\mathbf{P}$ such that  \n\n$$\n\\mathbf{P}^{\\scriptscriptstyle{-1}}\\mathbf{A}\\mathbf{P}=\\operatorname{diag}\\!\\left(\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}\\right)\n$$  \n\nB–9–6. Consider the following matrix A :  \n\n$$\n{\\bf A}=\\left[\\begin{array}{c r c}{{0}}&{{1}}\\\\ {{-2}}&{{-3\\sum}}\\end{array}\\right]\n$$  \n\nCompute $e^{\\mathbf{A}t}$ by three methods.  \n\nB–9–7. Given the system equation  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{2}&{1}&{0}\\\\ {0}&{2}&{1}\\\\ {0}&{0}&{2}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nfind the solution in terms of the initial conditions $x_{1}(0)$ ,$x_{2}(0)$ , and $x_{3}(0)$ .  \n\nB–9–8. Find $x_{1}(t)$ and $x_{2}(t)$ of the system described by  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {-3}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nwhere the initial conditions are  \n\n$$\n\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {x_{2}(0)\\underline{{\\right]}}=\\left[\\begin{array}{l}{1}\\\\ {-1\\underline{{\\right]}}}\\end{array}\\right]\n$$  \n\nB–9–9. Consider the following state equation and output equation:  \n\n$$\n{\\begin{array}{r l}{\\left[{\\frac{\\dot{x}_{1}}{\\dot{x}_{2}}}\\right]={\\left[\\begin{array}{l l l}{-6}&{1}&{0}\\\\ {-11}&{0}&{1}\\\\ {-6}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{2}\\\\ {6}\\\\ {2}\\end{array}\\right]}u}\\\\ {y={\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nShow that the state equation can be transformed into the following form by use of a proper transformation matrix:  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{z}}_{1}}\\\\ {{\\dot{z}}_{2}}\\\\ {{\\dot{z}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{0}&{-6}\\\\ {1}&{0}&{-11}\\\\ {0}&{1}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\\\ {z_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}u\n$$  \n\nThen obtain the output $y$ in terms of $z_{1},z_{2}$ , and $z_{3}$ .  \n\nB–9–10. Obtain a state-space representation of the following system with MATLAB:  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{10.4s^{2}\\,+\\,47s\\,+\\,160}{s^{3}\\,+\\,14s^{2}\\,+\\,56s\\,+\\,160}\n$$  \n\nB–9–11. Obtain a transfer-function representation of the following system with MATLAB:  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\perp}\\end{array}\\right]=\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {-1}&{-1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{l}{0}\\\\ {1}\\\\ {0}\\end{array}\\right]u}\\\\ &{\\qquad\\qquad y=[0}&{0}&{1]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\end{array}\n$$  \n\nB–9–15. Is the following system completely state controllable and completely observable?  \n\n$$\n{\\begin{array}{r}{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}u}\\\\ {y={\\left[20\\quad9\\quad1\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nB–9–12. Obtain a transfer-function representation of the following system with MATLAB:  \n\nB–9–16. Consider the system defined by  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l l}{2}&{1}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{1}&{3}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ {y=[1}&{0}&{0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\n$$\n{\\begin{array}{r}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}u}\\\\ {y={\\left[c_{1}\\quad c_{2}\\quad c_{3}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nB–9–13. Consider the system defined by  \n\nExcept for an obvious choice of $c_{1}=c_{2}=c_{3}=0$ , find an example of a set of $c_{1},\\,c_{2},$ $c_{3}$ that will make the system unobservable.  \n\nB–9–17. Consider the system  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]=\\left[\\begin{array}{c c c}{-1}&{-2}&{-2}\\\\ {0}&{-1}&{1}\\\\ {1}&{0}&{-1}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{l}{2}\\\\ {0}\\\\ {1}\\end{array}\\right]u}\\\\ &{\\qquad\\qquad y=[1}&{1}&{0]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\end{array}\n$$  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{2}&{0}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}{\\underline{{\\Gamma}}}_{3}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nIs the system completely state controllable and completely observable?  \n\nThe output is given by  \n\nB–9–14. Consider the system given by  \n\n$$\ny=[1\\quad1\\quad1]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\n$$\n{\\begin{array}{r l}&{{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{2}&{0}&{0}\\\\ {0}&{2}&{0}\\\\ {0}&{3}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{u_{1}}\\\\ {u_{2}}\\end{array}\\right]}}\\\\ &{{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nIs the system completely state controllable and completely observable? Is the system completely output controllable?  \n\n(a) Show that the system is not completely observable.  \n\n(b) Show that the system is completely observable if the output is given by  \n\n$$\n{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{1}&{1}&{1}\\\\ {1}&{2}&{3}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\n# Control Systems Design in State Space  \n\n10–1 INTRODUCTION  \n\nThis chapter discusses state-space design methods based on the pole-placement method, observers, the quadratic optimal regulator systems, and introductory aspects of robust control systems.The pole-placement method is somewhat similar to the root-locus method in that we place closed-loop poles at desired locations.The basic difference is that in the root-locus design we place only the dominant closed-loop poles at the desired locations, while in the pole-placement design we place all closed-loop poles at desired locations.  \n\nWe begin by presenting the basic materials on pole placement in regulator systems. We then discuss the design of state observers, followed by the design of regulator systems and control systems using the pole-placement-with-state-observer approach.Then, we discuss the quadratic optimal regulator systems. Finally, we present an introduction to robust control systems.  \n\nOutline of the Chapter. Section 10–1 has presented introductory material.Section 10–2 discusses the pole-placement approach to the design of control systems. We begin with the derivation of the necessary and sufficient conditions for arbitrary pole placement. Then we derive equations for the state feedback gain matrix Kfor pole placement.Section 10–3 presents the solution of the pole-placement problem with MATLAB. Section 10–4 discusses the design of servo systems using the pole-placement approach. Section 10–5 presents state observers.We discuss both full-order and minimum-order state observers. Also, transfer functions of observer controllers are derived. Section 10–6 presents the design of regulator systems with observers. Section 10–7 treats the design of control systems with observers.Section 10–8 discusses quadratic optimal regulator systems.Note that the state feedback gain matrix Kcan be obtained by both the pole-placement method and the quadratic optimal control method. Finally, Section 10–9 presents robust control systems.The discussions here are limited to introductory subjects only.  \n\nIn this section we shall present a design method commonly called the pole-placement or pole-assignment technique. We assume that all state variables are measurable and are available for feedback. It will be shown that if the system considered is completely state controllable, then poles of the closed-loop system may be placed at any desired locations by means of state feedback through an appropriate state feedback gain matrix.  \n\nThe present design technique begins with a determination of the desired closed-loop poles based on the transient-response and/or frequency-response requirements, such as speed, damping ratio, or bandwidth, as well as steady-state requirements.  \n\nLet us assume that we decide that the desired closed-loop poles are to be at $s=\\mu_{1}$ ,$s\\,=\\,\\mu_{2},\\ldots,s\\,=\\,\\mu_{n}.$ .By choosing an appropriate gain matrix for state feedback, it is possible to force the system to have closed-loop poles at the desired locations, provided that the original system is completely state controllable.  \n\nIn this chapter we limit our discussions to single-input, single-output systems. That is, we assume the control signal $u(t)$ and output signal $y(t)$ to be scalars. In the derivation in this section we assume that the reference input $r(t)$ is zero. [In Section 10–7 we discuss the case where the reference input $r(t)$ is nonzero.]  \n\nIn what follows we shall prove that a necessary and sufficient condition that the closed-loop poles can be placed at any arbitrary locations in the $s$ plane is that the system be completely state controllable. Then we shall discuss methods for determining the required state feedback gain matrix.  \n\nIt is noted that when the control signal is a vector quantity, the mathematical aspects of the pole-placement scheme become complicated.We shall not discuss such a case in this book. (When the control signal is a vector quantity, the state feedback gain matrix is not unique. It is possible to choose freely more than $n$ parameters; that is, in addition to being able to place nclosed-loop poles properly, we have the freedom to satisfy some or all of the other requirements, if any, of the closed-loop system.)  \n\nDesign by Pole Placement. In the conventional approach to the design of a singleinput, single-output control system, we design a controller (compensator) such that the dominant closed-loop poles have a desired damping ratio $\\zeta$ and a desired undamped natural frequency $\\omega_{n}$ .In this approach, the order of the system may be raised by 1 or 2 unless pole–zero cancellation takes place. Note that in this approach we assume the effects on the responses of nondominant closed-loop poles to be negligible.  \n\nDifferent from specifying only dominant closed-loop poles (the conventional design approach), the present pole-placement approach specifies all closed-loop poles. (There is a cost associated with placing all closed-loop poles, however, because placing all closedloop poles requires successful measurements of all state variables or else requires the inclusion of a state observer in the system.) There is also a requirement on the part of the system for the closed-loop poles to be placed at arbitrarily chosen locations.The requirement is that the system be completely state controllable.We shall prove this fact in this section.  \n\nConsider a control system  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,D u}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ($n$ -vector ) $y=$ output signal(scalar)$u=$ control signal (scalar )$\\mathbf{A}=n\\times n$ constant matrix $\\mathbf{B}\\,=\\,n\\,\\times\\,1$ constant matrix $\\mathbf{C}=1\\times n$ constant matrix  $D=$ constant(scalar)  \n\nWe shall choose the control signal to be  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nThis means that the control signal $u$ is determined by an instantaneous state. Such a scheme is called state feedback. The $1\\times n$ matrix $\\mathbf{K}$ is called the state feedback gain matrix. We assume that all state variables are available for feedback. In the following analysis we assume that $u$ is unconstrained.A block diagram for this system is shown in Figure 10–1.  \n\nThis closed-loop system has no input. Its objective is to maintain the zero output. Because of the disturbances that may be present, the output will deviate from zero.The nonzero output will be returned to the zero reference input because of the state feedback scheme of the system. Such a system where the reference input is always zero is called a regulator system. (Note that if the reference input to the system is always a nonzero constant, the system is also called a regulator system.)  \n\nSubstituting Equation (10–2) into Equation (10–1) gives  \n\n$$\n\\dot{\\mathbf{x}}(t)\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}(t)\n$$  \n\nThe solution of this equation is given by  \n\n$$\n\\mathbf{x}(t)\\,=\\,e^{(\\mathbf{A-BK})t}\\mathbf{x}(0)\n$$  \n\nwhere ${\\bf x}(0)$ is the initial state caused by external disturbances.The stability and transientresponse characteristics are determined by the eigenvalues of matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ . If matrix $\\mathbf{K}$ is chosen properly, the matrix $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ can be made an asymptotically stable matrix, and for all $\\mathbf{x}(0)\\neq\\mathbf{0}$ , it is possible to make ${\\bf x}(t)$ approach 0 as $t$ approaches infinity.The eigenvalues of matrix $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ are called the regulator poles.If these regulator poles are placed in the left-half $s$ plane, then ${\\bf x}(t)$ approaches 0 as $t$ approaches infinity.The problem of placing the regulator poles (closed-loop poles) at the desired location is called a pole-placement problem.  \n\n![](images/74f5531474d27de89cf01f4e12bd0a004e19e870417204c272985cc871adca56.jpg)  \nFigure 10–1 Closed-loop control system with $u\\,=\\,-\\mathbf{K}\\mathbf{x}$ .  \n\nIn what follows, we shall prove that arbitrary pole placement for a given system is possible if and only if the system is completely state controllable.  \n\nNecessary and Sufficient Condition for Arbitrary Pole Placement We shall now prove that a necessary and sufficient condition for arbitrary pole placement is that the system be completely state controllable.We shall first derive the necessary condition.We begin by proving that if the system is not completely state controllable, then there are eigenvalues of matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ that cannot be controlled by state feedback.  \n\nSuppose that the system of Equation (10–1) is not completely state controllable. Then the rank of the controllability matrix is less than $n$ ,or  \n\n$$\n\\operatorname{rank}[\\mathbf{B}\\mathbf{\\Pi}\\mid\\mathbf{\\DeltaA}\\mathbf{B}\\mathbf{\\Pi}\\mid\\mathbf{\\cdots}\\mid\\mathbf{\\DeltaA}^{n-1}\\mathbf{B}]=q<n\n$$  \n\nThis means that there are $q$ linearly independent column vectors in the controllability matrix. Let us define such $q$ linearly independent column vectors as $\\mathbf{f}_{1},\\mathbf{f}_{2},\\ldots,\\mathbf{f}_{q}$ .Also, let us choose $n\\mathrm{~-~}q$ additional $n$ -vectors $\\mathbf{\\Delta}\\mathbf{v}_{q}+\\mathbf{}1,\\,\\mathbf{v}_{q}+2,\\,\\dots,\\,\\mathbf{v}_{n}$ such that  \n\n$$\n\\mathbf{P}=\\left[\\mathbf{f}_{1}\\ \\ \\vdots\\ \\mathbf{f}_{2}\\ \\ {\\bf:}\\ \\cdots\\ {\\bf:}\\ \\ {\\bf f}_{q}\\ \\ {\\bf:}\\ \\ \\mathbf{v}_{q+1}\\ \\ {\\bf:}\\ \\ \\mathbf{v}_{q+2}\\ \\ {\\bf:}\\ \\ {\\bf\\cdots}\\ \\ {\\bf:}\\ \\ \\mathbf{v}_{n}\\right]\n$$  \n\nis of rank $n$ .Then it can be shown that  \n\n$$\n\\hat{\\mathbf{A}}\\,=\\,\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\,=\\,\\left[\\frac{\\mathbf{A}_{11}}{\\mathbf{0}\\,}\\,\\boxed{\\mathbf{A}_{12}}\\right],\\qquad\\hat{\\mathbf{B}}\\,=\\,\\mathbf{P}^{-1}\\mathbf{B}\\,=\\,\\left[\\frac{\\mathbf{B}_{11}}{\\mathbf{0}}\\right]\n$$  \n\n(See Problem A–10–1 for the derivation of these equations.) Now define  \n\n$$\n\\hat{\\mathbf{K}}=\\mathbf{K}\\mathbf{P}=\\left[\\mathbf{k}_{1}\\;\\vdots\\;\\mathbf{k}_{2}\\right]\n$$  \n\nThen we have  \n\n$$\n\\begin{array}{r l}{s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}]=\\left|\\mathbf{P}^{-1}(s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K})\\mathbf{P}\\right|}\\\\ {=\\left|s\\mathbf{I}-\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}+\\mathbf{P}^{-1}\\mathbf{B}\\mathbf{K}\\right|}\\\\ {=\\left|s\\mathbf{I}-\\hat{\\mathbf{A}}+\\hat{\\mathbf{B}}\\hat{\\mathbf{K}}\\right|}\\\\ {=\\left|s\\mathbf{I}-\\left[\\frac{\\mathbf{A}_{11}}{\\mathbf{\\Lambda}}\\right]\\mathbf{A}_{12}\\right|+\\left[\\frac{\\mathbf{B}_{11}}{\\mathbf{\\Lambda}}\\right]\\!\\!\\left[\\mathbf{k}_{1}\\!\\!\\right]\\!\\!}&{{}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$  \n\nwhere $\\mathbf{I}_{q}$ is a $q$ -dimensional identity matrix and $\\mathbf{I}_{n\\mathrm{~-~}q}$ is an $(n\\mathrm{~-~}q)$ -dimensional identity matrix.  \n\nNotice that the eigenvalues of $\\mathbf{A}_{22}$ do not depend on K. Thus, if the system is not completely state controllable, then there are eigenvalues of matrix A that cannot be arbitrarily placed.Therefore, to place the eigenvalues of matrix A -BK arbitrarily, the system must be completely state controllable (necessary condition).  \n\nNext we shall prove a sufficient condition: that is, if the system is completely state controllable, then all eigenvalues of matrix A can be arbitrarily placed.  \n\nIn proving a sufficient condition, it is convenient to transform the state equation given by Equation (10–1) into the controllable canonical form.  \n\nDefine a transformation matrix $\\mathbf{T}$ by  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}\n$$  \n\nwhere Mis the controllability matrix  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\mathbf{\\begin{array}{l}{\\vdots}\\\\ {\\vdots}\\end{array}}\\mathbf{A}\\mathbf{B}\\mathbf{\\begin{array}{l}{\\vdots}\\end{array}}\\cdots\\mathbf{\\begin{array}{l}{\\vdots}\\end{array}}\\mathbf{A}^{n-1}\\mathbf{B}\\right]\n$$  \n\nand  \n\n$$\n\\mathbf{W}={\\left[\\begin{array}{l l l l l}{a_{n-1}}&{a_{n-2}}&{\\cdots}&{a_{1}}&{1}\\\\ {a_{n-2}}&{a_{n-3}}&{\\cdots}&{1}&{0}\\\\ {\\cdot}&{\\cdot}&{}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{}&{\\cdot}&{\\cdot}\\\\ {a_{1}}&{1}&{\\cdots}&{0}&{0}\\\\ {1}&{0}&{\\cdots}&{0}&{0}\\end{array}\\right]}\n$$  \n\nwhere the $a_{i}$ ’s are coefficients of the characteristic polynomial  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\nDefine a new state vector $\\hat{\\bf x}$ by  \n\n$$\n\\mathbf{x}=\\mathbf{T}\\hat{\\mathbf{x}}\n$$  \n\nIf the rank of the controllability matrix $\\mathbf{M}$ is $n$ (meaning that the system is completely state controllable), then the inverse of matrix $\\mathbf{T}$ exists, and Equation (10–1) can be modified to  \n\n$$\n\\mathbf{\\dot{\\hat{x}}}\\,=\\,\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\hat{x}\\,+\\,\\mathbf{T}^{-1}\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}=\\left[\\begin{array}{c c c c c}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {.}&{.}&{.}&{}&{.}\\\\ {.}&{.}&{.}&{}&{.}\\\\ {.}&{.}&{.}&{}&{.}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]\n$$  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {0}\\\\ {1}\\end{array}\\right]}\n$$  \n\n[See Problems A–10–2 and A–10–3 for the derivation of Equations (10–8) and (10–9).] Equation (10–7) is in the controllable canonical form.Thus,given a state equation,Equation (10–1), it can be transformed into the controllable canonical form if the system is completely state controllable and if we transform the state vector $\\mathbf{X}$ into state vector $\\hat{\\bf x}$ by use of the transformation matrix Tgiven by Equation (10–4).  \n\nLet us choose a set of the desired eigenvalues as $\\mu_{1},\\mu_{2},\\ldots,\\mu_{n}$ .Then the desired characteristic equation becomes  \n\n$$\n(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\cdots(s\\,-\\,\\mu_{n})\\,=\\,s^{n}\\,+\\,\\alpha_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,\\alpha_{n-1}s\\,+\\,\\alpha_{n}=0\n$$  \n\nLet us write  \n\n$$\n\\mathbf{KT}\\,=\\,\\left[\\delta_{n}\\quad\\delta_{n-1}\\quad\\cdots\\quad\\delta_{1}\\right]\n$$  \n\nWhen $u=-\\mathbf{K}\\mathbf{T}\\hat{\\mathbf{x}}$ is used to control the system given by Equation (10–7), the system equation becomes  \n\n$$\n\\dot{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\hat{\\mathbf{x}}\\,-\\,\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{K}\\mathbf{T}\\hat{\\mathbf{x}}\n$$  \n\nThe characteristic equation is  \n\n$$\n\\left|s\\mathbf{I}\\,-\\,\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\,+\\,\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{K}\\mathbf{T}\\right|\\,=\\,0\n$$  \n\nThis characteristic equation is the same as the characteristic equation for the system, defined by Equation (10–1), when $u\\,=-\\mathbf{K}\\mathbf{x}$ is used as the control signal. This can be seen as follows: Since  \n\n$$\n{\\dot{\\mathbf{x}}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\n$$  \n\nthe characteristic equation for this system is  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}\\,+\\,\\mathbf{B}\\mathbf{K}|=\\left|\\mathbf{T}^{-1}(s\\mathbf{I}-\\mathbf{A}+\\,\\mathbf{B}\\mathbf{K})\\mathbf{T}\\right|=|s\\mathbf{I}-\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\mathbf{\\Sigma}+\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{K}\\mathbf{T}|=0\n$$  \n\nNow let us simplify the characteristic equation of the system in the controllable canonical form. Referring to Equations (10–8), (10–9), and (10–11), we have  \n\n$$\n\\left|s\\mathbf{I}\\,-\\,\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\,+\\,\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{K}\\mathbf{T}\\right|\n$$  \n\n$$\n={\\left|\\begin{array}{l}{\\mathbf{0}}&{1}\\\\ {\\mathbf{0}}&{\\mathbf{1}}\\\\ {\\mathbf{0}}&{\\mathbf{0}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\mathbf{0}}&{\\cdot}&{\\mathbf{1}}\\\\ {\\mathbf{0}}&{0}&{\\cdots}&{1}\\\\ {\\mathbf{-}a_{n}}&{\\mathbf{-}a_{n-1}}&{\\cdots}&{\\mathbf{-}a_{1}}\\end{array}\\right|}+{\\left[\\begin{array}{l}{\\mathbf{0}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\delta_{n-1}}\\\\ {\\cdots}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{0}}\\\\ {\\mathbf{1}}\\end{array}\\right]}\n$$  \n\n$$\n\\begin{array}{r l}&{\\quad\\quad-1\\quad\\quad\\cdots\\quad\\quad0}\\\\ &{\\quad=\\quad\\quad\\;\\;s\\quad\\quad\\cdots\\quad\\quad0}\\\\ &{\\quad\\quad\\cdot\\quad\\quad\\cdot\\quad\\quad\\cdot\\quad\\quad\\cdot}\\\\ &{\\quad\\quad\\cdot\\quad\\quad\\cdot\\quad\\quad\\quad\\cdot\\quad\\quad\\cdot}\\\\ &{\\quad\\quad\\cdot\\quad\\quad\\cdot\\quad\\quad\\quad\\cdot\\quad\\quad\\cdot}\\\\ &{\\quad\\quad\\cdot\\quad\\quad\\cdot\\quad\\quad\\quad\\cdot\\quad\\quad\\cdot}\\\\ &{\\quad\\quad\\quad\\quad\\quad a_{n}\\,\\,\\,a_{n-1}+\\,\\delta_{n-1}\\,\\,\\,\\cdots\\,\\,\\,s\\,+\\,a_{1}+\\,\\delta_{1}\\Big\\}}\\\\ &{=\\,s^{n}\\,+\\,\\big(a_{1}+\\,\\delta_{1}\\big)s^{n-1}\\,+\\,\\cdots\\,+\\,\\big(a_{n-1}+\\,\\delta_{n-1}\\big)s\\,+\\,\\big(a_{n}+\\,\\delta_{n}\\big)=0}\\end{array}\n$$  \n\nThis is the characteristic equation for the system with state feedback.Therefore, it must be equal to Equation (10–10), the desired characteristic equation. By equating the coefficients of like powers of $s$ ,we get  \n\n$$\n\\begin{array}{r}{a_{1}+\\,\\delta_{1}=\\,\\alpha_{1}}\\\\ {a_{2}+\\,\\delta_{2}=\\,\\alpha_{2}}\\end{array}\n$$  \n\n$$\na_{n}+\\,\\delta_{n}\\,=\\,\\alpha_{n}\n$$  \n\nSolving the preceding equations for the $\\delta_{i}$ ’s and substituting them into Equation (10–11), we obtain  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{K}={\\left[\\delta_{n}\\ \\delta_{n-1}\\ \\cdots\\ \\delta_{1}\\right]}\\mathbf{T}^{-1}}\\\\ &{\\quad={\\left[\\alpha_{n}\\,-\\,a_{n}\\ \\,{\\begin{array}{l}{\\alpha_{n-1}\\,-\\,a_{n-1}}\\end{array}}\\,{\\begin{array}{l}{!\\,\\cdots\\,{\\textrm{\\cite[]{a k2020202}}}}\\end{array}}\\right]}\\ \\cdots\\ {\\textrm{\\cite[]{a k220220202}}}}\\end{array}}\n$$  \n\nThus, if the system is completely state controllable, all eigenvalues can be arbitrarily placed by choosing matrix $\\mathbf{K}$ according to Equation (10–13) (sufficient condition).  \n\nWe have thus proved that a necessary and sufficient condition for arbitrary pole placement is that the system be completely state controllable.  \n\nIt is noted that if the system is not completely state controllable, but is stabilizable, then it is possible to make the entire system stable by placing the closed-loop poles at desired locations for $q$ controllable modes.The remaining $n\\mathrm{~-~}q$ uncontrollable modes are stable. So the entire system can be made stable.  \n\n# Determination of Matrix K Using Transformation Matrix T. Suppose that the system is defined by  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}u\n$$  \n\nand the control signal is given by  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nThe feedback gain matrix $\\mathbf{K}$ that forces the eigenvalues of $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ to be $\\mu_{1},\\mu_{2},\\ldots,\\mu_{n}$ (desired values) can be determined by the following steps (if $\\mu_{i}$ is a complex eigenvalue, then its conjugate must also be an eigenvalue of ${\\bf A}-{\\bf\\delta B K}$ ):  \n\nStep 1: Check the controllability condition for the system. If the system is completely state controllable, then use the following steps:  \n\nStep 2: From the characteristic polynomial for matrix A , that is,  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\ndetermine the values of $a_{1},a_{2},\\ldots,a_{n}$ .  \n\nStep 3: Determine the transformation matrix $\\mathbf{T}$ that transforms the system state equation into the controllable canonical form. (If the given system equation is already in the controllable canonical form, then $\\mathbf{T}=\\mathbf{I},$ ) It is not necessary to write the state equation in the controllable canonical form. All we need here is to find the matrix T. The transformation matrix $\\mathbf{T}$ is given by Equation (10–4), or  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}\n$$  \n\nwhere $\\mathbf{M}$ is given by Equation (10–5) and $\\mathbf{W}$ is given by Equation (10–6).  \n\nStep 4: Using the desired eigenvalues (desired closed-loop poles), write the desired characteristic polynomial:  \n\n$$\n(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\cdots(s\\,-\\,\\mu_{n})=\\,s^{n}\\,+\\,\\alpha_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,\\alpha_{n-1}s\\,+\\,\\alpha_{n}\n$$  \n\nand determine the values of $\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{n}$ .  \n\nStep 5: The required state feedback gain matrix $\\mathbf{K}$ can be determined from Equation (10–13), rewritten thus:  \n\n$$\n\\mathbf{K}={\\left[\\alpha_{n}\\,-\\,a_{n}\\,\\,\\,\\vdots\\,\\,\\,}\\alpha_{n-1}\\,-\\,a_{n-1}\\,\\,\\,\\vdots\\,\\,\\cdots\\,\\,\\vdots\\,\\,\\,\\alpha_{2}\\,-\\,a_{2}\\,\\,\\,\\vdots\\,\\,\\,\\alpha_{1}\\,-\\,a_{1}\\right]}\\mathbf{T}^{-1}\n$$  \n\nDetermination of Matrix K Using Direct Substitution Method. If the system is of low order $(n\\leq3)$ ), direct substitution of matrix $\\mathbf{K}$ into the desired characteristic polynomial may be simpler. For example, if $n\\,=\\,3$ ,then write the state feedback gain matrix $\\mathbf{K}$ as  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\right]\n$$  \n\nSubstitute this $\\mathbf{K}$ matrix into the desired characteristic polynomial $\\lvert s\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\mathrm{~+~}\\mathbf{B}\\mathbf{K}\\rvert$ and equate it to $(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})(s\\,-\\,\\mu_{3})$ , or  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{B}\\mathbf{K}|\\,=\\,(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})(s\\,-\\,\\mu_{3})\n$$  \n\nSince both sides of this characteristic equation are polynomials in $s$ ,by equating the coefficients of the like powers of $s$ on both sides, it is possible to determine the values of $k_{1},k_{2}$ ,and $k_{3}$ .This approach is convenient if $n\\,=\\,2$ or 3. (For $n=4,5,6,\\ldots,$ , this approach may become very tedious.)  \n\nNote that if the system is not completely controllable,matrix $\\mathbf{K}$ cannot be determined. (No solution exists.)  \n\nDetermination of Matrix K Using Ackermann’s Formula. There is a well-known formula, known as Ackermann’s formula, for the determination of the state feedback gain matrix K.We shall present this formula in what follows.  \n\nConsider the system  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{\\delta}\\mathbf{B}u\n$$  \n\nwhere we use the state feedback control $u\\,=\\,-\\mathbf{K}\\mathbf{x}$ . We assume that the system is completely state controllable.We also assume that the desired closed-loop poles are at $s\\,=\\,\\mu_{1},s\\,=\\,\\mu_{2},...\\,,s\\,=\\,\\mu_{n}$ .  \n\nUse of the state feedback control  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nmodifies the system equation to  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\n$$  \n\nLet us define  \n\n$$\n\\widetilde{\\bf A}={\\bf A}-{\\bf B}{\\bf K}\n$$  \n\nThe desired characteristic equation is  \n\n$$\n{\\begin{array}{r l}&{|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|=|s\\mathbf{I}-{\\widetilde{\\mathbf{A}}}|=(s-\\mu_{1})(s-\\mu_{2})\\cdots(s-\\mu_{n})}\\\\ &{\\qquad\\qquad\\qquad=s^{n}+\\alpha_{1}s^{n-1}+\\cdots+\\alpha_{n-1}s\\,+\\,\\alpha_{n}=0}\\end{array}}\n$$  \n\nSince the Cayley–Hamilton theorem states that $\\widetilde{\\bf A}$ satisfies its own characteristic equation, we have  \n\n$$\n\\phi(\\widetilde{\\mathbf{A}})=\\widetilde{\\mathbf{A}}^{n}+\\,\\alpha_{1}\\,\\widetilde{\\mathbf{A}}^{n-1}+\\dots+\\,\\alpha_{n-1}\\,\\widetilde{\\mathbf{A}}\\,+\\,\\alpha_{n}\\mathbf{I}=\\mathbf{0}\n$$  \n\nWe shall utilize Equation (10–15) to derive Ackermann’s formula. To simplify the derivation, we consider the case where $n\\,=\\,3$ .(For any other positive integer $n$ ,the following derivation can be easily extended.)  \n\nConsider the following identities:  \n\n$$\n\\begin{array}{r l}&{\\mathbf{I}=\\mathbf{I}}\\\\ &{\\widetilde{\\mathbf{A}}=\\mathbf{A}-\\mathbf{B}\\mathbf{K}}\\\\ &{\\widetilde{\\mathbf{A}}^{2}=(\\mathbf{A}-\\mathbf{B}\\mathbf{K})^{2}=\\mathbf{A}^{2}-\\mathbf{A}\\mathbf{B}\\mathbf{K}-\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}}\\\\ &{\\widetilde{\\mathbf{A}}^{3}=(\\mathbf{A}-\\mathbf{B}\\mathbf{K})^{3}=\\mathbf{A}^{3}-\\mathbf{A}^{2}\\mathbf{B}\\mathbf{K}-\\mathbf{A}\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}-\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}^{2}}\\end{array}\n$$  \n\nMultiplying the preceding equations in order by $\\alpha_{3},\\,\\alpha_{2},\\,\\alpha_{1}$ , and $\\alpha_{0}$ (where $\\alpha_{0}=1$ ), respectively, and adding the results, we obtain  \n\n$$\n\\begin{array}{r l}&{\\alpha_{3}\\mathbf{I}+\\alpha_{2}\\widetilde{\\mathbf{A}}\\,+\\,\\alpha_{1}\\widetilde{\\mathbf{A}}^{2}+\\widetilde{\\mathbf{A}}^{3}}\\\\ &{=\\alpha_{3}\\mathbf{I}+\\alpha_{2}(\\mathbf{A}-\\mathbf{B}\\mathbf{K})\\,+\\,\\alpha_{1}\\big(\\mathbf{A}^{2}-\\mathbf{A}\\mathbf{B}\\mathbf{K}-\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}\\big)\\,+\\,\\mathbf{A}^{3}-\\,\\mathbf{A}^{2}\\mathbf{B}\\mathbf{K}}\\\\ &{\\,\\,\\,\\,\\,\\,-\\,\\mathbf{A}\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}\\,-\\,\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}^{2}}\\\\ &{=\\alpha_{3}\\mathbf{I}+\\alpha_{2}\\mathbf{A}+\\,\\alpha_{1}\\mathbf{A}^{2}+\\,\\mathbf{A}^{3}-\\,\\alpha_{2}\\mathbf{B}\\mathbf{K}\\,-\\,\\alpha_{1}\\mathbf{A}\\mathbf{B}\\mathbf{K}-\\,\\alpha_{1}\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}\\,-\\,\\mathbf{A}^{2}\\mathbf{B}\\mathbf{K}}\\\\ &{\\,\\,\\,\\,\\,-\\,\\mathbf{A}\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}\\,-\\,\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{A}}^{2}}\\end{array}\n$$  \n\nReferring to Equation (10–15), we have  \n\n$$\n\\alpha_{3}\\mathbf{I}\\,+\\,\\alpha_{2}\\,\\widetilde{\\mathbf{A}}\\,+\\,\\alpha_{1}\\,\\widetilde{\\mathbf{A}}^{2}\\,+\\,\\widetilde{\\mathbf{A}}^{3}\\,=\\,\\phi\\big(\\widetilde{\\mathbf{A}}\\big)\\,=\\,\\mathbf{0}\n$$  \n\nAlso, we have  \n\n$$\n\\alpha_{3}\\mathbf{I}\\,+\\,\\alpha_{2}\\,\\mathbf{A}\\,+\\,\\alpha_{1}\\,\\mathbf{A}^{2}\\,+\\,\\mathbf{A}^{3}\\,=\\,\\phi(\\mathbf{A})\\,\\neq\\,\\mathbf{0}\n$$  \n\nSubstituting the last two equations into Equation (10–16), we have  \n\n$$\n\\phi(\\widetilde{\\bf A})\\,=\\,\\phi({\\bf A})\\,-\\,\\alpha_{2}\\,{\\bf B}{\\bf K}\\,-\\,\\alpha_{1}\\,{\\bf B}{\\bf K}\\,\\widetilde{\\bf A}\\,-\\,{\\bf B}{\\bf K}\\,\\widetilde{\\bf A}\\,^{2}\\,-\\,\\alpha_{1}\\,{\\bf A}{\\bf B}{\\bf K}\\,-\\,{\\bf A}{\\bf B}{\\bf K}\\,\\widetilde{\\bf A}\\,-\\,{\\bf A}^{2}{\\bf B}{\\bf K}\\,\\widetilde{\\bf A}\\,=\\,{\\bf B}{\\bf K}\\,\\widetilde{\\bf A}\\,.\n$$  \n\nSince $\\phi(\\widetilde{\\mathbf{A}})=\\mathbf{0}$ ,we obtain  \n\n$$\n\\begin{array}{r l}&{\\phi(\\mathbf{A})=\\mathbf{B}\\big(\\alpha_{2}\\mathbf{K}+\\alpha_{1}\\mathbf{K}\\widetilde{\\mathbf{A}}\\,+\\,\\mathbf{K}\\widetilde{\\mathbf{A}}^{2}\\big)+\\mathbf{A}\\mathbf{B}\\big(\\alpha_{1}\\mathbf{K}+\\mathbf{K}\\widetilde{\\mathbf{A}}\\big)+\\mathbf{A}^{2}\\mathbf{B}\\mathbf{K}}\\\\ &{\\qquad=\\big[\\mathbf{B}\\ \\mid\\ \\mathbf{A}\\mathbf{B}\\ \\mid\\ \\mathbf{A}^{2}\\mathbf{B}\\big]\\!\\!\\left[\\begin{array}{c}{\\alpha_{2}\\mathbf{K}+\\alpha_{1}\\mathbf{K}\\widetilde{\\mathbf{A}}\\,+\\,\\mathbf{K}\\widetilde{\\mathbf{A}}^{2}}\\\\ {\\alpha_{1}\\mathbf{K}+\\mathbf{K}\\widetilde{\\mathbf{A}}}\\\\ {\\mathbf{K}}\\end{array}\\right]}\\end{array}\n$$  \n\nSince the system is completely state controllable,the inverse of the controllability matrix  \n\n$$\n\\left[\\mathbf{B}\\right.\\ ;\\ \\mathbf{AB}\\ \\ ;\\ \\mathbf{A^{2}B}\\ ]\n$$  \n\nexists.Premultiplying both sides of Equation (10–17) by the inverse of the controllability matrix, we obtain  \n\n$$\n\\mathbf{\\left[B\\end{array}\\vdots\\begin{array}{c c c}{\\mathbf{AB}}&{\\vdots}&{\\mathbf{A^{2}B}}\\end{array}\\right]^{-1}}\\phi(\\mathbf{A})=\\left[\\begin{array}{c}{\\alpha_{2}\\mathbf{K}+\\alpha_{1}\\mathbf{K\\widetilde{A}}+\\mathbf{K\\widetilde{A}^{2}}}\\\\ {\\alpha_{1}\\mathbf{K}+\\mathbf{K\\widetilde{A}}}\\\\ {\\mathbf{K}}\\end{array}\\right]\n$$  \n\nPremultiplying both sides of this last equation by $\\begin{array}{r}{[0\\quad0\\quad1]}\\end{array}$ we obtain  \n\n$$\n\\begin{array}{r l}{0}&{{}1]\\big[\\mathbf{B}\\ :\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\mathbf{A}^{2}\\mathbf{B}\\big]^{-1}\\phi(\\mathbf{A})=[0\\quad0\\quad1]\\{\\binom{\\alpha_{2}\\mathbf{K}+\\ \\alpha_{1}\\mathbf{K}\\widetilde{\\mathbf{A}}\\ +\\ \\mathbf{K}\\widetilde{\\mathbf{A}}^{2}}{\\alpha_{1}\\mathbf{K}+\\mathbf{K}\\widetilde{\\mathbf{A}}}}=\\mathbf{K}}\\\\ {\\mathbf{K}\\quad}&{{}\\~\\\n$$  \n\nwhich can be rewritten as  \n\n$$\n\\mathbf{K}=[0\\quad0\\quad1]\\big[\\mathbf{B}\\quad;\\ \\mathbf{AB}\\ :\\ \\mathbf{A^{2}B}\\big]^{-1}\\phi(\\mathbf{A})\n$$  \n\nThis last equation gives the required state feedback gain matrix $\\mathbf{K}$ .For an arbitrary positive integer $n$ ,we have  \n\n$$\n\\mathbf{K}=[0\\quad0\\quad\\cdots\\quad0\\quad1][\\mathbf{B}\\quad\\{\\ \\mathbf{A}\\mathbf{B}\\ :\\ \\cdots\\ :\\ \\mathbf{A}^{n-1}\\mathbf{B}\\}^{-1}\\phi(\\mathbf{A})\n$$  \n\n#  \n\nEquation (10–18) is known as Ackermann’s formula for the determination of the state feedback gain matrix K.  \n\nRegulator Systems and Control Systems. Systems that include controllers can be divided into two categories: regulator systems (where the reference input is constant, including zero) and control systems (where the reference input is time varying). In what follows we shall consider regulator systems. Control systems will be treated in Section 10–7.  \n\nChoosing the Locations of Desired Closed-Loop Poles. The first step in the pole-placement design approach is to choose the locations of the desired closed-loop poles.The most frequently used approach is to choose such poles based on experience in the root-locus design,placing a dominant pair of closed-loop poles and choosing other poles so that they are far to the left of the dominant closed-loop poles.  \n\nNote that if we place the dominant closed-loop poles far from the jvaxis, so that the system response becomes very fast, the signals in the system become very large, with the result that the system may become nonlinear.This should be avoided.  \n\nAnother approach is based on the quadratic optimal control approach.This approach will determine the desired closed-loop poles such that it balances between the acceptable response and the amount of control energy required. (See Section 10–8.) Note that requiring a high-speed response implies requiring large amounts of control energy.Also, in general,increasing the speed of response requires a larger,heavier actuator,which will cost more.  \n\nConsider the regulator system shown in Figure 10–2.The plant is given by  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nThe system uses the state feedback control $\\mathbf{u}=-\\mathbf{K}\\mathbf{x}.$ .Let us choose the desired closed-loop poles at  \n\n$$\ns\\,=\\,-2\\,+\\,j4,\\qquad s\\,=\\,-2\\,-\\,j4,\\qquad s\\,=\\,-10\n$$  \n\n(We make such a choice because we know from experience that such a set of closed-loop poles will result in a reasonable or acceptable transient response.) Determine the state feedback gain matrix $\\mathbf{K}$ .  \n\n![](images/10028e4057116f4fd015310d97095066d93bf9e07e87d6455099f38b637ff167.jpg)  \nFigure 10–2 Regulator system.  \n\nFirst, we need to check the controllability matrix of the system. Since the controllability matrix $\\mathbf{M}$ is given by  \n\n$$\n\\mathbf{M}={\\left[\\mathbf{B}\\begin{array}{l}{\\vdots}\\\\ {\\vdots}\\end{array}\\right]}\\ \\mathbf{A}\\mathbf{B}\\ \\mid\\ \\mathbf{A}^{2}\\mathbf{B}{\\right]}={\\left[\\begin{array}{l l l}{0}&{0}&{1}\\\\ {0}&{1}&{-6}\\\\ {1}&{-6}&{31}\\end{array}\\right]}\n$$  \n\nwe find that $|\\mathbf{M}|=-1$ ,and therefore, rank $\\mathbf{M}=3,$ Thus, the system is completely state controllable and arbitrary pole placement is possible.  \n\nNext, we shall solve this problem.We shall demonstrate each of the three methods presented in this chapter.  \n\nMethod 1: The first method is to use Equation (10–13).The characteristic equation for the system is  \n\n$$\n{\\begin{array}{r}{|s\\mathbf{I}-\\mathbf{A}|={\\left|\\begin{array}{l l l}{s}&{-1}&{0}\\\\ {0}&{\\;s}&{-1}\\\\ {1}&{5}&{s+6}\\end{array}\\right|}}\\\\ {=s^{3}+6s^{2}+5s\\;+1}\\\\ {=s^{3}+a_{1}s^{2}+a_{2}s\\;+\\;a_{3}=0}\\end{array}}\n$$  \n\nHence,  \n\n$$\na_{1}=6,\\qquad a_{2}=5,\\qquad a_{3}=1\n$$  \n\nThe desired characteristic equation is  \n\n$$\n\\begin{array}{c}{(s\\mathrm{~+~}2\\mathrm{~-~}j4)(s\\mathrm{~+~}2\\mathrm{~+~}j4)(s\\mathrm{~+~}10)\\mathrm{~}=s^{3}\\mathrm{~+~}14s^{2}\\mathrm{~+~}60s\\mathrm{~+~}200}\\\\ {\\mathrm{~=~}s^{3}\\mathrm{~+~}\\alpha_{1}s^{2}\\mathrm{~+~}\\alpha_{2}s\\mathrm{~+~}\\alpha_{3}=0}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\alpha_{1}=14,\\qquad\\alpha_{2}=60,\\qquad\\alpha_{3}=200\n$$  \n\nReferring to Equation (10–13), we have  \n\n$$\n\\mathbf{K}={\\left[\\alpha_{3}\\,-\\,a_{3}\\,\\,\\,\\right\\{\\,\\,\\alpha_{2}\\,-\\,a_{2}\\,\\,\\,\\vdots\\,\\,\\,\\alpha_{1}\\,-\\,a_{1}\\big\\}}\\mathbf{T}^{-1}\n$$  \n\nwhere $\\mathbf{T}=\\mathbf{I}$ for this problem because the given state equation is in the controllable canonical form. Then we have  \n\n$$\n\\begin{array}{c}{{\\bf K}=[200\\mathrm{~-~}1\\mathrm{~}\\leftrightarrows\\mathrm{~}\\vdots\\mathrm{~}14\\mathrm{~-~}6]}\\\\ {{\\mathrm{~}}}\\\\ {{\\mathrm{~=~}[199\\mathrm{~}\\;55\\mathrm{~}\\;8]}}\\end{array}\n$$  \n\nMethod 2: By defining the desired state feedback gain matrix $\\mathbf{K}$ as  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\right]\n$$  \n\nand equating $\\lvert s\\mathbf{I}\\mathrm{~-~}\\mathbf{A}\\mathrm{~+~}\\mathbf{B}\\mathbf{K}\\rvert$ with the desired characteristic equation, we obtain  \n\n$$\n\\begin{array}{r l}{|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|=\\left|\\left[\\begin{array}{l l l}{s}&{0}&{0}\\\\ {0}&{s}&{0}\\\\ {0}&{0}&{s}\\end{array}\\right]-\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}\\right]+\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right][k_{1}}&{k_{2}}&{k_{3}]}\\right|}\\\\ {=\\left|\\begin{array}{l l l}{s}&{-1}&{0}\\\\ {0}&{s}&{-1}\\\\ {1+k_{1}}&{5+k_{2}}&{s+6+k_{3}}\\end{array}\\right|}\\\\ {=s^{3}+\\left(6+k_{3}\\right)s^{2}+\\left(5+k_{2}\\right)s+1+k_{1}}\\\\ {=s^{3}+14s^{2}+60s+200}\\end{array}\n$$  \n\nThus,  \n\n$$\n6\\,+\\,k_{3}=14,\\qquad5\\,+\\,k_{2}=60,\\qquad1\\,+\\,k_{1}=200\n$$  \n\nfrom which we obtain  \n\n$$\nk_{1}=199,\\qquad k_{2}=55,\\qquad k_{3}=8\n$$  \n\nor  \n\n$$\n\\mathbf{K}=[199\\quad55\\quad8]\n$$  \n\nMethod 3: The third method is to use Ackermann’s formula. Referring to Equation (10–18), we have  \n\n$$\n\\mathbf{K}=[0\\quad0\\quad1]\\big[\\mathbf{B}\\ \\ ;\\ \\ \\mathbf{A}\\mathbf{B}\\ \\ ;\\ \\ \\mathbf{A}^{2}\\mathbf{B}\\big]^{-1}\\phi(\\mathbf{A})\n$$  \n\nSince  \n\n$$\n\\phi(\\mathbf{A})=\\mathbf{A}^{3}\\,+\\,14\\,\\mathbf{A}^{2}\\,+\\,60\\,\\mathbf{A}\\,+\\,200\\,\\mathbf{I}\n$$  \n\n$$\n=\\left[{\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}}\\right]^{3}+14\\left[{\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}}\\right]^{2}\n$$  \n\n$$\n+\\ 60{\\left[\\begin{array}{l l l}{\\ 0}&{\\ 1}&{\\ 0}\\\\ {\\ 0}&{\\ 0}&{\\ 1}\\\\ {\\ -1}&{\\ -5}&{\\ -6}\\end{array}\\right]}+\\ 200{\\left[\\begin{array}{l l l}{\\ 1}&{\\ 0}&{\\ 0}\\\\ {\\ 0}&{1}&{\\ 0}\\\\ {\\ 0}&{0}&{1}\\end{array}\\right]}\n$$  \n\n$$\n={\\left[\\begin{array}{l l l}{199}&{55}&{8}\\\\ {-8}&{159}&{7}\\\\ {-7}&{-43}&{117}\\end{array}\\right]}\n$$  \n\nand  \n\n$$\n[\\mathbf{B}\\;\\;\\{\\;\\mathbf{A}\\mathbf{B}\\;\\;\\{\\;\\mathbf{A}^{2}\\mathbf{B}\\}}={\\left[\\begin{array}{l l l}{0}&{0}&{1}\\\\ {0}&{1}&{-6}\\\\ {1}&{-6}&{31}\\end{array}\\right]}\n$$  \n\nwe obtain  \n\n$$\n{\\begin{array}{r l}{\\mathbf{K}=[0\\quad0\\quad1]{\\left[\\begin{array}{l l l}{0}&{0}&{1}\\\\ {0}&{1}&{-6}\\\\ {1}&{-6}&{31}\\end{array}\\right]}^{-1}{\\left[\\begin{array}{l l l}{199}&{55}&{8}\\\\ {-8}&{159}&{7}\\\\ {-7}&{-43}&{117}\\end{array}\\right]}}\\\\ {=[0\\quad0\\quad1]{\\left[\\begin{array}{l l l}{5}&{6}&{1}\\\\ {6}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{199}&{55}&{8}\\\\ {-8}&{159}&{7}\\\\ {-7}&{-43}&{117}\\end{array}\\right]}}\\\\ {=[199\\quad55}&{8]}\\end{array}}\n$$  \n\nAs a matter of course, the feedback gain matrix Kobtained by the three methods are the same.   \nWith this state feedback, the closed-loop poles are placed at $s=-2\\pm j4$ and $s=-10$ ,as desired.  \n\nIt is noted that if the order $n$ of the system were 4 or higher, methods 1 and 3 are recommended, since all matrix computations can be carried out by a computer. If method 2 is used, hand computations become necessary because a computer may not handle the characteristic equation with unknown parameters $k_{1},k_{2},\\ldots,k_{n}$ .  \n\nComments. It is important to note that matrix $\\mathbf{K}$ is not unique for a given system, but depends on the desired closed-loop pole locations (which determine the speed and damping of the response) selected. Note that the selection of the desired closed-loop poles or the desired characteristic equation is a compromise between the rapidity of the response of the error vector and the sensitivity to disturbances and measurement noises.That is, if we increase the speed of error response, then the adverse effects of disturbances and measurement noises generally increase. If the system is of second order, then the system dynamics (response characteristics) can be precisely correlated to the location of the desired closed-loop poles and the zero(s) of the plant. For higher-order systems, the location of the closed-loop poles and the system dynamics (response characteristics) are not easily correlated. Hence, in determining the state feedback gain matrix Kfor a given system,it is desirable to examine by computer simulations the response characteristics of the system for several different matrices K(based on several different desired characteristic equations) and to choose the one that gives the best overall system performance.  \n\n# 10–3 SOLVING POLE-PLACEMENT PROBLEMS WITH MATLAB  \n\nPole-placement problems can be solved easily with MATLAB. MATLAB has two commands— acker and place —for the computation of feedback-gain matrix K.The command acker is based on Ackermann’s formula.This command applies to single-input systems only.The desired closed-loop poles can include multiple poles (poles located at the same place).  \n\nIf the system involves multiple inputs, for a specified set of closed-loop poles the state-feedback gain matrix Kis not unique and we have an additional freedom (or freedoms) to choose K.There are many approaches to constructively utilize this additional freedom (or freedoms) to determine K. One common use is to maximize the stability margin.The pole placement based on this approach is called the robust pole placement. The MATLAB command for the robust pole placement is place .  \n\nAlthough the command place can be used for both single-input and multiple-input systems, this command requires that the multiplicity of poles in the desired closed-loop poles be no greater than the rank of B. That is, if matrix $\\mathbf{B}$ is an $n\\times1$ matrix, the command place requires that there be no multiple poles in the set of desired closedloop poles.  \n\nFor single-input systems, the commands acker and place yield the same K. (But for multiple-input systems, one must use the command place instead of acker .)  \n\nIt is noted that when the single-input system is barely controllable, some computational problem may occur if the command acker is used. In such a case the use of the place command is preferred, provided that no multiple poles are involved in the desired set of closed-loop poles.  \n\nTo use the command acker or place , we first enter the following matrices in the program:  \n\n$$\n\\mathbf{A}\\;\\mathrm{matrix},\\qquad\\mathbf{B}\\;\\mathrm{matrix},\\qquad\\mathbf{J}\\;\\mathrm{matrix}\n$$  \n\nwhere Jmatrix is the matrix consisting of the desired closed-loop poles such that  \n\n$$\n\\mathbf{J}=\\left[\\mu_{1}\\quad\\mu_{2}\\quad\\ldots\\quad\\mu_{n}\\right]\n$$  \n\nThen we enter  \n\n$$\n\\mathsf{K}=\\mathsf{a c k e r}(\\mathsf{A},\\mathsf{B},\\mathsf{J})\n$$  \n\nor  \n\n$$\n\\mathsf{K}=\\mathsf{p l a c e}(\\mathsf{A},\\mathsf{B},\\mathsf{J})\n$$  \n\nIt is noted that the command eig $(\\mathsf{A}\\!-\\!\\mathsf{B}^{*}\\mathsf{K})$ may be used to verify that Kthus obtained gives the desired eigenvalues.  \n\nEXAMPLE 10–2 Consider the same system as treated in Example 10–1.The system equation is  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nBy using state feedback control $u\\,=\\,-\\mathbf{K}\\mathbf{x}.$ ,it is desired to have the closed-loop poles at $s=\\mu_{i}$ $(i=1,2,3)$ ), where  \n\n$$\n\\mu_{1}=-2\\,+\\,j4,\\qquad\\mu_{2}=-2\\,-\\,j4,\\qquad\\mu_{3}=-10\n$$  \n\nDetermine the state feedback-gain matrix $\\mathbf{K}$ with MATLAB.  \n\nMATLAB programs that generate matrix $\\mathbf{K}$ are shown in MATLAB Programs 10–1 and 10–2. MATLAB Program 10–1 uses command acker and MATLAB Program 10–2 uses command place.  \n\n![](images/eca03fffb2b653586bd866a17340e396b91bbf700c0dd139b9ff20ad9085a075.jpg)  \n\n![](images/21d2e0656d35ad39c247657e88b49c74792d2065d8e4e1da6d033b3f92e574c0.jpg)  \n\nConsider the same system as discussed in Example 10–1. It is desired that this regulator system have closed-loop poles at  \n\n$$\ns\\,=\\,-2\\,+\\,j4,\\qquad s\\,=\\,-2\\,-\\,j4,\\qquad s\\,=\\,-10\n$$  \n\nThe necessary state feedback gain matrix $\\mathbf{K}$ was obtained in Example 10–1 as follows:  \n\n$$\n\\mathbf{K}=[199\\quad55\\quad8]\n$$  \n\nUsing MATLAB, obtain the response of the system to the following initial condition:  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}\n$$  \n\nResponse to Initial Condition: To obtain the response to the given initial condition ${\\bf x}(0)$ , we substitute $u=-\\mathbf{K}\\mathbf{x}$ into the plant equation to get  \n\n$$\n{\\dot{\\mathbf{x}}}=(\\mathbf{A}-\\mathbf{BK})\\mathbf{x},\\qquad\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}\n$$  \n\nTo plot the response curves ( $x_{1}$ versus $t$ ,$x_{2}$ versus $t$ ,and $x_{3}$ versus $t$ ), we may use the command initial .We first define the state-space equations for the system as follows:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{Iu}}\\\\ {\\mathbf{y}\\,=\\,\\mathbf{Ix}\\,+\\,\\mathbf{Iu}}\\end{array}\n$$  \n\nwhere we included $\\mathbf{u}$ (a three-dimensional input vector). This $\\mathbf{u}$ vector is considered 0 in the computation of the response to the initial condition.Then we define  \n\n$$\n{\\mathsf{S y s}}={\\mathsf{s s}}({\\mathsf{A}}-{\\mathsf{B K}},\\,{\\mathsf{e y e}}(3),\\,{\\mathsf{e y e}}(3),\\,{\\mathsf{e y e}}(3))\n$$  \n\nand use the initial command as follows:  \n\n$$\n\\mathbf{\\Sigma}_{\\mathbf{X}}=\\operatorname*{initial}(\\operatorname{sys},\\;[1\\,;\\!0;\\!0],\\!0)\n$$  \n\nwhere tis the time duration we want to use, such as  \n\n$$\n\\mathrm{\\Deltat=0{:}0.01{:}4;}\n$$  \n\nThen obtain $\\times1\\,,\\,\\times2\\,,$ ,and $\\times3$ as follows:  \n\n$$\n\\begin{array}{r}{{\\boldsymbol{\\times}}{\\boldsymbol{1}}=[1\\ \\ 0\\ \\ 0]^{*}{\\boldsymbol{\\times}}^{\\prime};}\\\\ {{\\boldsymbol{\\times}}{\\boldsymbol{2}}=[0\\ \\ 1\\ \\ 0]^{*}{\\boldsymbol{\\times}}^{\\prime};}\\\\ {{\\boldsymbol{\\times}}{\\boldsymbol{3}}=[0\\ \\ 0\\ \\ 1]^{*}{\\boldsymbol{\\times}}^{\\prime};}\\end{array}\n$$  \n\nand use the plot command. This program is shown in MATLAB Program 10–3. The resulting response curves are shown in Figure 10–3.  \n\n![](images/82a8af7b80e6363cd2ab57ff5309780a8f109addee32a3a7603a559f3cbc5a8a.jpg)  \n\n![](images/8f5f380a5329e6aeee037446d4407fdd6999054037a8ee87d374acc1e338566b.jpg)  \n\nIn this section we shall discuss the pole-placement approach to the design of type 1 servo systems. Here we shall limit our systems each to have a scalar control signal $u$ and a scalar output y.  \n\nIn what follows we shall first discuss a problem of designing a type 1 servo system when the plant involves an integrator.Then we shall discuss the design of a type 1 servo system when the plant has no integrator.  \n\nDesign of Type 1 Servo System when the Plant Has An Integrator. Assume that the plant is defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector for the plant ($n$ -vector )  \n\n$$\n\\begin{array}{r l}&{u=\\mathrm{control~signal~(scalar)}}\\\\ &{y=\\mathrm{output~signal~(scalar)}}\\\\ &{\\mathbf{A}=n\\times n\\,\\mathrm{constant~matrix}}\\\\ &{\\mathbf{B}=n\\times1\\,\\mathrm{constant~matrix}}\\\\ &{\\mathbf{C}=1\\times n\\,\\mathrm{constant~matrix}}\\end{array}\n$$  \n\nAs stated earlier, we assume that both the control signal scalars. By a proper choice of a set of state variables, it is possible to choose the output A $u$ and the output signal $y$ are obtaining a state-space representation of the transfer function system in which the output o be equal to one the state variables. BSee the method presented in Chapter 2 for $y$ becomes equal to $x_{1}$ .  \n\nFigure 10–4 shows a general configuration of the type 1 servo system when the plant has an integrator. Here we assumed that $y\\,=\\,x_{1}$ .In the present analysis we assume that  \n\n![](images/23b8ef3b8320cdab9435a974ba2d4ca710c31b3c48f283b7ed81b5f1c2e9394e.jpg)  \nFigure 10–4 Type 1 servo system when the plant has an integrator.  \n\nthe reference input $r$ is a step function.In this system we use the following state-feedback control scheme:  \n\n$$\nu=-{\\left[\\begin{array}{l l l l l}{0}&{k_{2}}&{k_{3}}&{\\cdots}&{k_{n}}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {x_{n}}\\end{array}\\right]}+k_{1}(r\\,-\\,x_{1})\n$$  \n\n$$\n{\\bf\\alpha}=-{\\bf K}{\\bf x}\\,+\\,k_{1}r\n$$  \n\nwhere  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\ \\cdots\\ k_{n}\\right]\n$$  \n\nAssume that the reference input (step function) is applied at $t\\,=\\,0$ .Then, for $t>0$ ,the system dynamics can be described by Equations (10–19) and (10–21), or  \n\n$$\n{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u=(\\mathbf{A}-\\mathbf{B}\\mathbf{K})\\mathbf{x}+\\mathbf{B}k_{1}r\n$$  \n\nWe shall design the type 1 servo system such that the closed-loop poles are located at desired positions. The designed system will be an asymptotically stable system, $y(\\infty)$ will approach the constant value $r$ , and $u(\\infty)$ will approach zero. ( $r$ is a step input.)  \n\nNotice that at steady state we have  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}(\\infty)\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}(\\infty)\\,+\\,\\mathbf{B}k_{1}r(\\infty)}\\end{array}\n$$  \n\nNoting that $r(t)$ is a step input, we have $r(\\infty)\\,=\\,r(t)\\,=\\,r(\\mathrm{constant})$ ) for $t\\,>\\,0$ .By subtracting Equation (10–23) from Equation (10–22), we obtain  \n\n$$\n{\\dot{\\mathbf{x}}}(t)\\,-\\,{\\dot{\\mathbf{x}}}(\\infty)=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\big[\\mathbf{x}(t)\\,-\\,\\mathbf{x}(\\infty)\\big]\n$$  \n\nDefine  \n\n$$\n\\mathbf{x}(t)\\,-\\,\\mathbf{x}(\\infty)\\,=\\,\\mathbf{e}(t)\n$$  \n\nThen Equation (10–24) becomes  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{e}\n$$  \n\nEquation (10–25) describes the error dynamics.  \n\nThe design of the type 1 servo system here is converted to the design of an asymptotically stable regulator system such that $\\mathbf{e}(t)$ approaches zero, given any initial condition ${\\bf e}(0)$ .If the system defined by Equation (10–19) is completely state controllable, then, by specifying the desired eigenvalues $\\mu_{1},\\mu_{2},\\dots,\\mu_{n}$ for the matrix A -BK , matrix $\\mathbf{K}$ can be determined by the pole-placement technique presented in Section 10–2.  \n\nThe steady-state values of ${\\bf x}(t)$ and $u(t)$ can be found as follows: At steady state ($\\mathbf{\\chi}_{t}=\\infty,$ ), we have, from Equation (10–22),  \n\n$$\n\\dot{\\mathbf{x}}(\\infty)\\,=\\,\\mathbf{0}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}(\\infty)\\,+\\,\\mathbf{B}k_{1}r\n$$  \n\nSince the desired eigenvalues of $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{B}\\mathbf{K}$ are all in the left-half $s$ plane, the inverse of matrix A -BK exists. Consequently, $\\mathbf{x}(\\infty)$ can be determined as  \n\n$$\n\\mathbf{x}(\\infty)\\,=-(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})^{-1}\\mathbf{B}k_{1}r\n$$  \n\nAlso, $u(\\infty)$ can be obtained as  \n\n$$\nu(\\infty)\\,=-\\mathbf{K}\\mathbf{x}(\\infty)\\,+\\,k_{1}r\\,=\\,0\n$$  \n\n(See Example 10–4 to verify this last equation.)  \n\n# EXAMPLE 10–4  \n\nDesign a type 1 servo system when the plant transfer function has an integrator.Assume that the plant transfer function is given by  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{1}{s(s\\,+\\,1)(s\\,+\\,2)}}\n$$  \n\nThe desired closed-loop poles are $s\\,=\\,-2\\,\\pm\\,j2\\sqrt{3}$ and $s=-10$ .Assume that the system configuration is the same as that shown in Figure 10–4 and the reference input $r$ is a step function. Obtain the unit-step response of the designed system.  \n\nDefine state variables $x_{1},x_{2}$ ,and $x_{3}$ as follows:  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {\\qquad x_{2}=\\dot{x}_{1}}\\\\ {\\qquad x_{3}=\\dot{x}_{2}}\\end{array}\n$$  \n\nThen the state-space representation of the system becomes  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{-2}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{0}&{0]\n$$  \n\nReferring to Figure 10–4 and noting that $n\\,=\\,3$ ,the control signal $u$ is given by  \n\n$$\nu\\,=-\\big(k_{2}\\,x_{2}\\,+\\,k_{3}\\,x_{3}\\big)\\,+\\,k_{1}\\big(r\\,-\\,x_{1}\\big)\\,=-\\mathbf{K}\\mathbf{x}\\,+\\,k_{1}r\n$$  \n\nwhere  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\right]\n$$  \n\nThe state-feedback gain matrix $\\mathbf{K}$ can be obtained easily with MATLAB. See MATLAB Program 10–4.  \n\n![](images/172482dbfe0838cdd1684649f05d4b5bbaf406e9491aedd80269edfbcabd6c21.jpg)  \n\nThe state feedback gain matrix $\\mathbf{K}$ is thus  \n\n$$\n\\mathsf{K}=[1\\,60\\;\\;54\\;\\;11]\n$$  \n\nUnit-Step Response of the Designed System: The unit-step response of the designed system can be obtained as follows:  \n\nSince  \n\n$$\n\\mathbf{A}\\-\\mathbf{B}\\mathbf{K}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{-2}&{-3}\\end{array}\\right]}-{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}[160}&{54}&{11]={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-160}&{-56}&{-14}\\end{array}\\right]}\n$$  \n\nfrom Equation (10–22) the state equation for the designed system is  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\;\\;0}&{\\;\\;1}&{\\;\\;0}\\\\ {\\;\\;0}&{\\;\\;0}&{\\;\\;1}\\\\ {-160}&{-56}&{-14}\\end{array}\\right]}{\\left[\\begin{array}{l}{{x}_{1}}\\\\ {{x}_{2}}\\\\ {{x}_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\;\\;0}\\\\ {\\;\\;0}\\\\ {\\;\\;160}\\end{array}\\right]}r\n$$  \n\nand the output equation is  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nSolving Equations (10–29) and (10–30) for $y(t)$ when $r$ is a unit-step function gives the unit-step response curve $y(t)$ versus $t$ .MATLAB Program 10–5 yields the unit-step response. The resulting unit-step response curve is shown in Figure 10–5.  \n\n# MATLAB Program 10–5  \n\n![](images/7afc02bcd5c98e727e931632babb00a7e0eb78233fe11970c6a9e96eac9c38af.jpg)  \n\n![](images/4c349cf17ab697566e2a882c20b5a370a4d64182cb255776aa60042b3ad52ec8.jpg)  \nFigure 10–5 Unit-step response curve $y(t)$ versus $t$ for the system designed in Example 10–4.  \n\nNote that since  \n\n$$\nu(\\infty)=-\\mathbf{K}\\mathbf{x}(\\infty)\\,+\\,k_{1}r(\\infty)\\,=-\\mathbf{K}\\mathbf{x}(\\infty)\\,+\\,k_{1}r\n$$  \n\nwe have  \n\n$$\n{\\begin{array}{r l}{u(\\infty)=-[160}&{54}&{11]{\\left[\\begin{array}{l}{x_{1}(\\infty)}\\\\ {x_{2}(\\infty)}\\\\ {x_{3}(\\infty)}\\end{array}\\right]}+160r}\\\\ {=-[160}&{54}&{11]{\\left[\\begin{array}{l}{r}\\\\ {0}\\\\ {0}\\end{array}\\right]}+160r=0}\\end{array}}\n$$  \n\nAt steady state the control signal $u$ becomes zero.  \n\nDesign of Type 1 Servo System when the Plant Has No Integrator. If the plant has no integrator (type 0 plant), the basic principle of the design of a type 1 servo system is to insert an integrator in the feedforward path between the error comparator and the plant, as shown in Figure 10–6. (The block diagram of Figure 10–6 is a basic form of the type 1 servo system where the plant has no integrator.) From the diagram, we obtain  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {y=\\mathbf{C}\\mathbf{x}}\\\\ {u=-\\mathbf{K}\\mathbf{x}+k_{I}\\boldsymbol{\\xi}}\\\\ {\\dot{\\boldsymbol{\\xi}}=r-y=r-\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector of the plant ($n$ -vector )  \n\n![](images/ddc7128a1ce3e761d93824288b3d86a7d1c4c4d135ed4f9451ab23c30bb38cac.jpg)  \nFigure 10–6 Type 1 servo system.  \n\n$u=$ control signal (scalar )  \n $y=$ output signal(scalar)  \n$\\xi=$ output of the integrator (state variable of the system, scalar )  \n$r=$ reference input signal (step function, scalar )  \n$\\mathbf{A}=n\\times n$ constant matrix   \n$\\mathbf{B}\\,=\\,n\\,\\times\\,1$ constant matrix   \n$\\mathbf{C}=1\\times n$ constant matrix  \n\nWe assume that the plant given by Equation (10–31) is completely state controllable.The transfer function of the plant can be given by  \n\n$$\nG_{p}(s)\\,=\\,\\mathbf{C}(s\\mathbf{I}\\,-\\,\\mathbf{A})^{-1}\\mathbf{B}\n$$  \n\nTo avoid the possibility of the inserted integrator being canceled by the zero at the origin of the plant, we assume that $G_{p}(s)$ has no zero at the origin.  \n\nAssume that the reference input (step function) is applied at $t\\,=\\,0.$ Then, for $t>0$ ,the system dynamics can be described by an equation that is a combination of Equations (10–31) and (10–34):  \n\n$$\n{\\overset{\\lvert}{\\dot{\\xi}}}(t){\\overset{\\biggr]}{=}}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\mathbf{x}(t)}\\\\ {\\xi(t)}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {\\mathbf{0}}\\end{array}\\right]}u(t)+{\\left[\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{1}}\\end{array}\\right]}r(t)\n$$  \n\nWe shall design an asymptotically stable system such that #${\\bf x}(\\infty),\\boldsymbol\\xi(\\infty)$ ,and $u(\\infty)$ approach constant values, respectively.Then, at steady state, $\\dot{\\xi}(t)\\,=\\,0$ and we get $y(\\infty)\\,=\\,r$ .Notice that at steady state we have  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{\\mathbf{x}}(\\infty)}\\\\ {\\dot{\\xi}(\\infty)}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\mathbf{x}(\\infty)}\\\\ {\\xi(\\infty)}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]}u(\\infty)+{\\left[\\begin{array}{l}{\\mathbf{0}}\\\\ {1}\\end{array}\\right]}r(\\infty)\n$$  \n\nNoting that $r(t)$ is a step input, we have $r(\\infty)\\,=\\,r(t)\\,=\\,r$ (constant) for $t\\,>\\,0$ .By subtracting Equation (10–36) from Equation (10–35), we obtain  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{\\mathbf{x}}}(t)\\,-{\\dot{\\mathbf{x}}}(\\infty)\\right]={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\mathbf{x}(t)\\,-\\,\\mathbf{x}(\\infty)}\\\\ {\\xi(t)\\,-\\,\\xi(\\infty)}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]}{\\bigl[}u(t)\\,-\\,u(\\infty){\\bigr]}}\\end{array}}\n$$  \n\nDefine  \n\n$$\n\\begin{array}{r}{\\mathbf{x}(t)\\,-\\,\\mathbf{x}(\\infty)\\,=\\,\\mathbf{x}_{e}(t)}\\\\ {\\xi(t)\\,-\\,\\xi(\\infty)=\\,\\xi_{e}(t)}\\\\ {u(t)\\,-\\,u(\\infty)\\,=\\,u_{e}(t)}\\end{array}\n$$  \n\nThen Equation (10–37) can be written as  \n\n$$\n\\begin{array}{r}{\\left[\\dot{\\mathbf{x}}_{e}(t)\\right]=\\left[\\begin{array}{c c}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]\\!\\!\\left[\\mathbf{\\bar{\\alpha}}_{e}(t)\\right]+\\left[\\begin{array}{c}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]\\!\\!u_{e}(t)}\\end{array}\n$$  \n\nwhere  \n\n$$\nu_{e}(t)\\,=\\,-{\\bf K}{\\bf x}_{e}(t)\\,+\\,k_{I}\\xi_{e}(t)\n$$  \n\nDefine a new $(n+1)$ th-order error vector ${\\bf e}(t)$ by  \n\n$$\n\\mathbf{e}(t)\\,=\\,\\left[\\begin{array}{l}{\\mathbf{x}_{e}(t)}\\\\ {\\xi_{e}(t)\\smallskip}\\end{array}\\right]\\,=\\,(n\\,+\\,1)\\mathrm{-vector}\n$$  \n\nThen Equation (10–38) becomes  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\hat{\\mathbf{A}}\\mathbf{e}\\,+\\,\\hat{\\mathbf{B}}u_{e}\n$$  \n\nwhere  \n\n$$\n{\\hat{\\mathbf{A}}}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right]},\\qquad{\\hat{\\mathbf{B}}}={\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]}\n$$  \n\nand Equation (10–39) becomes  \n\n$$\nu_{e}=-\\hat{\\bf K}{\\bf e}\n$$  \n\nwhere  \n\n$$\n\\hat{\\mathbf{K}}=\\left[\\mathbf{K}\\right.\\left.\\mathrm{i}-k_{I}\\right]\n$$  \n\nThe state error equation can be obtained by substituting Equation (10–41) into Equation (10–40):  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,(\\hat{\\mathbf{A}}\\,-\\,\\hat{\\mathbf{B}}\\hat{\\mathbf{K}})\\mathbf{e}\n$$  \n\nIf the desired eigenvalues of matrix $\\hat{\\mathbf A}-\\hat{\\mathbf B}\\hat{\\mathbf K}$ (that is, the desired closed-loop poles) are specified as $\\mu_{1},\\mu_{2},\\dots,\\mu_{n+1}$ ,then the state-feedback gain matrix $\\mathbf{K}$ and the integral gain constant $k_{I}$ can be determined by the pole-placement technique presented in Section 10–2, provided that the system defined by Equation (10–40) is completely state controllable. Note that if the matrix  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]\n$$  \n\nhas rank $n\\,+\\,1$ ,then the system defined by Equation (10–40) is completely state controllable. (See Problem A–10–12 .)  \n\n![](images/4180559f333a7c34ce535b4d46847f6b89f65d746b0be68ca2125e331f0e2fb9.jpg)  \n\nFigure 10–7 Type 1 servo system with state observer.  \n\n# EXAMPLE 10–5  \n\nAs is usually the case, not all state variables can be directly measurable. If this is the case, we need to use a state observer. Figure 10–7 shows a block diagram of a type 1 servo system with a state observer. [In the figure, each block with an integral symbol represents an integrator $(1/s)$ .] Detailed discussions of state observers are given in Section 10–5.  \n\nConsider the inverted-pendulum control system shown in Figure 10–8. In this example, we are concerned only with the motion of the pendulum and motion of the cart in the plane of the page.  \n\nIt is desired to keep the inverted pendulum upright as much as possible and yet control the position of the cart—for instance, move the cart in a step fashion. To control the position of the cart, we need to build a type 1 servo system. The inverted-pendulum system mounted on a cart does not have an integrator.Therefore, we feed the position signal $y$ (which indicates the position of the cart) back to the input and insert an integrator in the feedforward path, as shown  \n\nFigure 10–8 Inverted-pendulum control system.  \n\n![](images/9974c3728ca1f1bd27dae069fb617e69223c10b8e0abecbb6924c659bc30fb2a.jpg)  \n\n![](images/7c0368d63af1297bd38fb4a59c2712f7767201d7617542c65b08c8fc15c88e1b.jpg)  \nFigure 10–9 Inverted-pendulum control system. (Type 1 servo system when the plant has no integrator.)  \n\n$\\sin\\theta\\div\\theta$ in Figure 10–9.We assume that the pendulum angle ,$\\cos\\theta\\doteq1$ ,and $\\dot{\\theta{\\dot{\\theta}}}^{2}\\div0$ #We also assume that the numerical values for $\\theta$ and the angular velocity are small, so that $M$ ,$_m$ ,and $l$ are given as  \n\n$$\nM\\,=\\,2\\,\\mathrm{kg},\\qquad m\\,=\\,0.1\\,\\mathrm{kg},\\qquad l\\,=\\,0.5\\,\\mathrm{m}\n$$  \n\nEarlier in Example 3–6 we derived the equations for the inverted-pendulum system shown in Figure 3–6, which is the same as that in Figure 10–8. Referring to Figure 3–6, we started with the force-balance and torque-balance equations and ended up with Equations (3–20) and (3–21) to model the inverted-pendulum system. Referring to Equations (3–20) and (3–21), the equations for the inverted-pendulum control system shown in Figure 10–8 are  \n\n$$\n\\begin{array}{l}{{M{\\ddot{\\theta}}=(M+m)g\\theta-u}}\\\\ {{{M\\ddot{x}}=u-m g\\theta}}\\end{array}\n$$  \n\nWhen the given numerical values are substituted, Equations (10–43) and (10–44) become  \n\n$$\n\\begin{array}{l}{\\ddot{\\theta}\\,=\\,20.601\\theta\\,-\\,u}\\\\ {\\ddot{x}\\,=\\,0.5u\\,-\\,0.4905\\theta}\\end{array}\n$$  \n\nLet us define the state variables $x_{1},x_{2},x_{3}$ ,and $x_{4}$ as  \n\n$$\n\\begin{array}{l}{x_{1}=\\theta}\\\\ {\\;x_{2}=\\dot{\\theta}}\\\\ {\\;x_{3}=x}\\\\ {\\;x_{4}=\\dot{x}}\\end{array}\n$$  \n\nThen,referring to Equations (10–45) and (10–46) and Figure 10–9 and considering the cart position $x$ as the output of the system, we obtain the equations for the system as follows:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {y=\\mathbf{C}\\mathbf{x}}\\\\ {u=-\\mathbf{K}\\mathbf{x}+k_{I}\\boldsymbol{\\xi}}\\\\ {\\dot{\\boldsymbol{\\xi}}=r-y=r-\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l l}{\\qquad0}&{1}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}\\\\ {\\qquad0}&{0}&{0}&{1}\\\\ {-0.4905}&{0}&{0}&{0}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{\\qquad0}\\\\ {-1}\\\\ {\\qquad0}\\\\ {0.5}\\end{array}\\right]},\\qquad\\mathbf{C}=[0}&{0}&{1}&{0]\n$$  \n\nFor the type 1 servo system, we have the state error equation as given by Equation (10–40):  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\hat{\\mathbf{A}}\\mathbf{e}\\,+\\,\\hat{\\mathbf{B}}u_{e}\n$$  \n\nwhere  \n\n$$\n{\\hat{\\mathbf{A}}}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {-\\mathbf{C}}&{0}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{0}\\\\ {-0.4905}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{-1}&{0}&{0}\\end{array}\\right]},\\qquad{\\hat{\\mathbf{B}}}={\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {\\mathbf{B}}\\\\ {0}\\\\ {0}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {-1}\\\\ {0}\\\\ {0.5}\\\\ {0}\\end{array}\\right]}\n$$  \n\nand the control signal is given by Equation (10–41):  \n\n$$\nu_{e}=-\\hat{\\bf K}{\\bf e}\n$$  \n\nwhere  \n\n$$\n\\hat{\\mathbf{K}}=\\left[\\mathbf{K}\\,\\,\\vdots\\,\\,-k_{I}\\right]=\\left[k_{1}\\,\\,\\,\\,\\,k_{2}\\,\\,\\,\\,\\,k_{3}\\,\\,\\,\\,\\,k_{4}\\,\\,\\,\\vdots\\,\\,\\,-k_{I}\\right]\n$$  \n\nexample, the settling time of approximately To obtain a reasonable speed and damping in the response of the designed system (for 1 $4\\sim5$ 1 sec and the maximum overshoot of $15\\%\\sim16\\%$ in the step response of the cart), let us choose the desired closed-loop poles at $s=\\mu_{i}$ $(i=1,2,3,4,5)$ ), where  \n\n$$\n\\mu_{1}=-1\\,+\\,j\\sqrt{3}\\,,\\qquad\\mu_{2}=-1\\,-\\,j\\sqrt{3}\\,,\\qquad\\mu_{3}=-5,\\qquad\\mu_{4}=-5,\\qquad\\mu_{5}=-5\n$$  \n\nWe shall determine the necessary state-feedback gain matrix by the use of MATLAB. Before we proceed further, we must examine the rank of matrix $\\mathbf{P}$ , where  \n\n$$\n\\mathbf{P}=\\left[\\begin{array}{c c}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]\n$$  \n\nMatrix $\\mathbf{P}$ is given by  \n\n$$\n\\mathbf{P}=\\left[\\mathbf{\\begin{array}{c c c c c c}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {-\\mathbf{C}}&{0}\\end{array}}\\right]=\\left[{\\begin{array}{c c c c c c}{0}&{1}&{0}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}&{-1}\\\\ {0}&{0}&{0}&{1}&{0}\\\\ {-0.4905}&{0}&{0}&{0}&{0.5}\\\\ {0}&{0}&{-1}&{0}&{0}\\end{array}}\\right]\n$$  \n\nThe rank of this matrix can be found to be 5.Therefore, the system defined by Equation (10–51) is completely state controllable, and arbitrary pole placement is possible. MATLAB Program 10–6 produces the state feedback gain matrix $\\mathbf{\\ddot{K}}$ .  \n\n![](images/44f09d9efa38ad6ba8986a03f50971f988537d2d99ae1ee1711e30d6f433cdfd.jpg)  \n\nThus, we get  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\quad k_{4}\\right]=\\left[-157.6336\\quad-35.3733\\quad-56.0652\\quad-36.7466\\right]\n$$  \n\nand  \n\n$$\nk_{I}=-50.9684\n$$  \n\nUnit Step-Response Characteristics of the Designed System. Once we determine the feedback gain matrix $\\mathbf{K}$ and the integral gain constant $k_{I}$ ,the step response in the cart position can be obtained by solving the following equation, which is obtained by substituting Equation (10–49) into Equation (10–35):  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{l}{\\dot{\\mathbf{x}}}\\\\ {\\dot{\\boldsymbol{\\xi}}}\\end{array}\\right]=\\left[\\begin{array}{l l}{\\mathbf{A}-\\mathbf{BK}}&{\\mathbf{B}k_{I}}\\\\ {-\\mathbf{\\xi}\\mathbf{C}}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\boldsymbol{\\xi}}\\end{array}\\right]+\\:\\left[\\begin{array}{l}{\\mathbf{0}}\\\\ {1}\\end{array}\\right]\\!\\!r}\\end{array}\n$$  \n\nThe output $y(t)$ of the system is $x_{3}(t)$ ,or  \n\n$$\ny=[0\\;\\;\\;0\\;\\;\\;1\\;\\;\\;0\\;\\;\\;0]{\\overset{\\textstyle\\sqrt{\\mathbf{x}}}{\\underset{\\xi}}}\\Biggr]+[0]r\n$$  \n\nDefine the state matrix, control matrix, output matrix, and direct transmission matrix of the system given by Equations (10–53) and (10–54) as AA, BB, CC, and DD, respectively. MATLAB Program 10–7 may be used to obtain the step-response curves of the designed system. Notice that, to obtain the unit-step response, we entered the command  \n\n$$\n[\\mathsf{y},\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{A A},\\mathsf{B B},\\mathsf{C C},\\mathsf{D D},1\\,,\\mathsf{t})\n$$  \n\n$\\left(=\\xi\\right)$ Figure 10–10 shows curves versus t. Notice th $y(t)\\left[=x_{3}(t)\\right]$ C$x_{1}$ versus D$t$ has approximately ,$x_{2}$ versus $t$ ,$x_{3}$ ($=$ output $15\\%$ overshoot and the settling time y) versus $t$ ,$x_{4}$ versus $t$ ,and $x_{5}$ is approximately 4.5 sec. $\\xi(t)\\,\\big[=\\,x_{5}(t)\\big]$ approaches 1.1.This result can be derived as follows: Since  \n\n$$\n{\\dot{\\mathbf{x}}}(\\infty)\\,=\\,\\mathbf{0}\\,=\\,\\mathbf{A}\\mathbf{x}(\\infty)\\,+\\,\\mathbf{B}u(\\infty)\n$$  \n\nor  \n\n$$\n{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\\\ {0}\\end{array}\\right]}={\\left[\\begin{array}{l l l l}{0}&{1}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {-0.4905}&{0}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {r}\\\\ {0}\\end{array}\\right]}+{\\left[\\begin{array}{l}{0}\\\\ {-1}\\\\ {0}\\\\ {0.5}\\end{array}\\right]}u(\\infty)\n$$  \n\n%\\*\\*\\*\\* The following program is to obtain step response $\\%$ of the inverted-pendulum system just designed \\*\\*\\*\\*\\* A = [0  1  0  0;20.601  0  0  0;0  0  0  1;-0.4905  0  0  0];$\\mathsf{B}=[0;\\!-1\\,;\\!0;\\!0.5]\\,;$   \n${\\mathsf{C}}=[0\\;\\;0\\;\\;1\\;\\;\\;0]$   \n$\\mathsf{D}=[0]$ ;  \n$\\mathsf{K}=[-1\\,57.6336\\ -35.3733\\ -56.0652\\ -36.7466];$   \n$\\mathsf{K I}=-50.9684$ ;  \n$\\mathsf{A A}=\\left[\\mathsf{A}-\\mathsf{B}^{*}\\mathsf{K}\\mathsf{\\Omega}\\mathsf{B}^{*}\\mathsf{K}\\mathsf{I};\\!\\!\\-\\!\\!\\mathrm{C}\\mathsf{\\Omega}\\mathsf{O}\\right];$   \n$\\mathrm{BB}=[0;\\!0;\\!0;\\!0;\\!1]$ ;  \n$C C=[C\\;\\;0]$ ;  \n$\\mathsf{D D}=[0]$ ;  \n$\\%^{*****}$ To obtain response curves $_{x1}$ versus t, $x2$ versus $^{\\mathrm{t,}}$ $\\%\\times3$ versus t, $x4$ versus $^{\\mathrm{t,}}$ and $x5$ versus $^{\\mathrm{t,}}$ separately, enter $\\%$ the following command \\*\\*\\*\\*\\*   \n$\\mathrm{t}=0{:}0.02{:}6;$ ;  \n$[\\mathsf{y},\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{A A},\\mathsf{B B},\\mathsf{C C},\\mathsf{D D},1\\,,\\mathsf{t});$   \n$\\times1\\,=\\,[1\\;\\;\\;0\\;\\;0\\;\\;0\\;\\;0]^{*}\\times{}^{\\prime},$ ;  \n$\\times2=[0\\;\\;1\\;\\;0\\;\\;0\\;\\;0]^{*}\\times^{\\prime};$ ;  \n$\\times3=[0\\;\\;0\\;\\;1\\;\\;0\\;\\;0]^{*}\\times^{\\prime};$ ;  \n$\\times4=[0\\;\\;0\\;\\;0\\;\\;1\\;\\;\\;0]^{*}\\times\";$   \n$\\times5=[0\\;\\;0\\;\\;0\\;\\;0\\;\\;1]^{*}\\times{}^{\\prime};$ ;  \nsubplot(3,2,1); $\\mathsf{p l o t}(\\mathsf{t},\\!\\times\\!\\mathsf{l}\\,)$ ; grid   \ntitle( $^{1}\\mathrm{x}\\,1$ versus t')  \nxlabel('t Sec'); ylabel('x1')   \nsubplot(3,2,2); plot(t,x2); grid   \ntitle( $^1\\mathrm{x}2$ versus t')  \nxlabel('t Sec'); ylabel('x2')   \nsubplot(3,2,3); plot(t,x3); grid   \ntitle( $^1\\mathrm{x3}$ versus t')  \nxlabel('t Sec'); ylabel('x3')   \nsubplot(3,2,4); plot(t,x4); grid   \ntitle( $^1\\mathrm{x4}$ versus t')  \nxlabel('t Sec'); ylabel('x4')   \nsubplot(3,2,5); plot(t,x5); grid   \ntitle( $^1\\mathrm{x}5$ versus t')  \nxlabel('t Sec'); ylabel('x5')  \n\n![](images/87c5dedb0c90c1e70497ba22d756980304d1ff424316dce9091b091dd13caf4f.jpg)  \nFigure 10–10 Curves $x_{1}$ versus $t$ ,$x_{2}$ versus $t$ ,$x_{3}$ $\\overset{\\cdot}{=}$ output $y$ ) versus $t$ ,$x_{4}$ versus $t$ ,and $x_{5}(=\\xi)$ versus $t$ .  \n\nwe get  \n\n$$\nu(\\infty)=0\n$$  \n\nSince $u(\\infty)\\,=\\,0$ ,we have, from Equation (10–33),  \n\n$$\nu(\\infty)\\,=\\,0\\,=-\\mathbf{K}\\mathbf{x}(\\infty)\\,+\\,k_{I}\\xi(\\infty)\n$$  \n\nand so  \n\n$$\n\\xi(\\infty)={\\frac{1}{k_{I}}}{\\big[}\\mathbf{K}\\mathbf{x}(\\infty){\\big]}={\\frac{1}{k_{I}}}k_{3}x_{3}(\\infty)={\\frac{-56.0652}{-50.9684}}\\,r=1.1r\n$$  \n\nHence, for $r\\,=\\,1$ ,we have  \n\n$$\n\\xi(\\infty)\\,=\\,1.1\n$$  \n\nIt is noted that, as in any design problem, if the speed and damping are not quite satisfactory, then we must modify the desired characteristic equation and determine a new matrix $\\hat{\\bf K}$ .Computer simulations must be repeated until a satisfactory result is obtained.  \n\n# 10–5 STATE OBSERVERS  \n\nIn the pole-placement approach to the design of control systems, we assumed that all state variables are available for feedback. In practice, however, not all state variables are available for feedback. Then we need to estimate unavailable state variables.  \n\nEstimation of unmeasurable state variables is commonly called observation .A device (or a computer program) that estimates or observes the state variables is called a state observer , or simply an observer . If the state observer observes all state variables of the system, regardless of whether some state variables are available for direct measurement, it is called a full-order state observer .There are times when this will not be necessary,when we will need observation of only the unmeasurable state variables, but not of those that are directly measurable as well. For example, since the output variables are observable and they are linearly related to the state variables,we need not observe all state variables, but observe only $n\\,-\\,m$ state variables, where $n$ is the dimension of the state vector and $_m$ is the dimension of the output vector.  \n\nAn observer that estimates fewer than $n$ state variables, where $n$ is the dimension of the state vector, is called a reduced-order state observer or, simply, a reduced-order observer . If the order of the reduced-order state observer is the minimum possible, the observer is called a minimum-order state observer or minimum-order observer . In this section, we shall discuss both the full-order state observer and the minimum-order state observer.  \n\nState Observer. A state observer estimates the state variables based on the measurements of the output and control variables. Here the concept of observability discussed in Section 9–7 plays an important role. As we shall see later, state observers can be designed if and only if the observability condition is satisfied.  \n\nIn the following discussions of state observers, we shall use the notation $\\widetilde{\\mathbf{x}}$ to designate the observed state vector. In many practical cases, the observed state vector $\\widetilde{\\mathbf{x}}$ is used in the state feedback to generate the desired control vector.  \n\nConsider the plant defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nThe observer is a subsystem to reconstruct the state vector of the plant. The mathematical model of the observer is basically the same as that of the plant, except that we include an additional term that includes the estimation error to compensate for inaccuracies in matrices A and $\\mathbf{B}$ and the lack of the initial error.The estimation error or observation error is the difference between the measured output and the estimated output.The initial error is the difference between the initial state and the initial estimated state.Thus, we define the mathematical model of the observer to be  \n\n$$\n\\begin{array}{r l}&{\\dot{\\widetilde{\\mathbf{x}}}\\ =\\mathbf{A}\\,\\widetilde{\\mathbf{x}}\\ +\\,\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}(y\\,-\\,\\mathbf{C}\\,\\widetilde{\\mathbf{x}}\\,)}\\\\ &{\\quad=\\big(\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\big)\\widetilde{\\mathbf{x}}\\ +\\,\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}y}\\end{array}\n$$  \n\nwhere $\\widetilde{\\mathbf{x}}$ is the estimated state and $\\mathbf{C}\\,\\mathbf{\\widetilde{x}}$ is the estimated output.The inputs to the observer are the output $y$ and the control input $u$ .Matrix $\\mathbf{K}_{e}$ , which is called the observer gain matrix, is a weighting matrix to the correction term involving the difference between the measured output $y$ and the estimated output $\\mathbf{C}\\,\\mathbf{\\widetilde{x}}$ .This term continuously corrects the model output and improves the performance of the observer.Figure 10–11 shows the block diagram of the system and the full-order state observer.  \n\n![](images/be6744aee3f3dfbeb3ff678c0137fee5bdb9fa19a93d287f9e81f8970e118766.jpg)  \nFigure 10–11 Block diagram of system and full-order state observer, when input $u$ and output $y$ are scalars.   \nFull-order state observer  \n\nFull-Order State Observer. The order of the state observer that will be discussed here is the same as that of the plant. Assume that the plant is defined by Equations (10–55) and (10–56) and the observer model is defined by Equation (10–57).  \n\nTo obtain the observer error equation, let us subtract Equation (10–57) from Equation (10–55):  \n\n$$\n\\begin{array}{r l}&{\\dot{\\mathbf{x}}\\,-\\,\\overset{\\cdot}{\\mathbf{\\Delta}\\mathbf{\\tilde{x}}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{A}\\,\\widetilde{\\mathbf{x}}\\,\\,-\\,\\mathbf{K}_{e}(\\mathbf{C}\\mathbf{x}\\,-\\,\\mathbf{C}\\,\\widetilde{\\mathbf{x}}\\,)}\\\\ &{\\qquad\\qquad\\quad=\\,\\big(\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\big)(\\mathbf{x}\\,-\\,\\widetilde{\\mathbf{x}}\\,)}\\end{array}\n$$  \n\nDefine the difference between $\\mathbf{X}$ and $\\widetilde{\\mathbf{x}}$ as the error vector e, or  \n\n$$\n\\mathbf{e}=\\mathbf{x}-\\widetilde{\\mathbf{\\deltax}}\n$$  \n\nThen Equation (10–58) becomes  \n\n$$\n\\dot{\\mathbf{e}}=(\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C})\\mathbf{e}\n$$  \n\nFrom Equation (10–59), we see that the dynamic behavior of the error vector is determined by the eigenvalues of matrix $\\mathbf{A}\\mathbf{\\Psi}-\\mathbf{\\Psi}\\mathbf{K}_{e}\\mathbf{\\Psi}\\mathbf{C}.$ . If matrix $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ is a stable matrix, the error vector will converge to zero for any initial error vector ${\\bf e}(0)$ .That is, $\\widetilde{\\mathbf{x}}\\left(t\\right)$ will converge to ${\\bf x}(t)$ regardless of the values of ${\\bf x}(0)$ and $\\widetilde{\\mathbf{x}}\\left(0\\right)$ .If the eigenvalues of matrix $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ are chosen in such a way that the dynamic behavior of the error vector is asymptotically stable and is adequately fast, then any error vector will tend to zero (the origin) with an adequate speed.  \n\nIf the plant is completely observable, then it can be proved that it is possible to choose matrix $\\mathbf{K}_{e}$ such that $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ has arbitrarily desired eigenvalues. That is, the observer gain matrix $\\mathbf{K}_{e}$ can be determined to yield the desired matrix ${\\bf A}\\mathrm{~-~}{\\bf K}_{e}\\,\\mathrm{~\\boldmath~\\Lambda~}$ C. We shall discuss this matter in what follows.  \n\nDual Problem. The problem of designing a full-order observer becomes that of determining the observer gain matrix $\\mathbf{K}_{e}$ such that the error dynamics defined by Equation (10–59) are asymptotically stable with sufficient speed of response. (The asymptotic stability and the speed of response of the error dynamics are determined by the eigenvalues of matrix $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}.$ ) Hence, the design of the full-order observer becomes that of determining an appropriate $\\mathbf{K}_{e}$ such that $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ has desired eigenvalues.Thus, the problem here becomes the same as the pole-placement problem we discussed in Section 10–2. In fact, the two problems are mathematically the same. This property is called duality.  \n\nConsider the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\mathrm{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nIn designing the full-order state observer, we may solve the dual problem, that is, solve the pole-placement problem for the dual system  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}=\\mathbf{A}^{*}\\mathbf{z}+\\mathbf{C}^{*}v}\\\\ {\\quad n=\\mathbf{B}^{*}\\mathbf{z}}\\end{array}\n$$  \n\nassuming the control signal $v$ to be  \n\n$$\nv\\,=\\,-\\mathbf{K}\\mathbf{z}\n$$  \n\nIf the dual system is completely state controllable, then the state feedback gain matrix $\\mathbf{K}$ can be determined such that matrix $\\mathbf{A}^{*}-\\mathbf{C}^{*}\\mathbf{K}$ will yield a set of the desired eigenvalues.  \n\nIf $\\mu_{1}$ ,$\\mu_{2},\\ldots,\\mu_{n}$ are the desired eigenvalues of the state observer matrix, then by taking the same ${\\pmb{\\mu}}_{i}^{\\prime}$ ’s as the desired eigenvalues of the state-feedback gain matrix of the dual system, we obtain  \n\n$$\n\\left|s\\mathbf{I}\\,-\\,(\\mathbf{A}^{*}\\,-\\,\\mathbf{C}^{*}\\mathbf{K})\\right|=(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\cdots(s\\,-\\,\\mu_{n})\n$$  \n\nNoting that the eigenvalues of $\\mathbf{A}^{*}-\\mathbf{C}^{*}\\mathbf{K}$ and those of $\\mathbf{A}-\\mathbf{K}^{*}\\mathbf{C}$ are the same, we have  \n\n$$\n\\left|s\\mathbf{I}\\,-\\,\\left(\\mathbf{A}^{*}\\,-\\,\\mathbf{C}^{*}\\mathbf{K}\\right)\\right|\\,=\\,\\left|s\\mathbf{I}\\,-\\,\\left(\\mathbf{A}\\,-\\,\\mathbf{K}^{*}\\mathbf{C}\\right)\\right|\n$$  \n\nComparing the characteristic polynomial $\\left|s\\mathbf{I}\\right.-\\left.\\left(\\mathbf{A}\\mathrm{~-~}\\mathbf{K}^{*}\\mathbf{C}\\right)\\right|$ @@and the characteristic polynomial $\\bar{|\\boldsymbol{s}\\mathbf{I}|}-\\left(\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\right)\\,\\,$ for the observer system [refer to Equation (10–57)], we find that $\\mathbf{K}_{e}$ and $\\mathbf{K}^{*}$ are related by  \n\n$$\n{\\bf K}_{e}={\\bf K}^{*}\n$$  \n\nThus,using the matrix $\\mathbf{K}$ determined by the pole-placement approach in the dual system, the observer gain matrix $\\mathbf{K}_{e}$ for the original system can be determined by using the relationship ${\\bf K}_{e}={\\bf K}^{*}$ . (See Problem A–10–10 for the details.)  \n\nNecessary and Sufficient Condition for State Observation. As discussed, a necessary and sufficient condition for the determination of the observer gain matrix $\\mathbf{K}_{e}$ for the desired eigenvalues of $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ is that the dual of the original system  \n\n$$\n\\dot{\\mathbf{z}}\\,=\\,\\mathbf{A}^{*}\\mathbf{z}\\,+\\,\\mathbf{C}^{*}v\n$$  \n\nbe completely state controllable. The complete state controllability condition for this dual system is that the rank of  \n\n$$\n\\left[\\mathbf{C}^{*}\\ \\ \\right]\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\nbe $n$ .This is the condition for complete observability of the original system defined by Equations (10–55) and (10–56).This means that a necessary and sufficient condition for the observation of the state of the system defined by Equations (10–55) and (10–56) is that the system be completely observable.  \n\nOnce we select the desired eigenvalues (or desired characteristic equation), the fullorder state observer can be designed, provided the plant is completely observable.The desired eigenvalues of the characteristic equation should be chosen so that the state observer responds at least two to five times faster than the closed-loop system considered.As stated earlier, the equation for the full-order state observer is  \n\n$$\n\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\right)\\widetilde{\\mathbf{x}}\\ +\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}y\n$$  \n\nIt is noted that thus far we have assumed the matrices $\\mathbf{A},\\mathbf{B}$ , and Cin the observer to be exactly the same as those of the physical plant. If there are discrepancies in A ,B,and Cin the observer and in the physical plant, the dynamics of the observer error are no longer governed by Equation (10–59). This means that the error may not approach zero as expected.Therefore, we need to choose $\\mathbf{K}_{e}$ so that the observer is stable and the error remains acceptably small in the presence of small modeling errors.  \n\nTransformation Approach to Obtain State Observer Gain Matrix $\\upkappa_{e}$ .By following the same approach as we used in deriving the equation for the state feedback gain matrix $\\mathbf{K}$ , we can obtain the following equation:  \n\n$$\n\\mathbf{K}_{e}=\\mathbf{Q}\\left[\\begin{array}{c}{\\alpha_{n}\\,-\\,a_{n}}\\\\ {\\alpha_{n-1}\\,-\\,a_{n-1}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\alpha_{1}\\,-\\,a_{1}}\\end{array}\\right]=\\,\\left(\\mathbf{W}\\mathbf{N}^{*}\\right)^{-1}\\left[\\begin{array}{c}{\\alpha_{n}\\,-\\,a_{n}}\\\\ {\\alpha_{n-1}\\,-\\,a_{n-1}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\alpha_{1}\\,-\\,a_{1}}\\end{array}\\right]\n$$  \n\nwhere $\\mathbf{K}_{e}$ is an $n\\times1$ matrix,  \n\n$$\n{\\bf Q}\\,=\\,({\\bf W}{\\bf N}^{*})^{-1}\n$$  \n\nand  \n\n$$\n\\mathbf{N}=\\left[\\mathbf{C}^{*}\\ \\ \\mathbf{\\Sigma}\\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\mathbf{\\Sigma}\\right]\\ \\cdots\\ \\mathbf{\\Sigma}\\mathbf{\\Sigma}\\left[\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\n$$\n\\mathbf{W}={\\left[\\begin{array}{l l l l l}{a_{n-1}}&{a_{n-2}}&{\\cdots}&{a_{1}}&{1}\\\\ {a_{n-2}}&{a_{n-3}}&{\\cdots}&{1}&{0}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {a_{1}}&{1}&{\\cdots}&{0}&{0}\\\\ {1}&{0}&{\\cdots}&{0}&{0}\\end{array}\\right]}\n$$  \n\n[Refer to Problem A–10–10 for the derivation of Equation (10–61).]  \n\nDirect-Substitution Approach to Obtain State Observer Gain Matrix $\\upkappa_{e}$ .Similar to the case of pole placement,if the system is of low order,then direct substitution of matrix $\\mathbf{K}_{e}$ into the desired characteristic polynomial may be simpler. For example, if $\\mathbf{X}$ is a 3-vector, then write the observer gain matrix $\\mathbf{K}_{e}$ as  \n\n$$\n\\mathbf{K}_{e}=\\left[\\begin{array}{l}{k_{e1}}\\\\ {k_{e2}}\\\\ {k_{e3}}\\end{array}\\right]\n$$  \n\nSubstitute this $\\mathbf{K}_{e}$ matrix into the desired characteristic polynomial:  \n\n$$\n\\left|s\\mathbf{I}\\,-\\,\\left(\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\right)\\right|\\,=\\,(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})(s\\,-\\,\\mu_{3})\n$$  \n\nBy equating the coefficients of the like powers of $s$ on both sides of this last equation, we can determine the values of $k_{e1},k_{e2}$ ,and $k_{e3}$ .This approach is convenient if $n=1$ ,2, or 3, where $n$ is the dimension of the state vector $\\mathbf{X}$ . (Although this approach can be used when $n=4,5,6,\\dots$ , the computations involved may become very tedious.)  \n\nAnother approach to the determination of the state observer gain matrix $\\mathbf{K}_{e}$ is to use Ackermann’s formula.This approach is presented in the following.  \n\nAckermann’s Formula. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\mathrm{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nIn Section 10–2 we derived Ackermann’s formula for pole placement for the system defined by Equation (10–62).The result was given by Equation (10–18), rewritten thus:  \n\n$$\n\\mathbf{K}=[0\\quad0\\,\\cdots\\,0\\quad1]\\big[\\mathbf{B}\\,\\,\\mid\\,\\mathbf{A}\\mathbf{B}\\,\\,\\mid\\,\\cdots\\,\\mid\\,\\mathbf{A}^{n-1}\\mathbf{B}\\big]^{-1}\\phi(\\mathbf{A})\n$$  \n\nFor the dual of the system defined by Equations (10–62) and (10–63),  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}=\\mathbf{A}^{*}\\mathbf{z}+\\mathbf{C}^{*}v}\\\\ {\\quad n=\\mathbf{B}^{*}\\mathbf{z}}\\end{array}\n$$  \n\nthe preceding Ackermann’s formula for pole placement is modified to  \n\n$$\n\\mathbf{K}=[0\\quad0\\,\\cdots\\,0\\quad1]\\big[\\mathbf{C}^{*}\\,\\mid\\,\\mathbf{A}^{*}\\mathbf{C}^{*}\\,\\mid\\,\\cdots\\,\\mid\\,(\\mathbf{A}^{*})^{n-1}\\mathbf{C}^{*}\\big]^{-1}\\phi(\\mathbf{A}^{*})\n$$  \n\nAs stated earlier, the state observer gain matrix $\\mathbf{K}_{e}$ is given by $\\mathbf{K}^{*}$ , where $\\mathbf{K}$ is given by Equation (10–64).Thus,  \n\n$$\n\\mathbf{K}_{e}=\\mathbf{K}^{*}=\\phi(\\mathbf{A}^{*})^{*}{\\left[\\begin{array}{l}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{C}\\mathbf{A}^{n-2}}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}}\\end{array}\\right]}^{-1}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {0}\\\\ {1}\\end{array}\\right]}=\\phi(\\mathbf{A}){\\left[\\begin{array}{l}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{C}\\mathbf{A}^{n-2}}\\\\ {\\mathbf{C}\\mathbf{A}^{n-1}}\\end{array}\\right]}^{-1}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nwhere $\\phi(s)$ is the desired characteristic polynomial for the state observer, or  \n\n$$\n\\phi(s)\\,=\\,{\\big(}s\\,-\\,\\mu_{1}{\\big)}{\\big(}s\\,-\\,\\mu_{2}{\\big)}\\cdots{\\big(}s\\,-\\,\\mu_{n}{\\big)}\n$$  \n\nwhere $\\mu_{1},\\mu_{2},\\ldots,\\mu_{n}$ are the desired eigenvalues. Equation (10–65) is called Ackermann’s formula for the determination of the observer gain matrix $\\mathbf{K}_{e}$ .  \n\nComments on Selecting the Best $\\upkappa_{e}$ .Referring to Figure 10–11, notice that the feedback signal through the observer gain matrix $\\mathbf{K}_{e}$ serves as a correction signal to the plant model to account for the unknowns in the plant. If significant unknowns are involved, the feedback signal through the matrix $\\mathbf{K}_{e}$ should be relatively large. However, if the output signal is contaminated significantly by disturbances and measurement noises, then the output $y$ is not reliable and the feedback signal through the matrix $\\mathbf{K}_{e}$ should be relatively small. In determining the matrix $\\mathbf{K}_{e}$ ,we should carefully examine the effects of disturbances and noises involved in the output $y$ .  \n\nRemember that the observer gain matrix $\\mathbf{K}_{e}$ depends on the desired characteristic equation  \n\n$$\n(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\cdots(s\\,-\\,\\mu_{n})\\,=\\,0\n$$  \n\nThe choice of a set of $\\mu_{1},\\mu_{2},\\ldots,\\mu_{n}$ is, in many instances, not unique.As a general rule, however, the observer poles must be two to five times faster than the controller poles to make sure the observation error (estimation error) converges to zero quickly. This means that the observer estimation error decays two to five times faster than does the state vector $\\mathbf{X}$ . Such faster decay of the observer error compared with the desired dynamics makes the controller poles dominate the system response.  \n\nIt is important to note that if sensor noise is considerable,we may choose the observer poles to be slower than two times the controller poles, so that the bandwidth of the system will become lower and smooth the noise. In this case the system response will be strongly influenced by the observer poles. If the observer poles are located to the right of the controller poles in the left-half $s$ plane, the system response will be dominated by the observer poles rather than by the control poles.  \n\nIn the design of the state observer, it is desirable to determine several observer gain matrices $\\mathbf{K}_{e}$ based on several different desired characteristic equations. For each of the several different matrices $\\mathbf{K}_{e}$ ,simulation tests must be run to evaluate the resulting system performance. Then we select the best $\\mathbf{K}_{e}$ from the viewpoint of overall system performance. In many practical cases, the selection of the best matrix $\\mathbf{K}_{e}$ boils down to a compromise between speedy response and sensitivity to disturbances and noises.  \n\nEXAMPLE 10–6 Consider the system  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\qquad\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\begin{array}{r l r l}&{\\mathbf{A}=\\left[\\begin{array}{l l}{0}&{20.6}\\\\ {1}&{0}\\end{array}\\right],}&&{\\mathbf{B}=\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right],}&&{\\mathbf{C}=[0}&{1]}\\end{array}\n$$  \n\nWe use the observed state feedback such that  \n\n$$\nu=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}\n$$  \n\nDesign a full-order state observer, assuming that the system configuration is identical to that shown in Figure 10–11.Assume that the desired eigenvalues of the observer matrix are  \n\n$$\n\\mu_{1}=-10,\\qquad\\mu_{2}=-10\n$$  \n\nThe design of the state observer reduces to the determination of an appropriate observer gain matrix $\\mathbf{K}_{e}$ .  \n\nLet us examine the observability matrix.The rank of  \n\n$$\n[\\mathbf{C}^{*}\\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}]={\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]}\n$$  \n\nis 2.Hence,the system is completely observable and the determination of the desired observer gain matrix is possible.We shall solve this problem by three methods.  \n\nMethod 1: We shall determine the observer gain matrix by use of Equation (10–61). The given system is already in the observable canonical form. Hence, the transformation matrix $\\mathbf{Q}\\,=\\,(\\mathbf{W}\\mathbf{N}^{*})^{-1}$ is I . Since the characteristic equation of the given system is  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}|={\\left|\\begin{array}{l l}{s}&{-20.6}\\\\ {-1}&{\\;\\;\\;s}\\end{array}\\right|}=s^{2}-20.6=s^{2}+a_{1}s\\,+\\,a_{2}=0\n$$  \n\nwe have  \n\n$$\na_{1}=0,\\qquad a_{2}=-20.6\n$$  \n\nThe desired characteristic equation is  \n\n$$\n(s\\,+\\,10)^{2}=s^{2}\\,+\\,20s\\,+\\,100\\,=\\,s^{2}\\,+\\,\\alpha_{1}s\\,+\\,\\alpha_{2}=0\n$$  \n\nHence,  \n\n$$\n\\alpha_{1}=20,\\;\\;\\;\\;\\;\\;\\alpha_{2}=100\n$$  \n\nThen the observer gain matrix $\\mathbf{K}_{e}$ can be obtained from Equation (10–61) as follows:  \n\n$$\n\\mathbf{K}_{e}=(\\mathbf{W}\\mathbf{N}^{*})^{-1}{\\left[\\begin{array}{l l}{\\alpha_{2}-\\,a_{2}}\\\\ {\\alpha_{1}-\\,a_{1}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l l}{100+20.6}\\\\ {20-\\,\\,\\,0}\\end{array}\\right]}={\\left[\\begin{array}{l}{120.6}\\\\ {20}\\end{array}\\right]}\n$$  \n\nMethod 2: Referring to Equation (10–59):  \n\n$$\n\\dot{\\mathbf{e}}=(\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C})\\mathbf{e}\n$$  \n\nthe characteristic equation for the observer becomes  \n\n$$\n\\left|s\\mathbf{I}\\right.-\\left.\\mathbf{A}\\right.+\\left.\\mathbf{K}_{e}\\mathbf{C}\\right|=0\n$$  \n\nDefine  \n\n$$\n\\mathbf{K}_{e}=\\left[\\begin{array}{l}{k_{e1}}\\\\ {k_{e2}}\\end{array}\\right]\n$$  \n\nThen the characteristic equation becomes  \n\n$$\n\\begin{array}{r l}&{\\left|\\left[\\begin{array}{l l}{s}&{0}\\\\ {0}&{s}\\end{array}\\right]-\\left[\\begin{array}{l l}{0}&{20.6}\\\\ {1}&{0}\\end{array}\\right]+\\left[\\begin{array}{l}{k_{e1}}\\\\ {k_{e2}}\\end{array}\\right][0}&{1]\\right|=\\left|\\begin{array}{l l}{s}&{-20.6\\,+\\,k_{e1}}\\\\ {-1}&{\\;\\;\\,s\\,+\\,k_{e2}}\\end{array}\\right|}\\\\ &{=s^{2}+\\,k_{e2}s-20.6\\,+\\,k_{e1}=0}\\end{array}\n$$  \n\nSince the desired characteristic equation is  \n\n$$\ns^{2}\\,+\\,20s\\,+\\,100\\,=\\,0\n$$  \n\nby comparing Equation (10–66) with this last equation, we obtain  \n\n$$\nk_{e1}=120.6,\\qquad k_{e2}=20\n$$  \n\nor  \n\n$$\n\\mathbf{K}_{e}=\\left[\\begin{array}{c}{120.6}\\\\ {20}\\end{array}\\right]\n$$  \n\nMethod 3: We shall use Ackermann’s formula given by Equation (10–65):  \n\n$$\n\\mathbf{K}_{e}=\\phi(\\mathbf{A})\\left[\\mathbf{C}_{\\mathbf{A}}^{\\mathrm{~C~}}\\right]^{-1}\\!\\!\\left[\\mathbf{\\Omega}_{1}\\right]\n$$  \n\nwhere  \n\n$$\n\\phi(s)\\,=\\,(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\,=\\,s^{2}\\,+\\,20s\\,+\\,100\n$$  \n\nThus,  \n\n$$\n\\phi(\\mathbf{A})\\,=\\,\\mathbf{A}^{2}\\,+\\,20\\mathbf{A}\\,+\\,100\\mathbf{I}\n$$  \n\nand  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{K}_{e}=\\left(\\mathbf{A}^{2}+20\\mathbf{A}+100\\mathbf{I}\\right){\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]}^{-1}{\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}}\\\\ &{\\qquad={\\left[\\begin{array}{l l}{120.6}&{412}\\\\ {20}&{120.6}\\end{array}\\right]}{\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}={\\left[\\begin{array}{l}{120.6}\\\\ {20}\\end{array}\\right]}}\\end{array}}\n$$  \n\nAs a matter of course, we get the same $\\mathbf{K}_{e}$ regardless of the method employed. The equation for the full-order state observer is given by Equation (10–57),  \n\n$$\n\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\right)\\widetilde{\\mathbf{x}}\\ +\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}y\n$$  \n\nor  \n\n$$\n{\\binom{\\dot{\\pi}_{1}}{\\dot{\\pi}_{2}}}={\\binom{0}{1}}\\ \\ 100{\\Biggl]}{\\Biggl[}{\\Biggl[}{\\tilde{x}_{1}}{\\atop{\\tilde{x}_{2}}}{\\Biggr]}+{\\Biggl[}{0{\\atop1}}{\\Biggr]}u+{\\Biggl[}{120.6}{\\Biggr]}y\n$$  \n\nFinally, it is noted that, similar to the case of pole placement, if the system order $n$ is 4 or higher, methods 1 and 3 are preferred, because all matrix computations can be carried out by a computer, while method 2 always requires hand computation of the characteristic equation involving unknown parameters $k_{e1}$ ,$k_{e2},\\ldots,k_{e n}$ .  \n\nEffects of the Addition of the Observer on a Closed-Loop System. In the pole-placement design process, we assumed that the actual state ${\\bf x}(t)$ was available for feedback. In practice, however, the actual state ${\\bf x}(t)$ may not be measurable, so we will need to design an observer and use the observed state $\\widetilde{\\mathbf{x}}\\left(t\\right)$ for feedback as shown in Figure 10–12. The design process, therefore, becomes a two-stage process, the first stage being the determination of the feedback gain matrix $\\mathbf{K}$ to yield the desired characteristic equation and the second stage being the determination of the observer gain matrix $\\mathbf{K}_{e}$ to yield the desired observer characteristic equation.  \n\nLet us now investigate the effects of the use of the observed state $\\widetilde{\\mathbf{x}}\\left(t\\right)$ ,rather than the actual state ${\\bf x}(t)$ ,on the characteristic equation of a closed-loop control system.  \n\n![](images/06467594d249982e7fec5c2098a387892270694927eaee1ef3402f20c44393d8.jpg)  \nFigure 10–12 Observed-state feedback control system.  \n\nConsider the completely state controllable and completely observable system defined by the equations  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\mathrm{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nFor the state-feedback control based on the observed state $\\widetilde{\\mathbf{x}}$ ,  \n\n$$\nu=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}\n$$  \n\nWith this control, the state equation becomes  \n\n$$\n{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\widetilde{\\mathbf{x}}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{K}(\\mathbf{x}\\,-\\,{\\widetilde{\\mathbf{x}}}\\,)\n$$  \n\nThe difference between the actual state ${\\bf x}(t)$ and the observed state $\\widetilde{\\mathbf{x}}\\left(t\\right)$ has been defined as the error $\\mathbf{e}(t)$ :  \n\n$$\n\\mathbf{e}(t)\\,=\\,\\mathbf{x}(t)\\,-\\,\\widetilde{\\mathbf{x}}\\,(t)\n$$  \n\nSubstitution of the error vector $\\mathbf{e}(t)$ into Equation (10–67) gives  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{Ke}\n$$  \n\nNote that the observer error equation was given by Equation (10–59), repeated here:  \n\n$$\n\\dot{\\mathbf{e}}=(\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C})\\mathbf{e}\n$$  \n\nCombining Equations (10–68) and (10–69), we obtain  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{c}{\\dot{\\mathbf{x}}}\\\\ {\\dot{\\mathbf{e}}}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\mathbf{A}-\\mathbf{\\nabla}\\mathbf{B}\\mathbf{K}}&{\\mathbf{B}\\mathbf{K}}\\\\ {\\mathbf{\\nabla}\\mathbf{0}}&{\\mathbf{A}-\\mathbf{\\nabla}\\mathbf{K}_{e}\\mathbf{C}}\\end{array}\\right]\\left[\\begin{array}{c}{\\mathbf{x}}\\\\ {\\mathbf{e}}\\end{array}\\right]}\\end{array}\n$$  \n\nChapter 10 /Control Systems Design in State Space  \n\nEquation (10–70) describes the dynamics of the observed-state feedback control system. The characteristic equation for the system is  \n\n$$\n\\begin{array}{r}{\\left|s\\mathbf{I}\\right.-\\left.\\mathbf{A}\\right.+\\left.\\mathbf{B}\\mathbf{K}\\begin{array}{c c}{\\left.-\\mathbf{B}\\mathbf{K}\\right.}\\\\ {\\left.\\mathbf{0}\\right.}\\end{array}\\right.}\\end{array}\n$$  \n\nor  \n\n$$\n\\big|s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{B}\\mathbf{K}\\big|\\big|s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{K}_{e}\\mathbf{C}\\big|\\,=\\,0\n$$  \n\nNotice that the closed-loop poles of the observed-state feedback control system consist of the poles due to the pole-placement design alone and the poles due to the observer design alone. This means that the pole-placement design and the observer design are independent of each other.They can be designed separately and combined to form the observed-state feedback control system. Note that, if the order of the plant is $n$ ,then the observer is also of nth order (if the full-order state observer is used), and the resulting characteristic equation for the entire closed-loop system becomes of order $2n$ .  \n\nTransfer Function of the Observer-Based Controller. Consider the plant defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\qquad\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nAssume that the plant is completely observable. Assume that we use observed-state feedback control $u=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}$ .Then, the equations for the observer are given by  \n\n$$\n\\begin{array}{r}{\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\mathbf{\\Sigma}-\\mathbf{B}\\mathbf{K}\\right)\\widetilde{\\textbf{x}}+\\mathbf{K}_{e}y}\\\\ {u=-\\mathbf{K}\\widetilde{\\mathbf{x}}}\\end{array}\n$$  \n\nwhere Equation (10–71) is obtained by substituting $u=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}$ into Equation (10–57).  \n\nBy taking the Laplace transform of Equation (10–71), assuming a zero initial condition, and solving for $\\widetilde{\\mathbf{X}}(s)$ we obtain  \n\n$$\n\\widetilde{\\mathbf{X}}(s)\\,=\\,\\bigl(s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{K}_{e}\\mathbf{C}\\,+\\,\\mathbf{B}\\mathbf{K}\\bigr)^{-1}\\mathbf{K}_{e}Y(s)\n$$  \n\nBy substituting this $\\widetilde{\\mathbf{X}}(s)$ into the Laplace transform of Equation (10–72), we obtain  \n\n$$\nU(s)=-\\mathbf{K}(s\\mathbf{I}-\\mathbf{A}+\\mathbf{K}_{e}\\mathbf{C}+\\mathbf{B}\\mathbf{K})^{-1}\\mathbf{K}_{e}Y(s)\n$$  \n\nThen the transfer function $U(s)/Y(s)$ can be obtained as  \n\n$$\n\\frac{U(s)}{Y(s)}=-\\mathbf{K}\\big(s\\mathbf{I}-\\mathbf{A}\\,+\\,\\mathbf{K}_{e}\\mathbf{C}\\,+\\,\\mathbf{B}\\mathbf{K}\\big)^{-1}\\mathbf{K}_{e}\n$$  \n\nFigure 10–13 shows the block diagram representation for the system. Notice that the transfer function  \n\n$$\n\\mathbf{K}\\big(s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{K}_{e}\\,\\mathbf{C}\\,+\\,\\mathbf{B}\\mathbf{K}\\big)^{-1}\\mathbf{K}_{e}\n$$  \n\nacts as a controller for the system. Hence, we call the transfer function  \n\n$$\n\\frac{U(s)}{-Y(s)}=\\frac{\\mathrm{num}}{\\mathrm{den}}=\\mathbf{K}\\big(s\\mathbf{I}-\\mathbf{A}+\\mathbf{K}_{e}\\mathbf{C}+\\mathbf{B}\\mathbf{K}\\big)^{-1}\\mathbf{K}_{e}\n$$  \n\n![](images/7f203dd783716f897612cf4ce42bc6396ca3de1a48e4013da29222afb01e5623.jpg)  \nFigure 10–13 Block diagram representation of system with a controller-observer.  \n\nthe observer-based controller transfer function or,simply,the observer-controller transfer function.  \n\nNote that the observer-controller matrix  \n\n$$\n\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\,-\\,\\mathbf{B}\\mathbf{K}\n$$  \n\nmay or may not be stable, although $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ and $\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}$ are chosen to be stable. In fact, in some cases the matrix A -$\\mathbf{K}_{e}$ C-BK may be poorly stable or even unstable.  \n\n# EXAMPLE 10–7  \n\nConsider the design of a regulator system for the following plant:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {20.6}&{0\\end{array}\\right]}},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1\\quad0]\n$$  \n\nSuppose that we use the pole-placement approach to the design of the system and that the desired closed-loop poles for this system are at $s=\\mu_{i}$ $(i=1,2)$ ), where $\\mu_{1}=-1.8\\,+\\,j2.4$ and $\\mu_{2}=-1.8\\,-\\,j2.4.$ The state-feedback gain matrix $\\mathbf{K}$ for this case can be obtained as follows:  \n\n$$\n\\mathbf{K}=[29.6\\quad3.6]\n$$  \n\nUsing this state-feedback gain matrix $\\mathbf{K}$ , the control signal $u$ is given by  \n\n$$\nu=-\\mathbf{K}\\mathbf{x}=-[29.6\\quad3.6]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nSuppose that we use the observed-state feedback control instead of the actual-state feedback control, or  \n\n$$\nu=-\\mathbf{K}\\widetilde{\\mathbf{x}}\\,=-[29.6\\,\\,\\,\\,\\,3.6]{\\left[\\begin{array}{l}{\\widetilde{x}_{1}}\\\\ {\\widetilde{x}_{2}}\\end{array}\\right]}\n$$  \n\nwhere we choose the observer poles to be at  \n\n$$\ns\\,=\\,-8,\\qquad s\\,=\\,-8\n$$  \n\nObtain the observer gain matrix $\\mathbf{K}_{e}$ and draw a block diagram for the observed-state feedback control system.Then obtain the transfer function $U(s)/[-Y(s)]$ for the observer controller, and draw another block diagram with the observer controller as a series controller in the feedforward path. Finally, obtain the response of the system to the following initial condition:  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]},\\qquad\\mathbf{e}(0)=\\mathbf{x}(0)\\,-\\,\\widetilde\\mathbf{x}\\,(0)={\\left[\\begin{array}{l}{0.5}\\\\ {0}\\end{array}\\right]}\n$$  \n\nChapter 10 /Control Systems Design in State Space  \n\nFor the system defined by Equation (10–75), the characteristic polynomial is  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}|=\\left|{\\begin{array}{c c}{s}&{-1}\\\\ {-20.6}&{s}\\end{array}}\\right|=s^{2}\\mathbf{\\Omega}-20.6\\,=s^{2}\\mathbf{\\Omega}+a_{1}s\\mathbf{\\Omega}+a_{2}\n$$  \n\nThus,  \n\n$$\na_{1}=0,\\qquad a_{2}=-20.6\n$$  \n\nThe desired characteristic polynomial for the observer is  \n\n$$\n\\begin{array}{c}{{(s\\,-\\,\\mu_{1})(s\\,-\\,\\mu_{2})\\,=\\,(s\\,+\\,8)(s\\,+\\,8)\\,=\\,s^{2}\\,+\\,16s\\,+\\,64}}\\\\ {{{}}}\\\\ {{=\\,s^{2}\\,+\\,\\alpha_{1}s\\,+\\,\\alpha_{2}}}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\alpha_{1}=16,\\ \\ \\ \\ \\ \\alpha_{2}=64\n$$  \n\nFor the determination of the observer gain matrix, we use Equation (10–61), or  \n\n$$\n\\mathbf{K}_{e}=\\,(\\mathbf{W}\\mathbf{N}^{*})^{-1}\\!\\left[\\begin{array}{l}{\\!\\alpha_{2}\\,-\\,a_{2}}\\\\ {\\!\\alpha_{1}\\,-\\,a_{1}}\\end{array}\\right]\n$$  \n\nwhere  \n\n$$\n\\mathbf{N}=[\\mathbf{C}^{*}\\ \\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}]={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}\n$$  \n\n$$\n\\mathbf{W}={\\left[\\begin{array}{l l}{a_{1}}&{1}\\\\ {1}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]}\n$$  \n\nHence,  \n\n$$\n\\begin{array}{r}{\\mathbf{K}_{e}=\\left\\{\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]\\right\\}^{-1}\\!\\left[\\begin{array}{l}{64\\mathrm{~+~}20.6}\\\\ {16\\mathrm{~-~}0}\\end{array}\\right]}\\\\ {=\\left[\\begin{array}{l l}{0}&{1}\\\\ {1}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{84.6}\\\\ {16}\\end{array}\\right]=\\left[\\begin{array}{l}{16}\\\\ {84.6}\\end{array}\\right]}\\end{array}\n$$  \n\nEquation (10–77) gives the observer gain matrix $\\mathbf{K}_{e}$ .The observer equation is given by Equation (10–60):  \n\n$$\n\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\right)\\widetilde{\\mathbf{x}}\\ +\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}y\n$$  \n\nSince  \n\n$$\nu=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}\n$$  \n\nEquation (10–78) becomes  \n\n$$\n\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\mathbf{\\Sigma}-\\mathbf{B}\\mathbf{K}\\right)\\widetilde{\\mathbf{x}}\\ +\\mathbf{K}_{e}y\n$$  \n\nor  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{\\tilde{x}}_{1}}\\\\ {\\dot{\\tilde{x}}_{2}}\\end{array}\\right]=\\left\\{\\left[\\begin{array}{l l}{0}&{1}\\\\ {20.6}&{0}\\end{array}\\right]-\\left[\\begin{array}{l}{16}\\\\ {84.6}\\end{array}\\right][1}&{0]-\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right][29.6}&{3.6]\\right\\}\\!\\!\\left[\\overset{\\widetilde{x}_{1}}{\\widetilde{x}_{2}}\\right]+\\left[\\begin{array}{l}{16}\\\\ {84.6}\\end{array}\\right]y}\\\\ &{\\qquad\\qquad=\\left[\\begin{array}{l l}{-16}&{1}\\\\ {-93.6}&{-3.6}\\end{array}\\right]\\!\\!\\left[\\overset{\\widetilde{x}_{1}}{\\widetilde{x}_{2}}\\right]+\\left[\\begin{array}{l}{16}\\\\ {84.6}\\end{array}\\right]y}\\end{array}\n$$  \n\nThe block diagram of the system with observed-state feedback is shown in Figure 10–14(a).  \n\n![](images/8aecf6dcb5aa1f6fe8f4e05b8c30c2ce5609d04c1031cd03bb7219c4986e37f0.jpg)  \nFigure 10–14 (a) Block diagram of system with observed-state feedback; (b) block diagram of transferfunction system.  \n\nReferring to Equation (10–74), the transfer function of the observer-controller is  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{U(s)}{-Y(s)}=\\mathbb{K}(s\\mathbf{I}-\\mathbf{A}+\\mathbf{K}_{e}\\mathbf{C}+\\mathbf{B}\\mathbf{K})^{-1}\\mathbb{K}_{e}}\\\\ {\\displaystyle=\\left[29.6\\quad3.6\\right]\\!\\left[\\!\\!\\begin{array}{c c}{\\displaystyle s+16}&{\\displaystyle-1}\\\\ {\\displaystyle93.6}&{\\displaystyle s+3.6\\right]^{-1}\\!\\!\\left[\\!\\!\\begin{array}{c}{16}\\end{array}\\!\\!\\!-1\\!\\!}\\\\ {\\displaystyle=\\frac{778.2s+3690.7}{s^{2}+19.6s+151.2}}\\end{array}\\!\\!\\!\\right]^{-1}\\!\\!\\left[\\!\\!\\begin{array}{c}{16}\\end{array}\\!\\!\\!-16\\!\\!\\!-16.65\\!\\!\\!\\right]^{-1}\\!\\!\\!=\\!\\!0.}\\end{array}\n$$  \n\nAs a matter of course, the same transfer function can be obtained with MATLAB. For example, MATLAB Program 10–8 produces the transfer function of the observer controller.Figure 10–14(b) shows a block diagram of the system.  \n\n![](images/90978ee24c8b79d8b64bfac682f514fb363a36a79fdee99531bf3728de1db69a.jpg)  \n\nThe dynamics of the observed-state feedback control system just designed can be described by the following equations: For the plant,  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}\\right]=\\left[\\begin{array}{l l}{0}&{1}\\\\ {20.6}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right]\\!\\!u}\\\\ {y=[1}&{0]\\!\\!\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nFor the observer,  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{c c}{-16}&{1}\\\\ {-93.6}&{-3.6}\\end{array}\\right]\\left[\\begin{array}{c}{\\widetilde{x}_{1}}\\\\ {\\widetilde{x}_{2}}\\end{array}\\right]+\\left[\\begin{array}{c}{16}\\\\ {84.6}\\end{array}\\right]y}\\\\ {u=-[29.6}&{3.6]\\left[\\begin{array}{c}{\\widetilde{x}_{1}}\\\\ {\\widetilde{x}_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nThe system, as a whole, is of fourth order.The characteristic equation for the system is  \n\n$$\n\\begin{array}{r}{\\vert s\\mathbf{I}-\\mathbf{A}\\,+\\,\\mathbf{B}\\mathbf{K}\\vert\\vert s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{K}_{e}\\mathbf{C}\\vert\\,=\\left(s^{2}\\,+\\,3.6s\\,+\\,9\\right)\\!\\!\\left(s^{2}\\,+\\,16s\\,+\\,64\\right)}\\\\ {=\\,s^{4}\\,+\\,19.6s^{3}\\,+\\,130.6s^{2}\\,+\\,374.4s\\,+\\,576\\,=\\,0\\qquad\\qquad}\\end{array}\n$$  \n\nThe characteristic equation can also be obtained from the block diagram for the system shown in Figure 10–14(b). Since the closed-loop transfer function is  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{778.2s\\,+\\,3690.7}{\\left(s^{2}\\,+\\,19.6s\\,+\\,151.2\\right)\\!\\left(s^{2}\\,-\\,20.6\\right)\\,+\\,778.2s\\,+\\,3690.7}\n$$  \n\nthe characteristic equation is  \n\n$$\n\\begin{array}{r l}&{\\big(s^{2}\\,+\\,19.6s\\,+\\,151.2\\big)\\big(s^{2}\\,-\\,20.6\\big)\\,+\\,778.2s\\,+\\,3690.7}\\\\ &{\\,\\,\\,\\,\\,\\,=s^{4}\\,+\\,19.6s^{3}\\,+\\,130.6s^{2}\\,+\\,374.4s\\,+\\,576\\,=\\,0}\\end{array}\n$$  \n\nAs a matter of course, the characteristic equation is the same for the system in state-space representation and in transfer-function representation.  \n\nFinally, we shall obtain the response of the system to the following initial condition:  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]},\\qquad\\mathbf{e}(0)={\\left[\\begin{array}{l}{0.5}\\\\ {0}\\end{array}\\right]}\n$$  \n\nReferring to Equation (10–70), the response to the initial condition can be determined from  \n\n$$\n\\begin{array}{r}{\\left[\\mathbf{\\overline{{\\kappa}}}_{\\mathbf{\\theta}}^{+}\\right]=\\left[\\mathbf{A}-\\mathbf{B}\\mathbf{K}\\begin{array}{c c}{\\mathbf{B}\\mathbf{K}}&{\\mathbf{B}\\mathbf{K}}\\\\ {\\mathbf{\\theta}}&{\\mathbf{A}-\\mathbf{K}_{e}\\mathbf{C}\\mathbf{\\overline{{\\kappa}}}\\mathbf{\\overline{{\\kappa}}}\\mathbf{\\overline{{e}}}\\mathbf{\\overline{{\\eta}}}}\\end{array}\\right]\\!\\!\\left[\\mathbf{{x}}^{\\mathbf{\\alpha}}\\right],\\quad\\left[\\mathbf{\\overline{{\\kappa}}}\\mathbf{x}(0)\\right]=\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0.5}\\\\ {0}\\end{array}\\right]}\\end{array}\n$$  \n\nA MATLAB Program to obtain the response is shown in MATLAB Program 10–9.The resulting response curves are shown in Figure 10–15.  \n\n![](images/886d6bd73e9dc87a2ce1ba4201af20faaca1c40e42aee61c6204674459a8e204.jpg)  \n\n![](images/dc34dc61980e23e76eff152b8d359b99b0abf5ec4b2388f34d3f10c5211d8531.jpg)  \nFigure 10–15 Response curves to initial condition.  \n\nMinimum-Order Observer. The observers discussed thus far are designed to reconstruct all the state variables. In practice, some of the state variables may be accurately measured. Such accurately measurable state variables need not be estimated.  \n\nSuppose that the state vector $\\mathbf{X}$ is an $n$ -vector and the output vector $\\mathbf{y}$ is an $_m$ -vector that can be measured. Since $_m$ output variables are linear combinations of the state variables, mstate variables need not be estimated. We need to estimate only $n\\,-\\,m$ state variables.Then the reduced-order observer becomes an $(n\\mathrm{~-~}m)$ th-order observer. Such an $(n\\mathrm{~-~}m)$ th-order observer is the minimum-order observer. Figure 10–16 shows the block diagram of a system with a minimum-order observer.  \n\n![](images/5ee315ac421aa22291a2f7bc172cb3663655ca7b7f77c704d4528173bd4e4bd4.jpg)  \nFigure 10–16 Observed-state feedback control system with a minimum-order observer.  \n\nIt is important to note, however, that if the measurement of output variables involves significant noises and is relatively inaccurate, then the use of the full-order observer may result in a better system performance.  \n\nTo present the basic idea of the minimum-order observer, without undue mathematical complications, we shall present the case where the output is a scalar (that is, $m\\,=\\,1$ ) and derive the state equation for the minimum-order observer. Consider the system  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\qquad\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nhe state vector D$\\mathbf{X}$ can be partitio d into two parts $x_{a}$ (a alar) and $\\mathbf{x}_{b}$ Can $(n\\mathrm{~-~}1)$ -vector . Here the state variable $x_{a}$ is equal to the output $y$ and thus can be directly measured, and $\\mathbf{x}_{b}$ is the unmeasurable portion of the state vector. Then the partitioned state and output equations become  \n\n$$\n\\begin{array}{r}{{\\small\\left[\\!\\!\\begin{array}{c}{{\\dot{x}}_{a}}\\\\ {{\\dot{\\mathbf{x}}}_{b}}\\end{array}\\!\\right]}={\\small\\left[\\!\\!\\begin{array}{c}{A_{a a}}\\\\ {\\mathbf{A}_{b a}}\\end{array}\\!\\!\\right]}{\\mathbf{A}}_{b b}{\\small\\left[\\!\\!\\begin{array}{c}{{\\mathbf{x}}_{a}}\\\\ {{\\mathbf{x}}_{b}}\\end{array}\\!\\!\\right]}+{\\small\\left[\\!\\!\\begin{array}{c}{B_{a}}\\\\ {\\mathbf{B}_{b}}\\end{array}\\!\\!\\right]}u}\\\\ {y={\\small\\left[\\!\\!\\begin{array}{c c}{1}&{!}\\\\ {\\mathbf{\\dot{x}}_{b}}\\end{array}\\!\\!\\right]}{\\small\\left[\\!\\!\\begin{array}{c}{x_{a}}\\\\ {\\mathbf{\\dot{x}}_{b}}\\end{array}\\!\\!\\right]}}\\end{array}\n$$  \n\nwhere $A_{a a}=$ scalar  \n\n$$\n\\begin{array}{r l}&{\\mathbf{A}_{a b}=1\\times\\left(n\\,-\\,1\\right)\\mathrm{matrix}}\\\\ &{\\mathbf{A}_{b a}=\\left(n\\,-\\,1\\right)\\times1\\,\\mathrm{matrix}}\\\\ &{\\mathbf{A}_{b b}=\\left(n\\,-\\,1\\right)\\times\\left(n\\,-\\,1\\right)\\mathrm{matrix}}\\\\ &{\\;\\;B_{a}=\\mathrm{scalar}}\\\\ &{\\mathbf{B}_{b}=\\left(n\\,-\\,1\\right)\\times1\\,\\mathrm{matrix}}\\end{array}\n$$  \n\nFrom Equation (10–81), the equation for the measured portion of the state becomes  \n\n$$\n\\dot{x}_{a}=A_{a a}x_{a}+\\mathbf{A}_{a b}\\mathbf{x}_{b}\\,+\\,B_{a}u\n$$  \n\nor  \n\n$$\n{\\dot{x}}_{a}\\,-\\,A_{a a}x_{a}\\,-\\,B_{a}u\\,=\\,{\\bf A}_{a b}\\,{\\bf x}_{b}\n$$  \n\nThe terms on the left-hand side of Equation (10–83) can be measured. Equation (10–83) acts as the output equation. In designing the minimum-order observer, we consider the left-hand side of Equation (10–83) to be known quantities.Thus,Equation (10–83) relates the measurable quantities and unmeasurable quantities of the state.  \n\nFrom Equation (10–81), the equation for the unmeasured portion of the state becomes  \n\n$$\n\\dot{\\mathbf{x}}_{b}=\\mathbf{A}_{b a}x_{a}+\\mathbf{A}_{b b}\\mathbf{x}_{b}+\\mathbf{B}_{b}u\n$$  \n\nNoting that terms ${\\bf A}_{b a}x_{a}$ and $\\mathbf{B}_{b}u$ are known quantities, Equation (10–84) describes the dynamics of the unmeasured portion of the state.  \n\nIn what follows we shall present a method for designing a minimum-order observer. The design procedure can be simplified if we utilize the design technique developed for the full-order state observer.  \n\nLet us compare the state equation for the full-order observer with that for the minimum-order observer.The state equation for the full-order observer is  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nand the “state equation” for the minimum-order observer is  \n\n$$\n\\dot{\\mathbf{x}}_{b}=\\mathbf{A}_{b b}\\mathbf{x}_{b}+\\mathbf{A}_{b a}x_{a}+\\mathbf{B}_{b}u\n$$  \n\nThe output equation for the full-order observer is  \n\n$$\ny=\\mathbf{C}\\mathbf{x}\n$$  \n\nand the “output equation” for the minimum-order observer is  \n\n$$\n{\\dot{x}}_{a}\\,-\\,A_{a a}x_{a}\\,-\\,B_{a}u\\,=\\,{\\bf A}_{a b}\\,{\\bf x}_{b}\n$$  \n\nThe design of the minimum-order observer can be carried out as follows: First, note that the observer equation for the full-order observer was given by Equation (10–57), which we repeat here:  \n\n$$\n\\dot{\\tilde{\\textbf{x}}}=\\left(\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{K}_{e}\\mathbf{C}\\right)\\widetilde{\\mathbf{x}}\\ +\\mathbf{B}u\\,+\\,\\mathbf{K}_{e}y\n$$  \n\nThen, making the substitutions of Table 10–1 into Equation (10–85), we obtain  \n\n$$\n\\begin{array}{r}{\\dot{\\widetilde{\\mathbf{x}}}_{\\,b}=\\left(\\mathbf{A}_{b b}\\mathbf{\\Phi}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right)\\widetilde{\\mathbf{x}}_{\\,b}+\\mathbf{A}_{b a}x_{a}+\\mathbf{B}_{b}u\\,+\\mathbf{K}_{e}\\big(\\dot{x}_{a}-\\mathbf{\\Phi}A_{a a}x_{a}-\\,B_{a}u\\big)}\\end{array}\n$$  \n\nwhere the state observer gain matrix $\\mathbf{K}_{e}$ is an $(n\\mathrm{~-~}1)\\times1$ matrix. In Equation (10–86), notice that in order to estimate $\\widetilde{\\mathbf{x}}_{\\,b}$ ,we need the derivative of $x_{a}$ .This presents a difficulty, because differentiation amplifies noise. If $x_{a}\\left(=y\\right)$ is noisy, the use of $\\dot{x}_{a}$ is unacceptable.  \n\nTable 10–1 List of Necessary Substitutions for Writing the Observer Equation for the Minimum-Order State Observer   \n\n![](images/6c6c2b5567bebcd04a7243dd657e1295e176c1805365b37d9c25935259f1365f.jpg)  \n\nTo avoid this difficulty, we eliminate $\\dot{x}_{a}$ in the following way. First rewrite Equation (10–86) as  \n\n$$\n\\begin{array}{r}{\\tilde{\\mathbf{x}}_{b}-\\mathbf{K}_{e}\\dot{x}_{a}=\\left(\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right)\\tilde{\\mathbf{x}}_{b}+\\left(\\mathbf{A}_{b a}-\\mathbf{K}_{e}A_{a a}\\right)y+\\left(\\mathbf{B}_{b}-\\mathbf{K}_{e}B_{a}\\right)\\boldsymbol{u}}\\\\ {=\\left(\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right)\\(\\tilde{\\mathbf{x}}_{b}-\\mathbf{K}_{e}y)\\quad}\\\\ {+\\left[\\left(\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right)\\mathbf{K}_{e}+\\mathbf{A}_{b a}-\\mathbf{K}_{e}A_{a a}\\right]y\\quad}\\\\ {+\\left(\\mathbf{B}_{b}-\\mathbf{K}_{e}B_{a}\\right)\\!\\right)\\quad}\\end{array}\n$$  \n\nDefine  \n\nand  \n\n$$\n\\begin{array}{r l}&{\\mathbf{x}_{b}-\\mathbf{K}_{e}y=\\mathbf{x}_{b}-\\mathbf{K}_{e}x_{a}=\\pmb{\\eta}}\\\\ &{}\\\\ &{\\widetilde{\\mathbf{x}}_{\\,b}-\\mathbf{K}_{e}y=\\widetilde{\\mathbf{x}}_{\\,b}-\\mathbf{K}_{e}x_{a}=\\widetilde{\\pmb{\\eta}}}\\end{array}\n$$  \n\nThen Equation (10–87) becomes  \n\n$$\n\\begin{array}{r l}&{\\dot{\\widetilde{\\pmb{\\eta}}}=\\big(\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\big)\\widetilde{\\pmb{\\eta}}\\,+\\big[\\big(\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\big)\\mathbf{K}_{e}}\\\\ &{\\qquad+\\mathbf{A}_{b a}-\\mathbf{K}_{e}\\boldsymbol{A}_{a a}\\big]y\\,+\\big(\\mathbf{B}_{b}-\\mathbf{K}_{e}\\boldsymbol{B}_{a}\\big)\\boldsymbol{u}}\\end{array}\n$$  \n\nDefine  \n\n$$\n\\begin{array}{l}{\\hat{\\mathbf{A}}=\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}}\\\\ {\\hat{\\mathbf{B}}=\\hat{\\mathbf{A}}\\mathbf{K}_{e}+\\mathbf{A}_{b a}-\\mathbf{K}_{e}A_{a a}}\\\\ {\\hat{\\mathbf{F}}=\\mathbf{B}_{b}-\\mathbf{K}_{e}B_{a}}\\end{array}\n$$  \n\nThen Equation (10–89) becomes  \n\n$$\n\\pmb{\\dot{\\eta}}=\\hat{\\mathbf{A}}\\pmb{\\widetilde{\\eta}}+\\hat{\\mathbf{B}}y\\,+\\,\\hat{\\mathbf{F}}u\n$$  \n\nEquation (10–90) and Equation (10–88) together define the minimum-order observer. Since  \n\n$$\n\\begin{array}{l}{{\\displaystyle{y=\\left[1\\;\\;\\vdots\\;\\;{\\bf0}\\right]\\left[\\frac{x_{a}}{\\bf x}\\right]}\\atop{\\displaystyle{\\bf x}=\\left[\\frac{x_{a}}{\\widetilde{\\bf x}_{b}}\\right]=\\left[\\frac{y}{\\widetilde{\\bf x}_{b}}\\right]=\\left[\\frac{\\bf0}{\\bf I}\\right]\\left[\\widetilde{\\bf x}_{b}-{\\bf K}_{e}y\\right]+\\Big[\\frac{1}{\\bf K}_{e}\\Big]y}}\\end{array}\n$$  \n\nwhere $\\mathbf{0}$ is a row vector consisting of $(n\\mathrm{~-~}1)$ zeros, if we define  \n\n$$\n\\hat{\\mathbf{C}}=\\left[\\frac{\\mathbf{0}}{\\mathbf{I}_{n-1}}\\right]\\!,\\qquad\\hat{\\mathbf{D}}=\\left[\\frac{1}{\\mathbf{K}_{e}}\\right]\n$$  \n\nthen we can write $\\widetilde{\\mathbf{x}}$ in terms of $\\widetilde{\\pmb{\\eta}}$ and $y$ as follows:  \n\n$$\n\\widetilde{\\textbf{x}}=\\hat{\\mathbf{C}}\\widetilde{\\pmb{\\eta}}+\\hat{\\mathbf{D}}y\n$$  \n\nThis equation gives the transformation from $\\widetilde{\\pmb{\\eta}}$ to $\\widetilde{\\mathbf{x}}$  \n\nFigure 10–17 shows the block diagram of the observed-state feedback control system with the minimum-order observer,based on Equations (10–79),(10–80),(10–90),(10–91) and $u=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}$ .  \n\nNext we shall derive the observer error equation. Using Equation (10–83), Equation (10–86) can be modified to  \n\n$$\n\\begin{array}{r}{\\tilde{\\mathbf{x}}_{\\,b}=\\left(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right)\\tilde{\\mathbf{x}}_{\\,b}+\\mathbf{A}_{b a}x_{a}\\,+\\,\\mathbf{B}_{b}u\\,+\\,\\mathbf{K}_{e}\\mathbf{A}_{a b}\\mathbf{x}_{b}}\\end{array}\n$$  \n\n![](images/09024ceba384d3e6c5b55e60f01d53e045c71967d87d89093ac3d59c974ff476.jpg)  \nFigure 10–17 System with observed-state feedback, where the observer is the minimum-order observer.   \nTransformation  \n\nBy subtracting Equation (10–92) from Equation (10–84), we obtain  \n\n$$\n\\dot{\\mathbf{x}}_{b}\\,-\\,\\stackrel{\\cdot}{\\mathbf{x}}_{b}=\\bigl({\\mathbf{A}}_{b b}\\,-\\,{\\mathbf{K}}_{e}\\,{\\mathbf{A}}_{a b}\\bigr)(\\mathbf{x}_{b}\\,-\\,\\widetilde{\\mathbf{x}}\\,_{b})\n$$  \n\nDefine  \n\n$$\n\\mathbf{e}\\,=\\,\\mathbf{x}_{b}\\,-\\,\\widetilde{\\mathbf{x}}\\,_{b}\\,=\\,\\pmb{\\eta}\\,-\\,\\widetilde{\\pmb{\\eta}}\n$$  \n\nThen Equation (10–93) becomes  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\mathbf{e}\n$$  \n\nThis is the error equation for the minimum-order observer. Note that eis an $(n\\mathrm{~-~}1)$ -vector.  \n\nThe error dynamics can be chosen as desired by following the technique developed for the full-order observer, provided that the rank of matrix  \n\n$$\n\\begin{array}{r}{\\ddot{\\mathbf{\\beta}}\\ \\ \\ \\ \\mathbf{A}_{a b}\\ \\ \\ \\ }\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}\\ \\ \\ }\\\\ {\\dot{\\mathbf{\\beta}}\\ \\ \\ \\ \\ }\\\\ {\\cdot\\mathbf{\\beta}}\\\\ {\\cdot\\mathbf{\\beta}}\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}^{n-2}}\\end{array}\n$$  \n\nis $n\\,-\\,1,$ .(This is the complete observability condition applicable to the minimum-order observer.)  \n\nThe characteristic equation for the minimum-order observer is obtained from Equation (10–94) as follows:  \n\n$$\n{\\begin{array}{r l}{\\left|s\\mathbf{I}-\\mathbf{A}_{b b}+\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right|={\\big(}s-\\mu_{1}{\\big)}(s-\\mu_{2})\\cdots{\\big(}s-\\mu_{n-1}{\\big)}}&{}\\\\ {=s^{n-1}+{\\hat{\\alpha}}_{1}s^{n-2}+\\cdots+{\\hat{\\alpha}}_{n-2}s+{\\hat{\\alpha}}_{n-1}=0}\\end{array}}\n$$  \n\nwhere $\\mu_{1},\\mu_{2},\\ldots,\\mu_{n-1}$ are desired eigenvalues for the minimum-order observer. The observer gain matrix $\\mathbf{K}_{e}$ can be determined by first choosing the desired eigenvalues for the minimum-order observer [that is, by placing the roots of the characteristic equation, Equation (10–95), at the desired locations] and then using the procedure developed for the full-order observer with appropriate modifications. For example, if the formula for determining matrix $\\mathbf{K}_{e}$ given by Equation (10–61) is to be used, it should be modified to  \n\n$$\n\\mathbf{K}_{e}=\\hat{\\mathbf{Q}}\\left[\\begin{array}{c}{\\hat{\\alpha}_{n-1}-\\hat{a}_{n-1}}\\\\ {\\hat{\\alpha}_{n-2}-\\hat{a}_{n-2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{\\alpha}_{1}-\\hat{a}_{1}}\\end{array}\\right]=\\left(\\hat{\\mathbf{W}}\\hat{\\mathbf{N}}^{*}\\right)^{-1}\\left[\\begin{array}{c}{\\hat{\\alpha}_{n-1}-\\hat{a}_{n-1}}\\\\ {\\hat{\\alpha}_{n-2}-\\hat{a}_{n-2}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\hat{\\alpha}_{1}-\\hat{a}_{1}}\\end{array}\\right]\n$$  \n\nwhere $\\mathbf{K}_{e}$ is an $(n\\mathrm{~-~}1)\\mathrm{~}\\!\\times\\!\\ 1$ matrix and  \n\n$$\n\\begin{array}{r l}&{\\hat{\\mathbf{N}}=\\left[\\mathbf{A}_{a b}^{\\dagger}\\mathbf{\\Lambda}^{\\dagger}\\mathbf{A}_{b b}^{\\dagger}\\mathbf{A}_{a b}^{\\phantom{\\dagger}}\\mathbf{\\Lambda}^{\\dagger}\\cdots\\mathbf{\\Lambda}\\right]\\,\\left(\\mathbf{A}_{b b}^{\\dagger}\\right)^{n-2}\\mathbf{A}_{a b}^{\\dagger}\\mathbf{\\Lambda}^{\\dagger}=\\left(n-1\\right)\\times\\left(n-1\\right)\\mathbf{n}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\quad\\quad\\times\\left[\\begin{array}{l l l l l}{\\hat{a}_{n-2}}&{\\hat{a}_{n-3}}&{\\cdots}&{\\hat{a}_{1}}&{1}\\\\ {\\hat{a}_{n-3}}&{\\hat{a}_{n-4}}&{\\cdots}&{1}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\hat{a}_{1}}&{1}&{\\cdots}&{0}&{0}\\\\ {1}&{0}&{\\cdots}&{0}&{0}\\end{array}\\right]}\\end{array}\n$$  \n\nNote that ${\\hat{a}}_{1},{\\hat{a}}_{2},\\ldots,{\\hat{a}}_{n-2}$ are coefficients in the characteristic equation for the state equation  \n\n$$\n\\left|s\\mathbf{I}\\right.-\\left.\\mathbf{A}_{b b}\\right|=s^{n-1}+\\left.\\hat{a}_{1}s^{n-2}+\\cdots+\\left.\\hat{a}_{n-2}s\\right.+\\left.\\hat{a}_{n-1}=0\n$$  \n\nAlso, if Ackermann’s formula given by Equation (10–65) is to be used, then it should be modified to  \n\n$$\n\\mathbf{K}_{e}=\\phi\\big(\\mathbf{A}_{b b}\\big)\\left[\\begin{array}{c}{\\mathbf{A}_{a b}}\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}^{n-3}}\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}^{n-2}}\\end{array}\\right]^{-1}\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {1}\\end{array}\\right]\n$$  \n\nwhere  \n\n$$\n\\phi\\big(\\mathbf{A}_{b b}\\big)=\\,\\mathbf{A}_{b b}^{n-1}\\,+\\,\\hat{\\alpha}_{1}\\mathbf{A}_{b b}^{n-2}\\,+\\,\\cdots\\,+\\,\\hat{\\alpha}_{n-2}\\mathbf{A}_{b b}\\,+\\,\\hat{\\alpha}_{n-1}\\mathbf{I}\n$$  \n\nObserved-State Feedback Control System with Minimum-Order Observer. For the case of the observed-state feedback control system with full-order state observer, we have shown that the closed-loop poles of the observed-state feedback control system consist of the poles due to the pole-placement design alone, plus the poles due to the observer design alone. Hence, the pole-placement design and the full-order observer design are independent of each other.  \n\nFor the observed-state feedback control system with minimum-order observer, the same conclusion applies.The system characteristic equation can be derived as  \n\n$$\n\\bigl|s\\mathbf{I}\\,-\\,\\mathbf{A}\\,+\\,\\mathbf{BK}\\bigl||s\\mathbf{I}\\,-\\,\\mathbf{A}_{b b}\\,+\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr|\\,=\\,0\n$$  \n\n(See Problem back control system with a minimum-order observer comprise the closed-loop poles due to pole placement A–10–11 for the details.) The closed-loop poles of the observed-state feedCthe eigenvalues of matrix C$\\left(\\mathbf{A}-\\mathbf{\\deltaBK}\\right)]$ a Dp poles due to the minimum-order observer the eigenvalues of matrix ( $(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b})\\big]$ .Therefore, the pole-placement design and the design of the minimum-order observer are independent of each other.  \n\nDetermining Observer Gain Matrix $\\upkappa_{e}$ with MATLAB. Because of the duality of pole-placement and observer design, the same algorithm can be applied to both the pole-placement problem and the observer-design problem. Thus, the commands acker and place can be used to determine the observer gain matrix $\\mathbf{K}_{e}$ .  \n\nThe closed-loop poles of the observer are the eigenvalues of matrix $\\mathbf{A}-\\mathbf{\\delta}\\mathbf{K}_{e}\\,\\mathbf{\\(}$ C.The closed-loop poles of the pole-placement are the eigenvalues of matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ .  \n\nReferring to the duality problem between the pole-placement problem and observerdesign problem,we can determine $\\mathbf{K}_{e}$ by considering the pole-placement problem for the dual system. That is, we determine $\\mathbf{K}_{e}$ by placing the eigenvalues of $\\mathbf{A}^{*}\\mathbf{\\Lambda}-\\mathbf{C}^{*}\\mathbf{K}_{e}$ at the desired place. Since ${\\bf K}_{e}={\\bf K}^{*}$ , for the full-order observer we use the command  \n\n$$\n\\mathsf{K}_{\\mathrm{e}}=\\mathsf{a c k e r}(\\mathsf{A}^{\\prime},\\mathsf{C}^{\\prime},\\mathsf{L})^{\\prime}\n$$  \n\nwhere Lis the vector of the desired eigenvalues for the observer. Similarly, for the fullorder observer, we may use  \n\n$$\n\\mathsf{K}_{\\mathrm{e}}=\\mathsf{p l a c e}(\\mathsf{A}^{\\prime},\\mathsf{C}^{\\prime},\\mathsf{L})^{\\prime}\n$$  \n\nprovided Ldoes not include multiple poles. [In the above commands, prime ( ') indicates the transpose.] For the minimum-order (or reduced-order) observers, use the following commands:  \n\n$$\n\\mathsf{K}_{\\mathrm{e}}=\\mathsf{a c k e r}(\\mathsf{A b b^{\\prime},A a b^{\\prime},L})^{\\mathrm{t}}\n$$  \n\nor  \n\n$$\n\\mathsf{K}_{\\mathrm{e}}=\\mathsf{p l a c e}(\\mathsf{A b b^{\\prime},A a b^{\\prime},L})^{\\mathrm{t}}\n$$  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\qquad\\mathbf{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1\\quad0\\quad0]\n$$  \n\nLet us assume that we want to place the closed-loop poles at  \n\n$$\ns_{1}=-2\\,+\\,j2\\sqrt{3}\\,,\\qquad s_{2}=-2\\,-\\,j2\\sqrt{3}\\,,\\qquad s_{3}=-6\n$$  \n\nThen the necessary state-feedback gain matrix $\\mathbf{K}$ can be obtained as follows:  \n\n$$\n\\mathbf{K}=[90\\quad29\\quad4]\n$$  \n\n(See MATLAB Program 10–10 for a MATLAB computation of this matrix $\\mathbf{K}$ .)  \n\nNext, let us assume that the output $y$ can be measured accurately so that state variable $x_{1}$ (which is equal to $y$ ) need not be estimated. Let us design a minimum-order observer. (The minimum-order observer is of second order.) Assume that we choose the desired observer poles to be at  \n\n$$\ns\\,=\\,-10,\\qquad s\\,=\\,-10\n$$  \n\nReferring to Equation (10–95), the characteristic equation for the minimum-order observer is  \n\n$$\n\\begin{array}{r l}{\\vert s\\mathbf{I}-\\mathbf{A}_{b b}+\\mathbf{K}_{e}\\mathbf{A}_{a b}\\vert=(s\\mathbf{\\Lambda}-\\mu_{1})(s\\mathbf{\\Lambda}-\\mu_{2})}&{}\\\\ {\\mathbf{\\Lambda}=(s\\mathbf{\\Lambda}+10)(s\\mathbf{\\Lambda}+10)}&{}\\\\ {\\mathbf{\\Lambda}=s^{2}+20s\\mathbf{\\Lambda}+100=0}\\end{array}\n$$  \n\nIn what follows, we shall use Ackermann’s formula given by Equation (10–97).  \n\n$$\n\\mathbf{K}_{e}=\\phi\\bigl(\\mathbf{A}_{b b}\\bigr)\\left[\\begin{array}{l}{\\mathbf{A}_{a b}}\\\\ {\\dots}\\\\ {\\mathbf{A}_{a b}\\mathbf{A}_{b b}}\\end{array}\\right]^{-1}\\!\\!\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]\n$$  \n\nwhere  \n\n$$\n\\phi\\big(\\mathbf{A}_{b b}\\big)=\\,\\mathbf{A}_{b b}^{2}\\,+\\,\\hat{\\alpha}_{1}\\,\\mathbf{A}_{b b}\\,+\\,\\hat{\\alpha}_{2}\\,\\mathbf{I}\\,=\\,\\mathbf{A}_{b b}^{2}\\,+\\,20\\mathbf{A}_{b b}\\,+\\,100\\mathbf{I}\n$$  \n\nSince  \n\n$$\n\\widetilde{\\mathbf{x}}\\,=\\,\\left[\\!\\!\\begin{array}{c}{x_{a}}\\\\ {\\widetilde{\\mathbf{x}}_{b}}\\end{array}\\!\\!\\right]\\,=\\,\\left[\\!\\!\\begin{array}{c}{x_{1}}\\\\ {\\ldots}\\\\ {\\widetilde{x}_{2}}\\\\ {\\widetilde{x}_{3}}\\end{array}\\!\\!\\right]\\!,\\qquad\\mathbf{A}\\,=\\,\\left[\\!\\!\\begin{array}{c c}{0\\,\\Big|\\quad1}&{0}\\\\ {0\\,\\Big|\\quad0}&{1}\\\\ {-6\\,\\Big|\\quad-11}&{-6}\\end{array}\\!\\!\\right]\\!,\\qquad\\mathbf{B}\\,=\\,\\left[\\!\\!\\begin{array}{c}{0}\\\\ {\\ldots}\\\\ {0}\\\\ {1}\\end{array}\\!\\!\\right]\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}&{A_{a a}=0,\\quad\\quad{\\bf A}_{a b}=[1\\quad0],\\quad\\quad{\\bf A}_{b a}=\\left[\\begin{array}{l}{\\phantom{-}0}\\\\ {-6}\\end{array}\\right]}\\\\ &{{\\bf A}_{b b}=\\left[\\begin{array}{l l}{\\phantom{-}0\\quad1}\\\\ {-11\\quad-6}\\end{array}\\right],\\quad\\quad B_{a}=0,\\quad\\quad{\\bf B}_{b}=\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\\end{array}\n$$  \n\nChapter 10 /Control Systems Design in State Space  \n\nEquation (10–99) now becomes  \n\n$$\n\\begin{array}{r l}&{\\mathbf{K}_{e}=\\left\\{\\left[\\begin{array}{l l}{\\phantom{-}0}&{1}\\\\ {-11}&{-6\\biggr]^{2}+20\\left[\\phantom{-}0}&{1}\\\\ {-11}&{-6\\biggr]+100\\left[\\phantom{-}\\!\\!\\!1\\right]+100\\left[\\phantom{-}\\!\\!\\!1\\right]+12\\right]\\right\\}\\left[\\phantom{-}1\\!\\!\\!1\\right]^{-1}\\!\\left[0\\right]^{-1}\\!\\left[1\\right]}\\\\ &{\\phantom{-}=\\left[\\phantom{-}89\\right.\\!\\!14\\left[\\phantom{-}\\!\\!\\!1\\right]\\left[0\\right]=\\left[\\phantom{-}14\\right]}\\end{array}\n$$  \n\n(A MATLAB computation of this $\\mathbf{K}_{e}$ is given in MATLAB Program 10–10.)  \n\n![](images/e4ac0e263ad0c7fbb6d87d77da88d9a394be2b038cd82ae39dc61f5745504891.jpg)  \n\nReferring to Equations (10–88) and (10–89),the equation for the minimum-order observer can be given by  \n\n$\\hat{\\tilde{\\eta}}\\,=\\,\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\tilde{\\eta}\\,+\\,\\bigl[\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\mathbf{K}_{e}\\,+\\,\\mathbf{A}_{b a}\\,-\\,\\mathbf{K}_{e}A_{a a}\\bigr]y\\,+\\,\\bigl(\\mathbf{B}_{b}\\,-\\,\\mathbf{K}_{e}B_{a}\\bigr)u$ A BCA BDA B(10–100) where  \n\n$$\n\\widetilde{\\pmb{\\eta}}\\;=\\;\\widetilde{\\mathbf{x}}_{\\,b}\\,-\\,\\mathbf{K}_{e}y\\,=\\,\\widetilde{\\mathbf{x}}_{\\,b}\\,-\\,\\mathbf{K}_{e}x_{1}\n$$  \n\nNoting that  \n\n$$\n\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}={\\left[\\begin{array}{l l}{\\phantom{-}0}&{\\phantom{-}1}\\\\ {-11}&{-6}\\end{array}\\right]}-{\\left[\\begin{array}{l}{14}\\\\ {5}\\end{array}\\right]}[1}&{0]={\\left[\\begin{array}{l l}{-14}&{1}\\\\ {-16}&{-6}\\end{array}\\right]}\n$$  \n\nthe equation for the minimum-order observer, Equation (10–100), becomes  \n\n$$\n\\begin{array}{r l}&{\\left[\\!\\!\\begin{array}{c}{\\dot{\\tilde{\\eta}}_{2}}\\\\ {\\dot{\\tilde{\\eta}}_{3}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c}{-14}&{1}\\\\ {-16}&{-6}\\end{array}\\!\\!\\right]\\!\\left[\\!\\begin{array}{c}{\\widetilde{\\eta}_{2}}\\\\ {\\widetilde{\\eta}_{3}}\\end{array}\\!\\!\\right]+\\left\\{\\!\\!\\left[\\!\\begin{array}{c c}{-14}&{1}\\\\ {-16}&{-6}\\end{array}\\!\\!\\right]\\!\\left[\\!\\begin{array}{c}{14}\\\\ {5}\\end{array}\\!\\!\\right]\\!\\right.}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\!\\left[\\!\\begin{array}{c}{0}\\\\ {-6}\\end{array}\\!\\!\\right]-\\left[\\!\\!\\begin{array}{c}{14}\\\\ {5}\\end{array}\\!\\!\\right]\\!\\!0\\right\\}\\!\\!,}\\end{array}\n$$  \n\nor  \n\n$$\n\\begin{array}{r}{{\\left[\\!\\!\\begin{array}{l}{\\dot{\\widetilde{\\eta}}_{2}}\\\\ {\\dot{\\widetilde{\\eta}}_{3}}\\end{array}\\!\\!\\right]}={\\left[\\!\\!\\begin{array}{l l}{-14}&{1}\\\\ {-16}&{-6}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l}{\\widetilde{\\eta}_{2}}\\\\ {\\widetilde{\\eta}_{3}}\\end{array}\\!\\!\\right]}+{\\left[\\!\\!\\begin{array}{l}{-191}\\\\ {-260}\\end{array}\\!\\!\\right]}y+{\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right]}u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\left[\\begin{array}{l}{\\widetilde{\\eta}_{2}}\\\\ {\\widetilde{\\eta}_{3}}\\end{array}\\right]=\\left[\\begin{array}{l}{\\widetilde{x}_{2}}\\\\ {\\widetilde{x}_{3}}\\end{array}\\right]-\\mathbf{K}_{e}y\n$$  \n\nor  \n\n$$\n\\left[\\begin{array}{l}{\\widetilde{x}_{2}}\\\\ {\\widetilde{x}_{3}}\\end{array}\\right]=\\left[\\begin{array}{l}{\\widetilde{\\eta}_{2}}\\\\ {\\widetilde{\\eta}_{3}}\\end{array}\\right]+\\mathbf{K}_{e}x_{1}\n$$  \n\nIf the observed-state feedback is used, then the control signal $u$ becomes  \n\n$$\nu=-\\mathbf{K}\\widetilde{\\mathbf{x}}\\,=-\\mathbf{K}\\Bigg[\\begin{array}{l}{x_{1}}\\\\ {\\widetilde{x}_{2}}\\\\ {\\widetilde{x}_{3}}\\end{array}\\Bigg]\n$$  \n\nwhere $\\mathbf{K}$ is the state feedback gain matrix. Figure 10–18 is a block diagram showing the configuration of the system with observed-state feedback, where the observer is the minimum-order observer.  \n\n![](images/a0e61c48d26c85e56acd10d56da3bbaa8bed2f95268d12d7b789cdca4c4dc6c6.jpg)  \nTransformation  \n\n# Figure 10–18  \n\nSystem with observed state feedback, where the observer is the minimum-order observer designed in Example 10–8.  \n\nTransfer Function of Minimum-Order Observer-Based Controller. In the minimum-order observer equation given by Equation (10–89):  \n\n$$\n\\overset{\\cdot}{\\underset{\\r{j}}{=}}\\,=\\big(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\mathbf{A}_{a b}\\big)\\widetilde{\\pmb{\\eta}}\\,+\\,\\big[\\big(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\mathbf{A}_{a b}\\big)\\mathbf{K}_{e}\\,+\\,\\mathbf{A}_{b a}\\,-\\,\\mathbf{K}_{e}\\mathbf{A}_{a a}\\big]y\\,+\\,\\big(\\mathbf{B}_{b}\\,-\\,\\mathbf{K}_{e}B_{a b}\\big)\\mathbf{K}_{e}\\,-\\,\\mathbf{K}_{e}\\mathbf{A}_{a b}\\big)\\widetilde{\\mathbf{T}}\\,.\n$$  \n\ndefine, similar to the case of the derivation of Equation (10–90),  \n\n$$\n\\begin{array}{l}{\\hat{\\mathbf{A}}=\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}}\\\\ {\\hat{\\mathbf{B}}=\\hat{\\mathbf{A}}\\mathbf{K}_{e}+\\mathbf{A}_{b a}-\\mathbf{K}_{e}A_{a a}}\\\\ {\\hat{\\mathbf{F}}=\\mathbf{B}_{b}-\\mathbf{K}_{e}B_{a}}\\end{array}\n$$  \n\nThen, the following three equations define the minimum-order oberver:  \n\n$$\n\\begin{array}{l}{\\dot{\\widetilde{\\pmb{\\eta}}}=\\hat{\\bf A}\\,\\widetilde{\\pmb{\\eta}}\\,+\\,\\hat{\\bf B}\\,y\\,+\\,\\hat{\\bf F}u}\\\\ {\\widetilde{\\pmb{\\eta}}=\\,\\widetilde{\\bf x}\\,_{b}\\,-\\,{\\bf K}_{e}y}\\\\ {u=-{\\bf K}\\widetilde{\\bf x}}\\end{array}\n$$  \n\nSince Equation (10–103) can be rewritten as  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\boldsymbol u}\\,=-{\\bf K}\\,\\widetilde{{\\bf x}}\\,=\\,-\\big[K_{a}\\quad{\\bf K}_{b}\\big]\\!\\!\\left[\\!\\!\\begin{array}{c}{{y}}\\\\ {{\\widetilde{{\\bf x}}\\,_{b}}}\\end{array}\\!\\!\\right]\\,=\\,-K_{a}y\\,-\\,{\\bf K}_{b}\\,\\widetilde{{\\bf x}}\\,_{b}\\,}}\\\\ {{\\displaystyle{\\mathrm{\\boldmath~\\Lambda~}}=\\,-{\\bf K}_{b}\\widetilde{\\eta}\\,-\\,\\big(K_{a}\\,+\\,{\\bf K}_{b}\\,{\\bf K}_{e}\\big)y}}\\end{array}\n$$  \n\nby substituting Equation (10–104) into Equation (10–101), we obtain  \n\n$$\n\\begin{array}{r l}&{\\dot{\\widetilde{\\pmb{\\eta}}}=\\hat{\\bf A}\\,\\widetilde{\\pmb{\\eta}}\\,+\\,\\hat{\\bf B}\\,y\\,+\\,\\hat{\\bf F}\\big[\\!-\\!{\\bf K}_{b}\\widetilde{\\pmb{\\eta}}\\,-\\,\\big(K_{a}\\,+\\,{\\bf K}_{b}{\\bf K}_{e}\\big)y\\big]}\\\\ &{\\quad=\\big(\\hat{\\bf A}\\,-\\,\\hat{\\bf F}{\\bf K}_{b}\\big)\\widetilde{\\pmb{\\eta}}\\,+\\,\\big[\\hat{\\bf B}\\,-\\,\\hat{\\bf F}\\big(K_{a}\\,+\\,{\\bf K}_{b}{\\bf K}_{e}\\big)\\big]y}\\end{array}\n$$  \n\nDefine  \n\n$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf{A}}=\\widehat{\\mathbf{A}}\\,-\\,\\widehat{\\mathbf{F}}\\mathbf{K}_{b}}\\\\ &{\\widetilde{\\mathbf{B}}=\\widehat{\\mathbf{B}}\\,-\\,\\widehat{\\mathbf{F}}\\big(X_{a}\\,+\\,\\mathbf{K}_{b}\\mathbf{K}_{e}\\big)}\\\\ &{\\widetilde{\\mathbf{C}}\\,=-\\mathbf{K}_{b}}\\\\ &{\\widetilde{D}\\,=-\\big(K_{a}\\,+\\,\\mathbf{K}_{b}\\mathbf{K}_{e}\\big)}\\end{array}\n$$  \n\nThen Equations (10–105) and (10–104) can be written as  \n\n$$\n\\begin{array}{r}{\\dot{\\pmb{\\eta}}=\\widetilde{\\mathbf{A}}\\,\\widetilde{\\pmb{\\eta}}+\\widetilde{\\mathbf{B}}\\,y}\\\\ {u=\\widetilde{\\mathbf{C}}\\,\\widetilde{\\pmb{\\eta}}+\\widetilde{D}y}\\end{array}\n$$  \n\nEquations (10–106) and (10–107) define the minimum-order observer-based controller. By considering $u$ as the output and $-y$ as the input, $U(s)$ can be written as  \n\n$$\n\\begin{array}{r l}&{U(s)=\\big[\\widetilde{\\mathbf{C}}\\big(s\\mathbf{I}-\\widetilde{\\mathbf{A}}\\big)^{-1}\\widetilde{\\mathbf{B}}\\,+\\,\\widetilde{D}\\big]Y(s)}\\\\ &{\\qquad\\quad=-\\big[\\widetilde{\\mathbf{C}}\\big(s\\mathbf{I}-\\widetilde{\\mathbf{A}}\\big)^{-1}\\widetilde{\\mathbf{B}}\\,+\\,\\widetilde{D}\\big][-Y(s)]}\\end{array}\n$$  \n\nSince the input to the observer controller is $-Y(s)$ ,rather than $Y(s)$ ,the transfer function of the observer controller is  \n\n$$\n\\frac{U(s)}{-Y(s)}=\\frac{\\mathrm{num}}{\\mathrm{den}}=-\\big[\\widetilde{\\mathbf{C}}\\big(s\\mathbf{I}-\\widetilde{\\textbf{A}}\\big)^{-1}\\widetilde{\\mathbf{B}}\\,+\\,\\widetilde{D}\\big]\n$$  \n\nThis transfer function can be easily obtained by using the following MATLAB statement:  \n\n$$\n[\\mathsf{n u m},\\mathsf{d e n}]=\\mathsf{s s2t f}(\\mathsf{A t i l d e},\\,\\mathsf{B t i l d e},\\,\\mathsf{-C t i l d e},\\,\\mathsf{-D t i l d e})\n$$  \n\nIn this section we shall consider a problem of designing regulator systems by using the pole-placement-with-observer approach.  \n\nConsider the regulator system shown in Figure 10–19. (The reference input is zero.) The plant transfer function is  \n\n$$\nG(s)=\\frac{10(s\\,+\\,2)}{s(s\\,+\\,4)(s\\,+\\,6)}\n$$  \n\nUsing the pole-placement approach, design a controller such that when the system is subjected to the following initial condition:  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]},\\qquad\\mathbf{e}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]}\n$$  \n\nwhere $\\mathbf{X}$ is the state vector for the plant and eis the observer error vector, the maximum undershoot of $y(t)$ is 25 to $35\\%$ and the settling time is about 4 sec.Assume that we use the minimum-order observer. (We assume that only the output $y$ is measurable.)  \n\nWe shall use the following design procedure:  \n\n1. Derive a state-space model of the plant.   \n2. Choose the desired closed-loop poles for pole placement. Choose the desired observer poles.   \n3. Determine the state feedback gain matrix $\\mathbf{K}$ and the observer gain matrix $\\mathbf{K}_{e}$ .  \n4. Using the gain matrices Kand $\\mathbf{K}_{e}$ obtained in step 3, derive the transfer function of the observer controller.If it is a stable controller,check the response to the given initial condition. If the response is not acceptable, adjust the closed-loop pole location and/or observer pole location until an acceptable response is obtained.  \n\nDesign step 1: We shall derive the state-space representation of the plant. Since the plant transfer function is  \n\n$$\n\\frac{Y(s)}{U(s)}=\\frac{10(s\\,+\\,2)}{s(s\\,+\\,4)(s\\,+\\,6)}\n$$  \n\nthe corresponding differential equation is  \n\n$$\n\\ddot{y}\\,+\\,10\\ddot{y}\\,+\\,24\\dot{y}\\,=\\,10\\dot{u}\\,+\\,20u\n$$  \n\nReferring to Section 2–5, let us define the state variables $x_{1},x_{2}$ ,and $x_{3}$ as follows:  \n\n![](images/5e2adc2ecb4f60bf1076fe1238e541a28f975d5abff450da46ca1311084f70a7.jpg)  \nFigure 10–19 Regulator system.  \n\nAlso, $\\dot{x}_{3}$ is defined by  \n\n$$\n\\begin{array}{c}{\\dot{x}_{3}=-a_{3}x_{1}\\,-\\,a_{2}x_{2}\\,-\\,a_{1}x_{3}\\,+\\,\\beta_{3}u}\\\\ {=-24x_{2}\\,-\\,10x_{3}\\,+\\,\\beta_{3}u}\\end{array}\n$$  \n\nwhere $\\beta_{0}=0,\\;\\beta_{1}=0,\\;\\beta_{2}=10,$ and $\\beta_{3}=-80$ .  \n\n[See Equation (2–35) for the calculation of $\\beta$ ’s.] Then the state-space equation and output equation can be obtained as  \n\n$$\n\\begin{array}{r l}&{\\left[\\phantom{\\,_{1}}\\!\\!\\!\\dot{x}_{1}\\right]=\\left[\\phantom{\\,_{0}}\\!\\!\\!1\\!\\!\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{\\!\\!\\!-24\\!\\!\\!}&{\\!\\!\\!-10\\!\\!\\!}\\end{array}\\right]\\!\\!\\left[\\phantom{\\,_{1}}\\!\\!\\!x_{1}\\right]+\\left[\\phantom{\\,_{0}}\\!\\!\\!1\\!\\!\\begin{array}{c}{0}\\\\ {10}\\\\ {\\!\\!\\!-80\\!\\!\\!}\\end{array}\\right]\\!\\!\\!u}\\\\ &{\\qquad\\qquad y=[1}&{0}&{0]\\!\\!\\!\\left[\\phantom{\\,_{1}}\\!\\!\\!x_{2}\\right]+[0]u}\\end{array}\n$$  \n\nDesign step 2: As the first trial, let us choose the desired closed-loop poles at  \n\n$$\ns\\,=\\,-1\\,+\\,j2,\\qquad s\\,=\\,-1\\,-\\,j2,\\qquad s\\,=\\,-5\n$$  \n\nand choose the desired observer poles at  \n\n$$\ns\\,=\\,-10,\\qquad s\\,=\\,-10\n$$  \n\nDesign step 3: We shall use MATLAB to compute the state feedback gain matrix $\\mathbf{K}$ and the observer gain matrix $\\mathbf{K}_{e}$ .MATLAB Program 10–11 produces matrices $\\mathbf{K}$ and $\\mathbf{K}_{e}$ .  \n\n# MATLAB Program 10–11  \n\n$\\%$ Obtaining the state feedback gain matrix K   \n$\\mathsf{A}=\\left[0\\;\\;1\\;\\;\\;0;0\\;\\;0\\;\\;1;0\\;\\;-24\\;\\;-1\\,0\\right],$ ;  \n$\\mathsf{B}=[0;10;\\!-80]$ ;  \n${\\mathsf{C}}=[1\\;\\;0\\;\\;0]$ ;  \n$\\mathsf{J}=[-1+\\mathsf{j}^{*}2\\mathsf{\\Omega}-1-\\mathsf{j}^{*}2\\mathsf{\\Omega}-5]$ ;  \nK = acker(A,B,J)   \n$\\mathsf{K}=$ 1.2500    1.2500    0.19375   \n$\\%$ Obtaining the observer gain matrix Ke   \nAaa $=0$ ;$\\mathrm{{Aab}=[1\\Delta~\\,0];\\,A b a=[0;0];\\,A b b=[0\\Delta~\\,1;-24\\,~-1\\,0];B a=0;\\,B b=[1\\,0;-80];}$   \n$L=[-10\\textrm{--}10]$ ;  \nKe = acker(Abb',Aab',L)'   \nKe = 10 -24  \n\nIn the program, matrices Jand Lrepresent the desired closed-loop poles for pole placement and the desired poles for the observer, respectively. The matrices $\\mathbf{K}$ and $\\mathbf{K}_{e}$ are obtained as  \n\n$$\n\\begin{array}{r}{\\mathbf{K}=\\left[1.25\\quad1.25\\quad0.19375\\right]}\\\\ {\\mathbf{K}_{e}=\\left[\\begin{array}{c}{10}\\\\ {-24}\\end{array}\\right]}\\end{array}\n$$  \n\nDesign step 4: We shall determine the transfer function of the observer controller. Referring to Equation (10–108), the transfer function of the observer controller can be given by  \n\n$$\nG_{c}(\\boldsymbol{s})=\\frac{U(\\boldsymbol{s})}{-Y(\\boldsymbol{s})}=\\frac{\\mathrm{num}}{\\mathrm{den}}=-\\big[\\widetilde{\\mathbf{C}}\\big(\\boldsymbol{s}\\mathbf{I}-\\widetilde{\\mathbf{A}}\\big)^{-1}\\widetilde{\\mathbf{B}}\\,+\\,\\widetilde{D}\\big]\n$$  \n\nWe shall use MATLAB to calculate the transfer function of the observer controller. MATLAB Program 10–12 produces this transfer function.The result is  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{G_{c}(s)=\\frac{9.1s^{2}\\,+\\,73.5s\\,+\\,125}{s^{2}\\,+\\,17s\\,-\\,30}}}\\\\ &{}&{\\,=\\frac{9.1(s\\,+\\,5.6425)(s\\,+\\,2.4344)}{(s\\,+\\,18.6119)(s\\,-\\,1.6119)}}\\end{array}\n$$  \n\nDefine the system with this observer controller as System 1. Figure 10–20 shows the block diagram of System 1.  \n\n![](images/3f99f34d3ce7498504eb349fee84147ee4ff647fdf594badc38806f2ebd0c82e.jpg)  \n\n# Figure 10–20 Block diagram of System 1.  \n\n![](images/04a177ed41772a173434f849001b525b604797de973467a18bfc2f5ec6ae9a8e.jpg)  \n\nThe observer controller has a pole in the right-half $s$ plane ($(s=1.6119)$ ). The existence of an open-loop right-half $s$ plane pole in the observer controller means that the system is open-loop unstable, although the closed-loop system is stable. The latter can be seen from the characteristic equation for the system:  \n\n$$\n\\begin{array}{r l}&{|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|\\cdot|s\\mathbf{I}-\\mathbf{A}_{b b}+\\mathbf{K}_{e}\\mathbf{A}_{a b}|}\\\\ &{=s^{5}+27s^{4}+255s^{3}+1025s^{2}+2000s+2500}\\\\ &{=(s+1+j2)(s+1-j2)(s+5)(s+10)(s+10)=0}\\end{array}\n$$  \n\n(See MATLAB Program 10–13 for the calculation of the characteristic equation.)  \n\nA disadvantage of using an unstable controller is that the system becomes unstable if the dc gain of the system becomes small. Such a control system is neither desirable nor acceptable. Hence, to get a satisfactory system, we need to modify the closed-loop pole location and/or observer pole location.  \n\n![](images/e538e366ed04ec5376b4c02fbab09becaf1a3de371fe7893101892654b57b476.jpg)  \n\nSecond trial: Let us keep the desired closed-loop poles for pole placement as before, but modify the observer pole locations as follows:  \n\n$$\ns\\,=\\,-4.5,\\qquad s\\,=\\,-4.5\n$$  \n\nThus,  \n\n$$\n\\mathbf{L}=[-4.5\\mathrm{~\\ensuremath~{~\\alpha~\\mathrm{~-4.5~}}~}\\mathrm{~\\ensuremath~{~\\alpha~\\mathrm{~-4.5~}}~}]\n$$  \n\nUsing MATLAB, we find the new $\\mathbf{K}_{e}$ to be  \n\n$$\n\\mathbf{K}_{e}=\\left[\\begin{array}{l}{-1}\\\\ {6.25}\\end{array}\\right]\n$$  \n\nNext, we shall obtain the transfer function of the observer controller. MATLAB Program 10–14 produces this transfer function as follows:  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{G_{c}(s)=\\frac{1.2109s^{2}+11.2125s\\ +\\ 25.3125}{s^{2}+\\ 6s\\ +\\ 2.1406}}}\\\\ &{}&{=\\frac{1.2109(s+\\ 5.3582)(s+\\ 3.9012)}{(s+\\ 5.619)(s\\ +\\ 0.381)}}\\end{array}\n$$  \n\n![](images/0e9c084c9bdec6bbd30532054c3494b194aa6faecfc77ae9a96708dbbdcc2dc5.jpg)  \n\nNotice that this is a stable controller. Define the system with this observer controller as System 2. We shall proceed to obtain the response of System 2 to the given initial condition:  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]},\\qquad\\mathbf{e}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]}\n$$  \n\nBy substituting $u=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}$ into the state-space equation for the plant, we obtain  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}-\\mathbf{B}\\mathbf{K}\\widetilde{\\mathbf{x}}\\ =\\mathbf{A}\\mathbf{x}-\\mathbf{B}\\mathbf{K}\\mathbf{\\left[\\begin{array}{l}{x_{a}}\\\\ {\\widetilde{\\mathbf{x}}_{b}}\\end{array}\\right]}=\\mathbf{A}\\mathbf{x}-\\mathbf{B}\\mathbf{K}\\mathbf{\\left[\\begin{array}{l l}{x_{a}}\\\\ {\\mathbf{x}_{b}-\\mathbf{e}}\\end{array}\\right]}}\\\\ {=\\mathbf{A}\\mathbf{x}-\\mathbf{B}\\mathbf{K}\\mathbf{\\left\\{x}-\\left[\\begin{array}{l}{0}\\\\ {\\mathbf{e}}\\end{array}\\right]}\\right\\}=\\mathbf{A}\\mathbf{x}-\\mathbf{B}\\mathbf{K}\\mathbf{x}+\\mathbf{B}\\mathbf{\\left[\\begin{array}{l l}{K_{a}}&{\\mathbf{K}_{b}}\\end{array}\\right]}\\mathbf{\\left[\\begin{array}{l}{0}\\\\ {\\mathbf{e}}\\end{array}\\right]}}\\end{array}\n$$  \n\nThe error equation for the minimum-order observer is  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\mathbf{e}\n$$  \n\nBy combining Equations (10–110) and (10–111), we get  \n\n$$\n\\begin{array}{r}{\\left[\\dot{\\mathbf{x}}\\right]=\\left[\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{BK}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{BK}_{b}\\mathbf{\\Sigma}\\right]\\left[\\mathbf{\\bar{c}}\\right]}\\\\ {\\mathbf{\\Phi}\\mathbf{\\bar{e}}\\mathbf{\\Psi}}\\end{array}\n$$  \n\nwith the initial condition  \n\n$$\n\\begin{array}{r}{\\left[\\mathbf{\\bar{x}}(0)\\right]=\\left[\\begin{array}{l}{\\ln}\\\\ {0}\\\\ {0}\\\\ {1}\\\\ {0}\\end{array}\\right]=\\left[\\begin{array}{l}{\\ln}\\\\ {0}\\\\ {0}\\\\ {0}\\\\ {1}\\\\ {0}\\end{array}\\right]}\\end{array}\n$$  \n\nMATLAB Program 10–15 produces the response to the given initial condition. The response curves are shown in Figure 10–21.They seem to be acceptable.  \n\n![](images/1d022ef7c77c7d68d8bdea2acb62ea9b5ccc8f213d439ad64845db216c552a84.jpg)  \n\n![](images/03d874e77869778ab13ed133be1fefc4c2ff68abb5ecd81cd60fbbf33cdffb4d.jpg)  \nFigure 10–21 Response to the given initial condition; $x_{1}(0)\\,=\\,1$ ,$x_{2}(0)\\,=\\,0$ ,$x_{3}(0)\\,=\\,0$ ,$e_{1}(0)\\,=\\,1$ ,$e_{2}(0)\\,=\\,0$ .  \n\n![](images/4dcc12dc783586d180aa2f09740bcb9bdc1d4ed8d2d541be0397b8f0e71eb97a.jpg)  \nFigure 10–22 Bode diagram for the open-loop transfer function of System 2.  \n\nNext,we shall check the frequency-response characteristics.The Bode diagram of the open-loop system just designed is shown in Figure 10–22.The phase margin is about $40^{\\circ}$ and the gain margin is $+\\infty$ dB.The Bode diagram of the closed-loop system is shown in Figure 10–23.The bandwidth of the system is approximately 3.8 rad/sec.  \n\n![](images/4833e5663a8ce98ad4d54b8d25ada2a3766b05296ba0d52c1aab64a227c05ac0.jpg)  \nFigure 10–23 Bode diagram for the closed-loop transfer function of System 2.  \n\nFinally,we shall compare the root-locus plots of the first system with $L=[-10\\textrm{~-}10]$ and the second system with $L=[-4.5\\mathrm{~~~}-4.5]$ The plot for the first system given in Figure 10–24(a) shows that the system is unstable for small dc gain and becomes stable for large dc gain.The plot for the second system given in Figure 10–24(b), on the other hand, shows that the system is stable for any positive dc gain.  \n\n![](images/0d16b156456b5011385d3587dc04bfa888720c8ea47a90fcba8ebc34aa15aeba.jpg)  \nRoot-Locus Plot of $(91s^{3}+917s^{2}+2720s+2500)/$ $(s^{5}+27s^{4}+164s^{3}+108s^{2}-720s)$   \nFigure 10–24  \n\n![](images/0c6c3f4e5b53aa56b54d04a6fa29b93bbdeb3d4b4ab25bac4c8f5d100d89f7f4.jpg)  \nRoot-Locus Plot of $(12.109s^{3}+136.343s^{2}+477.375s+506.25)/$ $(s^{5}+16s^{4}+86.1406s^{3}+165.406s^{2}+51.3744s)$   \n(a) Root-locus plot of the system with observer poles at $s=-10$ and $s=-10$ ;(b) root-locus plot of the system with observer poles at $s=-4.5$ and $s=-4.5$ .  \n\n# Comments  \n\n1. In designing regulator systems,note that if the dominant controller poles are placed far to the left of the $j\\omega$ axis, the elements of the state feedback gain matrix $\\mathbf{K}$ will become large. Large gain values will make the actuator output become large, so that saturation may take place. Then the designed system will not behave as designed.   \n2. Also, by placing the observer poles far to the left of the $j\\omega$ axis, the observer controller becomes unstable,although the closed-loop system is stable.An unstable observer controller is not acceptable.   \n3. If the observer controller becomes unstable, move the observer poles to the right in the left-half $s$ plane until the observer controller becomes stable.Also,the desired closed-loop pole locations may need to be modified.   \n4. Note that if the observer poles are placed far to the left of the jvaxis, the bandwidth of the observer will increase and will cause noise problems. If there is a serious noise problem, the observer poles should not be placed too far to the left of the jvaxis.The general requirement is that the bandwidth should be sufficiently low so that the sensor noise will not become a problem.   \n5. The bandwidth of the system with the minimum-order observer is higher than that of the system with the full-order observer, provided that the multiple observer poles are placed at the same place for both observers. If the sensor noise is a serious problem, use of a full-order observer is recomnended.  \n\n# 10–7 DESIGN OF CONTROL SYSTEMS WITH OBSERVERS  \n\nIn Section 10–6 we discussed the design of regulator systems with observers.(The systems did not have reference or command inputs.) In this section we consider the design of control systems with observers when the systems have reference inputs or command inputs. The output of the control system must follow the input that is time varying. In following the command input, the system must exhibit satisfactory performance (a reasonable rise time, overshoot, settling time, and so on).  \n\nIn this section we consider control systems that are designed by use of the poleplacement-with-observer approach. Specifically, we consider control systems using observer controllers. In Section 10–6 we discussed regulator systems, whose block diagram is shown in Figure 10–25. This system has no reference input, or $r\\,=\\,0$ .When the system has a reference input, several different block diagram configurations are conceivable, each having an observer controller.Two of these configurations are shown in Figures 10–26 (a) and (b); we shall consider them in this section.  \n\n![](images/ca9f88b3a79b84e23a235800b6ebc56b441a1c1dd69e51582089a11e97e15993.jpg)  \nFigure 10–25 Regulator system.  \n\n![](images/cdd2acf0bbd5bcb99ae699b8a09bc7616de158e340ba86123044bd1ad23cd073.jpg)  \nFigure 10–26 (a) Control system with observer controller in the feedforward path; (b) Control system with observer controller in the feedback path.   \n(b)  \n\nConfiguration 1: Consider the system shown in Figure 10–27. In this system the reference input is simply added at the summing point.We would like to design the observer controller such that in the unit-step response the maximum overshoot is less than $30\\%$ and the settling time is about 5 sec.  \n\nIn what follows we first design a regulator system.Then,using the observer controller designed, we simply add the reference input $r$ at the summing point.  \n\nBefore we design the observer controller, we need to obtain a state-space representation of the plant. Since  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{1}{s(s^{2}+1)}}\n$$  \n\nwe obtain  \n\n$$\n\\\"+\\dot{y}\\,=\\,u\n$$  \n\nBy choosing the state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {x_{2}=\\dot{y}}\\\\ {x_{3}=\\ddot{y}}\\end{array}\n$$  \n\nwe get  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad\\mathrm{y}=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\n![](images/6fca1a52427366db30b94c10d56ab88318cb400730938738c51dceacb3cffd93.jpg)  \nFigure 10–27 Control system with observer controller in the feedforward path.  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{-1}&{0}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{0}&{0]\n$$  \n\nNext, we choose the desired closed-loop poles for pole placement at  \n\n$$\ns\\,=\\,-1\\,+\\,j,\\qquad s\\,=\\,-1\\,-\\,j,\\qquad s\\,=\\,-8\n$$  \n\nand the desired observer poles at  \n\n$$\ns\\,=\\,-4,\\qquad s\\,=\\,-4\n$$  \n\nThe state feedback gain matrix $\\mathbf{K}$ and the observer gain matrix $\\mathbf{K}_{e}$ can be obtained as follows:  \n\n$$\n\\begin{array}{c c}{\\mathbf{K}=[16}&{17}&{10]}\\\\ {\\mathbf{K}_{e}=\\left[\\begin{array}{c}{8}\\\\ {15}\\end{array}\\right]}&{}\\end{array}\n$$  \n\nSee MATLAB Program 10–16.  \n\n![](images/d871b48ae9f7353108a005425c3e28b8fd2f2bef0c74e7decada8ec240a01415.jpg)  \n\nThe transfer function of the observer controller is obtained by use of MATLAB Program 10–17.The result is  \n\n$$\n\\begin{array}{l}{G_{c}(s)\\,=\\frac{302s^{2}\\,+\\,303s\\,+\\,256}{s^{2}\\,+\\,18s\\,+\\,113}}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ {\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}}\\end{array}{array}{array}{array}{array}{c{c}}{c}{c}{c}{c}{c}{c}}\\end{c}{c}{c}{c}{c}}\\end{c}{c}{c}{c}}\\end{c}{c}{c}{c}}\\end{c}{c}{c}{c}}{}{c}{}{c}{}{}{c}}{}{}{}{}{}{ \n$$  \n\n![](images/2d2d47c62bf028f8f8ef79cccbd8b1d531f90a6c1c08cb80191f7ebf2c24b95b.jpg)  \n\n![](images/9ecdda07dd5289e6c19b2b402bd766274382ef00ff20aac41d682d1c8314856c.jpg)  \nFigure 10–28 Regulator system with observer controller.   \nFigure 10–29 Control system with observer controller in the feedforward path.  \n\nFigure 10–28 shows the block diagram of the regulator system just designed. Figure 10–29 shows the block diagram of a possible configuration of the control system based on the regulator system shown in Figure 10–28. The unit-step response curve for this control system is shown in Figure 10–30.The maximum overshoot is about $28\\%$ and the settling time is about 4.5 sec.Thus, the designed system satisfies the design requirements.  \n\n![](images/cbb53f56babb707a4141a648b8dbe48859e21ae8e26bf5d7bbd77c4ce54ecc90.jpg)  \nFigure 10–30 Unit-step response of the control system shown in Figure 10–29.  \n\n![](images/96d73573bcbbca3d7c38c86eac1d096e7fbb81d9a461c7e17a09d0fb81deb20e.jpg)  \nFigure 10–31 Control system with observer controller in the feedback path.  \n\nConfiguration 2: A different configuration of the control system is shown in Figure 10–31.The observer controller is placed in the feedback path.The input $r$ is introduced into the closed-loop system through the box with gain $N.$ . From this block diagram, the closed-loop transfer function is obtained as  \n\n$$\n\\frac{Y(s)}{R(s)}=\\frac{N\\!\\left(s^{2}+18s\\,+113\\right)}{s\\!\\left(s^{2}+1\\right)\\!\\left(s^{2}+18s\\,+113\\right)\\,+\\,302s^{2}\\,+\\,303s\\,+\\,256}\n$$  \n\nWe determine the value of constant $N$ such that for a unit-step input $r$ , the output $y$ is unity as $t$ approaches infinity.Thus we choose  \n\n$$\nN=\\frac{256}{113}=2.2655\n$$  \n\nThe unit-step response of the system is shown in Figure 10–32. Notice that the maximum overshoot is very small, approximately $4\\%$ .The settling time is about 5 sec.  \n\n![](images/f9dfe1fbf63c22c4c878fa44a09aab5ccd6061e3b795d6873bf2d37a50b8ab39.jpg)  \nFigure 10–32 The unit-step response of the system shown in Figure 10–31. (The closed-loop poles for pole placement are at $s=-1\\pm j$ ,$s\\,=\\,-8,$ .The observer poles are at $s=-4$ ,$s=-4.$ )  \n\nComments. We considered two possible configurations for the closed-loop control systems using observer controllers.As stated earlier, other configurations are possible.  \n\nThe first configuration,which places the observer controller in the feedforward path,generally gives a fairly large overshoot.The second configuration,which places the observer controller in the feedback path,gives a smaller overshoot.This response curve is quite similar to that of the system designed by the pole-placement approach without using the observer controller. See the unit-step response curve of the system, shown in Figure 10–33, designed by the pole-placement approach without observer.Here the desired closed-loop poles used are  \n\n$$\ns\\,=-1\\,+\\,j,\\qquad s\\,=-1\\,-\\,j,\\qquad s\\,=-8\n$$  \n\n# Figure 10–33  \n\n![](images/bee57b7eaa1278625a833eba80c939469a73e5c955be996c52335227c7d4aa7b.jpg)  \nUnit-Step Response of System without Observer   \nThe unit-step response of the control system designed by the pole placement approach without observer. (The closed-loop poles are at $s\\,=\\,-1\\,\\pm\\,j,s\\,=\\,-8.$ )  \n\n![](images/ff0e9580acf931bbe9d38ee5f2f58ac9c448b52088d613c152ebc356f8e617b2.jpg)  \nFigure 10–34 Bode diagrams of closed-loop system 1 (shown in Figure 10–29) and closedloop system 2 (shown in Figure 10–31).  \n\nNote that, in these two systems, the rise time and settling time are determined primarily by the desired closed-loop poles for pole placement. (See Figures 10–32 and 10–33.)  \n\nThe Bode diagrams of closed-loop system 1 (shown in Figure 10–29) and closedloop system 2 (shown in Figure 10–31) are shown in Figure 10–34. From this figure, we find that the bandwidth of system 1 is 5 rad 'sec and that of system 2 is 1.3 rad 'sec.  \n\n# Summary of State-Space Design Method  \n\n1. The state-space design method based on the pole-placement-combined-withobserver approach is very powerful.It is a time-domain method.The desired closedloop poles can be arbitrarily placed, provided the plant is completely state controllable.   \n2. If not all state variables can be measured, an observer must be incorporated to estimate the unmeasurable state variables.   \n3. In designing a system using the pole-placement approach, several different sets of desired closed-loop poles need be considered, the response characteristics compared, and the best one chosen.   \n4. The bandwidth of the observer controller is generally large, because we choose observer poles far to the left in the $s$ plane. A large bandwidth passes highfrequency noises and causes the noise problem.   \n5. Adding an observer to the system generally reduces the stability margin. In some cases, an observer controller may have zero(s) in the right-half $s$ plane, which means that the controller may be stable but of nonminimum phase. In other cases, the controller may have pole(s) in the right-half $s$ plane—that is, the controller is unstable.Then the designed system may become conditionally stable.   \n6. When the system is designed by the pole-placement-with-observer approach, it is advisable to check the stability margins (phase margin and gain margin), using a  \n\nfrequency-response method. If the system designed has poor stability margins, it is possible that the designed system may become unstable if the mathematical model involves uncertainties.  \n\n7. Note that for minimum-order observer is used frequency-response methods) yield low-order compensators (first or second order). Since the observer-bas nth-order systems, classical design methods (root-locus and CDfor an s are nnth-order system, the designed system th-order DCor $(N\\mathrm{~-~}m)$ th-order if the will become 2 nth order $\\left[\\mathrm{or}\\,(2n\\mathrm{~-~}\\bar{m})\\right]$ th order .Since lower-order compensators are cheaper than higher-order ones, the designer should first apply classical methods and, if no suitable compensators can be determined, then try the pole-placementwith-observer design approach presented in this chapter.  \n\n# 10–8 QUADRATIC OPTIMAL REGULATOR SYSTEMS  \n\nAn advantage of the quadratic optimal control method over the pole-placement method is that the former provides a systematic way of computing the state feedback control gain matrix.  \n\nQuadratic Optimal Regulator Problems. We shall now consider the optimal regulator problem that, given the system equation  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\ndetermines the matrix $\\mathbf{K}$ of the optimal control vector  \n\n$$\n{\\mathbf u}(t)\\,=-{\\mathbf K}{\\mathbf x}(t)\n$$  \n\nso as to minimize the performance index  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{*}{\\bf Q}{\\bf x}\\,+\\,{\\bf u}^{*}{\\bf R}{\\bf u})\\,d t\n$$  \n\nwhere Qis a positive-definite (or positive-semidefinite) Hermitian or real symmetric matrix and $\\mathbf{R}$ is a positive-definite Hermitian or real symmetric matrix. Note that the second term on the right-hand side of Equation (10–114) accounts for the expenditure of the energy of the control signals. The matrices $\\mathbf{Q}$ and $\\mathbf{R}$ determine the relative importance of the error and the expenditure of this energy. In this problem, we assume that the control vector ${\\bf\\delta u}(t)$ is unconstrained.  \n\nAs will be seen later,the linear control law given by Equation (10–113) is the optimal control law. Therefore, if the unknown elements of the matrix $\\mathbf{K}$ are determined so as to minimize the performance index, then ${\\mathbf u}(t)\\,=\\,-{\\mathbf K}{\\mathbf x}(t)$ is optimal for any initial state ${\\bf x}(0)$ .The block diagram showing the optimal configuration is shown in Figure 10–35.  \n\nFigure 10–35   \nOptimal regulator system.  \n\n![](images/6fb56a1d01b24c9c057428751eb6062fe5d070c910679c67279477fafd5a60c8.jpg)  \n\nNow let us solve the optimization problem. Substituting Equation (10–113) into Equation (10–112), we obtain  \n\n$$\n{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}\\mathrm{~-~}\\mathbf{B}\\mathbf{K}\\mathbf{x}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\n$$  \n\nIn the following derivations, we assume that the matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ is stable, or that the eigenvalues of $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ have negative real parts.  \n\nSubstituting Equation (10–113) into Equation (10–114) yields  \n\n$$\n\\begin{array}{l}{{J=\\displaystyle\\int_{0}^{\\infty}({\\bf x}^{*}{\\bf Q}{\\bf x}+{\\bf x}^{*}{\\bf K}^{*}{\\bf R}{\\bf K}{\\bf x})\\,d t}}\\\\ {{\\displaystyle~~=\\,\\int_{0}^{\\infty}{\\bf x}^{*}({\\bf Q}\\,+\\,{\\bf K}^{*}{\\bf R}{\\bf K}){\\bf x}\\,d t}}\\end{array}\n$$  \n\nLet us set  \n\n$$\n\\mathbf{x}^{*}(\\mathbf{Q}\\,+\\,\\mathbf{K}^{*}\\mathbf{R}\\mathbf{K})\\mathbf{x}=-\\,{\\frac{d}{d t}}\\left(\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}\\right)\n$$  \n\nwhere $\\mathbf{P}$ is a positive-definite Hermitian or real symmetric matrix.Then we obtain  \n\n$$\n\\mathbf{x}^{*}(\\mathbf{Q}+\\mathbf{K}^{*}\\mathbf{R}\\mathbf{K})\\mathbf{x}=-\\dot{\\mathbf{x}}^{*}\\mathbf{P}\\mathbf{x}-\\mathbf{x}^{*}\\mathbf{P}\\dot{\\mathbf{x}}=-\\mathbf{x}^{*}\\big[(\\mathbf{A}-\\mathbf{B}\\mathbf{K})^{*}\\mathbf{P}+\\mathbf{P}(\\mathbf{A}-\\mathbf{B}\\mathbf{K})\\big]\\mathbf{x}\n$$  \n\nComparing both sides of this last equation and noting that this equation must hold true for any $\\mathbf{X}$ , we require that  \n\n$$\n(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})^{*}\\mathbf{P}\\,+\\,\\mathbf{P}(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\,=\\,-(\\mathbf{Q}\\,+\\,\\mathbf{K}^{*}\\mathbf{R}\\mathbf{K})\n$$  \n\nIt can be proved that if $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ is a stable matrix, there exists a positive-definite matrix $\\mathbf{P}$ that satisfies Equation (10–115). (See Problem A–10–15 .)  \n\nHence our procedure is to determine the elements of $\\mathbf{P}$ from Equation (10–115) and see if it is positive definite. (Note that more than one matrix $\\mathbf{P}$ may satisfy this equation. If the system is stable, there always exists one positive-definite matrix $\\mathbf{P}$ to satisfy this equation.This means that, if we solve this equation and find one positive-definite matrix $\\mathbf{P}$ ,the system is stable.Other $\\mathbf{P}$ matrices that satisfy this equation are not positive definite and must be discarded.)  \n\nThe performance index $J$ can be evaluated as  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!\\mathbf{x}^{*}(\\mathbf{Q}\\,+\\,\\mathbf{K}^{*}\\mathbf{R}\\mathbf{K})\\mathbf{x}\\,d t\\,=\\left.-\\mathbf{x}^{*}\\mathbf{P}\\mathbf{x}\\right|_{0}^{\\infty}=\\left.-\\mathbf{x}^{*}(\\infty)\\mathbf{P}\\mathbf{x}(\\infty)\\,+\\,\\mathbf{x}^{*}(0)\\,\\mathbf{P}\\mathbf{x}(0)\\right|_{0}^{\\infty}\n$$  \n\nSince all eigenvalues of $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ are assumed to have negative real parts, we have $\\mathbf{x}(\\infty)\\rightarrow\\mathbf{0}.$ .Therefore, we obtain  \n\n$$\nJ\\,=\\,{\\bf x}^{*}(0)\\,{\\bf P}{\\bf x}(0)\n$$  \n\nThus, the performance index $J$ can be obtained in terms of the initial condition ${\\bf x}(0)$ and $\\mathbf{P}$ .  \n\nTo obtain the solution to the quadratic optimal control problem, we proceed as follows: Since Rhas been assumed to be a positive-definite Hermitian or real symmetric matrix, we can write  \n\n$$\n\\mathbf{R}=\\mathbf{T}^{*}\\mathbf{T}\n$$  \n\nwhere $\\mathbf{T}$ is a nonsingular matrix.Then Equation (10–115) can be written as  \n\n$$\n\\big(\\mathbf{A}^{*}-\\mathbf{K}^{*}\\mathbf{B}^{*}\\big)\\mathbf{P}\\,+\\,\\mathbf{P}\\big(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K}\\big)\\,+\\,\\mathbf{Q}\\,+\\,\\mathbf{K}^{*}\\mathbf{T}^{*}\\mathbf{T}\\mathbf{K}\\,=\\,\\mathbf{0}\n$$  \n\nwhich can be rewritten as  \n\n$$\n\\mathbf{P}+\\mathbf{PA}+\\left[\\mathbf{TK}-(\\mathbf{T^{*}})^{-1}\\mathbf{B}^{*}\\mathbf{P}\\right]^{*}\\!\\left[\\mathbf{TK}-(\\mathbf{T^{*}})^{-1}\\mathbf{B}^{*}\\mathbf{P}\\right]-\\mathbf{PBR}^{-1}\\mathbf{B}^{*}\\mathbf{P}+\\mathbf{Q}=\\mathbf{0}\n$$A \\*  \n\nThe minimization of $J$ with respect to $\\mathbf{K}$ requires the minimization of  \n\n$$\n\\mathbf{x}^{*}[\\mathbf{T}\\mathbf{K}\\,-\\,(\\mathbf{T}^{*})^{-1}\\mathbf{B}^{*}\\mathbf{P}]^{*}[\\mathbf{T}\\mathbf{K}\\,-\\,(\\mathbf{T}^{*})^{-1}\\mathbf{B}^{*}\\mathbf{P}]\\mathbf{x}\n$$  \n\nwith respect to $\\mathbf{K}$ . (See Problem A–10–16 .) Since this last expression is nonnegative, the minimum occurs when it is zero, or when  \n\n$$\n\\mathbf{T}\\mathbf{K}\\,=\\,(\\mathbf{T}^{*})^{-1}\\mathbf{B}^{*}\\mathbf{P}\n$$  \n\nHence,  \n\n$$\n\\mathbf{K}=\\mathbf{T}^{-1}(\\mathbf{T}^{*})^{-1}\\mathbf{B}^{*}\\mathbf{P}=\\mathbf{R}^{-1}\\mathbf{B}^{*}\\mathbf{P}\n$$  \n\nEquation (10–117) gives the optimal matrix K.Thus,the optimal control law to the quadratic optimal control problem when the performance index is given by Equation (10–114) is linear and is given by  \n\n$$\n\\mathbf{u}(t)\\,=\\,-\\mathbf{K}\\mathbf{x}(t)\\,=\\,-\\mathbf{R}^{-1}\\mathbf{B}^{*}\\mathbf{P}\\mathbf{x}(t)\n$$  \n\nThe matrix $\\mathbf{P}$ in Equation (10–117) must satisfy Equation (10–115) or the following reduced equation:  \n\n$$\n\\mathbf{A}^{*}\\mathbf{P}\\,+\\,\\mathbf{PA}\\,-\\,\\mathbf{PBR}^{-1}\\mathbf{B}^{*}\\mathbf{P}\\,+\\,\\mathbf{Q}\\,=\\,\\mathbf{0}\n$$  \n\nEquation (10–118) is called the reduced-matrix Riccati equation.The design steps may be stated as follows:  \n\n1. Solve Equation (10–118), the reduced-matrix Riccati equation, for the matrix $\\mathbf{P}$ .[If a positive-definite matrix $\\mathbf{P}$ exists (certain systems may not have a positivedefinite matrix $\\mathbf{P}$ ), the system is stable, or matrix A -BK is stable.] 2. Substitute this matrix $\\mathbf{P}$ into Equation (10–117). The resulting matrix Kis the optimal matrix.  \n\nA design example based on this approach is given in Example 10–9. Note that if the matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ is stable, the present method always gives the correct result.  \n\nFinally, note that if the performance index is given in terms of the output vector rather than the state vector, that is,  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf y}^{*}{\\bf Q y}\\,+\\,{\\bf u}^{*}{\\bf R u})\\,d t\n$$  \n\nthen the index can be modified by using the output equation  \n\n$$\n\\mathbf{y}=\\mathbf{C}\\mathbf{x}\n$$  \n\nto  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}(\\mathbf{x}^{*}\\mathbf{C}^{*}\\mathbf{QCx}\\,+\\,\\mathbf{u}^{*}\\mathbf{R}\\mathbf{u})\\,d t\n$$  \n\nand the design steps presented in this section can be applied to obtain the optimal matrix $\\mathbf{K}$ .  \n\nConsider the system shown in Figure 10–36.Assuming the control signal to be  \n\n$$\nu(t)\\,=-{\\bf K}{\\bf x}(t)\n$$  \n\ndetermine the optimal feedback gain matrix $\\mathbf{K}$ such that the following performance index is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!({\\bf x}^{T}{\\bf Q x}\\,+\\,u^{2})\\,d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{\\mu}\\end{array}\\right]}\\qquad(\\mu\\geq0)\n$$  \n\nFrom Figure 10–36, we find that the state equation for the plant is  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n{\\bf A}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\end{array}\\right]},\\qquad{\\bf B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nWe shall demonstrate the use of the reduced-matrix Riccati equation in the design of the optimal control system. Let us solve Equation (10–118), rewritten as  \n\n$$\n\\mathbf{A}^{*}\\mathbf{P}\\,+\\,\\mathbf{PA}\\,-\\,\\mathbf{PBR}^{-1}\\mathbf{B}^{*}\\mathbf{P}\\,+\\,\\mathbf{Q}\\,=\\,\\mathbf{0}\n$$  \n\nNoting that matrix A is real and matrix Qis real symmetric, we see that matrix $\\mathbf{P}$ is a real symmetric matrix. Hence, this last equation can be written as  \n\n$$\n{\\begin{array}{r l}{{\\left[\\!\\!{\\begin{array}{l l}{0}&{0}\\\\ {1}&{0}\\\\ {0}&{0}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}}\\!\\!\\right]}+{\\left[\\!\\!{\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\\\ {0}&{0}\\end{array}}\\!\\!\\right]}}\\\\ &{\\qquad-\\left[\\!\\!{\\begin{array}{c c}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}}\\!\\!\\right]{\\left[\\!\\!{\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{l l}{1}&{p_{11}}\\\\ {0}&{1}\\end{array}}\\!\\!\\right]}{\\left[\\!\\!{\\begin{array}{c c}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}}\\!\\!\\right]}+{\\left[\\!\\!{\\begin{array}{l l}{1}&{0}\\\\ {0}&{\\mu}\\end{array}}\\!\\!\\right]}={\\left[\\!\\!{\\begin{array}{c c}{0}&{0}\\\\ {0}&{0}\\end{array}}\\!\\!\\right]}}\\end{array}}\n$$  \n\nThis equation can be simplified to  \n\n$$\n\\left[\\!\\!{\\begin{array}{c c}{0}&{0}\\\\ {p_{11}}&{p_{12}\\!}\\end{array}}\\!\\!\\right]+\\left[\\!\\!{\\begin{array}{c c}{0}&{p_{11}}\\\\ {0}&{p_{12}}\\end{array}}\\!\\!\\right]-\\left[\\!\\!{\\begin{array}{c c}{p_{12}^{2}}&{p_{12}p_{22}}\\\\ {p_{12}p_{22}}&{p_{22}^{2}}\\end{array}}\\!\\!\\right]+\\left[\\!\\!{\\begin{array}{c c}{1}&{0}\\\\ {0}&{\\mu}\\end{array}}\\!\\!\\right]=\\left[\\!\\!{\\begin{array}{c c}{0}&{0}\\\\ {0}&{0}\\end{array}}\\!\\!\\right]\n$$  \n\n![](images/97670c573d5c639c208c538b9167b141c36d8ffebb10973b836a3d880cf73c7a.jpg)  \nFigure 10–36 Control system.  \n\n![](images/3fa8a11720c6260d865e358e27201355d438b981f1db2465521489768e161c87.jpg)  \nFigure 10–37 Optimal control of the plant shown in Figure 10–36.  \n\nfrom which we obtain the following three equations:  \n\n$$\n\\begin{array}{r}{1-p_{12}^{2}=0}\\\\ {p_{11}-p_{12}p_{22}=0}\\\\ {\\mu\\,+\\,2p_{12}-\\,p_{22}^{2}=0}\\end{array}\n$$  \n\nSolving these three simultaneous equations for $p_{11},p_{12}$ ,and $p_{22}$ ,requiring $\\mathbf{P}$ to be positive definite, we obtain  \n\n$$\n\\mathbf{P}={\\left[\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{{\\sqrt{\\mu+2}}}&{1}\\\\ {1}&{{\\sqrt{\\mu+2}}}\\end{array}\\right]}\n$$  \n\nReferring to Equation (10–117), the optimal feedback gain matrix $\\mathbf{K}$ is obtained as  \n\n$$\n\\begin{array}{r l}{\\mathbf{K}=\\mathbf{R}^{-1}\\mathbf{B}^{*}\\mathbf{P}}&{}\\\\ {\\mathbf{\\Lambda}=[1][0}&{1]{\\left[\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}\\right]}}\\\\ {\\mathbf{\\Lambda}=\\left[p_{12}\\quad p_{22}\\right]}\\\\ {={\\left[1\\quad\\sqrt{\\mu+2}\\right]}}\\end{array}\n$$  \n\nThus, the optimal control signal is  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\\,=-x_{1}\\,-\\,{\\sqrt{\\mu\\,+\\,2\\,}}\\ x_{2}\n$$  \n\nNote that the control law given by Equation (10–120) yields an optimal result for any initial state 1 under the given performance index. Figure 10–37 is the block diagram for this system. Since the characteristic equation is  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|=s^{2}+{\\sqrt{\\mu\\mathbf{\\mu}+2\\mathbf{\\sigma}}}\\,\\,s\\mathbf{\\mu}+1=0\n$$  \n\nif $\\mu=1$ ,the two closed-loop poles are located at  \n\n$$\ns\\,=\\,-0.866\\,+\\,j\\,0.5,\\qquad s\\,=\\,-0.866\\,-\\,j\\,0.5\n$$  \n\nThese correspond to the desired closed-loop poles when $\\mu=1$ .  \n\nSolving Quadratic Optimal Regulator Problems with MATLAB. In MATLAB, the command  \n\nsolves the continuous-time,linear,quadratic regulator problem and the associated Riccati equation. This command calculates the optimal feedback gain matrix $\\mathbf{K}$ such that the feedback control law  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nminimizes the performance index  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{*}{\\bf Q}{\\bf x}\\,+\\,{\\bf u}^{*}{\\bf R}{\\bf u})\\,d t\n$$  \n\nsubject to the constraint equation  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{u}\n$$  \n\nAnother command  \n\n$$\n[\\mathsf{K},\\mathsf{P},\\mathsf{E}]=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R})\n$$  \n\nreturns the gain matrix K,eigenvalue vector $\\mathbf{E}$ ,and matrix $\\mathbf{P}$ ,the unique positive-definite solution to the associated matrix Riccati equation:  \n\n$$\n\\mathbf{PA}\\,+\\,\\mathbf{A}^{*}\\mathbf{P}\\,-\\,\\mathbf{PBR}^{-1}\\mathbf{B}^{*}\\mathbf{P}\\,+\\,\\mathbf{Q}\\,=\\,\\mathbf{0}\n$$  \n\nIf matrix $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{B}\\mathbf{K}$ is a stable matrix,such a positive-definite solution $\\mathbf{P}$ always exists.The eigenvalue vector $\\mathbf{E}$ gives the closed-loop poles of $\\mathbf{A}\\mathbf{\\Lambda}-\\mathbf{\\Lambda}\\mathbf{B}\\mathbf{K}$ .  \n\nIt is important to note that for certain systems matrix $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ cannot be made a stable matrix, whatever $\\mathbf{K}$ is chosen. In such a case, there does not exist a positive-definite matrix $\\mathbf{P}$ for the matrix Riccati equation. For such a case, the commands  \n\n$$\n\\mathsf{K}=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R})\n$$  \n\n$$\n[\\mathsf{K},\\mathsf{P},\\mathsf{E}]=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R})\n$$  \n\ndo not give the solution. See MATLAB Program 10–18.  \n\nEXAMPLE 10–10 Consider the system defined by  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{r l}{-1}&{1}\\\\ {0}&{2}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]+\\Big[1\\Big]u}\\end{array}\n$$  \n\nShow that the system cannot be stabilized by the state-feedback control scheme  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nwhatever matrix $\\mathbf{K}$ is chosen. (Notice that this system is not state controllable.) Define  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\right]\n$$  \n\nThen  \n\n$$\n\\begin{array}{r}{\\mathbf{A}\\mathbin{-}\\mathbf{B}\\mathbf{K}={\\left[\\begin{array}{l l}{\\phantom{-}1}&{1}\\\\ {\\phantom{-}0}&{2}\\end{array}\\right]}\\mathbin{-}{\\left[\\begin{array}{l}{1}\\\\ {0}\\end{array}\\right]}\\lbrack k_{1}\\quad k_{2}\\rbrack}\\\\ {\\mathbin{=}{\\left[\\begin{array}{l l}{\\phantom{-}1}&{\\phantom{-}k_{1}}&{1\\mathbin{-}k_{2}}\\\\ {\\phantom{-}0}&{2}\\end{array}\\right]}}\\end{array}\n$$  \n\nHence, the characteristic equation becomes  \n\n$$\n\\begin{array}{r}{|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|=\\left|\\begin{array}{l l}{s+1\\,+\\,k_{1}}&{-1\\,+\\,k_{2}}\\\\ {\\,}&{0\\qquad\\qquad\\,s\\,-\\,2}\\end{array}\\right|}\\\\ {=(s\\,+\\,1\\,+\\,k_{1})(s\\,-\\,2)\\,=0}\\end{array}\n$$  \n\nThe closed-loop poles are located at  \n\n$$\ns\\,=-1\\,-\\,k_{1},\\qquad s\\,=\\,2\n$$  \n\nSince the pole at $s\\,=\\,2$ is in the right-half s plane, the system is unstable whatever $\\mathbf{K}$ matrix is chosen. Hence, quadratic optimal control techniques cannot be applied to this system.  \n\nLet us assume that matrices $\\mathbf{Q}$ and $\\mathbf{R}$ in the quadratic performance index are given by  \n\n$$\n\\mathbf{Q}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]},\\qquad R=[1]\n$$  \n\nand that we write MATLAB Program 10–18.The resulting MATLAB solution is  \n\n$$\n{\\mathsf{K}}=[{\\mathsf{N a N}}\\;\\;{\\mathsf{N a N}}]\n$$  \n\n(NaN means ‘not a number.’) Whenever the solution to a quadratic optimal control problem does not exist, MATLAB tells us that matrix $\\mathbf{K}$ consists of NaN.  \n\n![](images/56b17f2e43bcb7dc376bc1c0fa002e98a8338a21a94cf593df24a2db801a360b.jpg)  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n{\\bf A}={\\left[\\begin{array}{l r}{0}&{1}\\\\ {0}&{-1}\\end{array}\\right]},\\qquad{\\bf B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nThe performance index $J$ is given by  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{\\prime}{\\bf Q}{\\bf x}\\,+\\,u^{\\prime}R u)\\,d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]},\\qquad R=[1]\n$$  \n\nAssume that the following control $u$ is used.  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nDetermine the optimal feedback gain matrix $\\mathbf{K}$ .  \n\nThe optimal feedback gain matrix $\\mathbf{K}$ can be obtained by solving the following Riccati equation for a positive-definite matrix $\\mathbf{P}$ :  \n\n$$\n\\begin{array}{c}{\\mathbf{A^{\\prime}P+P A-P B R^{-1}B^{\\prime}P+Q=\\mathbf{0}}}\\\\ {\\ \\ \\ \\ \\ }\\\\ {\\mathbf{P=\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{1}\\end{array}\\right]}}\\end{array}\n$$  \n\nThe result is  \n\nSubstituting this $\\mathbf{P}$ matrix into the following equation gives the optimal $\\mathbf{K}$ matrix:  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{K}=R^{-1}\\mathbf{B}^{\\prime}\\mathbf{P}}\\\\ &{\\qquad=[1][0\\quad1]{\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{1}\\end{array}\\right]}=[1\\quad1]}\\end{array}}\n$$  \n\nThus, the optimal control signal is given by  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\\,=-x_{1}\\,-\\,x_{2}\n$$  \n\nMATLAB 10–19 also yields the solution to this problem.  \n\n![](images/14e6dc5792eea547b76c149776a5a44deeb11631d5838ffec86f99c66da3b314.jpg)  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n{\\bf A}=\\left[\\begin{array}{r r r r}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-35}&{-27}&{-9}\\end{array}\\right],\\qquad{\\bf B}=\\left[\\begin{array}{r r r}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]\n$$  \n\nThe performance index $J$ is given by  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{\\prime}{\\bf Q}{\\bf x}\\,+\\,u^{\\prime}R u)\\,d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}=\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right],\\qquad R=[1]\n$$  \n\nObtain the positive-definite solution matrix $\\mathbf{P}$ of the Riccati equation, the optimal feedback gain matrix K, and the eigenvalues of matrix $\\mathbf{A}-\\mathbf{B}\\mathbf{K}$ .  \n\nMATLAB Program 10–20 will solve this problem.  \n\n![](images/0243ae1643227369c4a733734a0c5fce97050d26984114e219037795d6a5a76b.jpg)  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]}\n$$  \n\nWith state feedback $u\\,=\\,-\\mathbf{K}\\mathbf{x}$ , the state equation for the system becomes  \n\n$$\n{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\n$$  \n\nThen the system, or sys, can be given by  \n\n$$\n{\\mathsf{s y s}}={\\mathsf{s s}}({\\mathsf{A{-}B^{*}K}},\\,{\\mathsf{e y e}}(3),\\,{\\mathsf{e y e}}(3),\\,{\\mathsf{e y e}}(3))\n$$  \n\nMATLAB Program 10–21 produces the response to the given initial condition. The response curves are shown in Figure 10–38.  \n\n![](images/9136c6879e7ff5acf72df5e3c2250442bbe827110c84733471faaa151860b15e.jpg)  \n\nEXAMPLE 10–13 Consider the system shown in Figure 10–39. The plant is defined by the following state-space equations:  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}\\,+\\,D u}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\begin{array}{r l}&{\\mathbf{A}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{-2}&{-3}\\end{array}\\right],\\;\\ \\ \\mathbf{B}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {1}\\end{array}\\right],\\;\\ \\ \\ \\ \\mathbf{C}=[1}&{0}&{0],\\;\\ \\ \\ \\ D=[0]}\\end{array}\n$$  \n\nThe control signal $u$ is given by  \n\n$$\nu\\,=\\,k_{1}(r\\,-\\,x_{1})\\,-\\,\\left(k_{2}x_{2}\\,+\\,k_{3}x_{3}\\right)\\,=\\,k_{1}r\\,-\\,\\left(k_{1}x_{1}\\,+\\,k_{2}x_{2}\\,+\\,k_{3}x_{3}\\right)\n$$  \n\n![](images/d977201a18994f27c6ff8b044a53519b5ebd78b5751fe9077705461abfab8ba8.jpg)  \nFigure 10–38 Response curves to initial condition.  \n\nIn determining an optimal control law, we assume that the input is zero, or $r\\,=\\,0$ .Let us determine the state-feedback gain matrix $\\mathbf{K}$ , where  \n\n$$\n\\mathbf{K}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\right]\n$$  \n\nsuch that the following performance index is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{\\prime}{\\bf Q}{\\bf x}\\,+\\,u^{\\prime}R u)\\,d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}=\\left[\\begin{array}{c c c}{q_{11}}&{0}&{0}\\\\ {0}&{q_{22}}&{0}\\\\ {0}&{0}&{q_{33}}\\end{array}\\right],\\qquad R=1,\\qquad\\mathbf{x}=\\left[\\begin{array}{c}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]=\\left[\\begin{array}{c}{y}\\\\ {\\dot{y}}\\\\ {\\dot{\\nu}}\\end{array}\\right]\n$$  \n\n![](images/164d02f8f890a4ac4bec16d6510e9037d8dfb496914cecc45135c43b8cadbe90.jpg)  \nFigure 10–39 Control system.  \n\nTo get a fast response, $q_{11}$ must be sufficiently large compared with $q_{22},q_{33}$ ,and $R$ . In this problem, we choose  \n\n$$\nq_{11}=100,\\qquad q_{22}=q_{33}=1,\\qquad R=0.01\n$$  \n\nTo solve this problem with MATLAB, we use the command  \n\n$$\n\\mathsf{K}=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R})\n$$  \n\nMATLAB Program 10–22 yields the solution to this problem.  \n\n![](images/4d08e8e0467e1b7f9c7fc8a1d057d8859ca0cfef94d52bf6925151278ba93548.jpg)  \n\nNext we shall investigate the step-response characteristics of the designed system using the matrix Kthus determined.The state equation for the designed system is  \n\n$$\n\\begin{array}{r l}&{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}\\mathbf{\\Psi}+\\mathbf{B}u}\\\\ &{\\quad=\\mathbf{A}\\mathbf{x}+\\mathbf{B}\\mathbf{\\left(-K\\mathbf{x}\\mathbf{\\Psi}+\\lambda_{1}r\\right)}}\\\\ &{\\quad=\\mathbf{\\Phi}(\\mathbf{A}-\\mathbf{\\Phi}\\mathbf{B}\\mathbf{K})\\mathbf{x}+\\mathbf{B}k_{1}r}\\end{array}\n$$  \n\nand the output equation is  \n\n$$\ny=\\mathbf{C}\\mathbf{x}=\\left[1\\quad0\\quad0\\right]\\left[{\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}}\\right]\n$$  \n\nTo obtain the unit-step response, use the following command:  \n\n$$\n[\\mathsf{y},\\mathsf{x},\\mathsf{t}]=\\mathsf{s t e p}(\\mathsf{A A},\\mathsf{B B},\\mathsf{C C},\\mathsf{D D})\n$$  \n\nwhere  \n\n$$\n\\mathrm{AA}=\\mathbf{A}-\\mathbf{B}\\mathbf{K},\\qquad\\mathrm{BB}=\\mathbf{B}\\mathbf{k}_{1},\\qquad\\mathrm{CC}=\\mathbf{C},\\qquad\\mathrm{DD}=D\n$$  \n\nMATLAB Program 10–23 produces the unit-step response of the designed system. Figure 10–40 shows the response curves $x_{1},x_{2}$ ,and $x_{3}$ versus $t$ on one diagram.  \n\n![](images/eee47f62cda79794236fcb6847edca81507e2fc1d7a4bef6e732011e323362a3.jpg)  \n\n![](images/ec0a0b5e0af5f707aed3f7e654bcda066303fa1450de7f6531a08ae3bcfb7999.jpg)  \nFigure 10–40 Response curves $x_{1}$ versus $t$ ,$x_{2}$ versus $t$ ,and $x_{3}$ versus $t$ .  \n\n# Concluding Comments on Optimal Regulator Systems  \n\n1. Given any initial state ${\\bf x}(t_{0})$ ,the optimal regulator problem is to find an allowable control vector ${\\bf\\delta u}(t)$ that transfers the state to the desired region of the state space and for which the performance index is minimized. For the existence of an optimal control vector ${\\bf\\delta u}(t)$ ,the system must be completely state controllable.   \n2. The system that minimizes (or maximizes, as the case may be) the selected performance index is, by definition, optimal. Although the controller may have nothing to do with “optimality” in many practical applications, the important point is that the design based on the quadratic performance index yields a stable control system.   \n3. The characteristic of an optimal control law based on a quadratic performance index is that it is a linear function of the state variables, which implies that we need to feed back all state variables.This requires that all such variables be available for feedback. If not all state variables are available for feedback, then we need to employ a state observer to estimate unmeasurable state variables and use the estimated values to generate optimal control signals.  \n\nNote that the closed-loop poles of the system designed by the use of the quadratic optimal regulator approach can be found from  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}+\\mathbf{B}\\mathbf{K}|\\,=\\,0\n$$  \n\nSince these closed-loop poles correspond to the desired closed-loop poles in the pole-placement approach, the transfer functions of the observer controllers can be obtained from either Equation (10–74) if the observer is of full-order type or Equation (10–108) if the observer is of minimum-order type.  \n\n4. When the optimal control system is designed in the time domain, it is desirable to investigate the frequency-response characteristics to compensate for noise effects. The system frequency-response characteristics must be such that the system attenuates highly in the frequency range where noise and resonance of components are expected.(To compensate for noise effects,we must in some cases either modify the optimal configuration and accept suboptimal performance or modify the performance index.)   \n5. If the upper limit of integration in the performance index $J$ given by Equation (10–114) is finite, then it can be shown that the optimal control vector is still a linear function of the state variables,but with time-varying coefficients.(Therefore, the determination of the optimal control vector involves that of optimal timevarying matrices.)  \n\n# 10–9 ROBUST CONTROL SYSTEMS  \n\nSuppose that given a control object (i.e., a system with a flexible arm) we wish to design a control system. The first step in the design of a control system is to obtain a mathematical model of the control object based on the physical law. Quite often the model may be nonlinear and possibly with distributed parameters. Such a model may be difficult to analyze. It is desirable to approximate it by a linear constant-coefficient system that will approximate the actual object fairly well. Note that even though the model to be used for design purposes may be a simplified one, it is necessary that such a model must include any intrinsic character of the actual object.Assuming that we can get a model that approximates the actual system quite well, we must get a simplified model for the purpose of designing the control system that will require a compensator of lowest order possible. Thus, a model of a control object (whatever it may be) will probably include an error in the modeling process. Note that in the frequency-response approach to control systems design, we use phase and gain margins to take care of the modeling errors. However, in the state-space approach, which is based on the differential equations of the plant dynamics, no such “margins” are involved in the design process.  \n\nSince the actual plant differs from the model used in the design, a question arises whether the controller designed using a model will work satisfactorily with the actual plant. To ensure that it will do so, robust control theory has been developed since around 1980.  \n\nRobust control theory uses the assumption that the models we use in designing control systems have modeling errors.We shall present an introduction to this theory in this section. Basically, the theory assumes that there is an uncertainty or error between the actual plant and its mathematical model and includes such uncertainty or error in the design process of the control system.  \n\nSystems designed based on the robust control theory will possess the following properties:  \n\n(1) Robust stability .The control system designed is stable in the presence of perturbation.   \n(2) Robust performance .The control system exhibits predetermined response characteristics in the presence of perturbation.  \n\nThis theory requires considerations based on frequency-response analysis and timedomain analysis. Because of the mathematical complications associated with robust control theory, detailed discussion of robust control theory is beyond the scope of the senior engineering student. In this section, only introductory discussion of robust control theory is presented.  \n\nUncertain Elements in Plant Dynamics. The term uncertainty refers to the differences or errors between the model of the plant and the actual plant.  \n\nUncertain elements that may appear in practical systems may be classified as structured uncertainty and unstructured uncertainty.An example of structured uncertainty is any parametric variation in the plant dynamics, such as variations in poles and zeros of the plant transfer function. Examples of unstructured uncertainty include frequencydependent uncertainty, such as high-frequency modes that we normally neglect in modeling plant dynamics. For example, in the modeling of a flexible-arm system, the model may include a finite number of modes of oscillation.The modes of oscillation that are not included in the modeling behave as uncertainty of the system. Another example of uncertainty occurs in the linearization of a nonlinear plant. If the actual plant is nonlinear and its model is linear, then the difference acts as unstructured uncertainty.  \n\nIn this section we consider the case where the uncertainty is unstructured. In addition we assume that the plant involves only one uncertainty. (Some plants may involve multiple uncertain elements.)  \n\nIn the robust control theory, we define unstructured uncertainty as $\\Delta(s)$ . Since the exact description of $\\Delta(s)$ is unknown, we use an estimate of $\\Delta(s)$ (as to the magnitude and phase characteristics) and use this estimate in the design of the controller that stabilizes the control system. Stability of a system with unstructured uncertainty can then be examined by use of the small gain theorem to be given following the definition of the $H_{\\infty}$ norm.  \n\n$H_{\\infty}$ Norm. The $H_{\\infty}$ norm of a stable single-input–single-output system is the largest possible amplification factor of the steady-state response to sinusoidal excitation.  \n\nFor a scalar $\\Phi(s),\\|\\Phi\\|_{\\infty}$ gives the maximum value of $|\\Phi(j\\omega)|$ .It is called the $H_{\\infty}$ norm. See Figure 10–41.  \n\nIn robust control theory we measure the magnitude of the transfer function by the $H_{\\infty}$ norm. Assume that the transfer function $\\Phi(s)$ is proper and stable. [Note that a transfer function $\\Phi(s)$ is called proper if $\\Phi(\\infty)$ is limited and definite. If $\\Phi(\\infty)=0$ , it is called strictly proper.] The $H_{\\infty}$ norm of $\\Phi(s)$ is defined by  \n\n![](images/5aba738347437c086b80d4f3d4867047426cec9b9c32ec162095624e6b115764.jpg)  \nFigure 10–41 Bode diagram and the $H_{\\infty}$ norm $\\|\\Phi\\|_{\\infty}$ .  \n\n$$\n\\|\\Phi\\|_{\\infty}=\\overline{{\\sigma}}\\left[\\Phi(j\\omega)\\right]\n$$  \n\n$\\overline{{\\sigma}}\\left[\\Phi(j\\omega)\\right]$ means the maximum singular value of $\\left[\\Phi(j\\omega)\\right]$ . ( $\\overline{{\\sigma}}$ means $\\sigma_{\\mathrm{max}}$ .) Note that the singular value of a transfer function $\\Phi$ is defined by  \n\n$$\n\\sigma_{i}(\\Phi)\\,=\\,\\sqrt{\\lambda_{i}(\\Phi^{*}\\Phi)}\n$$  \n\nvalue. By making where $\\lambda_{i}(\\Phi^{*}\\Phi)$ is the $\\|\\Phi\\|_{\\infty}$ i th largest eigenvalue of smaller, we make the effect of input $\\Phi^{*}\\Phi$ and it is always a non-negative real $_w$ on the output 7 7 $z$ smaller. It is frequently the case that instead of using the maximum singular value $\\|\\Phi\\|_{\\infty}$ , we use the inequality  \n\n$$\n\\|\\Phi\\|_{\\infty}<\\gamma\n$$  \n\nand limit the magnitude of $\\Phi(s)$ by .To make the magnitude of $\\|\\Phi\\|_{\\infty}$ small, we choose a small $\\gamma$ and require that $\\|\\Phi\\|_{\\infty}<\\gamma$ .  \n\nFigure 10–42 Closed-loop system.  \n\n![](images/50f7c6c73e98b5a3a3d09ea29f9f10ec91c7ef872c69cc4abb853d4364f73032.jpg)  \n\nSmall-Gain Theorem. Consider the closed-loop system shown in Figure 10–42. In the figure $\\Delta(s)$ and $M(s)$ are stable and proper transfer functions.  \n\nThe small-gain theorem states that if  \n\n$$\n\\|\\Delta(s)M(s)\\|_{\\infty}<1\n$$  \n\nthen this closed-loop system is stable. That is, if the $H_{\\infty}$ norm of $\\Delta(s)M(s)$ is smaller than 1, this closed-loop system is stable. This theorem is an extension of the Nyquist stability criterion.  \n\nIt is important to note that the small-gain theorem gives a sufficient condition for stability.That is, a system may be stable even if it does not satisfy this theorem. However, if a system satisfies the small-gain theorem, it is always stable.  \n\nSystem with Unstructured Uncertainty. In some cases an unstructured uncertainty error may be considered multiplicative such that  \n\n$$\n\\widetilde{G}\\,=\\,G(1\\,+\\,\\Delta_{m})\n$$  \n\nwhere $\\widetilde{G}$ is the true plant dynamics and $G$ is the model plant dynamics. In other cases an unstructured uncertainty error may be considered additive such that  \n\n$$\n\\widetilde{G}\\,=\\,G\\,+\\,\\Delta_{a}\n$$  \n\nIn either case we assume that the norm of $\\Delta_{m}$ or $\\Delta_{a}$ is bounded such that  \n\n$$\n\\|\\Delta_{m}\\|\\,<\\,\\gamma_{m}\\,,\\qquad\\|\\Delta_{a}\\|\\,<\\,\\gamma_{a}\n$$  \n\nwhere $\\gamma_{m}$ and $\\gamma_{a}$ are positive constants.  \n\n# EXAMPLE 10–14  \n\nConsider a control system with unstructured multiplicative uncertainty.We shall consider robust stability and robust performance of the system. (A system with unstructured additive uncertainty will be discussed in Problem A–10–18 .)  \n\nRobust Stability. Let us define  \n\n$\\widetilde G=$ true plant dynamics   \n$G=$ model of plant dynamics   \n$\\Delta_{m}=$ unstructured multiplicative uncertainty  \n\nWe assume that $\\Delta_{m}$ is stable and its upper bound is known.We also assume that $\\widetilde{G}$ and $G$ are related by  \n\n$$\n\\widetilde{G}\\,=\\,G(I\\,+\\,\\Delta_{m})\n$$  \n\nConsider the system shown in Figure 10–43(a). Let us examine the transfer function between point $A$ and point $B$ . Notice that Figure 10–43(a) can be redrawn as shown in Figure 10-43(b).The transfer function between point $A$ and point $B$ can be given by  \n\n$$\n{\\frac{K G}{1+K G}}=(1\\,+\\,K G)^{-1}\\,K G\n$$  \n\nDefine  \n\n$$\n(1\\,+\\,K G)^{-1}\\,K G=T\n$$  \n\nUsing Equation (10–121) we can redraw Figure 10–43(b) as Figure 10–43(c).Applying the smallgain theorem to the system consisting of $\\Delta_{m}$ and $T$ as shown in Figure 10–43(c), we obtain the condition for stability to be  \n\n$$\n\\|\\Delta_{m}T\\|_{\\infty}<1\n$$  \n\nIn general, it is impossible to precisely model $\\Delta_{m}$ .Therefore, let us use a scalar transfer function $W_{m}(j\\omega)$ such that  \n\n$$\n\\overline{{\\sigma}}\\{\\Delta_{m}(j\\omega)\\}<|W_{m}(j\\omega)|\n$$  \n\nwhere $\\overline{{\\sigma}}\\{\\Delta_{m}(j\\omega)\\}$ is the largest singular value of $\\Delta_{m}(j\\omega)$ .  \n\nConsider, instead of Inequality (10–122), the following inequality:  \n\n$$\n\\|W_{m}T\\|_{\\infty}<1\n$$  \n\nIf Inequality (10–123) holds true, Inequality (10–122) will always be satisfied. By making the $H_{\\infty}$ norm of $W_{m}T$ to be less than 1, we obtain the controller $K$ that will make the system stable.  \n\nSuppose that we cut the line at point $A$ in Figure 10–43(a). Then we obtain Figure 10–43(d). Replacing $\\Delta_{m}$ by $W_{m}I$ , we obtain Figure 10–43(e). Redrawing Figure 10–43(e), we obtain Figure 10–43(f). Figure 10–43(f) is called a generalized plant diagram .  \n\nReferring to Equation (10–121), $T$ is given by  \n\n$$\nT=\\frac{K G}{1+K G}\n$$  \n\nThen Inequality (10–123) can be rewritten as  \n\n$$\n\\left\\|\\frac{W_{m}K(s)G(s)}{1+\\K(s)G(s)}\\right\\|_{\\infty}<1\n$$  \n\nClearly, for a stable plant model $G(s)$ ,$K(s)\\,=\\,0$ will satisfy Inequality (10–125). However, $K(s)\\,=\\,0$ is not the desirable transfer function for the controller. To find an acceptable transfer function for $K(s)$ , we may add another condition—for example, that the resulting system will have robust performance such that the system output follows the input with minimum error, or another reasonable condition. In what follows we shall obtain the condition for robust performance.  \n\n![](images/cbdc5cd9987044ea924f55457e25577d4bcf40faa9daaf3ff13d8fc6265b7127.jpg)  \n\n# Figure 10–43  \n\n(a) Block diagram of a system with unstructured multiplicative uncertainty;   \n(b)–(d) successive modifications of the block diagram of (a);   \n(e) block diagram showing a generalized plant with unstructured multiplicative uncertainty;   \n(f) generalized plant diagram.  \n\nRobust Performance. Consider the system shown in Figure 10–44. Suppose that we want the output $y(t)$ to follow the input $r(t)$ as closely as possible, or we wish to have  \n\n$$\n\\operatorname*{lim}_{t\\to\\infty}\\left[r(t)\\,-\\,y(t)\\right]\\;=\\;\\operatorname*{lim}_{t\\to\\infty}e(t)\\;\\to0\n$$  \n\nSince the transfer function $Y(s)/R(s)$ is  \n\n$$\n{\\frac{Y(s)}{R(s)}}\\;=\\;{\\frac{K G}{1\\;+\\;K G}}\n$$  \n\nwe have  \n\n$$\n{\\frac{E(s)}{R(s)}}={\\frac{R(s)\\,-\\,Y(s)}{R(s)}}=1\\,-\\,{\\frac{Y(s)}{R(s)}}={\\frac{1}{1\\,+\\,K G}}\n$$  \n\nDefine  \n\n$$\n{\\frac{1}{1\\,+\\,K G}}=S\n$$  \n\nwhere $S$ is commonly called the sensitivity function and $T$ defined by Equation (10–124) is called the complementary sensitivity function. In this robust performance problem we want to make the $H_{\\infty}$ norm of $S$ smaller than the desired transfer function $W_{s}^{-1}$ or $\\|\\boldsymbol{S}\\|_{\\infty}<W_{s}^{-1}$ which can be written as  \n\n$$\n\\|W_{s}S\\|_{\\infty}<1\n$$  \n\nCombining Inequalities (10–123) and (10–126), we get  \n\n$$\n\\left\\|W_{m}T\\right\\|_{\\infty}<1\n$$  \n\nwhere $T\\,+\\,S\\,=\\,1$ ,or  \n\n$$\n\\left\\|{\\cal W}_{m}(s)\\,\\frac{K(s){\\cal G}(s)}{1+K(s){\\cal G}(s)}\\right\\|_{\\;\\;<\\;1}<1\n$$  \n\nOur problem then becomes to find $K(s)$ that will satisfy Inequality (10–127). Note that depending on the chosen $W_{m}(s)$ and $W_{s}(s)$ there may be many $K(s)$ that satisfy Inequality (10–127), or may be no $K(s)$ that satisfies Inequality (10–127). Such a robust control problem using Inequality (10–127) is called a mixed-sensitivity problem.  \n\nFigure 10–45(a) is a generalized plant diagram, where two conditions (robust stability and robust performance) are specified.A simplified version of this diagram is shown in Figure 10–45(b).  \n\n![](images/69ea338a99dae3bd377985094c228628d1f52cb793e86cb414a717147bca9950.jpg)  \nFigure 10–44 Closed-loop system.  \n\n![](images/8ef1f157f6b005c368d364a228c3afcbf7d71f3388c81b28eefabc2ae0727f77.jpg)  \nFigure 10–45 (a) Generalized plant diagram; (b) simplfied version of the generalized plant diagram shown in (a).  \n\nFinding Transfer Function $z(s)/w(s)$ from a Generalized Plant Diagram. Consider the generalized plant diagram shown in Figure 10–46.  \n\nIn this diagram $w(s)$ is the exogenous disturbance and $u(s)$ is the manipulated variable. $z(s)$ is the controlled variable and $y(s)$ is the observed variable.  \n\nConsider this control system consisting of the generalized plant $P(s)$ and the controller $K(s)$ .The equation that relates the outputs $z(s)$ and $y(s)$ and the inputs $w(s)$ and $u(s)$ of the generalized plant $P(s)$ is  \n\n$$\n{\\left[\\begin{array}{l}{z(s)}\\\\ {y(s)}\\end{array}\\right]}={\\left[\\begin{array}{l l}{P_{11}}&{P_{12}}\\\\ {P_{21}}&{P_{22}}\\end{array}\\right]}{\\left[\\begin{array}{l}{w(s)}\\\\ {u(s)}\\end{array}\\right]}\n$$  \n\nThe equation that relates $u(s)$ and $y(s)$ is given by  \n\n$$\nu(s)\\,=\\,K(s)y(s)\n$$  \n\nDefine the transfer function that relates the controlled variable $\\mathbf{z}(\\mathbf{s})$ to the exogenous disturbance $w(s)$ as $\\Phi(s)$ .Then  \n\n$$\nz(s)\\,=\\,\\Phi(s)w(s)\n$$  \n\nFigure 10–46   \nA generalized plant diagram.  \n\n![](images/6972aba4d2f38bdfe8e8a940f9953c84f1f2e3b12c9974f722b25f9a80d4524d.jpg)  \n\nNote that $\\Phi(s)$ can be determined as follows: Since  \n\nwe obtain  \n\nHence  \n\n$$\n\\begin{array}{l}{{z(s)\\,=\\,P_{11}w(s)\\,+\\,P_{12}u(s)}}\\\\ {{\\ }}\\\\ {{y(s)\\,=\\,P_{21}w(s)\\,+\\,P_{22}u(s)}}\\\\ {{\\ }}\\\\ {{u(s)\\,=\\,K(s)y(s)}}\\end{array}\n$$  \n\nor  \n\nTherefore,  \n\nHence,  \n\n$$\ny(s)\\,=\\,P_{21}w(s)\\,+\\,P_{22}K(s)y(s)\n$$  \n\n$$\n[I\\,-\\,P_{22}K(s)]y(s)\\,=\\,P_{21}w(s)\n$$  \n\n$$\ny(s)\\,=\\,[I\\,-\\,P_{22}K(s)]^{-1}P_{21}w(s)\n$$  \n\n$$\n\\begin{array}{c}{{z(s)\\,=\\,P_{11}w(s)\\,+\\,P_{12}K(s)[I\\,-\\,P_{22}K(s)]^{-1}P_{21}w(s)}}\\\\ {{\\,}}\\\\ {{=\\,\\{P_{11}\\,+\\,P_{12}K(s)[I\\,-\\,P_{22}K(s)]^{-1}P_{21}\\}w(s)}}\\end{array}\n$$  \n\n$$\n\\Phi(s)=P_{11}+\\,P_{12}K(s)[I\\,-\\,P_{22}K(s)]^{-1}P_{21}\n$$  \n\n# EXAMPLE 10–15  \n\nLet us determine the $P$ matrix in the generalized plant diagram of the control system considered in Example 10–14. We derived Inequality (10–125) for the control system to be robust stable. Rewriting Inequality (10–125), we have  \n\n$$\n\\left\\|\\frac{W_{m}K G}{1+K G}\\right\\|_{\\infty}<1\n$$  \n\nIf we define  \n\n$$\n\\Phi_{1}={\\frac{W_{m}K G}{1+K G}}\n$$  \n\nthen Inequality (10–129) can be written as  \n\n$$\n\\|\\Phi_{1}\\|_{\\infty}<1\n$$  \n\nReferring to Equation (10–128), rewritten as  \n\n$$\n\\Phi\\,=\\,P_{11}\\,+\\,P_{12}K(I\\,-\\,P_{22}K)^{-1}P_{21}\n$$  \n\nnotice that if we choose the generalized plant $P$ matrix as  \n\n$$\nP=\\left[\\begin{array}{l l}{0}&{W_{m}G}\\\\ {I}&{-G}\\end{array}\\right]\n$$  \n\nThen we obtain  \n\n$$\n\\begin{array}{l}{{\\Phi\\,=\\,P_{11}\\,+\\,P_{12}K(I\\,-\\,P_{22}K)^{-1}P_{21}}}\\\\ {{\\ }}\\\\ {{=\\,W_{m}K G(I\\,+\\,K G)^{-1}}}\\end{array}\n$$  \n\nwhich is exactly the same as $\\Phi_{1}$ in Equation (10–130).  \n\nWe derived in Example 10–14 that if we wished to have the output $y$ follow the input $r$ as close as possible, we needed to make the $H_{\\infty}$ norm of $\\Phi_{2}(s)$ , where  \n\n$$\n\\Phi_{2}\\,=\\,{\\frac{W_{s}}{I\\,+\\,K G}}\n$$  \n\nless than 1. [See Inequality (10–126).]  \n\nNote that the controlled variable $z$ is related to the exogenous disturbance $w$ by  \n\n$$\nz\\,=\\,\\Phi(s)w\n$$  \n\nand referring to Equation (10–128)  \n\n$$\n\\Phi(s)\\,=\\,P_{11}\\,+\\,P_{12}K(I\\,-\\,P_{22}K)^{-1}P_{21}\n$$  \n\nNotice that if we choose the $P$ matrix as  \n\n$$\nP=\\left[{\\begin{array}{c}{W_{s}-W_{s}G}\\\\ {I}\\end{array}}\\right]\n$$  \n\nthen we obtain  \n\n$$\n\\begin{array}{l}{\\displaystyle\\Phi\\,=\\,P_{11}+P_{12}K(I-P_{22}K)^{-1}P_{21}}\\\\ {\\displaystyle\\quad=\\,W_{s}-W_{s}K G(I+K G)^{-1}}\\\\ {\\displaystyle\\quad=\\,W_{s}\\left[1-\\frac{K G}{1+K G}\\right]}\\\\ {\\displaystyle\\quad=\\,W_{s}\\left[\\frac{1}{1+K G}\\right]}\\end{array}\n$$  \n\nwhich is the same as $\\Phi_{2}$ in Equation (10–132).  \n\n![](images/704e14323a86dd722606be8a490a72959c973ec3f178253e913cc96c6bf3f05d.jpg)  \nFigure 10–47 Generalized plant of the system discussed in Example 10–15.  \n\nIf both the robust stability and robust performance conditions are required, the control system must satisfy the condition given by Inequality (10–127), rewritten as  \n\n$$\n\\left|{{W_{m}}\\,\\frac{K G}{1\\,+\\,K G}}\\right|}<1\n$$  \n\nFor the $P$ matrix, we combine Equations (10–133) and (10–131) and get  \n\n$$\nP\\,=\\,{\\left[\\begin{array}{l l}{W_{s}}&{-W_{s}G}\\\\ {0}&{W_{m}G}\\\\ {I}&{-G}\\end{array}\\right]}\n$$  \n\nIf we construct $P(s)$ as given by Equation (10–135), then the problem of designing a control system to satisfy both robust stability and robust performance conditions can be formulated by using the generalized plant represented by Equation (10–135). As mentioned earlier, such a problem is called a mixed-sensitivity problem. By using the generalized plant given by Equation (10–135) we are able to determine the controller $K(s)$ that satisfies Inequality (10–134). The generalized plant diagram for the system considered in Example 10–14 becomes as shown in Figure 10–47.  \n\nHInfinity Control Problem. To design a controller $K$ of a control system to satisfy various stability and performance specifications, we utilize the concept of the generalized plant.  \n\nAs mentioned earlier a generalized plant is a linear model consisting of a model of the plant and weighting functions corresponding to the specifications for the required performance. Referring to the generalized plant shown in Figure 10–48, the $H$ infinity control problem is a problem to design a controller $K$ that will make the $H_{\\infty}$ norm of the transfer function from the exogenous disturbance $_w$ to the controlled variable $z$ less than a specified value.  \n\n![](images/008a395a629f7da9a857c920dcad911c181ff3ceedf20e07ee42231038d36ad0.jpg)  \nFigure 10–48 A generalized plant diagram.  \n\nThe reason to use generalized plants, rather than individual block diagrams of control systems, is that a number of control systems with uncertain elements have been designed using generalized plants and, consequently, established design approaches using such plants are available.  \n\nNote that any weighting function, such as $W(s)$ , is an important parameter to influence the resulting controller $K(s)$ . In fact, the goodness of the resulting designed system depends on the choice of the weighting function or functions used in the design process.  \n\nNote that the controller that is the solution to the $H$ infinity control problem is commonly called the $H$ infinity controller.  \n\nSolving Robust Control Problems. There are three established approaches to solve robust control problems.They are  \n\n1 . Solve robust control problems by deriving the Riccati equations and solving them. 2. Solve robust control problems by using the linear matrix inequality approach. 3. Solve robust control problems that involve structural uncertainties by using the $\\mu$ analysis and $\\mu$ synthesis approach.  \n\nSolving robust control problems by use of any of the above methods requires a broad mathematical background.  \n\nIn this section we have presented only an introduction to the robust control theory. Solving any robust control problem requires mathematical background beyond the scope of the senior engineering student. Therefore, an interested reader may take a graduate-level control course at an established college or university and study this subject in detail.  \n\n# EXAMPLE PROBLEMS AND SOLUTIONS  \n\nA–10–1. Consider the system defined by  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nSuppose that this system is not completely state controllable.Then the rank of the controllability matrix is less than $n$ ,or  \n\n$$\n\\operatorname{rank}\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\;\\;\\vdots\\;\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}^{n-1}\\mathbf{B}\\right]=q<n\n$$  \n\nThis means that there are $q$ linearly independent column vectors in the controllability matrix. Let us define such $q$ linearly independent column vectors as $\\mathbf{f}_{1},\\mathbf{f}_{2},\\ldots,\\mathbf{f}_{q}$ .Also, let us choose $n\\mathrm{~-~}q$ additional $n$ -vectors $\\mathbf{\\Delta}\\mathbf{v}_{q}+\\mathbf{\\Delta}_{1},\\mathbf{\\Delta}\\mathbf{v}_{q}+\\mathbf{\\Delta}_{2},\\dots,\\mathbf{v}_{n}$ such that  \n\n$$\n\\mathbf{P}=\\left[\\mathbf{f}_{1}\\ \\ {\\centering}\\ \\mathbf{f}_{2}\\ \\ {\\centering}\\ \\cdots\\ {\\centering}\\ \\mathbf{f}_{q}\\ \\ {\\centering}\\ \\mathbf{v}_{q+1}\\ \\ {\\centering}\\ \\mathbf{v}_{q+2}\\ \\ {\\Biggl.}\\ \\cdots\\ {\\Biggl.}\\ \\ \\mathbf{v}_{n}\\right]\n$$  \n\nis of rank $n$ .By using matrix $\\mathbf{P}$ as the transformation matrix, define  \n\n$$\n\\mathbf{P}^{\\scriptscriptstyle-1}\\mathbf{A}\\mathbf{P}=\\hat{\\mathbf{A}},\\qquad\\mathbf{P}^{\\scriptscriptstyle-1}\\mathbf{B}=\\hat{\\mathbf{B}}\n$$  \n\nShow that $\\hat{\\mathbf A}$ can be given by  \n\n$$\n\\hat{\\mathbf{A}}=\\left[\\frac{\\mathbf{A}_{11}\\vdots\\mathbf{\\Lambda}\\mathbf{A}_{12}}{\\mathbf{0}\\,\\,\\,\\,\\vdots\\mathbf{\\Gamma}\\mathbf{A}_{22}}\\right]\n$$  \n\nwhere $\\mathbf{A}_{11}$ is a $q\\times q$ matrix, ${\\bf A}_{12}$ is a $q\\times(n\\mathrm{~-~}q)$ matrix, $\\mathbf{A}_{22}$ is an $(n\\mathrm{~-~}q)\\times(n\\mathrm{~-~}q)$ matrix, and $\\mathbf{0}$ is an $(n\\mathrm{~-~}q)\\times q$ matrix. Show also that matrix $\\hat{\\mathbf{B}}$ can be given by  \n\n$$\n\\hat{\\mathbf{B}}=\\left[\\begin{array}{c}{\\mathbf{B}_{11}}\\\\ {\\mathbf{0}}\\end{array}\\right]\n$$  \n\nwhere ${\\bf{B}}_{11}$ is a $q\\times1$ matrix and 0 is an $(n\\mathrm{~-~}q)\\times1$ matrix.  \n\nSolution. Notice that  \n\n$$\n\\mathbf{A}\\mathbf{P}=\\mathbf{P}\\hat{\\mathbf{A}}\n$$  \n\nor  \n\n$$\n\\begin{array}{r l}&{\\bigl[\\mathbf{A}\\mathbf{f}_{1}\\ :\\ \\mathbf{A}\\mathbf{f}_{2}\\ :\\ \\cdots\\ |\\ \\mathbf{A}\\mathbf{f}_{q}\\ :\\ \\mathbf{A}\\mathbf{v}_{q+1}\\ :\\ \\cdots\\ |\\ \\mathbf{A}\\mathbf{v}_{n}\\bigr]}\\\\ &{\\quad=\\bigl[\\mathbf{f}_{1}\\ :\\ \\mathbf{f}_{2}\\ :\\ \\cdots\\ |\\ \\mathbf{f}_{q}\\ :\\ \\mathbf{v}_{q+1}\\ :\\ \\cdots\\ |\\ \\mathbf{v}_{n}\\bigr]\\hat{\\mathbf{A}}}\\end{array}\n$$  \n\nAlso,  \n\n$$\n\\mathbf{B}=\\mathbf{P}\\hat{\\mathbf{B}}\n$$  \n\nSince we have $q$ linearly independent column vectors $\\mathbf{f}_{1},\\mathbf{f}_{2},\\ldots,\\mathbf{f}_{q}$ ,we can use the Cayley–Hamilton theorem to express vectors $\\mathbf{Af}_{1},\\mathbf{Af}_{2},\\ldots,\\mathbf{Af}_{q}$ in terms of these $q$ vectors.That is,  \n\n$$\n\\begin{array}{l}{\\mathbf{Af}_{1}=a_{11}\\mathbf{f}_{1}+a_{21}\\mathbf{f}_{2}+\\cdots+a_{q1}\\mathbf{f}_{q}}\\\\ {\\mathbf{Af}_{2}=a_{12}\\mathbf{f}_{1}+a_{22}\\mathbf{f}_{2}+\\cdots+a_{q2}\\mathbf{f}_{q}}\\end{array}\n$$  \n\n$$\n\\mathbf{A}\\mathbf{f}_{q}=a_{1q}\\mathbf{f}_{1}+a_{2q}\\mathbf{f}_{2}+\\cdots+a_{q q}\\mathbf{f}_{q}\n$$  \n\nHence, Equation (10–137) may be written as follows:  \n\n$$\n\\left[\\mathbf{A}\\mathbf{f}_{1}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{f}_{2}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{f}_{q}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{v}_{q+1}\\;\\;\\vdots\\;\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{v}_{n}\\right]\n$$  \n\nDefine  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{c c c}{a_{11}}&{\\cdots}&{a_{1q}}\\\\ {a_{21}}&{\\cdots}&{a_{2q}}\\\\ {.}&{.}\\\\ {.}&{.}\\\\ {.}&{.}\\\\ {a_{q1}}&{\\cdots}&{a_{q q}}\\end{array}\\right]=\\mathbf{A}_{11}}\\\\ {\\left[\\begin{array}{c c c}{a_{12}}&{\\cdots}&{a_{1q}}\\\\ {.}&{.}\\\\ {.}&{.}\\\\ {.}&{.}\\end{array}\\right]=\\mathbf{A}_{12}}\\\\ {\\left[\\begin{array}{c c c}{a_{1q1}}&{\\cdots}&{a_{q q}}\\\\ {a_{q q1}}&{\\cdots}&{a_{1q}}\\\\ {a_{q q1}}&{\\cdots}&{a_{2q n}}\\\\ {.}&{.}\\\\ {.}&{.}\\\\ {.}&{.}&{.}\\\\ {a_{q q1}}&{\\cdots}&{a_{m q}}\\end{array}\\right]}\\end{array}\n$$  \n\n$$\n\\left[\\begin{array}{c c c}{a_{q+1q+1}}&{\\cdots}&{a_{q+1n}}\\\\ {\\cdot}&&{\\cdot}\\\\ {\\cdot}&&{\\cdot}\\\\ {\\cdot}&&{\\cdot}\\\\ {a_{n q+1}}&{\\cdots}&{a_{n n}}\\end{array}\\right]={\\bf A}_{22}\n$$  \n\nThen Equation (10–137) can be written as  \n\n$$\n\\begin{array}{r l}&{\\bigl[\\mathbf{A}\\mathbf{f}_{1}\\ :\\ \\mathbf{A}\\mathbf{f}_{2}\\ :\\ \\cdots\\ |\\ \\mathbf{A}\\mathbf{f}_{q}\\ :\\ \\mathbf{A}\\mathbf{v}_{q+1}\\ :\\ \\cdots\\ |\\ \\mathbf{A}\\mathbf{v}_{n}\\bigr]}\\\\ &{=\\bigl[\\mathbf{f}_{1}\\ :\\ \\mathbf{f}_{2}\\ :\\ \\cdots\\ |\\ \\mathbf{f}_{q}\\ \\mid\\ \\mathbf{v}_{q+1}\\ |\\ \\cdots\\ |\\ \\mathbf{v}_{n}\\bigr]\\biggl[\\xrightarrow[\\mathbf{0}\\ ]{\\mathbf{A}_{11}\\ |\\ \\mathbf{A}_{12}}\\biggr]}\\end{array}\n$$  \n\nThus,  \n\n$$\n\\mathbf{AP}=\\mathbf{P}{\\left[\\frac{\\mathbf{A}_{11}\\left\\{\\begin{array}{l}{\\mathbf{A}_{12}}\\\\ {\\vdots}\\\\ {\\mathbf{0}}\\end{array}\\right]}\\mathbf{A}_{22}}{\\left[\\begin{array}{l}{\\mathbf{A}_{22}}\\end{array}\\right]}}\n$$  \n\nHence,  \n\n$$\n\\mathbf{P}^{\\mathrm{-1}}\\mathbf{A}\\mathbf{P}=\\hat{\\mathbf{A}}=\\left[\\frac{\\mathbf{A}_{11}\\left|\\begin{array}{l}{\\mathbf{A}_{12}}\\\\ {\\mathbf{\\bar{\\theta}}}\\end{array}\\right]}{\\mathbf{A}_{22}}\\right]\n$$  \n\nNext, referring to Equation (10–138), we have  \n\n$$\n\\mathbf{B}=\\left[\\mathbf{f}_{1}\\ \\ \\vdots\\ \\mathbf{f}_{2}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\ \\mathbf{f}_{q}\\ \\ \\vdots\\ \\mathbf{v}_{q+1}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\ \\mathbf{v}_{n}\\right]\\hat{\\mathbf{B}}\n$$  \n\nReferring to Equation (10–136), notice that vector $\\mathbf{B}$ can be written in terms of $q$ linearly independent column vectors $\\mathbf{f}_{1},\\mathbf{f}_{2},\\ldots,\\mathbf{f}_{q}$ .Thus, we have  \n\n$$\n\\mathbf{B}=b_{11}\\mathbf{f}_{1}\\mathbf{\\Phi}+b_{21}\\mathbf{f}_{2}\\mathbf{\\Phi}+\\cdots+b_{q1}\\mathbf{f}_{q}\n$$  \n\nConsequently, Equation (10–139) may be written as follows:  \n\n$$\nb_{11}\\,\\mathbf{f}_{1}\\,+\\,b_{21}\\,\\mathbf{f}_{2}\\,+\\,\\cdots\\,+\\,b_{q1}\\,\\mathbf{f}_{q}=\\left[\\mathbf{f}_{1}\\;\\;\\vdots\\;\\;\\mathbf{f}_{2}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{f}_{q}\\;\\;\\vdots\\;\\;\\mathbf{v}_{q+1}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{v}_{n}\\right]\\,\\,\\,\n$$  \n\nThus,  \n\n$$\n\\hat{\\mathbf{B}}=\\left[\\frac{\\mathbf{B}_{11}}{\\mathbf{0}}\\right]\n$$  \n\nwhere  \n\n$$\n\\mathbf{B}_{11}={\\left[\\begin{array}{l}{b_{11}}\\\\ {b_{21}}\\\\ {\\;\\cdot}\\\\ {\\;\\cdot}\\\\ {\\;\\cdot}\\\\ {b_{q1}}\\end{array}\\right]}.\n$$  \n\nA–10–2. Consider a completely state controllable system  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nDefine the controllability matrix as M:  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}^{n-1}\\mathbf{B}\\right]\n$$  \n\nShow that  \n\n$$\n\\mathbf{M}^{-1}\\mathbf{A}\\mathbf{M}={\\left[\\begin{array}{l l l l l}{0}&{0}&{\\cdots}&{0}&{-a_{n}}\\\\ {1}&{0}&{\\cdots}&{0}&{-a_{n-1}}\\\\ {0}&{1}&{\\cdots}&{0}&{-a_{n-2}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{\\cdots}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nwhere $a_{1},a_{2},\\ldots,a_{n}$ are the coefficients of the characteristic polynomial  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\nSolution. Let us consider the case where $n\\,=\\,3$ .We shall show that  \n\n$$\n\\mathbf{AM}=\\mathbf{M}{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nThe left-hand side of Equation (10–140) is  \n\n$$\n\\mathbf{AM}=\\mathbf{A}{\\left[\\mathbf{B}\\mathbf{\\begin{array}{l}{}{}\\end{array}\\vdots}\\mathbf{\\begin{array}{l}{\\mathbf{AB}}\\end{array}\\vdots}\\mathbf{\\begin{array}{l}{\\mathbf{A^{2}B}}\\end{array}}\\right]}=\\left[\\mathbf{AB}\\ \\mathbf{\\begin{array}{l l}{}&{\\mathbf{A^{2}B}}\\end{array}\\vdots}\\ \\mathbf{A^{3}B}\\ \\mathbf{\\begin{array}{l l}{}&{\\mathbf{A^{3}B}}\\end{array}}\\right]\n$$  \n\nThe right-hand side of Equation (10–140) is  \n\n$$\n{\\left[\\bf B\\!\\!\\!\\!\\!\\!\\begin{array}{l l l}{{\\bf B}}&{{\\!\\!\\!\\!\\perp}}&{{\\bf A}{\\bf B}}&{{\\!\\!\\!\\!\\perp}}{\\bf A}^{2}{\\bf B}}\\end{array}\\right]}{\\left[\\!\\!\\!\\!\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\!\\!\\!\\right]}={\\left[\\bf A{\\bf B}\\!\\!\\!\\!\\!\\!\\begin{array}{l}{{\\bf A}^{2}{\\bf B}}&{{\\!\\!\\!\\!\\perp}}\\\\ {{\\bf A}{\\bf B}}&{{\\!\\!\\!\\!\\!\\perp}}&{-a_{3}\\,{\\bf B}}&{-a_{2}\\,{\\bf A}{\\bf B}}&{-a_{1}\\,{\\bf A}^{2}{\\bf B}}\\end{array}\\right]}\n$$  \n\nThe Cayley–Hamilton theorem states that matrix $\\mathbf{A}$ satisfies its own characteristic equation or, in the case of $n\\,=\\,3$ ,  \n\n$$\n\\mathbf{A}^{3}\\,+\\,a_{1}\\,\\mathbf{A}^{2}\\,+\\,a_{2}\\,\\mathbf{A}\\,+\\,a_{3}\\,\\mathbf{I}\\,=\\,\\mathbf{0}\n$$  \n\nUsing Equation (10–142), the third column of the right-hand side of Equation (10–141) becomes  \n\n$$\n-a_{3}\\mathbf{B}\\,-\\,a_{2}\\mathbf{A}\\mathbf{B}\\,-\\,a_{1}\\mathbf{A}^{2}\\mathbf{B}\\,=\\left(-a_{3}\\mathbf{I}\\,-\\,a_{2}\\mathbf{A}\\,-\\,a_{1}\\,\\mathbf{A}^{2}\\right)\\!\\mathbf{B}\\,=\\,\\mathbf{A}^{3}\\mathbf{B}\n$$  \n\nThus, Equation (10–141) becomes  \n\n$$\n{\\left[{\\bf B}\\;\\;\\;{\\bf A B}\\;\\;\\;{\\bf i}\\;\\;{\\bf A^{2}B}\\right]}{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}={\\left[{\\bf A B}\\;\\;{\\bf i}\\;\\;{\\bf A^{2}B}\\;\\;{\\bf i}\\;\\;{\\bf A^{3}B}\\right]}\n$$  \n\nHence, the left-hand side and the right-hand side of Equation (10–140) are the same. We have thus shown that Equation (10–140) is true. Consequently,  \n\n$$\n\\mathbf{M}^{-1}\\mathbf{A}\\mathbf{M}={\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nThe preceding derivation can be easily extended to the general case of any positive integer $n$ .  \n\nA–10–3. Consider a completely state controllable system  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nDefine  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}^{n-1}\\mathbf{B}\\right]\n$$  \n\nand  \n\n$$\n\\mathbf{W}={\\left[\\begin{array}{l l l l l}{a_{n-1}}&{a_{n-2}}&{\\cdots}&{a_{1}}&{1}\\\\ {a_{n-2}}&{a_{n-3}}&{\\cdots}&{1}&{0}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {a_{1}}&{1}&{\\cdots}&{0}&{0}\\\\ {1}&{0}&{\\cdots}&{0}&{0}\\end{array}\\right]}\n$$  \n\nwhere the $a_{i}$ ’s are coefficients of the characteristic polynomial  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\nDefine also  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}\n$$  \n\nShow that  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}=\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{-a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right],\\qquad\\mathbf{T}^{-1}\\mathbf{B}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {0}\\\\ {1}\\end{array}\\right]\n$$  \n\nSolution. Let us consider the case where $n\\,=\\,3.$ .We shall show that  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}=(\\mathbf{M}\\mathbf{W})^{-1}\\mathbf{A}(\\mathbf{M}\\mathbf{W})=\\mathbf{W}^{-1}(\\mathbf{M}^{-1}\\mathbf{A}\\mathbf{M})\\mathbf{W}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nReferring to Problem A–10–2 , we have  \n\n$$\n\\mathbf{M}^{-1}\\mathbf{A}\\mathbf{M}={\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nHence, Equation (10–143) can be rewritten as  \n\n$$\n\\mathbf{W}^{-1}{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\\mathbf{W}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nTherefore, we need to show that  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]\\mathbf{W}=\\mathbf{W}\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\\end{array}\n$$  \n\nThe left-hand side of Equation (10–144) is  \n\n$$\n{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-a_{3}}&{0}&{0}\\\\ {0}&{a_{1}}&{1}\\\\ {0}&{1}&{0}\\end{array}\\right]}\n$$  \n\nThe right-hand side of Equation (10–144) is  \n\n$$\n{\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{-a_{3}}&{0}&{0}\\\\ {0}&{a_{1}}&{1}\\\\ {0}&{1}&{0}\\end{array}\\right]}\n$$  \n\nClearly, Equation (10–144) holds true.Thus, we have shown that  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nNext, we shall show that  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{B}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]\n$$  \n\nNote that Equation (10–145) can be written as  \n\n$$\n\\mathbf{B}=\\mathbf{T}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}=\\mathbf{M}\\mathbf{W}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nNoting that  \n\n$$\n\\mathbf{T}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}={\\left[\\mathbf{B}\\ \\ |\\ \\ \\mathbf{AB}\\ \\ |\\ \\ \\mathbf{A^{2}B}\\right]}{\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]}={\\left[\\mathbf{B}\\ \\ |\\ \\ \\mathbf{AB}\\ \\ |\\ \\ \\mathbf{A^{2}B}\\right]}{\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\\\ {0}\\end{array}\\right]}=\\mathbf{B}\n$$  \n\nwe have  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{B}=\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]\n$$  \n\nThe derivation shown here can be easily extended to the general case of any positive integer $n$ .  \n\nA–10–4. Consider the state equation  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{\\;\\;1}&{\\;\\;1}\\\\ {-4}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right]}\n$$  \n\nThe rank of the controllability matrix $\\mathbf{M}$ ,  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\ \\ {\\mathrm{~i~}}\\ \\mathbf{A}\\mathbf{B}\\right]=\\left[\\begin{array}{c c}{0}&{2}\\\\ {2}&{-6}\\end{array}\\right]\n$$  \n\nis 2.Thus, the system is completely state controllable.Transform the given state equation into the controllable canonical form.  \n\nSolution. Since  \n\n$$\n\\begin{array}{r}{|s\\mathbf{I}-\\mathbf{A}|=\\left|s\\begin{array}{l l}{-\\,1}&{\\;\\;-1}\\\\ {\\,4}&{s\\,+\\,3}\\end{array}\\right|=(s\\,-\\,1)(s\\,+\\,3)\\,+\\,4}\\\\ {=s^{2}+\\,2s\\,+\\,1\\,=s^{2}\\,+\\,a_{1}s\\,+\\,a_{2}}\\end{array}\n$$  \n\nwe have  \n\n$$\na_{1}=2,\\qquad a_{2}=1\n$$  \n\nDefine  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}\n$$  \n\nwhere  \n\n$$\n\\mathbf{M}={\\left[\\begin{array}{l l}{0}&{2}\\\\ {2}&{-6}\\end{array}\\right]},\\qquad\\mathbf{W}={\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{0}\\end{array}\\right]}\n$$  \n\nThen  \n\n$$\n\\mathbf{T}={\\left[\\begin{array}{l l l}{0}&{2}\\\\ {2}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l}{2}&{0}\\\\ {-2}&{2}\\end{array}\\right]}\n$$  \n\nand  \n\n$$\n\\mathbf{T}^{-1}=\\left[\\begin{array}{c c}{0.5}&{0}\\\\ {0.5}&{0.5\\perp}\\end{array}\\right]\n$$  \n\nDefine  \n\n$$\n\\mathbf{x}=\\mathbf{T}\\hat{\\mathbf{x}}\n$$  \n\nThen the state equation becomes  \n\n$$\n\\dot{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}\\hat{\\mathbf{x}}\\,+\\,\\mathbf{T}^{-1}\\mathbf{B}u\n$$  \n\nSince  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{A}\\mathbf{T}={\\left[\\begin{array}{l l}{0.5}&{0}\\\\ {0.5}&{0.5}\\end{array}\\right]}{\\left[\\begin{array}{l l}{1}&{1}\\\\ {-4}&{-3}\\end{array}\\right]}{\\left[\\begin{array}{l l}{2}&{0}\\\\ {-2}&{2}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {-1}&{-2}\\end{array}\\right]}\n$$  \n\nand  \n\n$$\n\\mathbf{T}^{-1}\\mathbf{B}={\\left[\\begin{array}{l l}{0.5}&{0}\\\\ {0.5}&{0.5}\\end{array}\\right]}{\\left[\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c}{\\dot{\\hat{x}}_{1}}\\\\ {\\dot{\\hat{x}}_{2}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c}{0}&{1}\\\\ {-1}&{-2}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\!\\begin{array}{c}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\end{array}\\!\\!\\right]+\\Big[\\!\\!\\begin{array}{c}{0}\\\\ {1}\\end{array}\\!\\!\\right]\\!\\!u}\\end{array}\n$$  \n\nwhich is in the controllable canonical form.  \n\nA–10–5. Consider a system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{\\,\\,\\,0}&{\\,\\,\\,1}\\\\ {\\!-2}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{\\,\\,0]\n$$  \n\nThe characteristic equation of the system is  \n\n$$\n|s\\mathbf{I}-\\mathbf{A}|={\\left|\\begin{array}{l l}{s}&{-1}\\\\ {2}&{s+3}\\end{array}\\right|}=s^{2}+3s+2=(s+1)(s+2)=0\n$$  \n\nThe eigenvalues of matrix $\\mathbf{A}$ are $-1$ and $^{-2}$ .  \n\nIt is desired to have eigenvalues at $^{-3}$ and $^{-5}$ by using a state-feedback control $u\\,=\\,-\\mathbf{K}\\mathbf{x}$ .Determine the necessary feedback gain matrix $\\mathbf{K}$ and the control signal $u$ .  \n\nSolution. The given system is completely state controllable, since the rank of  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\ \\ {\\mathrm{~i~}}\\ \\mathbf{A}\\mathbf{B}\\right]=\\left[\\begin{array}{c c}{0}&{2}\\\\ {2}&{-6}\\end{array}\\right]\n$$  \n\nis 2. Hence, arbitrary pole placement is possible.  \n\nSince the characteristic equation of the original system is  \n\n$$\ns^{2}\\,+\\,3s\\,+\\,2\\,=\\,s^{2}\\,+\\,a_{1}s\\,+\\,a_{2}=\\,0\n$$  \n\nwe have  \n\n$$\na_{1}=3,\\qquad a_{2}=2\n$$  \n\nThe desired characteristic equation is  \n\n$$\n(s\\,+\\,3)(s\\,+\\,5)\\,=\\,s^{2}\\,+\\,8s\\,+\\,15\\,=\\,s^{2}\\,+\\,\\alpha_{1}s\\,+\\,\\alpha_{2}=0\n$$  \n\nHence,  \n\n$$\n\\alpha_{1}=8,\\qquad\\alpha_{2}=15\n$$  \n\nIt is important to point out that the original state equation is not in the controllable canonical form, because matrix $\\mathbf{B}$ is not  \n\n$$\n\\left[0\\right]\n$$  \n\nHence, the transformation matrix $\\mathbf{T}$ must be determined.  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}={\\left[\\mathbf{B}\\ :\\ |\\ \\mathbf{A}\\mathbf{B}]}{\\left[\\begin{array}{l l}{a_{1}}&{1}\\\\ {1}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l}{0}&{2}\\\\ {2}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l l}{3}&{1}\\\\ {1}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l}{2}&{0}\\\\ {0}&{2}\\end{array}\\right]}\n$$  \n\nHence,  \n\n$$\n\\mathbf{T}^{-1}=\\left[\\begin{array}{c c}{0.5}&{0}\\\\ {0}&{0.5\\perp}\\end{array}\\right]\n$$  \n\nReferring to Equation (10–13), the necessary feedback gain matrix is given by  \n\n$$\n{\\begin{array}{r l}&{\\mathbf{K}={\\left[\\alpha_{2}\\,-\\,a_{2}\\,\\,\\left\\{\\begin{array}{l}{\\alpha_{1}\\,-\\,a_{1}}\\end{array}\\right]}\\mathbf{T}^{-1}}}\\\\ &{={\\left[15\\,-\\,2\\,\\,\\left\\{\\begin{array}{l l}{8\\,-\\,3}\\end{array}\\right]}\\{\\begin{array}{l l}{0.5}&{0}\\\\ {0}&{0.5}\\end{array}\\right]}=[6.5}}\\end{array}}\n$$  \n\nThus, the control signal $u$ becomes  \n\n$$\nu=-\\mathbf{K}\\mathbf{x}=-[6.5\\quad2.5]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right]}\n$$  \n\nA–10–6. A regulator system has a plant  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{10}{(s\\,+\\,1)(s\\,+\\,2)(s\\,+\\,3)}}\n$$  \n\nDefine state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {\\ }\\\\ {x_{2}=\\dot{x}_{1}}\\\\ {\\ }\\\\ {x_{3}=\\dot{x}_{2}}\\end{array}\n$$  \n\nBy use of the state-feedback control $u\\,=\\,-\\mathbf{K}\\mathbf{x}$ , it is desired to place the closed-loop poles at  \n\n$$\ns=-2\\,+\\,j2\\sqrt{3}\\,,\\qquad s=-2\\,-\\,j2\\sqrt{3}\\,,\\qquad s=-10\n$$  \n\nObtain the necessary state-feedback gain matrix $\\mathbf{K}$ with MATLAB.  \n\nSolution. The state-space equations for the system become  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\;0}&{\\;1}&{\\;0}\\\\ {\\;0}&{\\;0}&{\\;1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\;0}\\\\ {\\;0}\\\\ {\\;10}\\end{array}\\right]}u\n$$  \n\n$$\ny\\,=\\,[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\\,+\\,0u\n$$  \n\nHence,  \n\n$$\n{\\bf A}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right],\\qquad{\\bf B}=\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {10}\\end{array}\\right]\n$$  \n\n$$\n\\mathbf{C}=[1\\quad0\\quad0],\\qquad\\qquad\\qquad D=[0]\n$$  \n\n(Note that, for the pole placement, matrices Cand $D$ do not affect the state-feedback gain matrix K.)  \n\nTwo MATLAB programs for obtaining state-feedback gain matrix $\\mathbf{K}$ are given in MATLAB Programs 10–24 and 10–25.  \n\n![](images/efcb24e3dc513962f643920cb9f7161cdc21a896ad6993fb17e0955da6ab6e31.jpg)  \n\n![](images/a95b55b82f0b2a99de287825040f478571e1258fd31512bc97c766d2cd111e74.jpg)  \n\nA–10–7. Consider a completely observable system  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}}\\\\ {y=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nDefine the observability matrix as $\\mathbf{N}{\\mathrm{:}}$ :  \n\n$$\n\\mathbf{N}=\\left[\\mathbf{C}^{*}\\mid\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\mid\\ \\cdots\\ \\mid\\ (\\mathbf{A}^{*})^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\nShow that  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}(\\mathbf{N}^{*})^{-1}=\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{0}&{1}&{\\cdots}&{0}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&&{\\cdot}\\\\ {0}&{0}&{0}&{\\cdots}&{1}\\\\ {-a_{n}}&{a_{n-1}}&{-a_{n-2}}&{\\cdots}&{-a_{1}}\\end{array}\\right]\n$$  \n\nwhere $a_{1},a_{2},\\ldots,a_{n}$ are the coefficients of the characteristic polynomial  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\nSolution. Let us consider the case where $n\\,=\\,3,$ .Then Equation (10–146) can be written as  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}(\\mathbf{N}^{*})^{-1}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]\n$$  \n\nEquation (10–147) may be rewritten as  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\underline{{\\biggr]}}\\mathbf{N}^{*}}\\end{array}\\right]\\mathbf{N}^{*}\n$$  \n\nWe shall show that Equation (10–148) holds true.The left-hand side of Equation (10–148) is  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}=\\left[\\begin{array}{c}{\\mathbf{C}}\\\\ {\\mathbf{CA}}\\\\ {\\mathbf{CA}^{2}}\\end{array}\\right]\\mathbf{A}=\\left[\\begin{array}{c}{\\mathbf{CA}}\\\\ {\\mathbf{CA}^{2}}\\\\ {\\mathbf{CA}^{3}}\\end{array}\\right]\n$$  \n\nThe right-hand side of Equation (10–148) is  \n\n$$\n{\\begin{array}{r l}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\rfloor}\\end{array}\\right]}\\mathbf{N}^{*}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\rfloor}\\end{array}\\right]}{\\left[\\begin{array}{l}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\mathbf{C}\\mathbf{A}^{2}}\\end{array}\\right]}}\\\\ {={\\left[\\begin{array}{l l l}{\\mathbf{C}}&{\\mathbf{CA}}\\\\ {\\mathbf{C}\\mathbf{A}^{2}}&{\\mathbf{CA}}\\\\ {-a_{3}\\mathbf{C}-a_{2}\\mathbf{CA}-a_{1}\\mathbf{CA}^{2}}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThe Cayley–Hamilton theorem states that matrix A satisfies its own characteristic equation, or  \n\nHence,  \n\n$$\n\\begin{array}{r l}&{\\mathbf{A}^{3}\\,+\\,a_{1}\\,\\mathbf{A}^{2}\\,+\\,a_{2}\\,\\mathbf{A}\\,+\\,a_{3}\\,\\mathbf{I}\\,=\\,\\mathbf{0}}\\\\ &{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-a_{1}\\,\\mathbf{C}\\mathbf{A}^{2}\\,-\\,a_{2}\\,\\mathbf{C}\\mathbf{A}\\,-\\,a_{3}\\,\\mathbf{C}\\,=\\,\\mathbf{C}\\mathbf{A}^{3}}\\end{array}\n$$  \n\nThus, the right-hand side of Equation (10–150) becomes the same as the right-hand side of Equation (10–149). Consequently,  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\underline{{\\mathbf{\\pi}}}}\\end{array}\\right]\\mathbf{N}^{*}\n$$  \n\nwhich is Equation (10–148).This last equation can be modified to  \n\n$$\n\\mathbf{N}^{*}\\mathbf{A}(\\mathbf{N}^{*})^{-1}=\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]\n$$  \n\nThe derivation presented here can be extended to the general case of any positive integer $n$ .  \n\nA–10–8. Consider a completely observable system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{D}u}\\end{array}\n$$  \n\nDefine  \n\n$$\n\\mathbf{N}=\\left[\\mathbf{C}^{*}\\ \\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\ ;\\ \\cdots\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\n$$  \n\nand  \n\n$$\n\\mathbf{W}={\\left[\\begin{array}{l l l l l}{a_{n-1}}&{a_{n-2}}&{\\cdots}&{a_{1}}&{1}\\\\ {a_{n-2}}&{a_{n-3}}&{\\cdots}&{1}&{0}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}\\\\ {a_{1}}&{1}&{\\cdots}&{0}&{0}\\\\ {1}&{0}&{\\cdots}&{0}&{0}\\end{array}\\right]}\n$$  \n\nwhere the $a$ ’s are coefficients of the characteristic polynomial  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{n}\\,+\\,a_{1}s^{n-1}\\,+\\,\\cdots\\,+\\,a_{n-1}s\\,+\\,a_{n}\n$$  \n\nDefine also  \n\n$$\n\\mathbf{Q}\\,=\\,(\\mathbf{W}\\mathbf{N}^{*})^{-1}\n$$  \n\nShow that  \n\n$$\n\\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}={\\left[\\begin{array}{l l l l l}{0}&{0}&{\\cdots}&{0}&{-a_{n}}\\\\ {1}&{0}&{\\cdots}&{0}&{-a_{n-1}}\\\\ {0}&{1}&{\\cdots}&{0}&{-a_{n-2}}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}&{\\cdot}\\\\ {0}&{0}&{\\cdots}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\n$$\n\\mathbf{CQ}\\bumpeq[0\\quad0\\quad\\cdots\\quad0\\quad1]\n$$  \n\n$$\n\\mathbf{Q}^{-1}\\mathbf{B}={\\left[\\begin{array}{c}{b_{n}-a_{n}b_{0}}\\\\ {b_{n-1}-a_{n-1}b_{0}}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {\\cdot}\\\\ {b_{1}-a_{1}b_{0}}\\end{array}\\right]}\n$$  \n\nwhere the $b_{k}$ ’s $(k=0,1,2,\\dots,n)$ are those coefficients appearing in the numerator of the transfer function when $\\mathbf{C}(s\\mathbf{I}\\mathrm{~-~}\\mathbf{A})^{-1}\\mathbf{B}\\mathrm{~+~}D$ is written as follows:  \n\n$$\n\\mathbf{C}(s\\mathbf{I}-\\mathbf{A})^{-1}\\mathbf{B}+D={\\frac{b_{0}s^{n}+b_{1}s^{n-1}+\\dots+b_{n-1}s+b_{n}}{s^{n}+a_{1}s^{n-1}+\\dots+a_{n-1}s+a_{n}}}\n$$  \n\nwhere $D=b_{0}$ .  \n\nSolution. Let us consider the case where $n\\,=\\,3.$ .We shall show that  \n\n$$\n\\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}=(\\mathbf{W}\\mathbf{N}^{*})\\mathbf{A}(\\mathbf{W}\\mathbf{N}^{*})^{-1}=\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]\n$$  \n\nNote that, by referring to Problem A–10–7 , we have  \n\n$$\n(\\mathbf{W}\\mathbf{N}^{*})\\mathbf{A}(\\mathbf{W}\\mathbf{N}^{*})^{-1}=\\mathbf{W}\\big[\\mathbf{N}^{*}\\mathbf{A}(\\mathbf{N}^{*})^{-1}\\big]\\mathbf{W}^{-1}=\\mathbf{W}\\left[\\begin{array}{c c c}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\rfloor}\\end{array}\\right]\\mathbf{W}^{-1}\n$$  \n\nHence, we need to show that  \n\n$$\n\\mathbf{W}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}\\mathbf{W}^{-1}={\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\n$$  \n\nor  \n\n$$\n\\mathbf{W}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}\\mathbf{W}\n$$  \n\nThe left-hand side of Equation (10–154) is  \n\n$$\n{\\begin{array}{r l}{\\mathbf{w}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}{\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-a_{3}}&{-a_{2}}&{-a_{1}}\\end{array}\\right]}}\\\\ {={\\left[\\begin{array}{l l l}{-a_{3}}&{0}&{0}\\\\ {0}&{a_{1}}&{1}\\\\ {0}&{1}&{0}\\end{array}\\right]}}\\end{array}}\n$$  \n\nThe right-hand side of Equation (10–154) is  \n\n$$\n\\begin{array}{r l}{\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}\\lrcorner}\\end{array}\\right]\\mathbf{W}=\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}\\lrcorner}\\end{array}\\right]\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}\\\\ &{=\\left[\\begin{array}{l l l}{-a_{3}}&{0}&{0}\\\\ {0}&{a_{1}}&{1}\\\\ {0}&{1}&{0}\\end{array}\\right]}\\end{array}\n$$  \n\nThus, we see that Equation (10–154) holds true. Hence, we have proved Equation (10–153). Next we shall show that  \n\n$$\n\\mathbf{CQ}=\\left[0\\quad0\\quad1\\right]\n$$  \n\nor  \n\n$$\n\\mathbf{C}(\\mathbf{W}\\mathbf{N}^{*})^{-1}=[0\\quad0\\quad1]\n$$  \n\nNotice that  \n\n$$\n\\begin{array}{r l}{[0\\quad0\\quad1](\\mathbf{W}\\mathbf{N}^{*})\\,=\\,[0\\quad0\\quad0\\quad1]\\left[\\begin{array}{l l l}{a_{2}}&{a_{1}}&{1}\\\\ {a_{1}}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]\\left[\\begin{array}{l}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\mathbf{C}\\mathbf{A}^{2}}\\\\ {\\mathbf{C}\\mathbf{A}}\\end{array}\\right]}&{{}}\\\\ {\\,=\\,[1\\quad0\\quad0]\\left[\\begin{array}{l}{\\mathbf{C}}\\\\ {\\mathbf{C}\\mathbf{A}}\\\\ {\\mathbf{C}\\mathbf{A}^{2}}\\end{array}\\right]=\\mathbf{C}}\\end{array}\n$$  \n\nHence, we have shown that  \n\n$$\n[0\\mathrm{~\\ensuremath~{~0~}~\\,~}1]=\\mathbf{C}(\\mathbf{W}\\mathbf{N}^{*})^{-1}=\\mathbf{C}\\mathbf{Q}\n$$  \n\nNext define  \n\n$$\n\\mathbf{x}=\\mathbf{Q}\\hat{\\mathbf{x}}\n$$  \n\nThen Equation (10–151) becomes  \n\n$$\n\\dot{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}\\hat{\\mathbf{x}}\\,+\\,\\mathbf{Q}^{-1}\\mathbf{B}u\n$$  \n\nand Equation (10–152) becomes  \n\n$$\n\\mathbf{\\boldsymbol{y}}=\\mathbf{CQ\\hat{\\mathbf{x}}}\\,+\\,D\\boldsymbol{u}\n$$  \n\nReferring to Equation (10–153) ,Equation (10–155) becomes  \n\n$$\n{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{0}&{0}&{-a_{3}}\\\\ {1}&{0}&{-a_{2}}\\\\ {0}&{1}&{-a_{1}}\\end{array}\\right]}{\\left[\\begin{array}{l}{{\\dot{x}}_{1}}\\\\ {{\\dot{x}}_{2}}\\\\ {{\\dot{x}}_{3}}\\end{array}\\right]}+{\\left[\\begin{array}{l}{\\gamma_{3}}\\\\ {\\gamma_{2}}\\\\ {\\gamma_{1}}\\end{array}\\right]}u\n$$  \n\nwhere  \n\n$$\n\\left[\\begin{array}{l}{\\gamma_{3}}\\\\ {\\gamma_{2}}\\\\ {\\gamma_{1}}\\end{array}\\right]=\\mathbf{Q}^{-1}\\mathbf{B}\n$$  \n\nThe transfer function $G(s)$ for the system defined by Equations (10–155) and (10–156) is  \n\n$$\nG(s)\\,=\\,{\\bf C Q}{(s{\\bf I}\\,-\\,{\\bf Q}^{-1}{\\bf A}{\\bf Q})}^{-1}{\\bf Q}^{-1}{\\bf B}\\,+\\,D\n$$  \n\nNoting that  \n\n$$\n\\mathbf{CQ}=\\left[0\\quad0\\quad1\\right]\n$$  \n\nwe have  \n\n$$\nG(s)=[0\\quad0\\quad1]{\\left[\\begin{array}{l l l}{s}&{0}&{a_{3}}\\\\ {-1}&{s}&{a_{2}}\\\\ {0}&{-1}&{s+a_{1}{\\vphantom{\\bigg[}}\\Bigg]}^{-1}{\\left[\\begin{array}{l}{\\gamma_{3}}\\\\ {\\gamma_{2}}\\\\ {\\gamma_{1}}\\end{array}\\right]}+D\n$$  \n\nNote that $D\\,=\\,b_{0}$ .Since  \n\n$$\n\\left[\\begin{array}{c c c}{s}&{0}&{a_{3}}\\\\ {-1}&{s}&{a_{2}}\\\\ {0}&{-1}&{s+a_{1}}\\end{array}\\right]^{-1}=\\frac{1}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}\\left[\\begin{array}{c c c}{s^{2}+a_{1}s+a_{2}}&{-a_{3}}&{-a_{3}s}\\\\ {s+a_{1}}&{s^{2}+a_{1}s}&{-a_{2}s-a_{3}}\\\\ {1}&{s}&{s^{2}}\\end{array}\\right],\n$$  \n\nwe have  \n\n$$\n\\begin{array}{r l}&{G(s)=\\frac{1}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}[1\\-\\ s\\-\\ s^{2}]\\Bigg[\\frac{\\gamma_{3}}{\\gamma_{1}}\\Bigg]+D}\\\\ &{\\qquad=\\frac{\\gamma_{1}s^{2}+\\gamma_{2}s+\\gamma_{3}}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}+b_{0}}\\\\ &{\\qquad=\\frac{b_{0}s^{3}+\\big(\\gamma_{1}+a_{1}b_{0}\\big)s^{2}+\\big(\\gamma_{2}+a_{2}b_{0}\\big)s+\\gamma_{3}+a_{3}b_{0}}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}}\\\\ &{\\qquad=\\frac{b_{0}s^{3}+b_{1}s^{2}+b_{2}s+b_{3}}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}}\\end{array}\n$$  \n\nHence,  \n\n$$\n\\gamma_{1}=b_{1}-\\,a_{1}b_{0},\\qquad\\gamma_{2}=b_{2}-\\,a_{2}b_{0},\\qquad\\gamma_{3}=b_{3}-\\,a_{3}b_{0}\n$$  \n\nThus, we have shown that  \n\n$$\n\\mathbf{Q}^{-1}\\mathbf{B}={\\left[\\begin{array}{l}{\\gamma_{3}}\\\\ {\\gamma_{2}}\\\\ {\\gamma_{1}}\\end{array}\\right]}={\\left[\\begin{array}{l}{b_{3}-a_{3}b_{0}}\\\\ {b_{2}-a_{2}b_{0}}\\\\ {b_{1}-a_{1}b_{0}}\\end{array}\\right]}\n$$  \n\nNote that what we have derived here can be easily extended to the case when $n$ is any positive integer.  \n\nA–10–9. Consider a system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{1}&{1}\\\\ {-4}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{1]\n$$  \n\nThe rank of the observability matrix $\\mathbf{N}$ ,  \n\n$$\n\\mathbf{N}=\\left[\\mathbf{C}^{*}\\ \\left\\{\\begin{array}{l l}{\\mathbf{A}^{*}\\mathbf{C}^{*}\\right\\}=\\left[\\begin{array}{l l}{1}&{-3}\\\\ {1}&{-2}\\end{array}\\right]\\right.\n$$  \n\nis 2. Hence, the system is completely observable. Transform the system equations into the observable canonical form.  \n\nSolution. Since  \n\n$$\n|s\\mathbf{I}\\,-\\,\\mathbf{A}|\\,=\\,s^{2}\\,+\\,2s\\,+\\,1\\,=\\,s^{2}\\,+\\,a_{1}s\\,+\\,a_{2}\n$$  \n\nwe have  \n\n$$\na_{1}=2,\\qquad a_{2}=1\n$$  \n\nDefine  \n\n$$\n\\mathbf{Q}\\,=\\,(\\mathbf{W}\\mathbf{N}^{*})^{-1}\n$$  \n\nwhere  \n\n$$\n\\mathbf{N}={\\left[\\begin{array}{l l}{1}&{-3}\\\\ {1}&{-2}\\end{array}\\right]},\\qquad\\mathbf{W}={\\left[\\begin{array}{l l}{a_{1}}&{1}\\\\ {1}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{0}\\end{array}\\right]}\n$$  \n\nThen  \n\n$$\n\\mathbf{Q}={\\left\\{\\left[\\begin{array}{l l}{2}&{1}\\\\ {1}&{0}\\end{array}\\right]\\!\\!{\\left[\\begin{array}{l l}{1}&{1}\\\\ {-3}&{-2}\\end{array}\\right]}\\right\\}}^{-1}={\\left[\\begin{array}{l l}{-1}&{0}\\\\ {1}&{1}\\end{array}\\right]}^{-1}={\\left[\\begin{array}{l l}{-1}&{0}\\\\ {1}&{1}\\end{array}\\right]}\n$$  \n\nand  \n\n$$\n\\mathbf{Q}^{-1}=\\left[\\begin{array}{l l}{-1}&{0}\\\\ {1}&{1}\\end{array}\\right]\n$$  \n\nDefine  \n\n$$\n\\mathbf{x}=\\mathbf{Q}\\hat{\\mathbf{x}}\n$$  \n\nThen the state equation becomes  \n\n$$\n\\dot{\\hat{\\mathbf{x}}}\\,=\\,\\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}\\hat{\\mathbf{x}}\\,+\\,\\mathbf{Q}^{-1}\\mathbf{B}u\n$$  \n\nor  \n\n$$\n\\begin{array}{r l}&{\\left[\\!\\!\\begin{array}{r}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{r r}{-1}&{0}\\\\ {1}&{1}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\begin{array}{r r}{1}&{1}\\\\ {-4}&{-3}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\begin{array}{r r}{-1}&{0}\\\\ {1}&{1}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\begin{array}{r}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{r r}{-1}&{0}\\\\ {1}&{1}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\begin{array}{r}{0}\\\\ {0}\\end{array}\\!\\!\\right]u}\\\\ &{\\qquad=\\left[\\!\\!\\begin{array}{r r}{0}&{-1}\\\\ {1}&{-2}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\begin{array}{r}{\\hat{x}_{1}}\\\\ {\\hat{x}_{2}}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{r}{0}\\\\ {2}\\end{array}\\!\\!\\right]u}\\end{array}\n$$  \n\nThe output equation becomes  \n\n$$\n\\boldsymbol{y}\\,=\\,\\mathbf{CQ}\\hat{\\mathbf{x}}\n$$  \n\nor  \n\n$$\ny\\,=\\,[1\\quad1]{\\left[\\begin{array}{l l}{-1}&{0}\\\\ {\\,1}&{1}\\end{array}\\right]}{\\left[\\begin{array}{l}{{\\hat{x}}_{1}}\\\\ {{\\hat{x}}_{2}}\\end{array}\\right]}\\,=\\,[0\\quad1]{\\left[\\begin{array}{l}{{\\hat{x}}_{1}}\\\\ {{\\hat{x}}_{2}}\\end{array}\\right]}\n$$  \n\nEquations (10–157) and (10–158) are in the observable canonical form.  \n\nA–10–10. For the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nconsider the problem of designing a state observer such that the desired eigenvalues for the observer gain matrix are $\\mu_{1},\\mu_{2},\\dots,\\mu_{n}$ .  \n\nShow that the observer gain matrix given by Equation (10–61), rewritten as  \n\n$$\n\\mathbf{K}_{e}=\\left(\\mathbf{W}\\mathbf{N}^{*}\\right)^{-1}\\left[\\begin{array}{c}{\\alpha_{n}-\\left.a_{n}\\right.}\\\\ {\\alpha_{n-1}-\\left.a_{n-1}\\right.}\\\\ {\\left..}\\\\ {\\left..}\\\\ {\\left..}\\\\ {\\alpha_{1}-\\left.a_{1}\\right.}\\end{array}\\right]\n$$  \n\ncan be obtained from Equation (10–13) by considering the dual problem. That is, the matrix $\\mathbf{K}_{e}$ can be determined by considering the pole-placement problem for the dual system, obtaining the state-feedback gain matrix $\\mathbf{K}$ , and taking its conjugate transpose, or ${\\bf K}_{e}={\\bf K}^{*}$ .  \n\nSolution. The dual of the given system is  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{z}}=\\mathbf{A}^{*}\\mathbf{z}+\\mathbf{C}^{*}v}\\\\ {\\ }\\\\ {n=\\mathbf{B}^{*}\\mathbf{z}}\\end{array}\n$$  \n\nUsing the state-feedback control  \n\n$$\nv=-\\mathbf{K}\\mathbf{z}\n$$  \n\nEquation (10–160) becomes  \n\n$$\n\\pmb{\\dot{\\mathbf{z}}}=(\\mathbf{A}^{*}-\\mathbf{C}^{*}\\mathbf{K})\\mathbf{z}\n$$  \n\nEquation (10–13), which is rewritten here, is  \n\n$$\n\\mathbf{K}={\\left[\\alpha_{n}\\,-\\,a_{n}\\,\\,\\,\\vdots\\,\\,\\,}\\alpha_{n-1}\\,-\\,a_{n-1}\\,\\,\\,\\vdots\\,\\,\\cdots\\,\\,\\vdots\\,\\,\\,\\alpha_{2}\\,-\\,a_{2}\\,\\,\\,\\vdots\\,\\,\\,\\alpha_{1}\\,-\\,a_{1}\\right]}\\mathbf{T}^{-1}\n$$  \n\nwhere  \n\n$$\n\\mathbf{T}=\\mathbf{M}\\mathbf{W}=\\left[\\mathbf{C}^{*}\\ \\ \\vdots\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ \\ \\vdots\\ \\cdots\\ \\vdots\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]\\mathbf{W}\n$$  \n\nFor the original system, the observability matrix is  \n\n$$\n\\left[\\mathbf{C}^{*}\\ :\\ \\mathbf{A}^{*}\\mathbf{C}^{*}\\ :\\ \\backslash\\ \\cdots\\ :\\ \\left(\\mathbf{A}^{*}\\right)^{n-1}\\mathbf{C}^{*}\\right]=\\mathbf{N}\n$$  \n\nHence, matrix $\\mathbf{T}$ can also be written as  \n\n$$\n\\mathbf{T}=\\mathbf{NW}\n$$  \n\nSince $\\mathbf{W}\\,=\\,\\mathbf{W}^{*}$ \\*, we have  \n\n$$\n\\mathbf{T}^{*}=\\mathbf{W}^{*}\\mathbf{N}^{*}=\\mathbf{W}\\mathbf{N}^{*}\n$$  \n\nand  \n\n$$\n(\\mathbf{T}^{*})^{-1}\\,=\\,(\\mathbf{W}\\mathbf{N}^{*})^{-1}\n$$  \n\nTaking the conjugate transpose of both sides of Equation (10–146), we have  \n\n$$\n\\mathbf{K}^{*}=\\left(\\mathbf{T}^{-1}\\right)^{*}\\left[\\begin{array}{c}{\\alpha_{n}\\mathrm{~-~}a_{n}}\\\\ {\\alpha_{n-1}\\mathrm{~-~}a_{n-1}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\alpha_{1}\\mathrm{~-~}a_{1}}\\end{array}\\right]=\\left(\\mathbf{T}^{*}\\right)^{-1}\\left[\\begin{array}{c}{\\alpha_{n}\\mathrm{~-~}a_{n}}\\\\ {\\alpha_{n-1}\\mathrm{~-~}a_{n-1}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\alpha_{1}\\mathrm{~-~}a_{1}}\\end{array}\\right]=\\left(\\mathbf{W}\\mathbf{N}^{*}\\right)^{-1}\\left[\\begin{array}{c}{\\alpha_{n}\\mathrm{~-~}a_{n}}\\\\ {\\alpha_{n-1}\\mathrm{~-~}a_{n-1}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\mathrm{~}\\cdot\\mathrm{~}}\\\\ {\\alpha_{1}\\mathrm{~-~}a_{1}}\\end{array}\\right]\n$$  \n\nSince ${\\bf K}_{e}={\\bf K}^{*}$ , this last equation is the same as Equation (10–159).Thus, we obtained Equation (10–159) by considering the dual problem.  \n\nA–10–11. Consider an observed-state feedback control system with a minimum-order observer described by the following equations:  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {y=\\mathbf{C}\\mathbf{x}}\\\\ {u=-\\mathbf{K}\\widetilde{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{x}=\\binom{\\overline{{x}}_{a}}{\\overline{{\\mathbf{x}}}_{b}},\\qquad\\widetilde{\\mathbf{x}}\\,=\\,\\left[\\!\\!\\begin{array}{l}{x_{a}}\\\\ {\\widetilde{\\mathbf{x}}_{b}}\\end{array}\\!\\!\\right]\n$$  \n\n$x_{a}$ is the state variable that can be directly measured, and $\\widetilde{\\mathbf{x}}_{\\,b}$ corresponds to the observed state variables.  \n\nplacement Show that the closed-loop poles of the system comprise the closed-loop poles due to pole Cthe eigenvalues of matrix $(\\mathbf{A}-\\mathbf{\\deltaB}\\mathbf{K})]$ A B D osed-loop poles due to the minimumorder observer [the eigenvalues of matrix $\\left(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\right)]$  \n\nSolution. The error equation for the minimum-order observer may be derived as given by Equation (10–94), rewritten thus:  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\mathbf{e}\n$$  \n\nwhere  \n\n$$\n\\mathbf e=\\mathbf x_{b}\\,-\\,\\widetilde{\\mathbf x}\\,_{b}\n$$  \n\nFrom Equations (10–162) and (10–163), we obtain  \n\n$$\n{\\begin{array}{r l}&{{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\widetilde{\\mathbf{x}}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\left[\\underbrace{\\mathbf{\\dot{x}}_{a}}_{\\widetilde{\\mathbf{x}}_{b}}\\right]}=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\left[\\underbrace{\\mathbf{\\dot{x}}_{a}}_{\\mathbf{B}_{b}\\,-\\,\\mathbf{e}}\\right]}}\\\\ &{\\quad=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\left\\{\\mathbf{x}\\,-\\,{\\left[\\underbrace{0}_{\\mathbf{e}}\\right]}\\right\\}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{K}{\\left[\\underbrace{0}_{\\mathbf{e}}\\right]}}\\end{array}}\n$$  \n\nCombining Equations (10–164) and (10–165) and writing  \n\n$$\n\\mathbf{K}=\\left[K_{a}\\ :\\ \\mathbf{K}_{b}\\right]\n$$  \n\nwe obtain  \n\n$$\n\\begin{array}{r}{\\left[\\dot{\\mathbf{x}}\\right]=\\left[\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{BK}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{BK}_{b}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\right]\\left[\\mathbf{\\bar{c}}\\right]}\\\\ {\\mathbf{0}\\mathbf{\\Sigma}\\mathbf{\\Phi}\\mathbf{\\Xi}\\mathbf{A}_{b b}-\\mathbf{K}_{e}\\mathbf{A}_{a b}\\right]\\left[\\mathbf{\\bar{c}}\\right]}\\end{array}\n$$  \n\nEquation (10–166) describes the dynamics of the observed-state feedback control system with a minimum-order observer.The characteristic equation for this system is  \n\n$$\n\\begin{array}{r}{\\vert s\\mathbf{I}\\vert-\\mathbf{A}+\\mathbf{B}\\mathbf{K}}\\\\ {\\qquad\\mathbf{0}\\qquad\\qquad s\\mathbf{I}-\\mathbf{A}_{b b}+\\mathbf{K}_{e}\\mathbf{A}_{a b}\\vert=0}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\vert s\\mathbf{I}-\\mathbf{A}_{b}\\vert=0}\\end{array}\n$$  \n\nor  \n\nThe closed-loop poles of the observed-state feedback control system with a minimum-order observer consist of the closed-loop poles due to pole placement and the closed-loop poles due to the minimum-order observer. (Therefore, the pole-placement design and the design of the minimum-order observer are independent of each other.)  \n\nA–10–12. Consider a completely state controllable system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere $\\mathbf{X}=$ state vector ($n$ -vector )$u=$ control signal (scalar ) $y=$ output signal(scalar)$\\mathbf{A}=n\\times n$ constant matrix $\\mathbf{B}\\,=\\,n\\,\\times\\,1$ constant matrix $\\mathbf{C}=1\\times n$ constant matrix Suppose that the rank of the following $(n\\,+\\,1)\\,\\times\\,(n\\,+\\,1)$ matrix  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{-C}}&{\\mathbf{0}}\\end{array}\\right]\n$$  \n\nis $n\\,+\\,1.$ . Show that the system defined by  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\hat{\\mathbf{A}}\\mathbf{e}\\,+\\,\\hat{\\mathbf{B}}u_{e}\n$$  \n\nwhere  \n\n$$\n\\hat{\\mathbf{A}}=\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right],\\;\\;\\;\\;\\;\\;\\hat{\\mathbf{B}}=\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {\\mathbf{0}}\\end{array}\\right],\\;\\;\\;\\;\\;\\;u_{e}=u(t)\\,-\\,u(\\infty)\n$$  \n\nis completely state controllable.  \n\nSolution. Define  \n\n$$\n\\mathbf{M}=\\left[\\mathbf{B}\\;\\;\\vdots\\;\\;\\mathbf{A}\\mathbf{B}\\;\\;\\vdots\\;\\cdots\\;\\;\\vdots\\;\\;\\mathbf{A}^{n-1}\\mathbf{B}\\right]\n$$  \n\nBecause the system given by Equation (10–167) is completely state controllable,the rank of matrix $\\mathbf{M}$ is $n$ .Then the rank of  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{M}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{1}\\end{array}\\right]\n$$  \n\nis $n\\,+\\,1$ .Consider the following equation:  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right]\\!\\!\\left[\\begin{array}{l l}{\\mathbf{M}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{1}\\end{array}\\right]=\\left[\\begin{array}{l l}{\\mathbf{A}\\mathbf{M}}&{\\mathbf{B}}\\\\ {\\mathbf{-C}\\mathbf{M}}&{0}\\end{array}\\right]\n$$  \n\nSince matrix  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{-C}}&{\\mathbf{0}}\\end{array}\\right]\n$$  \n\nis of rank $n\\,+\\,1$ ,the left-hand side of Equation (10–169) is of rank $n\\,+\\,1,$ Therefore,the right-hand side of Equation (10–169) is also of rank $n\\,+\\,1$ .Since  \n\n$$\n{\\begin{array}{r l}{\\left[\\mathbf{\\Lambda}\\mathbf{A}\\mathbf{M}\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{B}\\right]={\\left[\\begin{array}{l}{\\mathbf{A}[\\mathbf{B}\\mid\\mathbf{\\Lambda}\\mathbf{A}\\mathbf{B}\\mid\\mathbf{\\Lambda}\\cdots\\mid\\mathbf{A}^{n-1}\\mathbf{B}]}&{\\mathbf{B}}\\\\ {\\mathbf{-C}[\\mathbf{B}\\mid\\mathbf{\\Lambda}\\mathbf{A}\\mathbf{B}\\mid\\mathbf{\\Lambda}\\cdots\\mid\\mathbf{A}^{n-1}\\mathbf{B}]}&{0}\\end{array}\\right]}}\\\\ &{\\qquad={\\left[\\begin{array}{l l l l l l}{\\mathbf{A}\\mathbf{B}\\mid\\mathbf{\\Lambda}\\mathbf{A}^{2}\\mathbf{B}}&{\\mid\\cdots\\mid\\mathbf{\\Lambda}\\mathbf{A}^{n}\\mathbf{B}}&{\\mid\\mathbf{B}}&{\\mid\\mathbf{\\Lambda}\\mathbf{B}}\\\\ {\\mathbf{-C}\\mathbf{B}\\mid\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{-C}\\mathbf{A}\\mathbf{B}\\mid\\cdots\\mid\\mathbf{-C}\\mathbf{A}^{n-1}\\mathbf{B}}&{\\mid\\mathbf{\\Lambda}0}\\end{array}\\right]}}\\\\ &{\\qquad={\\left[\\hat{\\mathbf{A}}\\hat{\\mathbf{B}}\\mid\\hat{\\mathbf{A}}^{2}\\hat{\\mathbf{B}}\\ \\mid\\cdots\\mid\\hat{\\mathbf{A}}^{n}\\hat{\\mathbf{B}}\\ \\mid\\hat{\\mathbf{B}}\\right]}}\\end{array}}\n$$  \n\nwe find that the rank of  \n\n$$\n[\\hat{\\bf B}\\vdots\\hat{\\bf\\Large A}\\hat{\\bf B}\\vdots\\hat{\\bf\\Large A}^{2}\\hat{\\bf\\Large B}\\vdots\\cdots\\vdots\\hat{\\bf\\Large A}^{n}\\hat{\\bf B}]\n$$  \n\nis $n\\,+\\,1.$ .Thus, the system defined by Equation (10–168) is completely state controllable.  \n\nA–10–13. Consider the system shown in Figure 10–49. Using the pole-placement-with-observer approach, 1 1 A Bdesign a regulator system such that the system will maintain the zero position $y_{1}=0$ and $y_{2}=0$ in the presence of disturbances. Choose the desired closed-loop poles for the pole-placement part to be  \n\n$$\ns=-2\\,+\\,j2\\sqrt{3}\\,,\\qquad s=-2\\,-\\,j2\\sqrt{3}\\,,\\qquad s=-10,\\qquad s=-10\n$$  \n\nand the desired poles for the minimum-order observer to be  \n\n$$\ns=-15,\\qquad s=-16\n$$  \n\nFirst, determine the state feedback gain matrix $\\mathbf{K}$ and observer gain matrix $\\mathbf{K}_{e}$ .Then, obtain the response of the system to an arbitrary initial condition—for example,  \n\n$$\n\\begin{array}{l l l l}{{y_{1}(0)\\,=\\,0.1,}}&{{\\quad y_{2}(0)\\,=\\,0,}}&{{\\dot{y}_{1}(0)\\,=\\,0,}}&{{\\quad\\dot{y}_{2}(0)\\,=\\,0}}\\\\ {{}}&{{}}&{{}}&{{}}\\\\ {{e_{1}(0)\\,=\\,0.1,}}&{{\\quad e_{2}(0)\\,=\\,0.05}}&{{}}&{{}}\\end{array}\n$$  \n\nwhere $e_{1}$ and $e_{2}$ are defined by  \n\n$$\n\\begin{array}{r}{e_{1}=y_{1}-\\,\\widetilde{y}_{\\,1}}\\\\ {e_{2}=y_{2}-\\,\\widetilde{y}_{\\,2}}\\end{array}\n$$  \n\nAssume that $m_{1}=1\\,\\mathrm{kg},m_{2}=2\\,\\mathrm{kg},k=36\\,\\mathrm{N/m}$ , and $b\\,=\\,0.6\\,\\mathrm{N{-s/m}}$ .  \n\nSolution. The equations for the system are  \n\n$$\n\\begin{array}{r l}&{m_{1}\\ddot{y}_{1}=k\\big(y_{2}\\,-\\,y_{1}\\big)\\,+\\,b\\big(\\dot{y}_{2}\\,-\\,\\dot{y}_{1}\\big)\\,+\\,u}\\\\ &{m_{2}\\ddot{y}_{2}=k\\big(y_{1}\\,-\\,y_{2}\\big)\\,+\\,b\\big(\\dot{y}_{1}\\,-\\,\\dot{y}_{2}\\big)}\\end{array}\n$$  \n\nBy substituting the given numerical values for $m_{1},m_{2},k$ ,and $^b$ and simplifying, we obtain  \n\n$$\n\\begin{array}{l}{\\ddot{y}_{1}=-36y_{1}+36y_{2}-0.6\\dot{y}_{1}+0.6\\dot{y}_{2}+u}\\\\ {\\ddot{y}_{2}=18y_{1}-18y_{2}+0.3\\dot{y}_{1}-0.3\\dot{y}_{2}}\\end{array}\n$$  \n\nLet us choose the state variables as follows:  \n\n$$\n\\begin{array}{l}{x_{1}=y_{1}}\\\\ {x_{2}=y_{2}}\\\\ {x_{3}=\\dot{y}_{1}}\\\\ {x_{4}=\\dot{y}_{2}}\\end{array}\n$$  \n\n![](images/9b2be1fc7d51c82d00e3dec070b9e3b02149b9b1a62d89c88060ced52c96f8b5.jpg)  \nFigure 10–49 Mechanical system.  \n\nThen, the state-space equations become  \n\n$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\\\ {\\dot{x}_{4}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c}{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {-36}&{36}&{-0.6}&{0.6}\\\\ {18}&{-18}&{0.3}&{-0.3}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]+\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\\\ {0}\\end{array}\\right]{u}}\\\\ &{\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\end{array}\\right]=\\left[\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\\\ {-0}&{0}&{0}&{0}\\\\ {18}&{-18}&{0}&{-0.3}\\end{array}\\right]\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]}\\end{array}\n$$  \n\nDefine  \n\n$$\n\\mathbf{A}=\\left[\\begin{array}{c c}{\\left.0}&{0}\\\\ {0}&{0}\\\\ {-36}&{36}\\\\ {18}&{-18}\\end{array}\\right|-0.6\\quad0.6}\\\\ {\\left.0\\right.}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\left.\\mathbf{A}_{a a}\\right|\\left.\\mathbf{A}_{a b}\\right]}\\\\ {\\left.\\mathbf{A}_{b a}\\right|\\left.\\mathbf{A}_{b b}\\right]},\\qquad\\mathbf{B}=\\left[\\begin{array}{c}{\\left.0}\\\\ {0}\\\\ {\\dots}\\\\ {1}\\\\ {0}\\end{array}\\right]=\\left[\\begin{array}{l}{\\mathbf{B}_{a}}\\\\ {\\dots}\\\\ {\\mathbf{B}_{b}}\\end{array}\\right]\n$$  \n\nThe state feedback gain matrix $\\mathbf{K}$ and observer gain matrix $\\mathbf{K}_{e}$ can be obtained easily by use of MATLAB as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathbf{K}=[130.4444\\_\\_-41.5556\\_\\ 23.1000\\ \\ 15.4185]}\\\\ &{\\mathbf{K}_{e}=\\left[\\begin{array}{c c}{14.4\\_\\ 0.6}\\\\ {0.3\\ \\ 15.7}\\end{array}\\right]}\\end{array}\n$$  \n\n(See MATLAB Program 10–26.)  \n\n![](images/ab2b83d603adb7aa80425ee431b68dc79268862cb8d0b1775f221eb5762e3b00.jpg)  \n\nResponse to Initial Condition: Next, we obtain the response of the designed system to the given initial condition. Since  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {u=-\\mathbf{K}\\widetilde{\\mathbf{x}}}\\\\ {\\widetilde{\\mathbf{x}}\\;=\\left[\\begin{array}{l}{\\mathbf{x}_{a}}\\\\ {\\widetilde{\\mathbf{x}}_{b}}\\end{array}\\right]=\\left[\\begin{array}{l}{\\mathbf{y}}\\\\ {\\widetilde{\\mathbf{x}}_{b}}\\end{array}\\right]}\\end{array}\n$$  \n\n$$\n{\\dot{\\mathbf{x}}}=\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}{\\widetilde{\\mathbf{x}}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{K}(\\mathbf{x}\\,-\\,{\\widetilde{\\mathbf{x}}}\\,)\n$$  \n\nNote that  \n\n$$\n\\mathbf{x}\\,-\\,{\\widetilde{\\mathbf{x}}}\\,=\\,{\\left[\\!\\!\\begin{array}{l}{\\mathbf{x}_{a}}\\\\ {\\mathbf{x}_{b}}\\end{array}\\!\\!\\right]}\\,-\\,{\\left[\\!\\!\\begin{array}{l}{\\mathbf{x}_{a}}\\\\ {{\\widetilde{\\mathbf{x}}}_{\\,b}}\\end{array}\\!\\!\\right]}\\,=\\,{\\left[\\!\\!\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{x}_{b}\\,-\\,{\\widetilde{\\mathbf{x}}}_{\\,b}}\\end{array}\\!\\!\\right]}\\,=\\,{\\left[\\!\\!\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{e}}\\end{array}\\!\\!\\right]}\\,=\\,{\\left[\\!\\!\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{I}}\\end{array}\\!\\!\\right]}\\mathbf{e}\\,=\\,\\mathbf{Fe}\n$$  \n\nwhere  \n\n$$\n\\mathbf{F}=\\left[{\\begin{array}{l}{\\mathbf{0}}\\\\ {\\dots}\\\\ {\\mathbf{I}}\\end{array}}\\right]\n$$  \n\nThen, Equation (10–170) can be written as  \n\n$$\n{\\dot{\\mathbf{x}}}\\,=\\,\\left(\\mathbf{A}\\,-\\,\\mathbf{BK}\\right)\\mathbf{x}\\,+\\,\\mathbf{BKFe}\n$$  \n\nSince, from Equation (10–94), we have  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\bigl(\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}\\bigr)\\mathbf{e}\n$$  \n\nby combining Equations (10–171) and (10–172) into one equation, we have  \n\n$$\n\\left[\\overbrace{\\mathbf{x}}^{\\star}\\right]=\\left[\\overbrace{\\mathbf{0}\\,\\,\\,\\,\\,\\,\\,\\mathbf{B}\\mathbf{K}}^{\\mathbf{A}\\,\\,\\,-\\,\\,\\,\\mathbf{B}\\mathbf{K}}\\overbrace{\\mathbf{A}_{b b}\\,-\\,\\mathbf{K}_{e}\\,\\mathbf{A}_{a b}}^{\\mathbf{B}\\mathbf{K}\\mathbf{F}}\\right]\\left[\\overbrace{\\mathbf{e}}^{\\mathbf{X}}\\right]\n$$  \n\nThe state matrix here is a $6\\times6$ matrix.The response of the system to the given initial condition can be obtained easily with MATLAB. (See MATLAB Program 10–27.) The resulting response curves are shown in Figure 10–50.The response curves seem to be acceptable.  \n\n![](images/65daf461cd3f9db76a75b4103598679d0276ed3f663dc1596317983ff5d0c3c2.jpg)  \nFigure 10–50 Response curves to initial condition.  \n\n![](images/d99d938867b763563fc1268891c8e0fb7b86cab6e27dcdf35a98d2b68d92e275.jpg)  \n\nA–10–14. Consider the system shown in Figure 10–51.Design both the full-order and minimum-order observers for the plant.Assume that the desired closed-loop poles for the pole-placement part are located at  \n\n$$\ns\\,=\\,-2\\,+\\,j2{\\sqrt{3}}\\,,\\qquad s\\,=\\,-2\\,-\\,j2\\,{\\sqrt{3}}\n$$  \n\nAssume also that the desired observer poles are located at   \n(a) $s=-8,s\\,=-8$   \n(b) $s=-8$ for the minimum-order observer Compare the responses to the initial conditions specified below:   \n(a) for the full-order observer:  \n\n$$\nx_{1}(0)\\,=\\,1,\\qquad x_{2}(0)\\,=\\,0,\\qquad e_{1}(0)\\,=\\,1,\\qquad e_{2}(0)\\,=\\,0\n$$  \n\nFigure 10–51 Regulator system.  \n\n![](images/0f96f894164ccac45c1db380183516c5e5368de673970a6afa6bbb7d60effe40.jpg)  \n\n(b) for the minimum-order observer:  \n\n$$\nx_{1}(0)\\,=\\,1,\\qquad x_{2}(0)\\,=\\,0,\\qquad e_{1}(0)\\,=\\,1\n$$  \n\nAlso, compare the bandwidths of both systems.  \n\nSolution. We first determine the state-space representation of the system. By defining state variables $x_{1}$ and $x_{2}$ as  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {x_{2}=\\dot{y}}\\end{array}\n$$  \n\nwe obtain  \n\n$$\n\\begin{array}{r}{\\left[\\dot{x}_{1}\\right]=\\left[0\\!\\!\\!\\!\\slash\\left.\\!\\!\\!\\begin{array}{r}{\\left.\\phantom{\\sum_{i}}\\right]}\\\\ {\\left.\\phantom{\\sum_{i}}\\dot{x}_{2}\\right]=\\left[\\!\\!\\!\\begin{array}{r}{0}\\\\ {0}\\end{array}\\right.\\!\\!-2\\!\\!\\!\\rfloor\\!\\!\\!\\left[\\!\\!\\!\\begin{array}{r}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\!\\right]+\\left[\\!\\!\\!\\begin{array}{r}{0}\\\\ {4}\\end{array}\\!\\!\\!\\right]u}\\\\ {y=[1\\!\\!\\!\\!\\slash\\!\\!\\!\\slash\\left.\\!\\!\\!\\slash\\left.\\!\\!\\!\\begin{array}{r}{0}\\\\ {x_{2}}\\end{array}\\right]}\\end{array}\n$$  \n\nFor the pole-placement part,we determine the state feedback gain matrix K.Using MATLAB, we find $\\mathbf{K}$ to be  \n\n$$\n\\mathbf{K}\\,=\\,[4\\quad0.5]\n$$  \n\n(See MATLAB Program 10–28.)  \n\nNext, we determine the observer gain matrix $\\mathbf{K}_{e}$ for the full-order observer. Using MATLAB, we find $\\mathbf{K}_{e}$ to be  \n\n$$\n\\mathbf{K}_{e}=\\left[\\begin{array}{l}{14}\\\\ {36}\\end{array}\\right]\n$$  \n\n(See MATLAB Program 10–28.)  \n\n![](images/be56292f8c8e1ed3055a41ac3f31e726339c52b53ad613ffd02977ef2de8ba95.jpg)  \n\nNow we find the response of this system to the given initial condition. Referring to Equation (10–70), we have  \n\n$$\n\\begin{array}{r}{\\left[\\begin{array}{l}{\\dot{\\mathbf{x}}}\\\\ {\\dot{\\mathbf{e}}}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K}}&{\\mathbf{B}\\mathbf{K}}\\\\ {\\mathbf{0}}&{\\mathbf{A}\\,-\\,\\mathbf{K}_{e}\\mathbf{C}\\end{array}\\right]\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\mathbf{e}}\\end{array}\\right]}\\end{array}\n$$  \n\nThis equation defines the dynamics of the designed system using the full-order observer.MATLAB Program 10–29 produces the response to the given initial condition.The resulting response curves are shown in Figure 10–52.  \n\n![](images/e07c5c745d6aa0131aff441e797cc3a6350b0b6d915a551ab8f7b01219c8f6df.jpg)  \n\n![](images/a0082af042ae540c7545649142a4c4b8d1196263d5bda9a3ec9add8da6b09d02.jpg)  \nFigure 10–52 Response curves to initial condition.  \n\nTo obtain the transfer function of the observer controller, we use MATLAB. MATLAB Program 10–30 produces this transfer function.The result is  \n\n$$\n{\\frac{\\mathrm{num}}{\\mathrm{den}}}={\\frac{74s\\,+\\,256}{s^{2}\\,+\\,18s\\,+\\,108}}={\\frac{74(s\\,+\\,3.4595)}{(s\\,+\\,9\\,+\\,j5.1962)(s\\,+\\,9\\,-\\,j5.1962)}}\n$$  \n\nMATLAB Program 10–30   \n$\\%$ Determination of transfer function of observer controller ---- full-order observer $\\mathsf{A}=[0\\;\\;1\\,;0\\;\\;-2]$ ;  \n$\\mathsf{B}=[0;\\!4]$ ;  \n$C=[1\\ \\ 0]$ ;  \n$\\mathsf{K}=[4\\ \\ 0.5]$ ;  \n$\\begin{array}{r}{\\mathsf{K e}=\\left[14;36\\right]\\,}\\end{array}$ ;  \n[num,den] $=$ ss2tf(A-Ke\\*C-B\\*K, Ke,K,0)   \nnum $=$   \n0   74.0000   256.0000   \nden $=$   \n1   18   108  \n\nNext, we obtain the observer gain matrix $K_{e}$ for the minimum-order observer. MATLAB Program 10–31 produces $K_{e}$ .The result is  \n\n$$\nK_{e}=6\n$$  \n\n![](images/687d945932d8292ca338a1eb5b1275e6bb62190013928ab41af1d6459c05c556.jpg)  \n\nThe response of the system with minimum-order observer to the initial condition can be obtained as follows: By substituting $u=-\\mathbf{K}\\,\\widetilde{\\mathbf{x}}$ into the plant equation given by Equation (10–79)  \n\nwe find  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}\\,\\widetilde{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,-\\,\\mathbf{B}\\mathbf{K}\\mathbf{x}\\,+\\,\\mathbf{B}\\mathbf{K}(\\mathbf{x}\\,-\\,\\widetilde{\\mathbf{x}}\\,)}\\\\ {\\;}\\\\ {=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}\\big[K_{a}\\quad K_{b}\\big]\\!\\!\\left[\\!\\!\\begin{array}{l}{0}\\\\ {e\\!\\!\\!\\int\\!\\!\\!\\mathbf{d}}\\end{array}\\!\\!\\!\\right]}\\end{array}\n$$  \n\nor  \n\n$$\n\\dot{\\mathbf{x}}\\,=\\,(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\mathbf{x}\\,+\\,\\mathbf{B}K_{b}e\n$$  \n\nThe error equation is  \n\n$$\n{\\dot{e}}\\,=\\,\\bigl(A_{b b}\\,-\\,K_{e}A_{a b}\\bigr)e\n$$  \n\nHence the system dynamics are defined by  \n\n$$\n\\left[\\begin{array}{l}{\\dot{\\mathbf{x}}}\\\\ {\\dot{e}}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\mathbf{A}-\\mathbf{B}\\mathbf{K}}&{\\mathbf{B}K_{b}}\\\\ {0}&{A_{b b}-K_{e}A_{a b}\\right]\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {e}\\end{array}\\right]\n$$  \n\nBased on this last equation, MATLAB Program 10–32 produces the response to the given initial condition.The resulting response curves are shown in Figure 10–53.  \n\n![](images/391ef236b1052f24f3d0c142317bfa2f82013efb261460fd1625130b5cf7841e.jpg)  \n\n![](images/5ce334e2a583f3648158fb61324f16c48967720f18ccc2dcb3b8fadd4b3f9abb.jpg)  \nFigure 10–53 Response curves to initial condition.  \n\nThe transfer function of the observer controller, when the system uses the minimum-order observer, can be obtained by use of MATLAB Program 10–33.The result is  \n\n$$\n{\\frac{\\mathrm{num}}{\\mathrm{den}}}={\\frac{7s\\,+\\,32}{s\\,+\\,10}}={\\frac{7(s\\,+\\,4.5714)}{s\\,+\\,10}}\n$$  \n\n# MATLAB Program 10–33  \n\n![](images/12d4b416b99fcc4bf051cf7234a084ec7c7a6fcae2f342d40fc6e8ccdb3ce8aa.jpg)  \n\n# Figure 10–54  \n\nBode diagrams of System 1 (system with full-order   \nobserver) and System 2   \n(system with minimum  \norder observer).   \nSystem $1=$   \n$(296s\\,+\\,1024)/$   \n$(s^{4}\\,+\\,20s^{3}\\,+\\,144s^{2}$   \n$+\\ 512s\\,+\\,1024\\}$ );   \nSystem $2=~(28s~+~128)/$ $(s^{3}\\,+\\,12s^{2}\\,+\\,48s\\,+\\,128)$ .  \n\n![](images/b2abcc5219c0b3597b1bb3ca555bdc8499d98f5834d879ee3074ddf8229b7a81.jpg)  \nFrequency (rad/sec)  \n\nThe observer controller is clearly a lead compensator.  \n\nThe Bode diagrams of System 1 (closed-loop system with full-order observer) and of System 2 (closed-loop system with minimum-order observer) are shown in Figure 10–54. Clearly, the bandwidth of System 2 is wider than that of System 1. System 1 has a better high-frequency noiserejection characteristic than System 2.  \n\nA–10–15. Consider the system  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nwhere $\\mathbf{X}$ is a state vector ( $n$ -vector) and A is an $n\\times n$ constant matrix.We assume that A is nonsingular. Prove that if the equilibrium state $\\mathbf{x}=\\mathbf{0}$ of the system is asymptotically stable (that is, if $\\mathbf{A}$ is a stable matrix), then there exists a positive-definite Hermitian matrix $\\mathbf{P}$ such that  \n\n$$\n\\mathbf{A}^{*}\\mathbf{P}+\\mathbf{P}\\mathbf{A}=-\\mathbf{Q}\n$$  \n\nwhere $\\mathbf{Q}$ is a positive-definite Hermitian matrix.  \n\nSolution. The matrix differential equation.  \n\n$$\n{\\dot{\\mathbf{X}}}=\\mathbf{A}^{*}\\mathbf{X}+\\mathbf{X}\\mathbf{A},\\qquad\\mathbf{X}(0)=\\mathbf{Q}\n$$  \n\nhas the solution  \n\n$$\n\\mathbf{X}=e^{\\mathbf{A}^{*}t}\\mathbf{Q}e^{\\mathbf{A}t}\n$$  \n\nIntegrating both sides of this matrix differential equation from $t\\,=\\,0$ to $t=\\infty$ , we obtain  \n\n$$\n{\\bf X}(\\infty)\\,-\\,{\\bf X}(0)={\\bf A}^{*}\\bigg(\\int_{0}^{\\infty}\\!{\\bf X}\\,d t\\bigg)\\,+\\,\\bigg(\\int_{0}^{\\infty}\\!{\\bf X}\\,d t\\bigg){\\bf A}\n$$  \n\nNoting that $\\mathbf{A}$ is a stable matrix and, therefore, $\\mathbf{X}(\\infty)\\,=\\,\\mathbf{0}$ ,we obtain  \n\n$$\n-\\mathbf{X}(0)\\,=\\,-\\mathbf{Q}\\,=\\,\\mathbf{A}^{*}{\\biggl(}\\int_{0}^{\\infty}\\!\\mathbf{X}\\,d t{\\biggr)}\\,+\\,{\\biggl(}\\int_{0}^{\\infty}\\!\\mathbf{X}\\,d t{\\biggr)}\\mathbf{A}\n$$  \n\nLet us put  \n\n$$\n\\mathbf{P}=\\,\\int_{0}^{\\infty}\\!\\mathbf{X}\\,d t=\\,\\int_{0}^{\\infty}\\!e^{\\mathbf{A}^{*}t}\\mathbf{Q}e^{\\mathbf{A}t}\\,d t\n$$  \n\nNote that the elements of $e^{\\mathbf{A}t}$ are finite sums of terms like ${\\mathbf\\Lambda}_{\\zeta}^{\\lambda_{i}t},\\,t e^{\\lambda_{i}t}\\,\\dots,\\,t^{m_{i}-1}\\,e^{\\lambda_{i}t}$ ,where the $\\lambda_{i}$ are the eigenvalues of $\\mathbf{A}$ and $m_{i}$ is the multiplicity of $\\lambda_{i}$ .Since the $\\lambda_{i}$ possess negative real parts,  \n\n$$\n\\int_{0}^{\\infty}\\!e^{\\mathbf{A}^{*}t}\\mathbf{Q}e^{\\mathbf{A}t}\\,d t\n$$  \n\nexists. Note that  \n\n$$\n\\mathbf{P}^{*}=\\,\\int_{0}^{\\infty}\\!e^{\\mathbf{A}^{*}t}\\mathbf{Q}e^{\\mathbf{A}t}\\,d t=\\mathbf{P}\n$$  \n\nThus $\\mathbf{P}$ is Hermitian (or symmetric if $\\mathbf{P}$ is a real matrix).We have thus shown that for a stable A and for a positive-definite Hermitian matrix Q, there exists a Hermitian matrix $\\mathbf{P}$ such that $\\mathbf{A}^{*}\\mathbf{P}+\\mathbf{P}\\mathbf{A}=-\\mathbf{Q}.$ We now need to prove that $\\mathbf{P}$ is positive definite. Consider the following Hermitian form:  \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\bf x}^{*}{\\bf P}{\\bf x}={\\bf x}^{*}\\int_{0}^{\\infty}e^{{\\bf A}^{*}t}{\\bf Q}e^{{\\bf A}t}\\,d t\\,{\\bf x}\\ ~}}\\\\ {{\\displaystyle~~~~~~~~=~\\int_{0}^{\\infty}(e^{{\\bf A}t}{\\bf x})^{*}{\\bf Q}\\big(e^{{\\bf A}t}{\\bf x}\\big)d t>0,~~~~~\\mathrm{for~}{\\bf x}\\ne{\\bf0}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~=~0,~~~~~\\mathrm{for~}{\\bf x}={\\bf0}}}\\end{array}\n$$  \n\nHence, $\\mathbf{P}$ is positive definite.This completes the proof.  \n\nA–10–16. Consider the control system described by  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n{\\bf A}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\end{array}\\right]},\\qquad{\\bf B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nAssuming the linear control law  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\\,=-k_{1}x_{1}\\,-\\,k_{2}x_{2}\n$$  \n\ndetermine the constants $k_{1}$ and $k_{2}$ so that the following performance index is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!\\mathbf{x}^{T}\\mathbf{x}\\,d t\n$$  \n\nChapter 10 /Control Systems Design in State Space  \n\nConsider only the case where the initial condition is  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{c}\\\\ {0}\\end{array}\\right]}\n$$  \n\nChoose the undamped natural frequency to be 2 rad 'sec.  \n\nSolution. Substituting Equation (10–174) into Equation (10–173), we obtain  \n\n$$\n\\dot{\\bf x}\\,=\\,{\\bf A}{\\bf x}\\,-\\,{\\bf B}{\\bf K}{\\bf x}\n$$  \n\nor  \n\n$$\n{\\begin{array}{r l}{\\left[{\\dot{x}}_{1}\\right]={\\left[\\!\\!\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}+{\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right]}[-k_{1}x_{1}-k_{2}x_{2}]}\\\\ &{={\\left[\\!\\!\\begin{array}{l l}{0}&{1}\\\\ {-k_{1}}&{-k_{2}}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\!\\!\\right]}}\\end{array}}\n$$  \n\nThus,  \n\n$$\n\\mathbf{A}\\mathrm{~-~}\\mathbf{B}\\mathbf{K}=\\left[\\begin{array}{c c}{0}&{1}\\\\ {-k_{1}}&{-k_{2}}\\end{array}\\right]\n$$  \n\nElimination of $x_{2}$ from Equation (10–175) yields  \n\n$$\n\\ddot{x}_{1}\\,+\\,k_{2}\\dot{x}_{1}\\,+\\,k_{1}x_{1}=0\n$$  \n\nSince the undamped natural frequency is specified as 2 rad 'sec, we obtain  \n\n$$\nk_{1}=4\n$$  \n\nTherefore,  \n\n$$\n\\mathbf{A}-\\mathbf{B}\\mathbf{K}=\\left[{\\begin{array}{c c}{0}&{1}\\\\ {-4}&{-k_{2}}\\end{array}}\\right]\n$$  \n\nA -BK is a stable matrix if $k_{2}>0$ .Our problem now is to determine the value of $k_{2}$ so that the performance index  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!{\\mathbf{x}}^{T}{\\mathbf{x}}\\,d t={\\mathbf{x}}^{T}(0)\\,{\\mathbf{P}}(0){\\mathbf{x}}(0)\n$$  \n\nis minimized, where the matrix $\\mathbf{P}$ is determined from Equation (10–115), rewritten  \n\n$$\n(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})^{*}\\mathbf{P}\\,+\\,\\mathbf{P}(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\,=\\,-(\\mathbf{Q}\\,+\\,\\mathbf{K}^{*}\\mathbf{R}\\mathbf{K})\n$$  \n\nSince in this system $\\mathbf{Q}=\\mathbf{I}$ and $\\mathbf{R}=\\mathbf{0}$ , this last equation can be simplified to  \n\n$$\n\\mathbf{\\left(A\\beta-\\frac{\\partial\\mathbf{K}}{\\partial\\mathbf{K}}\\right)}^{*}\\mathbf{P}\\,+\\,\\mathbf{P}(\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{K})\\,=\\,-\\mathbf{I}\n$$  \n\nSince the system involves only real vectors and real matrices, $\\mathbf{P}$ becomes a real symmetric matrix. Then Equation (10–176) can be written as  \n\n$$\n{\\left[\\!\\!\\begin{array}{l l}{0}&{-4}\\\\ {1}&{-k_{2}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}\\end{array}\\!\\!\\right]}+{\\left[\\!\\!\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}\\end{array}\\!\\!\\right]}{\\left[\\!\\!\\begin{array}{l l}{0}&{1}\\\\ {-4}&{-k_{2}\\end{array}\\!\\!\\right]}={\\left[\\!\\!\\begin{array}{l l}{-1}&{0}\\\\ {0}&{-1}\\end{array}\\!\\!\\right]}\n$$  \n\nSolving for the matrix $\\mathbf{P}$ , we obtain  \n\n$$\n\\mathbf{P}={\\left[\\begin{array}{l l}{p_{11}}&{p_{12}}\\\\ {p_{12}}&{p_{22}}\\end{array}\\right]}={\\left[\\begin{array}{l l}{{\\frac{5}{2k_{2}}}+{\\frac{k_{2}}{8}}}&{{\\frac{1}{8}}}\\\\ {{\\frac{1}{8}}}&{{\\frac{5}{8k_{2}}}}\\end{array}\\right]}\n$$  \n\nThe performance index is then  \n\n$$\n{\\begin{array}{r l}&{J=\\mathbf{x}^{T}(0)\\mathbf{P}\\mathbf{x}(0)}\\\\ &{\\quad=[c\\quad0]{\\Biggl[}\\!\\!\\left[p_{11}\\quad p_{12}\\right]\\!\\!{\\Biggl[}\\!\\!\\left[c\\,\\right]\\!\\!{\\Biggr]}=p_{11}c^{2}}\\\\ &{\\quad={\\Biggl(}{\\frac{5}{2k_{2}}}+{\\frac{k_{2}}{8}}{\\Biggl)}c^{2}}\\end{array}}\n$$  \n\nTo minimize $J$ , we differentiate $J$ with respect to $k_{2}$ and set $\\partial J/\\partial k_{2}$ equal to zero as follows:  \n\n$$\n\\frac{\\partial J}{\\partial k_{2}}=\\left(\\frac{-5}{2k_{2}^{2}}+\\frac{1}{8}\\right)c^{2}=0\n$$  \n\nHence,  \n\n$$\nk_{2}=\\sqrt{20}\n$$  \n\nWith this value of $k_{2}$ ,we have $\\partial^{2}J/\\partial k_{2}^{2}>0.$ Thus, the minimum value of $J$ is obtained by substituting $k_{2}=\\sqrt{20}$ into Equation (10–177), or  \n\n$$\nJ_{\\operatorname*{min}}={\\frac{\\sqrt{5}}{2}}\\,c^{2}\n$$  \n\nThe designed system has the control law  \n\n$$\nu\\,=\\,-4x_{1}\\,-\\,\\sqrt{20}x_{2}\n$$  \n\nThe designed system is optimal in that it results in a minimum value for the performance index $J$ under the assumed initial condition.  \n\nA–10–17. Consider the same inverted-pendulum system as discussed in Example 10–5.The system is shown in Figure 10–8, where $M\\,=\\,2\\,\\mathrm{kg},m\\,=\\,0.1\\,\\mathrm{kg}.$ , and $l\\,=\\,0.5\\:\\mathrm{m}$ .The block diagram for the system is shown in Figure 10–9.The system equations are given by  \n\n$$\n\\begin{array}{r l}&{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ &{\\dot{y}=\\mathbf{C}\\mathbf{x}}\\\\ &{u=-\\mathbf{K}\\mathbf{x}+k_{I}\\boldsymbol{\\xi}}\\\\ &{\\dot{\\boldsymbol{\\xi}}=r-y=r-\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l l}{\\qquad0}&{1}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}\\\\ {\\qquad0}&{0}&{0}&{1}\\\\ {-0.4905}&{0}&{0}&{0}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{\\qquad0}\\\\ {-1}\\\\ {\\qquad0}\\\\ {0.5}\\end{array}\\right]},\\qquad\\mathbf{C}=[0}&{0}&{1}&{0]\n$$  \n\nReferring to Equation (10–51), the error equation for the system is given by  \n\n$$\n\\dot{\\mathbf{e}}\\,=\\,\\hat{\\mathbf{A}}\\mathbf{e}\\,+\\,\\hat{\\mathbf{B}}u_{e}\n$$  \n\nwhere  \n\n$$\n{\\hat{\\mathbf{A}}}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\right]}={\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{0}\\\\ {-0.4905}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{-1}&{0}&{0}\\end{array}\\right]},\\qquad{\\hat{\\mathbf{B}}}={\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {\\mathbf{B}}\\\\ {0}\\\\ {0}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {-1}\\\\ {0}\\\\ {0.5}\\\\ {0}\\end{array}\\right]}\n$$  \n\nand the control signal is given by Equation (10–41):  \n\n$$\nu_{e}=-\\hat{\\bf K}{\\bf e}\n$$  \n\nwhere  \n\n$$\n\\begin{array}{r l}&{\\hat{\\mathbf{K}}=\\left[\\mathbf{K}\\mathbf{\\beta}\\right]\\mathbf{\\beta}\\{\\mathbf{\\beta}-k_{I}\\}=\\left[k_{1}\\quad k_{2}\\quad k_{3}\\quad k_{4}\\ \\}-k_{I}\\right]}\\\\ &{}\\\\ &{\\mathbf{e}=\\left[\\mathbf{x}_{e}\\mathbf{\\beta}\\right]=\\left[\\mathbf{\\beta}\\mathbf{x}(t)\\mathbf{\\beta}-\\mathbf{x}(\\infty)\\right]}\\\\ &{\\mathbf{\\beta}}\\end{array}\n$$  \n\n$$\n\\mathbf{x}={\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\\\ {x_{4}}\\end{array}\\right]}={\\left[\\begin{array}{l}{\\theta}\\\\ {{\\dot{\\theta}}}\\\\ {x}\\\\ {{\\dot{x}}}\\end{array}\\right]}\n$$  \n\nUsing MATLAB, determine the state feedback gain matrix $\\hat{\\bf K}$ such that the following performance index $J$ is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf e}^{*}{\\bf Q}{\\bf e}\\,+\\,u^{*}R u)\\,d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}=\\left[\\begin{array}{r r r r r}{100}&{0}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right],\\qquad R=0.01\n$$  \n\nObtain the unit-step response of the system designed.  \n\nSolution. A MATLAB program to determine $\\hat{\\bf K}$ is given in MATLAB Program 10–34.The result is  \n\n$$\n-188.0799,\\qquad k_{2}=-37.0738,\\qquad k_{3}=-26.6767,\\qquad k_{4}=-30.5824,\\qquad k_{I}=-10.0061.\n$$  \n\n# MATLAB Program 10–34  \n\n$\\%$ Design of quadratic optimal control system   \nA = [0  1  0  0;20.601  0  0  0;0  0  0  1;-0.4905  0  0  0];  \n$\\mathsf{B}=[0;\\!-1;\\!0;\\!0.5]$ ;  \n${\\mathsf{C}}=[0\\;\\;0\\;\\;1\\;\\;\\;0]\\,;$   \n$\\mathsf{D}=[0]$ ;  \nAhat $=$ [A zeros(4,1);-C  0];  \n$\\mathrm{Bhat}=[\\mathsf{B};\\boldsymbol{0}]$ ;  \nQ = [100  0  0  0  0;0  1  0  0  0;0  0  1  0  0;0  0  0  1  0;0  0  0  0  1]; $\\mathsf{R}=[0.01]$ ;  \nKhat $=$ lqr(Ahat,Bhat,Q,R)   \nKhat $=$   \n-188.0799  -37.0738  -26.6767  -30.5824  10.0000  \n\nUnit-Step Response .Once we have determined the feedback gain matrix $\\mathbf{K}$ and the integral gain constant $k_{I}$ ,we can determine the unit-step response of the designed system.The system equation is  \n\n$$\n{\\overset{\\triangledown}{\\left[{\\dot{\\mathbf{x}}}\\right]}}={\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{0}}\\\\ {\\mathbf{-C}}&{0}\\end{array}\\right]}{\\overset{\\triangledown}{\\left[{\\mathbf{\\xi}}\\right]}}+{\\left[\\begin{array}{l}{\\mathbf{B}}\\\\ {0}\\end{array}\\right]}u+{\\left[\\begin{array}{l}{\\mathbf{0}}\\\\ {1}\\end{array}\\right]}r\n$$  \n\n[Refer to Equation (10–35).] Since  \n\n$$\nu\\,=\\,-\\mathbf{K}\\mathbf{x}\\,+\\,k_{I}\\xi\n$$  \n\nEquation (10–178) can be written as follows:  \n\n$$\n\\left[\\!\\!\\begin{array}{l}{\\dot{\\mathbf{x}}}\\\\ {\\dot{\\boldsymbol{\\xi}}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{l l}{\\mathbf{A}-\\mathbf{BK}}&{\\mathbf{B}k_{I}}\\\\ {-\\mathbf{C}}&{0}\\end{array}\\!\\!\\right]\\!\\!\\left[\\!\\!\\begin{array}{l}{\\mathbf{x}}\\\\ {\\boldsymbol{\\xi}}\\end{array}\\!\\!\\right]+\\left[\\!\\!\\begin{array}{l}{\\mathbf{0}}\\\\ {\\boldsymbol{1}}\\end{array}\\!\\!\\right]r\n$$  \n\nThe output equation is  \n\n$$\ny=[\\mathbf{C}\\quad0]{\\left[\\begin{array}{l}{\\mathbf{x}}\\\\ {\\xi}\\end{array}\\right]}+[0]r\n$$  \n\nMATLAB Program 10–35 gives the unit-step response of the system given by Equation (10–179). The resulting response curves are presented in Figure 10–55. It shows response curves versus t, $\\dot{\\theta}[=x_{2}(\\dot{t})]$ #CDversus t$y\\left[=x_{3}^{\\circ}(t)\\right]$ CDversus t, $\\dot{y}\\tilde{[}=x_{4}(t)]$ CDversus $t$ ,and $\\dot{\\boldsymbol{\\xi}}\\left[=\\boldsymbol{x}_{5}(t)\\right]$ CDversus $\\theta\\left[=x_{1}(t)\\right]$ $t$ ,where the input $r(t)$ to the cart is a unit-step function $\\bar{[{r}(t)=1\\mathrm{\\,m}]}$ .All initial conditions are set equal to zero. Figure 10–56 is an enlarged version of the cart position $y\\left[=x_{3}(t)\\right]$ versus $t$ .The cart moves backward a very small amount for the first 0.6 sec or so. (Notice that the cart velocity is negative for the first $0.4\\:\\mathrm{sec.}$ .) This is due to the fact that the inverted-pendulum-on-the-cart system is a nonminimum-phase system.  \n\n![](images/a278a8328516d6416987c479185f16db73b1ed9be4d920ad4a0b351c6db53de8.jpg)  \n\nnotice that the response of the present system is less oscillatory and exhibits less maximum Comparing the step-response characteristics of this system with those of Example 10–5, we A Bovershoot in the position response $x_{3}$ versus $t$ . The system designed by use of the quadratic optimal regulator approach generally gives such characteristics—less oscillatory and well damped.  \n\n![](images/57fba72ceffe9f0356c097be3e36a4c7ae6eec2498f7deabe97d22134e4c05e4.jpg)  \nFigure 10–55 Response curves to a unit-step input.  \n\n![](images/32d968ec7b4c0db1c0fe77e989617954c3b6c09cb383963aef641a92888190f1.jpg)  \nFigure 10–56 Cart position versus tcurve.   \nA–10–18. Consider the stability of a system with unstructured additive uncertainty as shown in Figure 10–57(a). Define  \n\n$\\tilde{G}=$ true plant dynamics $G=$ model of plant dynamics $\\Delta_{a}=$ unstructured additive uncertainty  \n\n![](images/bb211ce28470b53beac5765f609f084f77703fed95cac8c031c2cfa5f85885a1.jpg)  \n\n# Figure 10–57  \n\n(a) Block diagram of a system with unstructured additive uncertainty;   \n(b)–(d) successive modifications of the block diagram of (a);   \n(e) block diagram showing a generalized plant with unstructured additive uncertainty;   \n(f) generalized plant diagram.  \n\nAssume that $\\Delta_{a}$ is stable and its upper bound is known.Assume also that $\\tilde{G}$ and $G$ are related by  \n\n$$\n\\widetilde{G}\\,=\\,G\\,+\\,\\Delta_{a}\n$$  \n\nObtain the condition that the controller $K$ must satisfy for robust stability.Also, obtain a generalized plant diagram for this system.  \n\nSolution. Let us obtain the transfer function between point $A$ and point $B$ in Figure 10–57(a). Redrawing Figure 10–57(a), we obtain Figure 10–57(b).Then the transfer function between points $A$ and $B$ can be obtained as  \n\n$$\n\\frac{K}{1\\,+\\,G K}\\,=\\,K(1\\,+\\,G K)^{-1}\n$$  \n\nDefine  \n\n$$\nK(1\\,+\\,G K)^{-1}\\,=\\,T_{a}\n$$  \n\nThen Figure 10–57(b) can be redrawn as Figure 10–57(c).By using the small-gain theorem,the condition for the robust stability of the closed-loop system can be obtained as  \n\n$$\n\\|\\Delta_{a}T_{a}\\|_{\\infty}<1\n$$  \n\nSince it is impossible to model $\\Delta_{a}$ precisely, we need to find a scalar transfer function $W_{a}(j\\omega)$ such that  \n\n$$\n\\overline{{{\\sigma}}}\\{\\Delta_{a}(j\\omega)\\}\\,<\\,|W_{a}(j\\omega)|\\quad\\mathrm{~for~all~}\\omega\n$$  \n\nand use this $W_{a}(j\\omega)$ instead of $\\Delta_{a}$ . Then, the condition for the robust stability of the closed-loop system can be given by  \n\n$$\n\\|W_{a}T_{a}\\|_{\\infty}<1\n$$  \n\nIf Inequality (10–181) holds true, then it is evident that Inequality (10–180) also holds true. So this is the condition to guarantee the robust stability of the designed system. In Figure 10–57(e), $\\Delta_{a}$ in Figure 10–57(d) was replaced by $W_{a}I$ .  \n\nTo summarize, if we make the $H_{\\infty}$ norm of the transfer function from $w$ to $z$ to be less than 1, the controller $K$ that satisfies Inequality (10–181) can be determined. Figure 10–57(e) can be redrawn as that shown in Figure 10–57(f), which is the generalized plant diagram for the system considered. Note that for this problem the $\\Phi$ matrix that relates the controlled variable $z$ and the exogenous disturbance $_w$ is given by  \n\n$$\nz\\,=\\,\\Phi(s)w\\,=\\,(W_{a}T_{a})w\\,=\\,[W_{a}K(I\\,+\\,G K)^{-1}]w\n$$  \n\nNoting that $u(s)\\,=\\,K(s)y(s)$ and referring to Equation (10–128), $\\Phi(s)$ is given by the elements of the $P$ matrix as follows:  \n\n$$\n\\Phi(s)\\,=\\,P_{11}\\,+\\,P_{12}K(I\\,-\\,P_{22}K)^{-1}P_{21}\n$$  \n\nTo make this $\\Phi(s)$ equal to $W_{a}K(I\\,+\\,G K)^{-1}$ ,we may choose $P_{11}=0,P_{12}=W_{a},P_{21}=I$ ,and $P_{22}=-G.$ .Then, the $P$ matrix for this problem can be obtained as  \n\n$$\nP=\\left[{0\\atop I-G}\\right]\n$$  \n\n# PROBLEMS  \n\nB–10–1. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{-1}&{\\;\\;\\;0}&{\\;\\;\\;1}\\\\ {\\;\\;1}&{-2}&{\\;\\;\\;0}\\\\ {\\;\\;0}&{\\;\\;\\;0}&{-3}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{\\;\\;1\\;\\;\\;0]\n$$  \n\nTransform the system equations into (a) controllable canonical form and (b) observable canonical form.  \n\nB–10–2. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{-1}&{\\;\\;0}&{\\;\\;1}\\\\ {\\;\\;1}&{-2}&{\\;\\;0}\\\\ {\\;\\;0}&{\\;\\;0}&{-3}\\end{array}\\right]},\\;\\;\\;\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\\\ {1}\\end{array}\\right]},\\;\\;\\;\\mathbf{C}=[1}&{\\;\\;1}&{\\;\\;1]\n$$  \n\nTransform the system equations into the observable canonical form.  \n\nB–10–3. Consider the system defined by  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-5}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\\\ {1}\\end{array}\\right]}\n$$  \n\nBy using the state-feedback control $u=-\\mathbf{K}\\mathbf{x}.$ ,it is desired to have the closed-loop poles at $s\\,=\\,-2\\,\\pm\\,j4,s\\,=\\,-10.$ .Determine the state-feedback gain matrix $\\mathbf{K}$ .  \n\nB–10–4. Solve Problem B–10–3 with MATLAB.  \n\nFigure 10–58 Type 1 servo system.  \n\nB–10–5. Consider the system defined by  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{0}{0}}-{\\binom{1}{2}}{\\binom{\\dot{x}_{1}}{x_{2}}}+{\\binom{1}{0}}u\n$$  \n\nShow that this system cannot be stabilized by the statefeedback control $u=-\\mathbf{K}\\mathbf{x}$ ,whatever matrix $\\mathbf{K}$ is chosen.  \n\nB–10–6. A regulator system has a plant  \n\n$$\n{\\frac{Y(s)}{U(s)}}={\\frac{10}{(s\\,+\\,1)(s\\,+\\,2)(s\\,+\\,3)}}\n$$  \n\nDefine state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=y}\\\\ {x_{2}=\\dot{x}_{1}}\\\\ {x_{3}=\\dot{x}_{2}}\\end{array}\n$$  \n\nBy use of the state-feedback control $u=-\\mathbf{K}\\mathbf{x}.$ ,it is desired to place the closed-loop poles at  \n\n$$\ns=-2\\,+\\,j2\\sqrt{3}\\,,\\qquad s=-2\\,-\\,j2\\sqrt{3}\\,,\\qquad s=-10\n$$  \n\nDetermine the necessary state-feedback gain matrix $\\mathbf{K}$ .  \n\nB–10–7. Solve Problem B–10–6 with MATLAB.  \n\nB–10–8. Consider the type 1 servo system shown in Figure 10–58. Matrices A ,B, and Cin Figure 10–58 are given by  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {0}&{-5}&{-6}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{0}&{0]\n$$  \n\nDetermine the feedback gain constants $k_{1},k_{2}$ ,and $k_{3}$ such that the closed-loop poles are located at  \n\n$$\ns\\,=\\,-2\\,+\\,j4,\\qquad s\\,=\\,-2\\,-\\,j4,\\qquad s\\,=\\,-10\n$$  \n\nObtain the unit-step response and plot the output $y(t)$ -versus$\\cdot t$ curve.  \n\n![](images/d3034f5d4d6c9a98bb4b3b2752259748434b5078254164ac7ba2a0ed5e49e1c3.jpg)  \n\nB–10–9. Consider the inverted-pendulum system shown in Figure 10–59.Assume that  \n\n$$\nM=2\\,\\mathrm{kg},\\qquad m=0.5\\,\\mathrm{kg},\\qquad l=1\\,\\mathrm{m}\n$$  \n\nDefine state variables as  \n\n$$\nx_{1}=\\theta,\\qquad x_{2}=\\dot{\\theta},\\qquad x_{3}=\\,x,\\qquad x_{4}=\\dot{x}\n$$  \n\nand output variables as  \n\n$$\ny_{1}=\\theta=x_{1},\\qquad y_{2}=x=x_{3}\n$$  \n\nDerive the state-space equations for this system.  \n\nIt is desired to have closed-loop poles at  \n\n$$\ns=-4\\,+\\,j4,\\qquad s=-4\\,-\\,j4,\\qquad s=-20,\\qquad s=-20\n$$  \n\nDetermine the state-feedback gain matrix $\\mathbf{K}$ .  \n\nUsing the state-feedback gain matrix $\\mathbf{K}$ thus determined, examine the performance of the system by computer simulation.Write a MATLAB program to obtain the response of the system to an arbitrary initial condition. Obtain the response curves $x_{1}(t)$ versus $t$ ,$x_{2}(t)$ versus $t$ ,$x_{3}(t)$ versus $t$ ,and $x_{4}(t)$ versus $t$ for the following set of initial condition:  \n\n$$\nx_{1}(0)\\,=\\,0,\\ \\ x_{2}(0)\\,=\\,0,\\ \\ x_{3}(0)\\,=\\,0,\\ \\ x_{4}(0)\\,=\\,1\\,\\mathrm{m/s}\n$$  \n\n![](images/83304c5f4dbf9516bf941103638fe3329032939aa5383e6c85488d83b8158405.jpg)  \n\nFigure 10–59 Inverted-pendulum system.  \n\nB–10–10. Consider the system defined by  \n\n$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}}\\\\ {y=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{-1}&{\\;\\;\\;1}\\\\ {\\;\\;\\;1}&{-2}\\end{array}\\right]},\\quad\\;\\;\\mathbf{C}=[1{\\begin{array}{l l}{\\;\\;\\;0}]\n$$  \n\nDesign a full-order state observer. The desired observer poles are $s\\,=-5$ and $s=-5$ .  \n\nB–10–11. Consider the system defined by  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u}\\\\ {\\qquad y=\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{\\,\\,0}&{\\,\\,1}&{\\,0}\\\\ {\\,\\,0}&{\\,\\,0}&{1}\\\\ {-5}&{-6}&{0}\\end{array}\\right]},\\qquad\\mathbf{B}={\\left[\\begin{array}{l}{\\!\\,0}\\\\ {\\!\\,0}\\\\ {\\!\\,1}\\end{array}\\right]},\\qquad\\mathbf{C}=[1}&{\\,\\,0}&{\\,0]\n$$  \n\nDesign a full-order state observer, assuming that the desired poles for the observer are located at  \n\n$$\ns\\,=\\,-10,\\qquad s\\,=\\,-10,\\qquad s\\,=\\,-15\n$$  \n\nB–10–12. Consider the system defined by  \n\n$$\n{\\left[\\begin{array}{l}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{x}_{3}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{\\ 0}&{\\ 1}&{\\ 0}\\\\ {\\ 0}&{\\ 0}&{\\ 1}\\\\ {\\ 1.244}&{\\ 0.3956}&{-3.145}\\end{array}\\right]}{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\n$$\n+\\left[{\\begin{array}{c}{0}\\\\ {0}\\\\ {1.244}\\end{array}}\\right]u\n$$  \n\n$$\ny=[1\\quad0\\quad0]{\\left[\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}}\\end{array}\\right]}\n$$  \n\nGiven the set of desired poles for the observer to be  \n\n$$\ns=-5\\,+\\,j5\\sqrt{3}\\,,\\qquad s=-5\\,-\\,j5\\sqrt{3}\\,,\\qquad s=-10\n$$  \n\ndesign a full-order observer.  \n\nB–10–13. Consider the double integrator system defined by  \n\n$$\n\\operatorname{i}\\!{})=u\n$$  \n\nIf we choose the state variables as  \n\n$$\n\\begin{array}{l}{x_{1}=\\,y}\\\\ {x_{2}=\\,\\dot{y}}\\end{array}\n$$  \n\nthen the state-space representation for the system becomes as follows:  \n\n$$\n\\begin{array}{r l}{\\left[\\dot{x}_{1}\\right]=\\left[0\\quad1\\right]\\left[\\!\\!\\left[x_{1}\\right]\\!\\!\\right]+\\left[0\\right]\\!\\!\\right]u}\\\\ {\\dot{x}_{2}\\!\\!\\,\\Bigg]=\\left[\\!\\!\\left[0\\quad0\\right]\\!\\!\\left[\\!\\!\\left[x_{2}\\right]\\!\\!\\right]\\!\\!+\\left[\\!\\!\\left[0\\right]\\!\\!\\right]\\!\\!u\\right.}\\\\ {\\left.y=\\left[1\\right.\\quad0\\right]\\!\\!\\left[\\!\\!\\left[x_{1}\\right]\\!\\!\\!\\right]\\!\\!}\\end{array}\n$$  \n\nIt is desired to design a regulator for this system. Using the pole-placement-with-observer approach,design an observer controller.  \n\nChoose the desired closed-loop poles for the poleplacement part to be  \n\n$$\ns\\,=\\,-0.7071\\,+\\,j0.7071,\\qquad s\\,=\\,-0.7071\\,-\\,j0.7071\n$$  \n\nand assuming that we use a minimum-order observer,choose the desired observer pole at  \n\n$$\ns=-5\n$$  \n\nB–10–14. Consider the system  \n\n$$\n\\begin{array}{l}{\\dot{\\mathbf{x}}\\,=\\,\\mathbf{A}\\mathbf{x}\\,+\\,\\mathbf{B}u}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{\\beta}}\\\\ {y\\,=\\,\\mathbf{C}\\mathbf{x}}\\end{array}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-6}&{-11}&{-6}\\end{array}\\right]},\\quad\\mathbf{B}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {1}\\end{array}\\right]},\\quad\\mathbf{C}=[1}&{0}&{0]\n$$  \n\nDesign a regulator system by the pole-placement-withobserver approach. Assume that the desired closed-loop poles for pole placement are located at  \n\n$$\ns\\,=\\,-1\\,+\\,j,\\qquad s\\,=\\,-1\\,-\\,j,\\qquad s\\,=\\,-5\n$$  \n\nThe desired observer poles are located at  \n\n$$\ns\\,=\\,-6,\\qquad s\\,=\\,-6,\\qquad s\\,=\\,-6\n$$  \n\nAlso, obtain the transfer function of the observer controller.  \n\nB–10–15. Using the pole-placement-with-observer approach, design observer controllers (one with a full-order observer and the other with a minimum-order observer) for the system shown in Figure 10–60.The desired closed-loop poles for the pole-placement part are  \n\n$$\ns\\,=\\,-1\\,+\\,j2,\\qquad s\\,=\\,-1\\,-\\,j2,\\qquad s\\,=\\,-5\n$$  \n\n![](images/0fd7e2c78685b7bf18e890e210c88b285867fc17ba4d180406c3bfbcadbe0b7a.jpg)  \n\n# Figure 10–60  \n\nControl system with observer controller in the feedforward path.  \n\nThe desired observer poles are  \n\n$$\n\\begin{array}{l l l l}{{s=-10,}}&{{s=-10,}}&{{s=-10}}&{{\\mathrm{for\\,the\\,full-order\\,observer}}}\\\\ {{}}&{{}}&{{}}\\\\ {{s=-10,}}&{{s=-10}}&{{\\mathrm{for\\,the\\,minimum-order\\,observer}.}}\\end{array}\n$$  \n\nCompare the unit-step responses of the designed systems.   \nCompare also the bandwidths of both systems.  \n\nB–10–16. Using the pole-placement-with-observer approach, design the control systems shown in Figures 10–61(a) and (b). Assume that the desired closed-loop poles for the pole placement are located at  \n\n$$\ns\\,=\\,-2\\,+\\,j2,\\qquad s\\,=\\,-2\\,-\\,j2\n$$  \n\nand the desired observer poles are located at  \n\n$$\ns\\,=\\,-8,\\qquad s\\,=\\,-8\n$$  \n\nObtain the transfer function of the observer controller. Compare the unit-step responses of both systems.[In System (b), determine the constant $N$ so that the steady-state output $y(\\infty)$ is unity when the input is a unit-step input.]  \n\n![](images/1b6f375ab62a41b0784691e022648115ba0d7958fb979691d6395ddabcd7de56.jpg)  \n\n# Figure 10–61  \n\nControl systems with observer controller: (a) observer controller in the feedforward path; (b) observer controller in the feedback path.  \n\nB–10–17. Consider the system defined by  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{Ax}\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{0}&{1}&{0}\\\\ {0}&{0}&{1}\\\\ {-1}&{-2}&{-a}\\end{array}\\right]}\n$$  \n\n$a=$ adjustable parameter $>0$  \n\nDetermine the value of the parameter $a$ so as to minimize the following performance index:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!\\mathbf{x}^{T}\\mathbf{x}\\,d t\n$$  \n\nAssume that the initial state ${\\bf x}(0)$ is given by  \n\n$$\n\\mathbf{x}(0)={\\left[\\begin{array}{l}{c_{1}}\\\\ {0}\\\\ {0}\\end{array}\\right]}\n$$  \n\nB–10–18. Consider the system shown in Figure 10–62. Determine the value of the gain $K$ so that the damping ratio $\\zeta$ of the closed-loop system is equal to 0.5. Then determine also the undamped natural frequency $\\omega_{n}$ of the closed-loop system.Assuming that $e(0)=1$ and $\\dot{e}(0)\\,=\\,0$ #evaluate  \n\n$$\n\\int_{0}^{\\infty}\\!e^{2}(t)\\,d t\n$$  \n\n![](images/7ef3408f37f84c0f1119921457c5bd335bc87d6cb69e92dd1da3c0f63c9dda84.jpg)  \nFigure 10–62 Control system.  \n\nB–10–19. Determine the optimal control signal $u$ for the system defined by  \n\n$$\n\\dot{\\mathbf{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n{\\bf A}={\\left[\\begin{array}{l r}{0}&{1}\\\\ {0}&{-1}\\end{array}\\right]},\\qquad{\\bf B}={\\left[\\begin{array}{l}{0}\\\\ {1}\\end{array}\\right]}\n$$  \n\nsuch that the following performance index is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}({\\bf x}^{T}{\\bf x}\\,+\\,u^{2})\\,d t\n$$  \n\nB–10–20. Consider the system  \n\n$$\n{\\binom{\\dot{x}_{1}}{\\dot{x}_{2}}}={\\binom{0}{0}}-{\\binom{1}{0}}{\\binom{\\dot{x}_{1}}{x_{2}}}+{\\binom{0}{1}}u\n$$  \n\nIt is desired to find the optimal control signal $u$ such that the performance index  \n\n$$\nJ\\,=\\;\\int_{0}^{\\infty}\\!({\\bf x}^{T}{\\bf Q}{\\bf x}\\,+\\,u^{2}\\big)\\,d t,\\;\\;\\;\\;\\;\\;{\\bf Q}\\,=\\,\\left[\\begin{array}{l l}{{1}}&{{0}}\\\\ {{0}}&{{\\mu}}\\end{array}\\right]\n$$  \n\nis minimized. Determine the optimal signal $u(t)$ .  \n\nB–10–21. Consider the inverted-pendulum system shown in Figure 10–59. It is desired to design a regulator system that will maintain the inverted pendulum in a vertical position in the presence of disturbances in terms of angle $\\theta$ and/or angular velocity $\\dot{\\theta}$ .The regulator system is required to return the cart to its reference position at the end of each control process. (There is no reference input to the cart.)  \n\nThe state-space equation for the system is given by  \n\n$$\n\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}+\\mathbf{B}u\n$$  \n\nwhere  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l l}{\\qquad0}&{\\qquad1}&{0}&{0}\\\\ {20.601}&{0}&{0}&{0}\\\\ {\\qquad0}&{0}&{0}&{1}\\\\ {\\qquad-0.4905}&{0}&{0}&{0}\\end{array}\\right]}\n$$  \n\n$$\n\\mathbf{B}=\\left[\\begin{array}{c}{0}\\\\ {-1}\\\\ {0}\\\\ {0.5}\\end{array}\\right],\\qquad\\mathbf{x}=\\left[\\begin{array}{c}{\\phantom{-}\\theta}\\\\ {\\dot{\\theta}}\\\\ {x}\\\\ {\\dot{x}}\\end{array}\\right]\n$$  \n\nWe shall use the state-feedback control scheme  \n\n$$\nu\\,=-\\mathbf{K}\\mathbf{x}\n$$  \n\nUsing MATLAB, determine the state-feedback gain matrix $\\mathbf{K}=\\left[k_{1}\\;\\;k_{2}\\;\\;k_{3}\\;\\;k_{4}\\right]$ such that the following performance index $J$ is minimized:  \n\n$$\nJ\\,=\\,\\int_{0}^{\\infty}\\!\\bigl({\\bf x}^{*}{\\bf Q}{\\bf x}\\,+\\,u^{*}R u\\bigr)d t\n$$  \n\nwhere  \n\n$$\n\\mathbf{Q}=\\left[\\begin{array}{r r r r}{100}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right],\\qquad R=1\n$$  \n\nThen obtain the system response to the following initial condition:  \n\n$$\n{\\left[\\begin{array}{l}{x_{1}(0)}\\\\ {x_{2}(0)}\\\\ {x_{3}(0)}\\\\ {x_{4}(0)}\\end{array}\\right]}={\\left[\\begin{array}{l}{0.1}\\\\ {0}\\\\ {0}\\\\ {0}\\end{array}\\right]}\n$$  \n\nPlot response curves $\\theta$ versus $t$ ,$\\dot{\\theta}$ versus $t,x$ versus $t$ ,and $\\dot{x}$ versus $t$ .  \n\n# Appendix  \n\n# Laplace Transform Tables  \n\nAppendix A first presents the complex variable and complex function.Then it presents tables of Laplace transform pairs and properties of Laplace transforms.Finally,it presents frequently used Laplace transform theorems and Laplace transforms of pulse function and impulse function.  \n\nComplex Variable. A complex number has a real part and an imaginary part, both of which are constant. If the real part and/or imaginary part are variables, a complex quantity is called a complex variable . In the Laplace transformation we use the notation $s$ as a complex variable; that is,  \n\n$$\ns\\,=\\,\\sigma\\,+\\,j\\omega\n$$  \n\nwhere $\\sigma$ is the real part and $\\omega$ is the imaginary part.  \n\nComplex Function. A complex function $G(s)$ , a function of $s$ , has a real part and an imaginary part or  \n\n$$\nG(s)\\,=\\,G_{x}\\,+\\,j G_{y}\n$$  \n\nwhere $G_{x}$ and $G_{y}$ are real quantities. The magnitude of A B$G(s)$ is $\\sqrt{G_{x}^{2}+G_{y}^{2}}$ ,and the angle $\\theta$ of $G(s)$ is $\\tan^{-1}\\!\\left(G_{y}/\\hat{G}_{x}\\right)\\!.$ .The angle is measured counterclockwise from the positive real axis.The complex conjugate of $G(s)$ is $\\bar{G}(s)\\,=\\,G_{x}\\,-\\,j G_{y}$ .  \n\nComplex functions commonly encountered in linear control systems analysis are single-valued functions of $s$ and are uniquely determined for a given value of $s$ .  \n\nA complex function $G(s)$ is said to be analytic in a region if $G(s)$ and all its derivatives exist in that region.The derivative of an analytic function $G(s)$ is given by  \n\n$$\n{\\frac{d}{d s}}\\,G(s)\\,=\\,\\operatorname*{lim}_{\\Delta s\\to0}{\\frac{G(s\\,+\\,\\Delta s)\\,-\\,G(s)}{\\Delta s}}=\\operatorname*{lim}_{\\Delta s\\to0}{\\frac{\\Delta G}{\\Delta s}}\n$$  \n\nSince $\\Delta s\\,=\\,\\Delta\\sigma\\,+\\,j\\Delta\\omega,\\,\\Delta s$ can approach zero along an infinite number of different paths. It can be shown, but is stated without a proof here, that if the derivatives taken along two particular paths, that is, $\\Delta s\\,=\\,\\Delta\\sigma$ and $\\Delta s=j\\Delta\\omega$ ,are equal, then the derivative is unique for any other path $\\Delta s\\,=\\,\\Delta\\sigma\\,+\\,j\\Delta\\omega$ and so the derivative exists.  \n\nFor a particular path $\\Delta s\\,=\\,\\Delta\\sigma$ (which means that the path is parallel to the real axis),  \n\n$$\n{\\frac{d}{d s}}\\,G(s)\\,=\\,\\operatorname*{lim}_{\\Delta\\sigma\\rightarrow0}\\left({\\frac{\\Delta G_{x}}{\\Delta\\sigma}}+\\,j\\,{\\frac{\\Delta G_{y}}{\\Delta\\sigma}}\\right)\\,=\\,{\\frac{\\partial G_{x}}{\\partial\\sigma}}\\,+\\,j\\,{\\frac{\\partial G_{y}}{\\partial\\sigma}}\n$$  \n\nFor another particular path $\\Delta s=j\\Delta\\omega$ (which means that the path is parallel to the imaginary axis),  \n\n$$\n{\\frac{d}{d s}}G(s)\\,=\\,\\operatorname*{lim}_{j\\Delta\\omega\\to0}\\!\\left({\\frac{\\Delta G_{x}}{j\\Delta\\omega}}+\\,j\\,{\\frac{\\Delta G_{y}}{j\\Delta\\omega}}\\right)\\,=\\,-j\\,{\\frac{\\partial G_{x}}{\\partial\\omega}}\\,+\\,{\\frac{\\Delta G_{y}}{\\partial\\omega}}\n$$  \n\nIf these two values of the derivative are equal,  \n\n$$\n\\frac{\\partial G_{x}}{\\partial\\sigma}+\\,j\\,\\frac{\\partial G_{y}}{\\partial\\sigma}=\\frac{\\partial G_{y}}{\\partial\\omega}-\\,j\\,\\frac{\\partial G_{x}}{\\partial\\omega}\n$$  \n\nor if the following two conditions  \n\n$$\n{\\frac{\\partial G_{x}}{\\partial\\sigma}}={\\frac{\\partial G_{y}}{\\partial\\omega}}\\qquad{\\mathrm{and}}\\qquad{\\frac{\\partial G_{y}}{\\partial\\sigma}}=-\\,{\\frac{\\partial G_{x}}{\\partial\\omega}}\n$$  \n\nare satisfied,then the derivative $d G\\left(s\\right)/d s$ is uniquely determined.These two conditions are known as the Cauchy–Riemann conditions.If these conditions are satisfied,the function $G(s)$ is analytic.  \n\nAs an example, consider the following $G(s)$ :  \n\n$$\nG(s)={\\frac{1}{s+1}}\n$$  \n\nThen  \n\n$$\nG(\\sigma\\,+\\,j\\omega)\\,=\\frac{1}{\\sigma\\,+\\,j\\omega\\,+\\,1}=G_{x}\\,+\\,j G_{y}\n$$  \n\nwhere  \n\n$$\nG_{x}=\\frac{\\sigma\\,+\\,1}{(\\sigma\\,+\\,1)^{2}\\,+\\,\\omega^{2}}\\qquad\\mathrm{and}\\qquad G_{y}=\\frac{-\\omega}{(\\sigma\\,+\\,1)^{2}\\,+\\,\\omega^{2}}\n$$  \n\nIt can be seen that, except at $s=-1$ (that is, $\\sigma=-1$ ,$\\omega\\,=\\,0$ ),$G(s)$ satisfies the Cauchy–Riemann conditions:  \n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial G_{x}}{\\partial\\sigma}=\\frac{\\partial G_{y}}{\\partial\\omega}=\\frac{\\omega^{2}\\,-\\,(\\sigma\\,+\\,1)^{2}}{\\left[(\\sigma\\,+\\,1)^{2}\\,+\\,\\omega^{2}\\right]^{2}}\\,\\ }\\\\ {\\displaystyle\\frac{\\partial G_{y}}{\\partial\\sigma}=-\\,\\frac{\\partial G_{x}}{\\partial\\omega}=\\frac{2\\omega(\\sigma\\,+\\,1)}{\\left[(\\sigma\\,+\\,1)^{2}\\,+\\,\\omega^{2}\\right]^{2}}}\\end{array}\n$$  \n\nHence $G(s)\\,=\\,1/(s\\,+\\,1)$ is analytic in the entire $s$ plane except at $s\\,=\\,-1.$ .The derivative $d G\\left(s\\right)/\\left.d s$ , except at $s=1$ , is found to be  \n\n$$\n\\begin{array}{c}{\\displaystyle{\\frac{d}{d s}G(s)=\\frac{\\partial G_{x}}{\\partial\\sigma}+j\\frac{\\partial G_{y}}{\\partial\\sigma}=\\frac{\\partial G_{y}}{\\partial\\omega}-j\\frac{\\partial G_{x}}{d\\omega}}}\\\\ {\\displaystyle{=-\\,\\frac{1}{(\\sigma\\,+\\,j\\omega\\,+\\,1)^{2}}=-\\,\\frac{1}{(s\\,+\\,1)^{2}}}}\\end{array}\n$$  \n\nNote that the derivative of an analytic function can be obtained simply by differentiating $G(s)$ with respect to $s$ . In this example,  \n\n$$\n\\frac{d}{d s}\\left(\\frac1{s\\,+\\,1}\\right)\\,=\\,-\\,\\frac1{(s\\,+\\,1)^{2}}\n$$  \n\nPoints in the $s$ plane at which the function $G(s)$ is analytic are called ordinary points, while points in the $s$ plane at which the function $G(s)$ is not analytic are called singular points. Singular points at which the function $G(s)$ or its derivatives approach infinity are called poles . Singular points at which the function $G(s)$ equals zero are called zeros .  \n\nIf $G(s)$ approaches infinity as $s$ approaches $-p$ and if the function  \n\n$$\nG(s)(s\\,+\\,p)^{n},\\qquad\\mathrm{for}\\,n=1,2,3,\\ldots\n$$  \n\nhas a finite, nonzero value at $s=-p$ , then $s=-p$ is called a pole of order $n$ . If $n=1$ ,the pole is called a simple pole. If $n\\,=\\,2,3,\\ldots$ , the pole is called a second-order pole, a third-order pole, and so on.  \n\nTo illustrate, consider the complex function  \n\n$$\nG(s)\\,=\\frac{K(s\\,+\\,2)(s\\,+\\,10)}{s(s\\,+\\,1)(s\\,+\\,5)(s\\,+\\,15)^{2}}\n$$  \n\n$G(s)$ has zeros at $s\\,=-2$ ,$s=-10$ , simple poles at $s\\,=\\,0,s\\,=-1,s\\,=-5$ , and a double pole (multiple pole of order 2) at $s\\,=\\,-15.$ Note that $G(s)$ becomes zero at $s=\\infty$ . Since for large values of $s$  \n\n$$\nG(s)\\div{\\frac{K}{s^{3}}}\n$$  \n\n$G(s)$ possesses a triple zero (multiple zero of order 3) at $s=\\infty,$ . If points at infinity are included, $G(s)$ has the same number of poles as zeros.To summarize, $G(s)$ has five zeros $[s=-2$ ,$s=-10$ ,$s=\\infty$ ,$s=\\infty$ ,$s=\\infty$ )and five poles $(s\\,=\\,0,\\,s\\,=\\,-1,\\,s\\,=\\,-5$ ,$s=-15,s=-15)$ .  \n\nLaplace Transformation. Let us define  \n\n$f(t)=$ a function of time $t$ such that $f(t)\\,=\\,0$ for $t<0$ $s=$ a complex variable $\\mathcal{L}=$ an operational symbol indicating that the quantity that it prefixes is to be transformed by the Laplace integral $\\textstyle\\int_{0}^{\\infty}\\!e^{-s t}\\,d t$   \n$F(s)=$ Laplace transform of $f(t)$  \n\nThen the Laplace transform of $f(t)$ is given by  \n\n$$\n{\\mathcal{L}}\\!\\left[f(t)\\right]=F(s)=\\,\\int_{0}^{\\infty}\\!\\!e^{-s t}\\,d t\\!\\left[f(t)\\right]=\\,\\int_{0}^{\\infty}\\!\\!f(t)e^{-s t}\\,d t\n$$  \n\nThe reverse process of finding the time function $f(t)$ from the Laplace transform $F(s)$ is called the inverse Laplace transformation .The notation for the inverse Laplace transformation is $\\mathcal{L}^{-1}$ , and the inverse Laplace transform can be found from $F(s)$ by the following inversion integral:  \n\n$$\n{\\mathcal{L}}^{-1}{\\big[}F(s){\\big]}=f(t)={\\frac{1}{2\\pi j}}\\int_{c-j\\infty}^{c+j\\infty}F(s)e^{s t}\\,d s,\\qquad{\\mathrm{for~}}t>0\n$$  \n\nwhere $c$ , the abscissa of convergence, is a real constant and is chosen larger than the real parts of all singular points of $F(s)$ .Thus, the path of integration is parallel to the $j\\omega$ axis and is displaced by the amount $c$ from it.This path of integration is to the right of all singular points.  \n\nEvaluating the inversion integral appears complicated.In practice,we seldom use this integral for finding $f(t)$ .We frequently use the partial-fraction expansion method given in Appendix B.  \n\nIn what follows we give Table A–1, which presents Laplace transform pairs of commonly encountered functions, and Table A–2, which presents properties of Laplace transforms.  \n\nTable A–1 Laplace Transform Pairs Table A–1 (continued )  \n\n![](images/e21a606183f0b288f4ff0c4eae9a0dbb1b6b68c300202dfa7759d43e812db2f1.jpg)  \n\n![](images/bb9110f955aa5f98e434a7731794dd7298387cf19c4fdd2a15e3df151de46420.jpg)  \n\nTable A–2 Properties of Laplace Transforms  \n\n![](images/fe32d8caa0c4d0ce46254070d21c48a5ec12d1df31f072fdd4161474f5cec189.jpg)  \n\nFinally, we present two frequently used theorems, together with Laplace transforms of the pulse function and impulse function.  \n\n![](images/01d24745028856b3ad261c1c49dd3757592ad6a8ab3a9c2bf2721b47aa49548b.jpg)  \n\n# Appendix  \n\n# Partial-Fraction Expansion  \n\nBefore we present MATLAB approach to the partial-fraction expansions of transfer functions, we discuss the manual approach to the partial-fraction expansions of transfer functions.  \n\nPartial-Fraction Expansion when $F(s)$ Involves Distinct Poles Only. Consider $F(s)$ written in the factored form  \n\n$$\nF(s)\\,=\\frac{B(s)}{A(s)}=\\frac{K(s\\,+\\,z_{1})(s\\,+\\,z_{2})\\cdots(s\\,+\\,z_{m})}{(s\\,+\\,p_{1})(s\\,+\\,p_{2})\\cdots(s\\,+\\,p_{n})},\\qquad\\mathrm{for}\\,m<n\n$$  \n\nwhere $p_{1},p_{2},\\ldots,p_{n}$ and $z_{1},z_{2},\\dots,z_{m}$ are either real or complex quantities,but for each complex $p_{i}$ or $z_{j}$ there will occur the complex conjugate of $p_{i}$ or $z_{j}$ ,respectively.If $F(s)$ involves distinct poles only,then it can be expanded into a sum of simple partial fractions as follows:  \n\n$$\nF(s)={\\frac{B(s)}{A(s)}}={\\frac{a_{1}}{s\\,+\\,p_{1}}}+{\\frac{a_{2}}{s\\,+\\,p_{2}}}+\\cdots+{\\frac{a_{n}}{s\\,+\\,p_{n}}}\n$$  \n\nwhere $a_{k}\\,(k=1,2,\\dots,n)$ are constants.The coefficient $a_{k}$ is called the residue at the pole at $s=-p_{k}$ . The value of $a_{k}$ can be found by multiplying both sides of Equation (B–1) by $\\left(s+\\ p_{k}\\right)$ and letting $s=-p_{k}$ , which gives  \n\n$$\n\\begin{array}{r l l}{\\displaystyle\\left(s\\,+\\,p_{k}\\right)\\frac{B(s)}{A(s)}\\Bigg]_{s=-p_{k}}=\\Bigg[\\frac{a_{1}}{s\\,+\\,p_{1}}\\big(s\\,+\\,p_{k}\\big)+\\frac{a_{2}}{s\\,+\\,p_{2}}\\big(s\\,+\\,p_{k}\\big)}&{\\quad s\\,+\\,p_{k}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\cdots+\\frac{a_{k}}{s\\,+\\,p_{k}}\\big(s\\,+\\,p_{k}\\big)+\\cdots+\\frac{a_{n}}{s\\,+\\,p_{n}}\\big(s\\,+\\,p_{k}\\big)\\Bigg]}&{{}=\\displaystyle\\,a_{k}}\\end{array}\n$$  \n\nWe see that all the expanded terms drop out with the exception of $a_{k}$ .Thus the residue $a_{k}$ is found from  \n\n$$\na_{k}=\\left[\\left(s\\,+\\,p_{k}\\right)\\frac{B(s)}{A(s)}\\right]_{s=-p_{k}}\n$$  \n\nNote that, since $f(t)$ is a real function of time, if $p_{1}$ and $p_{2}$ are complex conjugates, then the residues $a_{1}$ and $a_{2}$ are also complex conjugates. Only one of the conjugates, $a_{1}$ or $a_{2}$ ,needs to be evaluated, because the other is known automatically.  \n\nSince  \n\n$$\n\\mathcal{L}^{-1}\\bigg[\\frac{a_{k}}{s\\,+\\,p_{k}}\\bigg]\\,=\\,a_{k}e^{-p_{k}t}\n$$  \n\n$f(t)$ is obtained as  \n\n$$\nf(t)={\\mathcal{L}}^{-1}{\\big[}F(s){\\big]}=a_{1}e^{-p_{1}t}+a_{2}e^{-p_{2}t}+\\cdots+\\,a_{n}e^{-p_{n}t},\\qquad{\\mathrm{for~}}t\\geq0\n$$  \n\nEXAMPLE B–1 Find the inverse Laplace transform of  \n\n$$\nF(s)=\\frac{s+3}{(s+1)(s+2)}\n$$  \n\nThe partial-fraction expansion of $F(s)$ is  \n\n$$\nF(s)={\\frac{s\\,+\\,3}{(s\\,+\\,1)(s\\,+\\,2)}}={\\frac{a_{1}}{s\\,+\\,1}}+{\\frac{a_{2}}{s\\,+\\,2}}\n$$  \n\nwhere $a_{1}$ and $a_{2}$ are found as  \n\n$$\n{\\begin{array}{r l}&{a_{1}=\\left[(s\\,+\\,1)\\,{\\cfrac{s\\,+\\,3}{(s\\,+\\,1)(s\\,+\\,2)}}\\right]_{s=-1}=\\left[{\\cfrac{s\\,+\\,3}{s\\,+\\,2}}\\right]_{s=-1}=2}\\\\ &{a_{2}=\\left[(s\\,+\\,2)\\,{\\cfrac{s\\,+\\,3}{(s\\,+\\,1)(s\\,+\\,2)}}\\right]_{s=-2}=\\left[{\\cfrac{s\\,+\\,3}{s\\,+\\,1}}\\right]_{s=-2}=-1}\\end{array}}\n$$  \n\nThus  \n\n$$\n\\begin{array}{l}{{\\displaystyle f(t)\\,=\\,\\mathcal{L}^{-1}\\big[F(s)\\big]}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,\\,\\mathcal{L}^{-1}\\bigg[\\frac{2}{s\\,+\\,1}\\bigg]\\,+\\,\\mathcal{L}^{-1}\\bigg[\\frac{-1}{s\\,+\\,2}\\bigg]}}\\\\ {{\\,\\,\\,\\,\\,\\,\\,=\\,2e^{-t}\\,-\\,e^{-2t},\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{for}\\,t\\geq0}}\\end{array}\n$$  \n\nEXAMPLE B–2 Obtain the inverse Laplace transform of  \n\n$$\nG(s)=\\frac{s^{3}+5s^{2}+9s+7}{(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nHere, since the degree of the numerator polynomial is higher than that of the denominator polynomial, we must divide the numerator by the denominator.  \n\n$$\nG(s)\\,=\\,s\\,+\\,2\\,+\\frac{s\\,+\\,3}{(s\\,+\\,1)(s\\,+\\,2)}\n$$  \n\nNote that the Laplace transform of the unit-impulse function $\\delta(t)$ is 1 and that the Laplace transform of $d\\delta(t)/d t$ is $s$ .The third term on the right-hand side of this last equation is $F(s)$ in Example B–1. So the inverse Laplace transform of $G(s)$ is given as  \n\n$$\ng(t)\\,=\\frac{d}{d t}\\,\\delta(t)\\,+\\,2\\delta(t)\\,+\\,2e^{-t}\\,-\\,e^{-2t},\\qquad\\mathrm{for}\\,t\\,\\geq\\,0-\n$$  \n\nEXAMPLE B–3 Find the inverse Laplace transform of  \n\n$$\nF(s)={\\frac{2s\\,+\\,12}{s^{2}\\,+\\,2s\\,+\\,5}}\n$$  \n\nNotice that the denominator polynomial can be factored as  \n\n$$\ns^{2}\\,+\\,2s\\,+\\,5\\,=\\,(s\\,+\\,1\\,+\\,j2)(s\\,+\\,1\\,-\\,j2)\n$$  \n\nIf the function $F(s)$ involves a pair of complex-conjugate poles, it is convenient not to expand $F(s)$ into the usual partial fractions but to expand it into the sum of a damped sine and a damped cosine function.  \n\nNoting that $s^{2}\\,+\\,2s\\,+\\,5\\,=\\,(s\\,+\\,1)^{2}\\,+\\,2^{2}$ and referring to the Laplace transforms of $e^{-\\alpha t}\\sin\\omega t$ and $e^{-\\alpha t}\\cos\\omega t$ , rewritten thus,  \n\n$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}\\big[e^{-\\alpha t}\\sin\\omega t\\big]\\,=\\,\\frac{\\omega}{(s\\,+\\,\\alpha)^{2}\\,+\\,\\omega^{2}}}\\\\ {\\displaystyle\\mathcal{L}\\big[e^{-\\alpha t}\\cos\\omega t\\big]\\,=\\,\\frac{s\\,+\\,\\alpha}{(s\\,+\\,\\alpha)^{2}\\,+\\,\\omega^{2}}}\\end{array}\n$$  \n\nthe given $F(s)$ can be written as a sum of a damped sine and a damped cosine function:  \n\n$$\n\\begin{array}{c}{{F(s)=\\displaystyle\\frac{2s\\,+\\,12}{s^{2}\\,+\\,2s\\,+\\,5}=\\displaystyle\\frac{10\\,+\\,2(s\\,+\\,1)}{(s\\,+\\,1)^{2}\\,+\\,2^{2}}}}\\\\ {{=\\displaystyle5\\,\\displaystyle\\frac{2}{(s\\,+\\,1)^{2}\\,+\\,2^{2}}+\\,2\\,\\displaystyle\\frac{s\\,+\\,1}{(s\\,+\\,1)^{2}\\,+\\,2^{2}}}}\\end{array}\n$$  \n\nIt follows that  \n\n$$\n\\begin{array}{l}{\\displaystyle f(t)\\,=\\,\\mathcal{L}^{-1}\\big[F(s)\\big]}\\\\ {\\,=\\,5\\mathcal{L}^{-1}\\bigg[\\frac{2}{(s\\,+\\,1)^{2}\\,+\\,2^{2}}\\bigg]\\,+\\,2\\mathcal{L}^{-1}\\bigg[\\frac{s\\,+\\,1}{(s\\,+\\,1)^{2}\\,+\\,2^{2}}\\bigg]}\\\\ {\\,=\\,5e^{-t}\\sin2t\\,+\\,2e^{-t}\\cos2t,\\qquad\\qquad\\mathrm{~for}\\,t\\ge0}\\end{array}\n$$  \n\nPartial-Fraction Expansion when $F(s)$ Involves Multiple Poles. Instead of discussing the general case, we shall use an example to show how to obtain the partialfraction expansion of $F(s)$ .  \n\nConsider the following $F(s)$ :  \n\n$$\nF(s)=\\frac{s^{2}+2s\\,+\\,3}{(s+1)^{3}}\n$$  \n\nThe partial-fraction expansion of this $F(s)$ involves three terms,  \n\n$$\nF(s)=\\frac{B(s)}{A(s)}=\\frac{b_{1}}{s\\,+\\,1}+\\frac{b_{2}}{(s\\,+\\,1)^{2}}+\\frac{b_{3}}{(s\\,+\\,1)^{3}}\n$$  \n\nwhere $b_{3},\\ b_{2}$ , and $b_{1}$ are determined as follows. By multiplying both sides of this last equation by $(s+1)^{3}$ , we have  \n\n$$\n(s\\,+\\,1)^{3}{\\frac{B(s)}{A(s)}}=b_{1}(s\\,+\\,1)^{2}\\,+\\,b_{2}(s\\,+\\,1)\\,+\\,b_{3}\n$$  \n\nThen letting $s=-1$ , Equation (B–2) gives  \n\n$$\n\\bigg[(s\\,+\\,1)^{3}\\,\\frac{B(s)}{A(s)}\\bigg]_{s=-1}=b_{3}\n$$  \n\nAlso, differentiation of both sides of Equation (B–2) with respect to $s$ yields  \n\n$$\n\\frac{d}{d s}\\bigg[(s\\,+\\,1)^{3}\\,\\frac{B(s)}{A(s)}\\bigg]\\,=\\,b_{2}\\,+\\,2b_{1}(s\\,+\\,1)\n$$  \n\nIf we let $s=-1$ in Equation (B–3), then  \n\n$$\n\\frac{d}{d s}\\bigg[(s\\,+\\,1)^{3}\\,\\frac{B(s)}{A(s)}\\bigg]_{s=-1}\\,=\\,b_{2}\n$$  \n\nBy differentiating both sides of Equation (B–3) with respect to $s$ , the result is  \n\n$$\n{\\frac{d^{2}}{d s^{2}}}\\bigg[\\,(s\\,+\\,1)^{3}\\,{\\frac{B(s)}{A(s)}}\\bigg]\\,=\\,2b_{1}\n$$  \n\nFrom the preceding analysis it can be seen that the values of $b_{3},b_{2}$ , and $b_{1}$ are found systematically as follows:  \n\n$$\n\\begin{array}{r l}&{b_{3}=\\bigg[(s+1)^{3}\\frac{B(s)}{A(s)}\\bigg]_{s=-1}}\\\\ &{\\quad=(s^{2}+2s+3)_{s=-1}}\\\\ &{\\quad=2}\\\\ &{b_{2}=\\bigg\\{\\frac{d}{d s}\\Big[(s+1)^{3}\\frac{B(s)}{A(s)}\\Big]\\bigg\\}_{s=-1}}\\\\ &{\\quad=\\bigg[\\frac{d}{d s}\\big(s^{2}+2s+3\\big)\\bigg]_{s=-1}}\\\\ &{\\quad=(2s+2)_{s=-1}}\\\\ &{\\quad=0}\\end{array}\n$$  \n\n$$\n\\begin{array}{l}{\\displaystyle b_{1}=\\frac{1}{2!}\\bigg\\{\\frac{d^{2}}{d s^{2}}\\bigg[(s+1)^{3}\\frac{B(s)}{A(s)}\\bigg]\\bigg\\}_{s=-1}}\\\\ {\\displaystyle=\\frac{1}{2!}\\bigg[\\frac{d^{2}}{d s^{2}}\\big(s^{2}+2s+3\\big)\\bigg]_{s=-1}}\\\\ {\\displaystyle=\\frac{1}{2}\\,(2)=1}\\end{array}\n$$  \n\nWe thus obtain  \n\n$$\n\\begin{array}{l}{f(t)=\\mathcal{L}^{-1}\\big[F(s)\\big]}\\\\ {\\quad\\quad=\\mathcal{L}^{-1}\\bigg[\\cfrac{1}{s+1}\\bigg]+\\mathcal{L}^{-1}\\bigg[\\cfrac{0}{(s+1)^{2}}\\bigg]+\\mathcal{L}^{-1}\\bigg[\\cfrac{2}{(s+1)^{3}}\\bigg]}\\\\ {\\quad\\quad=e^{-t}+0+t^{2}e^{-t}}\\\\ {\\quad\\quad=\\big(1+t^{2}\\big)e^{-t},\\quad\\quad\\mathrm{for}\\,t\\geq0}\\end{array}\n$$  \n\nComments. For complicated functions with denominators involving higher-order polynomials, partial-fraction expansion may be quite time consuming. In such a case, use of MATLAB is recommended.  \n\nPartial-Fraction Expansion with MATLAB. MATLAB has a command to obtain the partial-fraction expansion of $B(s)/A(s)$ . Consider the following function $B(s)/A(s)$ :  \n\n$$\n{\\frac{B(s)}{A(s)}}={\\frac{\\mathrm{num}}{\\mathrm{den}}}={\\frac{b_{0}s^{n}+b_{1}s^{n-1}+\\cdots+b_{n}}{s^{n}+a_{1}s^{n-1}+\\cdots+a_{n}}}\n$$  \n\nwhere some of $a_{i}$ and $b_{j}$ may be zero. In MATLAB row vectors num and den specify the coefficients of the numerator and denominator of the transfer function.That is,  \n\n$$\n\\begin{array}{c}{{\\mathsf{n u m}}=[\\mathsf{b}_{0}\\ \\mathsf{\\ b}_{1}\\ \\dots\\ \\mathsf{b}_{\\mathsf{n}}]}\\\\ {{\\mathsf{d e n}}=[1\\ \\ \\mathsf{a}_{1}\\ \\ \\dots\\ \\mathsf{a}_{\\mathsf{n}}]}\\end{array}\n$$  \n\nThe command  \n\n$$\n[\\mathsf{r},\\mathsf{p},\\mathsf{k}]=\\mathsf{r e s i d u e}(\\mathsf{n u m},\\mathsf{d e n})\n$$  \n\nfinds the residues ( r), poles ( p), and direct terms ( k) of a partial-fraction expansion of the ratio of two polynomials $B(s)$ and $A(s)$ .  \n\nThe partial-fraction expansion of $B(s)/A(s)$ is given by  \n\n$$\n{\\frac{B(s)}{A(s)}}={\\frac{r(1)}{s\\,-\\,p(1)}}+{\\frac{r(2)}{s\\,-\\,p(2)}}+\\cdots+{\\frac{r(n)}{s\\,-\\,p(n)}}+\\,k(s)\n$$  \n\nComparing Equations (B–1) and (B–4), we note that $p(1)\\,=\\,-p_{1}$ ,$p(2)\\,=\\,-p_{2},\\ldots$ ,$p(n)=-p_{n};r(1)=a_{1},r(2)=a_{2},\\ldots,r(n)=a_{n}.\\left[k(s)\\right]$ is a direct term.]  \n\nEXAMPLE B–4 Consider the following transfer function,  \n\n$$\n{\\frac{B(s)}{A(s)}}={\\frac{2s^{3}+5s^{2}+3s+6}{s^{3}+6s^{2}+11s+6}}\n$$  \n\nFor this function,  \n\n$\\mathrm{den}=[1\\ \\ 6\\ \\ 1\\ \\ 6]$  \n\nThe command  \n\ngives the following result:  \n\n![](images/0bb39b76804c9b18e0952871ac248d7ae836d70ae08f32a1789dc681391e7d91.jpg)  \n\n(Note that the residues are returned in column vector r, the pole locations in column vector p, and the direct term in row vector $\\mathrm{k}.$ .) This is the MATLAB representation of the following partialfraction expansion of $B(s)/A(s)$ :  \n\n$$\n\\begin{array}{c}{{\\displaystyle\\frac{B(s)}{A(s)}=\\frac{2s^{3}\\,+\\,5s^{2}\\,+\\,3s\\,+\\,6}{s^{3}\\,+\\,6s^{2}\\,+\\,11s\\,+\\,6}}}\\\\ {{{}}}\\\\ {{=\\displaystyle\\frac{-6}{s\\,+\\,3}+\\frac{-4}{s\\,+\\,2}+\\frac{3}{s\\,+\\,1}+\\,2}}\\end{array}\n$$  \n\nthat if $p(j)\\,=\\,p(j\\,+\\,1)\\,=\\,\\cdots\\,=\\,p(j\\,+\\,m\\,-\\,1)$ that is, $p_{j}=p_{j+1}=\\cdots=p_{j+m-1}\\!\\Big]$ the pole $p(j)$ is a pole of multiplicity m. In such a case, the expansion includes terms of the form  \n\n$$\n{\\frac{r(j)}{s\\,-\\,p(j)}}+{\\frac{r(j+1)}{\\left[s\\,-\\,p(j)\\right]^{2}}}+\\cdots+{\\frac{r(j\\,+\\,m\\,-\\,1)}{\\left[s\\,-\\,p(j)\\right]^{m}}}\n$$  \n\nFor details, see Example B–5.  \n\n# EXAMPLE B–5 Expand the following $B(s)/A(s)$ into partial fractions with MATLAB.  \n\n$$\n{\\frac{B(s)}{A(s)}}={\\frac{s^{2}+2s+3}{(s+1)^{3}}}={\\frac{s^{2}+2s+3}{s^{3}+3s^{2}+3s+1}}\n$$  \n\nFor this function, we have  \n\n$$\n\\begin{array}{l}{\\mathsf{n u m}=\\left[1\\ \\ 2\\ \\ 3\\right]}\\\\ {\\mathsf{d e n}=\\left[1\\ \\ 3\\ \\ 3\\ \\ 1\\right]}\\end{array}\n$$  \n\nThe command  \n\n$$\n[\\mathsf{r},\\mathsf{p},\\mathsf{k}]=\\mathsf{r e s i d u e}(\\mathsf{n u m},\\mathsf{d e n})\n$$  \n\ngives the result shown next:  \n\nIt is the MATLAB representation of the following partial-fraction expansion of $B(s)/A(s)$ :  \n\n$$\n\\frac{B(s)}{A(s)}=\\frac{1}{s\\,+\\,1}+\\frac{0}{(s\\,+\\,1)^{2}}+\\frac{2}{(s\\,+\\,1)^{3}}\n$$  \n\nNote that the direct term $\\mathsf{k}$ is zero.  \n\n# Vector-Matrix Algebra  \n\nIn this appendix we first review the determinant of a matrix, then we define the adjoint matrix, the inverse of a matrix, and the derivative and integral of a matrix.  \n\nDeterminant of a Matrix. For each square matrix, there exists a determinant.The determinant of a square matrix A is usually written as $|\\mathbf{A}|$ or det A . The determinant has the following properties:  \n\n1. If any two consecutive rows or columns are interchanged,the determinant changes its sign.   \n2. If any row or any column consists only of zeros, then the value of the dererminant is zero.   \n3. If the elements of any row (or any column) are exactly $k$ times those of another row (or another column), then the value of the determinant is zero.   \n4. If, to any row (or any column), any constant times another row (or column) is added, the value of the determinant remains unchanged.   \n5. If a determinant is multiplied by a constant, then only one row (or one column) is multiplied by that constant. Note, however, that the determinant of $k$ times an $n\\times n$ matrix $\\mathbf{A}$ is $k^{n}$ times the determinant of $\\mathbf{A}$ , or  \n\n$$\n\\left|k\\mathbf{A}\\right|=k^{n}\\big|\\mathbf{A}\\big|\n$$  \n\nThis is because  \n\n$$\nk\\mathbf{A}=\\left[\\begin{array}{c c c c}{k a_{11}}&{k a_{12}}&{\\ldots}&{k a_{1m}}\\\\ {k a_{21}}&{k a_{22}}&{\\ldots}&{k a_{2m}}\\\\ {\\vdots}&{\\vdots}&{}&{\\vdots}\\\\ {k a_{n1}}&{k a_{n2}}&{\\ldots}&{k a_{n m}}\\end{array}\\right]\n$$  \n\n6. The determinant of the product of two square matrices $\\mathbf{A}$ and $\\mathbf{B}$ is the product of determinants, or  \n\n$$\n\\left|\\mathbf{AB}\\right|=\\left|\\mathbf{A}\\right|\\left|\\mathbf{B}\\right|\n$$  \n\nIf $\\mathbf{B}\\,=\\,n\\,\\times\\,m$ matrix and $\\mathbf{C}=m\\times n$ matrix, then  \n\n$$\n\\operatorname*{det}(\\mathbf{I}_{n}+\\,\\mathbf{BC})=\\operatorname*{det}(\\mathbf{I}_{m}+\\,\\mathbf{CB})\n$$  \n\nIf $\\mathbf A\\neq\\mathbf0$ and $\\mathbf{D}=m\\times m$ matrix, then  \n\n$$\n\\operatorname*{det}{\\left[\\!\\!{\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{C}}&{\\mathbf{D}}\\end{array}}\\right]}=\\operatorname*{det}\\mathbf{A}\\!\\cdot\\!\\operatorname*{det}\\mathbf{S}\n$$  \n\nwhere $\\mathbf{S}=\\mathbf{D}-\\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}$ .If $\\mathbf{D}\\neq\\mathbf{0}$ , then  \n\n$$\n\\begin{array}{r}{\\mathsf{d e t}\\overset{\\mathbf{\\Delta}\\mathbf{A}}{\\mathbf{C}}}&{{}\\mathbf{D}\\overset{\\mathbf{\\Delta}\\mathbf{\\top}}{=}\\mathsf{d e t}\\,\\mathbf{D}\\!\\cdot\\!\\operatorname*{det}\\mathbf{T}}\\end{array}\n$$  \n\nwhere $\\mathbf{T}=\\mathbf{A}\\mathbf{\\Sigma}-\\mathbf{\\Sigma}\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C}$ .If $\\mathbf B=\\mathbf0$ or $\\mathbf{C}=\\mathbf{0}$ ,then  \n\n$$\n{\\begin{array}{r l}{\\operatorname*{det}{\\left[\\mathbf{A}\\right]}\\;}&{\\;\\mathbf{0}}\\\\ {\\mathbf{C}\\;}&{\\mathbf{D}}\\end{array}}=\\operatorname*{det}\\mathbf{A}\\cdot\\operatorname*{det}\\mathbf{D}}\\\\ {\\operatorname*{det}{\\left[\\mathbf{A}\\right]}\\;}&{\\;\\mathbf{B}}\\\\ {\\operatorname*{det}{\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{0}}&{\\mathbf{D}}\\end{array}\\right]}\\;=\\;\\operatorname*{det}\\mathbf{A}\\cdot\\operatorname*{det}\\mathbf{D}}\\end{array}}\n$$  \n\nRank of Matrix. A matrix A is said to have rank $_m$ if there exists an $m\\times m$ sub  \nmatrix Mof A such that the determinant of Mis nonzero and the determinant of every   \n$r\\times r$ submatrix (where $r\\geq m\\,+\\,1)$ ) of $\\mathbf{A}$ is zero. As an example, consider the following matrix:  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l l}{1}&{2}&{3}&{4}\\\\ {0}&{1}&{-1}&{0}\\\\ {1}&{0}&{1}&{2}\\\\ {1}&{1}&{0}&{2}\\end{array}\\right]}\n$$  \n\nNote that $|\\mathbf{A}|=0$ . One of a number of largest submatrices whose determinant is not equal to zero is  \n\n$$\n\\begin{array}{r}{\\left[{1\\quad2\\quad\\quad3}\\,\\right]}\\\\ {\\mathrm{~0~}\\,1\\quad-1}\\\\ {\\mathrm{~1~}\\,0\\quad\\quad1}\\end{array}\n$$  \n\nHence, the rank of the matrix $\\mathbf{A}$ is 3.  \n\nMinor $M_{i j}.$ If the i th row and $j$ th column are deleted from an $n\\times n$ matrix A ,the resulting matrix is an $(n\\,-\\,1)\\,\\times\\,(n\\,-\\,1)$ matrix. The determinant of this $(n\\,-\\,1)\\times(n\\,-\\,1)$ matrix is called the minor $M_{i j}$ of the matrix A .  \n\nCofactor $A_{i j}.$ .The cofactor $A_{i j}$ of the element $a_{i j}$ of the $n\\times n$ matrix A is defined by the equation  \n\n$$\nA_{i j}\\,=\\,(-1)^{i+j}M_{i j}\n$$  \n\nThat is, the cofactor $A_{i j}$ of the element $a_{i j}$ is ${(-1)}^{i+j}$ times the determinant of the matrix formed by deleting the i th row and the $j$ th column from A . Note that the cofactor $A_{i j}$ of the element $a_{i j}$ is the coefficient of the term $a_{i j}$ in the expansion of the determinant $\\left\\lceil\\mathbf{A}\\right\\rceil$ ,since it can be shown that  \n\n$$\na_{i1}A_{i1}\\,+\\,a_{i2}A_{i2}\\,+\\,\\cdots\\,+\\,a_{i n}A_{i n}\\,=\\,\\left|{\\bf A}\\right|\n$$  \n\nIf $a_{i1},a_{i2},\\dots,a_{i n}$ are replaced by $a_{j1},a_{j2},\\dots,a_{j n}.$ ,then  \n\n$$\na_{j1}A_{i1}\\,+\\,a_{j2}A_{i2}\\,+\\,\\cdots\\,+\\,a_{j n}A_{i n}=0\\,\\qquad i\\ne j\n$$  \n\nbecause the determinant of $\\mathbf{A}$ in this case possesses two identical rows. Hence, we obtain  \n\n$$\n\\sum_{k=1}^{n}a_{j k}A_{i k}\\,=\\,\\delta_{j i}\\big|{\\bf A}\\big|\n$$  \n\nSimilarly,  \n\n$$\n\\sum_{k=1}^{n}a_{k i}A_{k j}\\,=\\,\\delta_{i j}\\vert\\mathbf{A}\\vert\n$$  \n\nAdjoint Matrix .The matrix $\\mathbf{B}$ whose element in the i th row and $j$ th column equals $A_{j i}$ is called the adjoint of $\\mathbf{A}$ and is denoted by adj A , or  \n\n$$\n\\mathbf{B}\\,=\\,(b_{i j})\\,=\\,(A_{j i})\\,=\\,{\\mathrm{adj}}\\ \\mathbf{A}\n$$  \n\nThat is, the adjoint of $\\mathbf{A}$ is the transpose of the matrix whose elements are the cofactors of $\\mathbf{A}$ , or  \n\n$$\n\\operatorname{adj}\\mathbf{A}={\\left[\\begin{array}{l l l l}{A_{11}}&{A_{21}}&{\\dots}&{A_{n1}}\\\\ {A_{12}}&{A_{22}}&{\\dots}&{A_{n2}}\\\\ {\\vdots}&{\\vdots}&{}&{\\vdots}\\\\ {A_{1n}}&{A_{2n}}&{\\dots}&{A_{n m}}\\end{array}\\right]}\n$$  \n\nNote that the element of the $j$ th row and $i$ th column of the product A (adj A )is  \n\n$$\n\\sum_{k=1}^{n}a_{j k}b_{k i}=\\ \\sum_{k=1}^{n}a_{j k}A_{i k}=\\delta_{j i}\\vert\\mathbf{A}\\vert\n$$  \n\nHence, A (adj A )is a diagonal matrix with diagonal elements equal to $|\\mathbf{A}|$ , or  \n\n$$\n\\mathbf{A(adj\\,\\hat{A})}=\\left|\\mathbf{A}\\right|\\mathbf{I}\n$$  \n\nSimilarly, the element in the $j$ th row and i th column of the product (adj $\\mathbf{A})\\mathbf{A}$ is  \n\n$$\n\\sum_{k=1}^{n}b_{j k}a_{k i}=\\ \\sum_{k=1}^{n}A_{k j}a_{k i}=\\delta_{i j}\\vert\\mathbf{A}\\vert\n$$  \n\nHence, we have the relationship  \n\n$$\n\\mathbf{A}(\\mathrm{adj}\\ \\mathbf{A})=(\\mathrm{adj}\\ \\mathbf{A})\\mathbf{A}=\\left|\\mathbf{A}\\right|\\mathbf{I}\n$$  \n\nThus  \n\n$$\n\\mathbf{A}^{-1}={\\frac{\\operatorname{adj}\\mathbf{A}}{|\\mathbf{A}|}}={\\left[\\begin{array}{l l l l}{{\\cfrac{A_{11}}{|\\mathbf{A}|}}}&{{\\cfrac{A_{21}}{|\\mathbf{A}|}}}&{\\dots}&{{\\cfrac{A_{n1}}{|\\mathbf{A}|}}}\\\\ {{\\cfrac{A_{12}}{|\\mathbf{A}|}}}&{{\\cfrac{A_{22}}{|\\mathbf{A}|}}}&{\\dots}&{{\\cfrac{A_{n2}}{|\\mathbf{A}|}}}\\\\ {\\vdots}&{\\vdots}&&{\\vdots}\\\\ {{\\cfrac{A_{1n}}{|\\mathbf{A}|}}}&{{\\cfrac{A_{2n}}{|\\mathbf{A}|}}}&{\\dots}&{{\\cfrac{A_{n n}}{|\\mathbf{A}|}}}\\end{array}\\right]}\n$$  \n\nwhere $A_{i j}$ is the cofactor of $a_{i j}$ of the matrix A .Thus, the terms in the i th column of ${\\mathbf{A}}^{-1}$ are $1/|\\mathbf{A}|$ times the cofactors of the i th row of the original matrix A . For example, if  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l r}{1}&{2}&{0}\\\\ {3}&{-1}&{-2}\\\\ {1}&{0}&{-3}\\end{array}\\right]}\n$$  \n\nthen the adjoint of $\\mathbf{A}$ and the determinant $|\\mathbf{A}|$ are respectively found to be  \n\n$$\n\\operatorname{adj}\\mathbf{A}={\\left[\\begin{array}{l l l l l l}{-1}&{-2}\\\\ {0}&{-3}\\end{array}\\right]}\\,+\\,{\\overline{{\\left|2}\\begin{array}{l l}{\\quad0}&{0}\\\\ {0}&{-3}\\end{array}\\right|}}&{\\,\\,\\left|{\\begin{array}{l l}{\\,2}&{\\;\\,0}\\\\ {-1}&{-2}\\end{array}}\\right|}\\\\ {\\operatorname{adj}\\mathbf{A}={\\left[\\begin{array}{l l}{3}&{-2}\\\\ {-{\\Big|}_{1}}&{-3}\\end{array}\\right]}}&{\\,\\,{\\Big|}1}&{\\,\\,0{\\Big|}}&{\\,\\,-{\\Big|}_{3}}&{\\,\\,0{\\Big|}}\\\\ {{\\Big|}3}&{-1}&{{-{\\Big|}_{1}}}&{\\,\\,{2}}&{\\,\\,{\\Big|}1}&{\\,\\,2{\\Big|}}\\\\ {{\\Big|}1}&{\\,\\,0{\\Big|}}&{\\,\\,-{\\Big|}1}&{\\,\\,0{\\Big|}}&{\\,{\\Big|}3}&{\\,\\,-1}\\end{array}\\right|}\n$$  \n\n$$\n=\\left[{\\begin{array}{r r r}{3}&{6}&{-4}\\\\ {7}&{-3}&{2}\\\\ {1}&{2}&{-7}\\end{array}}\\right]\n$$  \n\nand  \n\n$$\n\\left|\\mathbf{A}\\right|=17\n$$  \n\nHence, the inverse of $\\mathbf{A}$ is  \n\n$$\n\\mathbf{A}^{-1}={\\frac{\\operatorname{adj}\\mathbf{A}}{|\\mathbf{A}|}}={\\left[\\begin{array}{l l l}{{\\frac{3}{17}}}&{{\\frac{6}{17}}}&{-{\\frac{4}{17}}}\\\\ {{\\frac{7}{17}}}&{-{\\frac{3}{17}}}&{{\\frac{2}{17}}}\\\\ {{\\frac{1}{17}}}&{{\\frac{2}{17}}}&{-{\\frac{7}{17}}}\\end{array}\\right]}\n$$  \n\nIn what follows, we give formulas for finding inverse matrices for the $2\\times2$ matrix and the $3\\times3$ matrix. For the $2\\times2$ matrix  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l}{a}&{b}\\\\ {c}&{d}\\end{array}\\right]}\\qquad\\quad{\\mathrm{where~}}a d\\,-\\,b c\\neq0\n$$  \n\nthe inverse matrix is given by  \n\n$$\n\\mathbf{A}^{-1}={\\frac{1}{a d\\,-\\,b c}}{\\left[\\begin{array}{l l}{d}&{-b}\\\\ {-c}&{\\;\\;\\;a}\\end{array}\\right]}\n$$  \n\nFor the $3\\times3$ matrix  \n\n$$\n{\\mathbf A}={\\left[\\begin{array}{l l l}{a}&{b}&{c}\\\\ {d}&{e}&{f}\\\\ {g}&{h}&{i}\\end{array}\\right]}\\qquad{\\mathrm{where~}}\\vert{\\mathbf A}\\vert\\neq0\n$$  \n\nthe inverse matrix is given by  \n\n$$\n\\mathbf{A}^{-1}={\\frac{1}{|\\mathbf{A}|}}{\\left[\\begin{array}{l l}{e}&{f}\\\\ {h}&{i}\\end{array}\\right]}\\,\\,\\,-{\\left|\\begin{array}{l l}{b}&{c}\\\\ {h}&{i}\\end{array}\\right|}\\,\\,\\,{\\left|\\begin{array}{l l}{b}&{c}\\\\ {e}&{f}\\end{array}\\right|}\n$$  \n\nNote that  \n\n$$\n\\begin{array}{r l}&{(\\mathbf{A}^{-1})^{-1}=\\mathbf{A}}\\\\ &{\\mathbf{\\Phi}(\\mathbf{A}^{-1})^{\\prime}=(\\mathbf{A}^{\\prime})^{-1}}\\\\ &{\\mathbf{\\Phi}(\\mathbf{A}^{-1})^{*}=(\\mathbf{A}^{*})^{-1}}\\end{array}\n$$  \n\nThere are several more useful formulas available. Assume that $\\mathbf{A}=n\\times n$ matrix, $\\mathbf{B}\\,=\\,n\\,\\times\\,m$ matrix, $\\mathbf{C}=m\\times n$ matrix, and $\\mathbf{D}=m\\times m$ matrix.Then  \n\n$$\n[\\mathbf{A}+\\mathbf{BC}]^{-1}=\\mathbf{A}^{-1}-\\mathbf{A}^{-1}\\mathbf{B}[\\mathbf{I}_{m}+\\mathbf{CA}^{-1}\\mathbf{B}]^{-1}\\mathbf{CA}^{-1}\n$$  \n\nIf $|\\mathbf{A}|\\neq0$ and $\\vert\\mathbf{D}\\vert\\neq0$ ,then  \n\n$$\n\\begin{array}{r l}&{\\left[\\mathbf{A}\\mathbf{\\,}}&{\\mathbf{B}\\right]^{-1}=\\left[\\mathbf{A}^{-1}\\mathbf{\\,}}&{-\\mathbf{A}^{-1}\\mathbf{\\,B}\\mathbf{D}^{-1}\\right]}\\\\ &{\\mathbf{D}}\\end{array}\n$$  \n\nIf $\\left|\\mathbf{A}\\right|\\neq0,\\mathbf{S}=\\mathbf{D}-\\mathbf{CA}^{-1}\\mathbf{B},\\left|\\mathbf{S}\\right|\\neq0$ ,then  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{C}}&{\\mathbf{D}}\\end{array}\\right]^{-1}=\\left[\\begin{array}{c c}{\\mathbf{A}^{-1}+\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{S}^{-1}\\mathbf{C}\\mathbf{A}^{-1}}&{-\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{S}^{-1}}\\\\ {-\\mathbf{S}^{-1}\\mathbf{C}\\mathbf{A}^{-1}}&{\\mathbf{S}^{-1}}\\end{array}\\right]\n$$  \n\nIf $\\left\\vert\\mathbf{D}\\right\\vert\\,\\neq\\,0,\\mathbf{T}=\\mathbf{A}\\,-\\,\\mathbf{B}\\mathbf{D}^{-1}\\,\\mathbf{C},$ , and $\\left\\vert\\mathbf{T}\\right\\vert\\neq0$ ,then  \n\n$$\n\\left[\\begin{array}{l l}{\\mathbf{A}}&{\\mathbf{B}}\\\\ {\\mathbf{C}}&{\\mathbf{D}}\\end{array}\\right]^{-1}=\\left[\\mathbf{-D^{-1}C T^{-1}}\\begin{array}{r r}{\\mathbf{\\Gamma}-\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{D}^{-1}}\\\\ {\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{T}^{-1}\\mathbf{B}\\mathbf{D}^{-1}}\\end{array}\\right]\n$$  \n\nFinally, we present the MATLAB approach to obtain the inverse of a square matrix. If all elements of the matrix are given as numerical values, this approach is best.  \n\nMATLAB Approach to Obtain the Inverse of a Square Matrix. The inverse of a square matrix A can be obtained with the command  \n\nFor example, if matrix A is given by  \n\n$$\n\\mathbf{A}={\\left[\\begin{array}{l l l}{1}&{1}&{2}\\\\ {3}&{4}&{0}\\\\ {1}&{2}&{5}\\end{array}\\right]}\n$$  \n\nthen the inverse of matrix A is obtained as follows:  \n\n![](images/6cfc417e69595dfee131049661d401d65671ed0daaeb5c8106430c3cb5d7d5d0.jpg)  \n\nThat is  \n\n$$\n\\mathbf{A}^{-1}=\\left[\\begin{array}{r r r}{2.2222}&{-0.1111}&{-0.8889}\\\\ {-1.6667}&{0.3333}&{0.6667}\\\\ {0.2222}&{-0.1111}&{0.1111}\\end{array}\\right]\n$$  \n\nMATLAB Is Case Sensitive. It is important to note that MATLAB is case sensitive.That is, MATLAB distinguishes between upper- and lowercase letters.Thus, $\\boldsymbol{\\mathrm{x}}$ and Xare not the same variable. All function names must be in lowercase, such as inv(A) ,eig(A) , and poly(A) .  \n\nDifferentiation and Integration of Matrices. The derivative of an $n\\times m$ matrix $\\mathbf{A}(t)$ is defined to be the $n\\times m$ matrix, each element of which is the derivative of the corresponding element of the original matrix, provided that all the elements $a_{i j}(t)$ have derivatives with respect to $t,$ .That is,  \n\n$$\n{\\frac{d}{d t}}\\,\\mathbf{A}(t)\\,=\\,\\left({\\frac{d}{d t}}\\,a_{i j}(t)\\right)\\,=\\,{\\left[\\begin{array}{l l l l}{{\\displaystyle{\\frac{d}{d t}}}\\,a_{11}(t)}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{12}(t)}&{\\cdots}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{1m}(t)}\\\\ {{\\displaystyle{\\frac{d}{d t}}}\\,a_{21}(t)}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{22}(t)}&{\\cdots}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{2m}(t)}\\\\ {\\vdots}&{\\vdots}&&{\\vdots}\\\\ {{\\displaystyle{\\frac{d}{d t}}}\\,a_{n1}(t)}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{n2}(t)}&{\\cdots}&{{\\displaystyle{\\frac{d}{d t}}}\\,a_{n m}(t)}\\end{array}\\right]}\n$$  \n\nSimilarly, the integral of an $n\\times m$ matrix $\\mathbf{A}(t)$ is defined to be  \n\n$$\n\\int\\mathbf{A}(t)\\,d t={\\left(\\int a_{i j}(t)\\,d t\\right)}={\\left[\\begin{array}{l l l l}{\\int a_{11}(t)\\,d t}&{\\int a_{12}(t)\\,d t}&{\\dots}&{\\int a_{1m}(t)\\,d t}\\\\ {\\int a_{21}(t)\\,d t}&{\\int a_{22}(t)\\,d t}&{\\dots}&{\\int a_{2m}(t)\\,d t}\\\\ {\\qquad\\vdots}&{\\vdots}&&{\\qquad\\vdots}\\\\ {\\int a_{n1}(t)\\,d t}&{\\int a_{2n}(t)\\,d t}&{\\dots}&{\\int a_{n m}(t)\\,d t}\\end{array}\\right]}\n$$  \n\nDifferentiation of the Product of Two Matrices. If the matrices $\\mathbf{A}(t)$ and ${\\bf\\delta B}(t)$ can be differentiated with respect to $t$ , then  \n\n$$\n{\\frac{d}{d t}}\\left[\\mathbf{A}(t)\\mathbf{B}(t)\\right]={\\frac{d\\mathbf{A}(t)}{d t}}\\,\\mathbf{B}(t)\\,+\\,\\mathbf{A}(t)\\,{\\frac{d\\mathbf{B}(t)}{d t}}\n$$  \n\nHere again the multiplication of $\\mathbf{A}(t)$ and $d\\mathbf{B}(t)/d t$ [or $d\\mathbf{A}(t)/d t$ and ${\\bf B}(t)]$ is, in general, not commutative.  \n\nDifferentiation of $\\mathbf{A}^{-1}(t)$ .If a matrix $\\mathbf{A}(t)$ and its inverse $\\mathbf{A}^{-1}(t)$ are differentiable with respect to $t$ , then the derivative of $\\mathbf{A}^{-1}(t)$ is given by  \n\n$$\n\\frac{d{\\bf A}^{-1}(t)}{d t}=-{\\bf A}^{-1}(t)\\,\\frac{d{\\bf A}(t)}{d t}\\,{\\bf A}^{-1}(t)\n$$  \n\nThe derivative may be obtained by differentiating $\\mathbf{A}(t)\\mathbf{A}^{-1}(t)$ with respect to $t$ . Since  \n\n$$\n\\frac{d}{d t}\\left[{\\bf A}(t){\\bf A}^{-1}(t)\\right]=\\frac{d{\\bf A}(t)}{d t}\\,{\\bf A}^{-1}(t)\\,+\\,{\\bf A}(t)\\,\\frac{d{\\bf A}^{-1}(t)}{d t}\n$$  \n\nand  \n\n$$\n\\frac{d}{d t}\\left[{\\bf A}(t){\\bf A}^{-1}(t)\\right]=\\frac{d}{d t}{\\bf I}={\\bf0}\n$$  \n\nwe obtain  \n\n$$\n\\mathbf{A}(t)\\,\\frac{d\\mathbf{A}^{-1}(t)}{d t}=-\\,\\frac{d\\mathbf{A}(t)}{d t}\\,\\mathbf{A}^{-1}(t)\n$$  \n\nor  \n\n$$\n\\frac{d{\\bf A}^{-1}(t)}{d t}=-{\\bf A}^{-1}(t)\\,\\frac{d{\\bf A}(t)}{d t}\\,{\\bf A}^{-1}(t)\n$$  \n\n![](images/66cd8968f3043f3764841c7c356633681bb0c6a6ba011e327b5e5eb524a7cdee.jpg)  \n\n# References  \n\nA–1 Anderson, B. D. O., and J. B. Moore, Linear Optimal Control . Upper Saddle River, NJ: Prentice Hall, 1971.   \nA–2 Athans, M., and P. L. Falb, Optimal Control: An Introduction to the Theory and Its Applications . New York: McGraw-Hill Book Company, 1965.   \nB–1 Barnet, S., “Matrices, Polynomials, and Linear Time-Invariant Systems,” IEEE Trans. Automatic Control ,AC-18 (1973), pp. 1–10.   \nB–2 Bayliss, L. E., Living Control Systems. London: English Universities Press Limited, 1966.   \nB–3 Bellman, R., Introduction to Matrix Analysis . New York: McGraw-Hill Book Company, 1960.   \nB–4 Bode, H.W., Network Analysis and Feedback Design . New York:Van Nostrand Reinhold, 1945.   \nB–5 Brogan,W. L., Modern Control Theory . Upper Saddle River, NJ: Prentice Hall, 1985.   \nB–6 Butman, S., and R. Sivan (Sussman),“On Cancellations, Controllability and Observability,” IEEE Trans.Automatic Control ,AC-9 (1964), pp. 317–8.   \nC–1 Campbell, D. P, Process Dynamics . New York: John Wiley & Sons, Inc., 1958.   \nC–2 Cannon, R., Dynamics of Physical Systems . New York: McGraw-Hill Book Company, 1967.   \nC–3 Chang, P. M., and S. Jayasuriya, “An Evaluation of Several Controller Synthesis Methodologies Using a Rotating Flexible Beam as a Test Bed,” ASME J. Dynamic Systems, Measurement, and Control ,117 (1995), pp. 360–73.   \nC–4 Cheng, D. K., Analysis of Linear Systems . Reading, MA: Addison-Wesley Publishing Company, Inc., 1959.   \nC–5 Churchill,R.V., Operational Mathematics ,3rd ed.New York:McGraw-Hill Book Company, 1972.   \nC–6 Coddington, E.A., and N. Levinson, Theory of Ordinary Differential Equations . New York: McGraw-Hill Book Company, 1955.   \nC–7 Craig, J. J., Introduction to Robotics, Mechanics and Control . Reading, MA:AddisonWesley Publishing Company, Inc., 1986.   \nC–8 Cunningham, W J., Introduction to Nonlinear Analysis. New York: McGraw-Hill Book Company, 1958.   \nD–1 Dorf, R. C., and R. H. Bishop, Modern Control Systems , 9th ed. Upper Saddle River, NJ: Prentice Hall, 2001.   \nE–1 Enns, M., J. R. Greenwood III, J. E. Matheson, and F. T. Thompson, “Practical Aspects of State-Space Methods Part I: System Formulation and Reduction,” IEEE Trans. Military Electronics ,MIL-8 (1964), pp. 81–93.   \nE–2 Evans, W. R., “Graphical Analysis of Control Systems,” AIEE Trans. Part II ,67 (1948), pp. 547-51.   \nE–3 Evans,W. R.,“Control System Synthesis by Root Locus Method,” AIEE Trans Part II ,69 (1950), pp. 66–9.   \nE–4 Evans,W.R.,“The Use of Zeros and Poles for Frequency Response or Transient Response,” ASME Trans .76 (1954), pp. 1135–44.   \nE–5 Evans,W. R., Control System Dynamics . New York: McGraw-Hill Book Company, 1954.   \nF–1 Franklin, G. F, J. D. Powell, and A. Emami-Naeini, Feedback Control of Dynamic Systems ,3rd ed. Reading, MA:Addison-Wesley Publishing Company, Inc., 1994.   \nF–2 Friedland, B., Control System Design . New York: McGraw-Hill Book Company, 1986.   \nF–3 Fu, K. S., R. C. Gonzalez, and C. S. G. Lee, Robotics: Control, Sensing, Vision, and Intelligence. New York: McGraw-Hill Book Company, 1987.   \nG–1 Gantmacher, F. R., Theory of Matrices , Vols. I and II. NewYork: Chelsea Publishing Company, Inc., 1959.   \nG–2 Gardner, M. F, and J. L. Barnes, Transients in Linear Systems. New York: John Wiley & Sons, Inc., 1942.   \nG–3 Gibson, J. E., Nonlinear Automatic Control . New York: McGraw-Hill Book Company, 1963.   \nG–4 Gilbert,E.G.,“Controllability and Observability in Multivariable Control Systems,” J.SIAM Control , ser.A, 1 (1963) , pp. 128–51.   \nG–5 Graham, D., and R. C. Lathrop,“The Synthesis of Optimum Response: Criteria and Standard Forms,” AIEE Trans. Part II ,72 (1953), pp. 273–88.   \nH–1 Hahn,W., Theory and Application of Liapunov’s Direct Method . Upper Saddle River, NJ: Prentice Hall, 1963.   \nH–2 Halmos, P. R., Finite Dimensional Vector Spaces. New York:Van Nostrand Reinhold, 1958.   \nH–3 Higdon, D. T., and R. H. Cannon, Jr., “On the Control of Unstable Multiple-Output Mechanical Systems,” ASME Paper no. 63 -WA-148 , 1963.   \nI–1 Irwin, J. D., Basic Engineering Circuit Analysis . New York: Macmillan, Inc., 1984.   \nJ–1 Jayasuriya, S., “Frequency Domain Design for Robust Performance Under Parametric, Unstructured, or Mixed Uncertainties,” ASME J. Dynamic Systems, Measurement, and Control ,115 (1993), pp. 439–51.   \nK–1 Kailath,T., Linear Systems . Upper Saddle River, NJ: Prentice Hall, 1980.   \nK–2 Kalman, R. E., “Contributions to the Theory of Optimal Control,” Bol. Soc Mat. Mex. ,5 (1960), pp. 102–19.   \nK–3 Kalman,R.E.,“On the General Theory of Control Systems,” Proc.First Intern.Cong.IFAC, Moscow ,1960, Automatic and Remote Control .London:Butterworths & Company Limited, 1961, pp. 481–92.   \nK–4 Kalman, R. E.,“Canonical Structure of Linear Dynamical Systems,” Proc. Natl. Acad. Sci., USA ,48 (1962), pp. 596–600.   \nK–5 Kalman, R. E.,“When Is a Linear Control System Optimal?” ASMEJ. Basic Engineering ,ser. D, 86 (1964), pp. 51–60.   \nK–6 Kalman, R. E., and J. E. Bertram, “Control System Analysis and Design via the Second Method of Lyapunov: I Continuous-Time Systems,” ASME J. Basic Engineering , ser. D, 82 (1960), pp. 371–93.   \nK–7 Kalman, R. E.,Y. C. Ho, and K. S. Narendra,“Controllability of Linear Dynamic Systems,” in Contributions to Differential Equations ,Vol. 1. New York:Wiley-Interscience Publishers, Inc., 1962.   \nK–8 Kautsky, J., and N. Nichols,“Robust Pole Assignment in Linear State Feedback,” Intern. J. Control ,41 (1985), pp 1129–55.   \nK–9 Kreindler, E., and P. E. Sarachick, “On the Concepts of Controllability and Observability of Linear Systems,” IEEE Trans.Automatic Control ,AC-9 (1964), pp. 129–36.   \nK–10 Kuo, B. C., Automatic Control Systems , 6th ed. Upper Saddle River, NJ: Prentice Hall, 1991.   \nL–1 LaSalle, J. P, and S. Lefschetz, Stability by Liapunov’s Direct Method with Applications . New York:Academic Press, Inc., 1961.   \nL–2 Levin,W. S., The Control Handbook . Boca Raton, FL: CRC Press, 1996.   \nL–3 Levin,W. S. Control System Fundamentals . Boca Raton, FL: CRC Press, 2000.   \nL–4 Luenberger, D. G.,“Observing the State of a Linear System,” IEEE Trans. Military Electr ., MIL-8 (1964), pp. 74–80.   \nL–5 Luenberger, D. G., “An Introduction to Observers,” IEEE Trans. Automatic Control ,AC-16 (1971), pp. 596–602.   \nL–6 Lur’e,A.I.,and E.N.Rozenvasser,“On Methods of Constructing Liapunov Functions in the Theory of Nonlinear Control Systems,” Proc. First Intern. Cong. IFAC , Moscow, 1960, Automatic and Remote Control . London: Butterworths & Company Limited, 1961, pp. 928–33.   \nM–1 MathWorks, Inc., The Student Edition of MATLAB , version 5. Upper Saddle River, NJ: Prentice Hall, 1997.   \nM–2 Melbourne, W. G., “Three Dimensional Optimum Thrust Trajectories for Power-Limited Propulsion Systems,” ARS J ., 31 (1961), pp. 1723–8.   \nM–3 Melbourne,W. G., and C. G. Sauer, Jr.,“Optimum Interplanetary Rendezvous with PowerLimited Vehicles,” AIAA J ., 1 (1963), pp. 54–60.   \nM–4 Minorsky, N., Nonlinear Oscillations . New York:Van Nostrand Reinhold, 1962.   \nM–5 Monopoli,R.V.,“Controller Design for Nonlinear and Time-Varying Plants,” NASA CR152 ,Jan., 1965.   \nN–1 Noble, B., and J. Daniel, Applied Linear Algebra , 2nd ed. Upper Saddle River, NJ: Prentice Hall, 1977.   \nN–2 Nyquist, H.,“Regeneration Theory,” Bell System Tech. J ., 11 (1932), pp. 126–47.   \nO–1 Ogata, K., State Space Analysis of Control Systems . Upper Saddle River, NJ: Prentice Hall, 1967.   \nO–2 Ogata, K., Solving Control Engineering Problems with MATLAB . Upper Saddle River, NJ: Prentice Hall, 1994.   \nO–3 Ogata, K., Designing Linear Control Systems with MATLAB . Upper Saddle River, NJ: Prentice Hall, 1994.   \nO–4 Ogata, K., Discrete-Time Control Systems , 2nd ed. Upper Saddle River, NJ: Prentice Hall, 1995.   \nO–5 Ogata, K., System Dynamics , 4th ed. Upper Saddle River, NJ: Prentice Hall, 2004.   \nO–6 Ogata, K., MATLAB for Control Engineers. Upper Saddle River, NJ: Pearson Prentice Hall, 2008.   \nP–1 Phillips, C. L., and R. D. Harbor, Feedback Control Systems . Upper Saddle River, NJ: Prentice Hall, 1988.   \nP–2 Pontryagin, L. S., V. G. Boltyanskii, R. V. Gamkrelidze, and E. F. Mishchenko, The Mathematical Theory of Optimal Processes . New York: John Wiley & Sons, Inc., 1962.   \nR–1 Rekasius, Z.V.,“A General Performance Index for Analytical Design of Control Systems,” IRE Trans.Automatic Control ,AC-6 (1961), pp. 217–22.   \nR–2 Rowell,G.,and D.Wormley, System Dynamics .Upper Saddle River,NJ:Prentice Hall,1997.   \nS–1 Schultz, W. C., and V. C. Rideout, “Control System Performance Measures: Past, Present, and Future,” IRE Trans.Automatic Control ,AC-6 (1961), pp. 22–35.   \nS–2Smith,R.J.,Electronics:Circuits and Devices,2d ed.New York:John Wiley & Sons,Inc.,1980.  \nS–3 Staats, P. F. “A Survey of Adaptive Control Topics,” Plan B paper , Dept. of Mech. Eng., University of Minnesota, March 1966.   \nS–4 Strang, G., Linear Algebra and Its Applications . New York:Academic Press, Inc., 1976.   \nT–1 Truxal, J. G., Automatic Feedback Systems Synthesis . New York: McGraw-Hill Book Company, 1955.   \nU–1 Umez-Eronini, E., System Dynamics and Control . Pacific Grove, CA: Brooks 'Cole Publishing Company, 1999.   \nV–1 Valkenburg, M. E., Network Analysis. Upper Saddle River, NJ: Prentice Hall, 1974.   \nV–2 Van Landingham, H. F., and W. A. Blackwell, “Controller Design for Nonlinear and Time-Varying Plants,” Educational Monograph , College of Engineering, Oklahoma State University, 1967.   \nW–1 Webster, J. G., Wiley Encyclopedia of Electrical and Electronics Engineering , Vol. 4. New York: John Wiley & Sons, Inc., 1999.   \nW–2 Wilcox, R. B., “Analysis and Synthesis of Dynamic Performance of Industrial Organizations—The Application of Feedback Control Techniques to Organizational Systems,” IRE Trans.Automatic Control ,AC-7 (1962), pp. 55–67.   \nW–3 Willems, J. C., and S. K. Mitter, “Controllability, Observability, Pole Allocation, and State Reconstruction,” IEEE Trans.Automatic Control ,AC-16 (1971), pp. 582–95.   \nW–4 Wojcik, C. K.,“Analytical Representation of the Root Locus,” ASME J. Basic Engineering ,ser. D, 86 (1964), pp. 37–43.   \nW–5 Wonham,W. M.,“On Pole Assignment in Multi-Input Controllable Linear Systems,” IEEE Trans.Automatic Control ,AC-12 (1967), pp. 660–65.   \nZ–1 Zhou, K., J. C. Doyle, and K. Glover, Robust and Optimal Control. Upper Saddle River, NJ: Prentice Hall, 1996.   \nZ–2 Zhou, K., and J. C. Doyle, Essentials of Robust Control, Upper Saddle River, NJ: Prentice Hall, 1998.   \nZ–3 Ziegler, J. G., and N. B. Nichols, “Optimum Settings for Automatic Controllers,” ASME Trans .64 (1942), pp. 759–68.   \nZ–4 Ziegler,J.G.,and N.B.Nichols,“Process Lags in Automatic Control Circuits,” ASME Trans .65 (1943), pp. 433–44.   \nA   \nAbsolute stability, 160   \nAckermann’s formula: for observer gain matrix, 756–57 for pole placement, 730–31   \nActuating error, 8   \nActuator, 21–22   \nAdjoint matrix, 876   \nAir heating system, 150   \nAircraft elevator control system, 156   \nAnalytic function, 860   \nAngle: of arrival, 286 of departure, 280, 286   \nAngle condition, 271   \nAsymptotes: Bode diagram, 406–07 root loci, 274–75, 284–85   \nAttenuation, 165   \nAttitude-rate control system, 386   \nAutomatic controller, 21   \nAutomobile suspension system, 86   \nAuxiliary polynomial, 216   \nB  \nBack emf, 95 constant, 95   \nBandwidth, 474, 539   \nBasic control actions: integral, 24 on-off, 22 proportional, 24 proportional-plus-derivative, 25 proportional-plus-integral, 24 proportional-plus-integral-plusderivative, 35 two-position, 22–23   \nBleed-type relay, 111   \nBlock, 17   \nBlock diagram, 17–18 reduction, 27–28, 48   \nBode diagram, 403 error in asymptotic expression of, 403 of first-order factors, 406–07, 409 general procedure for plotting, 413 plotting with MATLAB, 422–25 of quadratic factors, 410–12 of system defined in state space, 426–27   \nBranch point, 18   \nBreak frequency, 406   \nBreakaway point, 275–76, 285–86, 351   \nBreak-in point, 276, 281, 285–86, 351   \nBridged-T networks, 90, 520   \nBusiness system, 5   \nC  \nCanonical forms: controllable, 649 diagonal, 650 Jordan, 651, 653 observable, 650   \nCapacitance: of pressure system, 107–09 of thermal system, 137 of water tank, 103   \nCancellation of poles and zeros, 288   \nCascaded system, 20   \nCascaded transfer function, 20   \nCauchy–Riemann conditions, 860–61   \nCauchy’s theorem, 526   \nCayley–Hamilton theorem, 668, 701   \nCharacteristic equation, 652   \nCharacteristic polynomial, 34   \nCharacteristic roots, 652   \nCircular root locus, 282   \nClassical control theory, 2   \nClassification of control systems, 225   \nClosed-loop control system, 8   \nClosed-loop system, 20   \nClosed-loop frequency response, 477   \nClosed-loop frequency response curves: desirable shapes of, 492 undesirable shapes of, 492   \nClosed-loop transfer function, 19–20   \nCofactor, 876   \nCommand compensation, 630   \nCompensation: feedback, 308 parallel, 308 series, 308   \nCompensator: lag, 323, 503–04 lag–lead, 332–34, 511–13 lead, 312–13, 495–96   \nComplete observability, 683–84 conditions for, 684–85 in the splane, 684   \nComplete output controllablility, 714   \nComplete state controllability, 676–81 in the $s$ plane, 680–81   \nComplex-conjugate poles: cancellation of undesirable, 520   \nComplex function, 859   \nComplex impedence, 75   \nComplex variable, 859   \nComputational optimization approach to design PID controller, 583–89   \nConditional stability, 299–300, 510–11   \nConditionally stable system, 299–300, 458, 510–11   \nConduction heat transfer, 137  \n\n![](images/be03b8122d54b78736b1998c8a5bb24691f0798f0a8a3e2ce3a7c256a5d06620.jpg)  \n\nConformal mapping, 447, 462–64   \nConical water tank system, 152   \nConstant-gain loci, 302–03   \nConstant-magnitude loci (M circles),   \n478–79   \nConstant phase-angle loci (N circles),   \n480–81   \nConstant $\\omega_{n}$ loci, 296   \nConstant $\\zeta$ lines, 298   \nConstant $\\zeta$ loci, 296   \nControl actions, 21   \nControl signal, 3   \nControllability, 675–81   \nmatrix, 677   \noutput, 681   \nControllable canonical form, 649, 688   \nControlled variable, 3   \nController, 22   \nConvection heat transfer, 137   \nConventional control theory, 29   \nConvolution, integral, 16   \nCorner frequency, 406   \nCritically damped system, 167   \nCutoff frequency, 474   \nCutoff rate, 475   \nD  \nDamped natural frequency, 167   \nDamper, 64, 132   \nDamping ratio, 165   \nlines of constant, 296   \nDashpot, 64, 132–33   \nDead space, 43   \nDecade, 405   \nDecibel, 403   \nDelay time, 169–70   \nDerivative control action, 118–20, 222   \nDerivative gain, 84   \nDerivative time, 25, 61   \nDetectability, 688   \nDeterminant, 874   \nDiagonal canonical form, 694   \nDiagonalization of $n\\times n$ matrix, 652   \nDifferential amplifier, 78   \nDifferential gap, 23, 24   \nDifferentiating system, 231   \nDifferentiation:   \nof inverse matrix, 881   \nof matrix, 880   \nof product of two matrices, 880   \nDifferentiator:   \napproximate, 617   \nDirect transmission matrix, 31   \nDisturbance, 3, 26   \nDominant closed-loop poles, 182   \nDuality, 754  \n\n$E$   \n$e^{\\mathbf{A}t}$ :computation of, 670–71   \nEigenvalue, 652 invariance of, 655   \nElectromagnetic valve, 23   \nElectronic controller, 77, 83   \nEngineering organizational system, 5–6   \nEquivalent moment of inertia, 234   \nEquivalent spring constant, 64   \nEquivalent viscous-friction coefficient, 65, 234   \nEvans,W. R., 2, 11, 269   \nExponential response curve, 162   \n$F$   \nFeedback compensation, 308–09, 342, 519   \nFeedback control, 3   \nFeedback control system, 7   \nFeedback system, 20   \nFeedforward transfer function, 19   \nFinal value theorem, 866   \nFirst-order lag circuit, 80   \nFirst-order system, 161–64 unit-impulse response of, 163 unit-ramp response of, 162–63 unit-step response of, 161–62   \nFlapper, 110 valve, 156   \nFluid systems: mathematical modeling of, 100   \nFree-body diagram, 69–70   \nFrequency response, 398 correlation between step response and, 471–74 lag compensation based on, 502–11 lag–lead compensation based on, 511–17 lead compensation based on, 493–502   \nFull-order state observer, 752–53   \nFunctional block, 17   \nG  \nGain crossover frequency, 467–69   \nGain margin, 464–67   \nGas constant, 108 for air, 142 universal, 108   \nGear train, 232 system, 232–34   \nGeneralized plant, 813, 815–17 diagram, 810–16, 853–54   \nH  \nH infinity control problem, 816   \nH infinity norm, 6, 808   \nHazen, 2, 11   \nHigh-pass filter, 495   \nHigher-order systems, 179 transient response of, 180–81   \nHurwitz determinants, 252–58   \nHurwitz stability criterion, 252–53, 255–58 equivalence of Routh’s stability criterion and, 255–57   \nHydraulic controller: integral, 130 jet-pipe, 147 proportional, 131 proportional-plus-derivative, 134–35 proportional-plus-integral, 133–34 proportional-plus-integral-plusderivative, 135–36   \nHydraulic servo system, 124–25   \nHydraulic servomotor, 128, 130, 156   \nHydraulic system, 106, 123–39, 149 advantages and disadvantages of, 124 compared with pneumatic system, 106  \n\n# I  \n\nIdeal gas law, 108   \nImpedance: approach to obtain transfer function, 75–76   \nImpulse function, 866   \nImpulse response, 163, 178–79, 195–97 function, 16–17   \nIndustrial controllers, 22   \nInitial condition: response to, 203–11   \nInitial value theorem, 866   \nInput filter, 261, 630   \nInput matrix, 31   \nIntegral control, 220   \nIntegral control action, 24–25, 218   \nIntegral controller, 22   \nIntegral gain, 61   \nIntegral time, 25, 61   \nIntegration of matrix, 880   \nInverse Laplace transform: partial-fraction expansion method for obtaining, 867–73   \nInverse Laplace transformation, 862   \nInverse of a matrix: MATLAB approach to obtain, 879   \nInverse polar plot, 461–62, 537–38   \nInverted-pendulum system, 68–72, 98   \nInverted-pendulum control system, 746–51   \nInverting amplifier, 78   \nI-PD control, 591–92   \nI-PD-controlled system, 592, 628–29, 643 with feedforward control, 642   \nJ  \nJet-pipe controller, 146–47   \nJordan blocks, 679   \nJordan canonical form, 651, 695, 706–07   \nK  \nKalman, R. E., 12, 675   \nKirchhoff’s current law, 72   \nKirchhoff’s loop law, 72   \nKirchhoff’s node law, 72   \nKirchhoff’s voltage law, 72   \nL  \nLag compensation, 321   \nLag compensator, 311, 321, 502 Bode diagram of, 503 design by frequency-response method, 502–11 design by root-locus method, 321, 323 polar plot of, 503   \nLag network, 82, 542   \nLag–lead compensation, 330, 335, 338, 377, 511–18   \nLag–lead compensator: Bode diagram of, 558 design by frequency-response method, 513–17 design by root-locus method, 331–32, 380–82 electronic, 330–32 polar plot of, 512   \nLag–lead network: electronic, 330–32 mechanical, 366   \nLagrange polynomial, 708   \nLagrange’s interpolation formula, 708   \nLaminar-flow resistance, 102   \nLaplace transform, 862 properties of, 865 table of, 863–64   \nLead compensator, 311, 493 Bode diagram of, 494 design by frequency-response method, 493–502 design by root-locus method, 311–18 polar plot of, 494   \nLead, lag, and lag–lead compensators: comparison of, 517–18   \nLead network, 542 electronic, 82 mechanical, 365   \nLead time, 5   \nLinear approximation: of nonlinear mathematical models, 43   \nLinear system, 14 constant coefficient, 14   \nLinear time-invariant system, 14, 164   \nLinear time-varying system, 14   \nLinearization: of nonlinear systems, 43   \nLiquid-level control system, 157   \nLiquid-level systems, 101, 103–04, 140–41   \nLog-magnitude curves of quadratic transfer function, 411   \nLogarithmic decrement, 237   \nLogarithmic plot, 403   \nLog-magnitude versus phase plot, 403, 443–44   \nLRC circuit, 72–73   \nM  \nM circles, 478–79 a family of constant, 479   \nMagnitude condition, 271   \nManipulated variable, 3   \nMapping theorem, 448–49   \nMathematical model, 13   \nMATLAB commands: MATLAB: obtaining maximum overshoot with, 194 obtaining peak time with, 194 obtaining response to initial condition with, 266 partial-fraction expansion with, 871–73 plotting Bode diagram with, 422–23 plotting root loci with, 290–91 writing text in diagrams with, 188–89 [A,B,C,D] $=$ tf2ss(num,den), 40, 656, 698 bode(A,B,C,D), 422, 426 bode(A,B,C,D,iu), 426–27 bode(A,B,C,D,iu,w), 422 bode(A,B,C,D,w), 422 bode(num,den), 422 bode(num,den,w), 422, 425, 551 bode(sys), 422 bode(sys,w), 552 c = step(num,den,t), 190 for loop, 243, 249, 584 [Gm,pm,wcp,wcg,] $=$ margin(sys), 468–69 gtext ('text'), 189 impulse(A,B,C,D), 195 impulse(num, den), 195 initial(A,B,C,D,[initial condition],t), 209 inv(A), 879 $\\mathsf{K}=$ acker(A,B,J), 736 $\\mathsf{K}=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R})$ , 798 $\\mathsf{K}=$ place(A,B,J), 736  \n\n$\\mathsf{K}_{\\mathrm{e}}=\\mathsf{a c k e r}(\\mathsf{A}^{\\prime},\\mathsf{C}^{\\prime},\\mathsf{L})^{\\prime}.$ , 773   \n$\\mathsf{K_{e}}=\\mathsf{a c k e r}(\\mathsf{A b b,A a b,L})^{\\ast}$ , 773   \n$\\mathsf{K}_{\\mathrm{e}}=\\mathsf{p l a c e}(\\mathsf{A}^{\\prime},\\mathsf{C}^{\\prime},\\mathsf{L})^{\\prime}$ , 773   \n$\\mathsf{K}_{\\mathrm{e}}=\\mathsf{p l a c e}(\\mathsf{A b b^{\\prime},A a b^{\\prime},L})^{\\mathrm{t}}$ , 773   \n$[\\mathsf{K},\\mathsf{P},\\mathsf{E}]=\\mathsf{l q r}(\\mathsf{A},\\mathsf{B},\\mathsf{Q},\\mathsf{R}),$ , 798   \n$\\left[\\mathsf{K},\\mathsf{r}\\right]=$ rlocfind(num,den), 303   \nlogspace(d1,d2), 422   \nlogspace(d1,d2,n), 422–23   \nlqr(A,B,Q,R), 797   \nlsim(A,B,C,D,u,t), 201   \nlsim(num,den,r,t), 201   \nmagd $\\mathtt{3}=20^{*}|\\mathrm{og}10(\\mathrm{mag}),422$   \n[mag,phase,w] $=$ bode(A,B,C,D), 422   \n[mag,phase,w] $=$ bode(A,B,C,D,iu,w),   \n422   \n[mag,phase,w] $=$ bode(A,B,C,D,w),   \n422   \n[mag,phase,w] $=$ bode(num,den), 422   \n[mag,phase,w] $=$ bode(num,den,w),   \n422, 476   \n[mag,phase,w] $=$ bode(sys), 422   \n[mag,phase,w] $=$ bode(sys,w), 476   \nmesh, 192   \nmesh(y), 192, 249   \nmesh(y'), 192, 249   \n$[\\mathsf{M p},\\mathsf{k}]=\\mathsf{m a x}(\\mathsf{m a g}),\\,476$   \nNaN, 799   \n[num,den] $=$ feedback(num1,den1,   \nnum2,den2), 20–21   \n[num,den] $=$ parallel(num1,den1,   \nnum2,den2), 20–21   \n[num,den] $=$ series(num1,den1,   \nnum2,den2), 20–21   \n[num,den] $=$ ss2tf(A,B,C,D), 41, 657   \n[num,den] $=$ ss2tf(A,B,C,D,iu), 41–42,   \n58, 657   \n[NUM,den] $=$ ss2tf(A,B,C,D,iu), 59,   \n659   \nnyquist(A,B,C,D), 436, 441–42   \nnyquist(A,B,C,D,iu), 441   \nnyquist(A,B,C,D,iu,w), 436, 441   \nnyquist(A,B,C,D,w), 436   \nnyquist(num,den), 436   \nnyquist(num, den,w), 436   \nnyquist(sys), 436   \npolar(theta,r), 545   \nprintsys(num,den), 20–21, 189   \nprintsys(num,den, $\"S\"$ ), 189   \n$\\mathbf{r}=\\mathsf{a}\\mathsf{b}\\mathsf{s}(z)$ , 544   \n$\\left[\\mathsf{r},\\mathsf{p},\\mathsf{k}\\right]=$ residue(num,den), 239, 871–72   \n[re,im,w] $=$ nyquist(A,B,C,D), 436   \n[re,im,w] $=$ nyquist(A,B,C,D,iu,w), 436   \n[re,im,w] $=$ nyquist(A,B,C,D,w), 436   \n[re,im,w] $=$ nyquist(num,den), 436   \n[re,im,w] $=$ nyquist(num,den,w), 436   \n[re,im,w] $=$ nyquist(sys), 436   \nresidue, 867   \nresonant_frequency $\\mathbf{\\theta}=\\mathbf{w}(\\mathbf{k})$ , 476   \nresonant_peak $=20^{*}|_{\\mathrm{Og}}10(\\mathsf{M}\\mathsf{p})|$ , 476   \nrlocfind, 303   \nrlocus(A,B,C,D), 295   \nrlocus(A,B,C,D,K), 290, 295   \nrlocus(num,den), 290–91   \nrlocus(num,den,K), 290   \nsgrid, 297   \nsortsolution, 584   \nstep(A,B,C,D), 184, 186   \nstep(A,B,C,D,iu), 184   \nstep(num,den), 184   \nstep(num,den,t), 184   \nstep(sys), 184   \n$\\mathsf{S Y S}=\\mathsf{s s}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D}),$ , 184   \nsys $=$ tf(num,den), 184   \ntext, 188   \ntheta $=$ angle(z), 544   \nw$=$ logspace(d2,d3,100), 425   \n $\\mathsf{y}=|\\mathsf{s i m}(\\mathsf{A},\\mathsf{B},\\mathsf{C},\\mathsf{D},\\mathsf{u},\\mathsf{t}),$ , 201  \n$\\upgamma=$ lsim(num,den,r,t), 201   \n[y, x, t] $=$ impulse(A,B,C,D), 195   \n[y, x, t] $=$ impulse(A,B,C,D,iu), 195   \n[y, x, t] $=$ impulse(A,B,C,D,iu,t), 195   \n[y, x, t] $=$ impulse(num,den), 195   \n[y, $\\mathrm{{x,}}$ t] $=$ impulse(num,den,t), 195   \n[y, $\\mathrm{{x,}}$ t] $=$ step(A,B,C,D,iu), 184   \n[y, $\\mathrm{{x,}}$ t] $=s\\mathrm{t}\\epsilon$ p(A,B,C,D,iu,t), 184   \n[y, x, t] $=$ step(num,den,t), 184, 190   \n$z={\\sf r e}+{\\sf j}^{*}{\\sf i m},$ , 544  \n\n# End of MATLAB commands  \n\nMatrix exponential, 661, 669–674 closed solution for, 663   \nMatrix Riccati equation, 798, 800   \nMaximum overshoot: in unit-impulse response, 179 in unit-step response, 170, 172 versus $\\zeta$ curve, 174   \nMaximum percent overshoot, 170   \nMaximum phase lead angle, 494, 498   \nMeasuring element, 21   \nMechanical lag–lead system, 366   \nMechanical lead system, 365   \nMechanical vibratory system, 236   \nMercury thermometer system, 151   \nMinimal polynomial, 669, 704–06   \nMinimum-order observer, 767–77 based controller, 777   \nMinimum-order state observer, 752   \nMinimum-phase system, 415–16   \nMinimum-phase transfer function, 415   \nMinor, 876   \nModern control theory, 7, 29 versus conventional control theory, 29   \nMotor torque constant, 95   \nMotorcycle suspension system, 87   \nMultiple-loop system, 458–59   \n$N$   \nN circles, 480–81 a family of constant, 481   \nNewton’s second law, 66   \nNichols, 2, 11, 398   \nNichols chart, 482–85   \nNichols plots, 403   \nNonbleed-type relay, 111   \nNonhomogeneous state equation: solution of, 666–67   \nNoninverting amplifier, 79   \nNonlinear mathematical models: linear approximation of, 43–45   \nNonlinear system, 43   \nNonminimum-phase systems, 300–01, 415, 417   \nNonminimum-phase transfer function, 415, 488   \nNonuniqueness: of a set of state variables, 655   \nNozzle-flapper amplifier, 110   \nNumber-decibel conversion line, 404   \nNyquist, H., 2, 11, 398   \nNyquist path, 545   \nNyquist plot, 403, 439–40, 443 of positive-feedback system, 535–37 of system defined in state space, 440–43   \nNyquist stability analysis, 454–62   \nNyquist stability criterion, 445–54 applied to inverse polar plots, 461–62   \nO  \nObservability, 675, 682–88 complete, 683–85 matrix, 653   \nObservable canonical form, 650, 692   \nObservation, 752   \nObserved-state feedback control system, 761   \nObserver, 753 design of control system with, 786–93 full-order, 753 mathematical model of, 752 minimum-order, 767–73   \nObserver-based controller: transfer function of, 761   \nObserver controller: in the feedback path of control system, 787, 790–93 in the feedforward path of control system, 787–90   \nObserver-controller matrix, 762   \nObserver-controller transfer function, 761–62   \nObserver error equation, 753   \nObserver gain matrix, 755 MATLAB determination of, 773   \nOctave, 405   \nOffset, 258   \nOn-off control action, 22–23   \nOn-off controller, 22   \nOne-degree-of-freedom control system, 593   \nop amps, 78   \nOpen-loop control system, 8 advantages of, 9 disadvantages of, 9   \nOpen-loop frequency response curves: reshaping of, 493   \nOpen-loop transfer function, 19   \nOperational amplifier, 78   \nOperational amplifier circuits, 93–94 for lead or lag compensator: table of, 85   \nOptimal regulator problem, 806   \nOrdinary point, 861   \nOrthogonality: of root loci and constant gain loci, 301–02   \nOutput controllability, 681   \nOutput equation, 31   \nOutput matrix, 31   \nOverdamped system, 168–69   \nOverlapped spool valve, 146   \nOverlapped valve, 130   \n$P$   \nParallel compensation, 308–09, 342–43   \nPartial-fraction expansion, 867–73 with MATLAB, 871–73   \nPD control, 373   \nPD controller, 614–15   \nPeak time, 170, 172, 193   \nPerformance index, 793   \nPerformance specifications, 9   \nPhase crossover frequency, 467–69   \nPhase margin, 464–67 versus $\\zeta$ curve, 472   \nPI controller, 2, 614–15   \nPI-D control, 590–92   \nPID control system, 572–77, 583, 587, 617–21, 628–29, 642–43 basic, 590 with input filter, 629 two-degrees-of-freedom, 592–95   \nPID controller, 567, 577, 614–16, 620, 632 modified, 616 using operational amplifiers, 83–84   \nPilot valve, 124, 130   \nPI-PD control, 592   \nPID-PD control, 592   \nPlant, 3   \nPneumatic actuating valve, 117–18   \nPneumatic controllers, 144–45, 154–55   \nPneumatic nozzle-flapper amplifier, 110   \nPneumatic on-off controller, 115   \nPneumatic pressure system, 142   \nPneumatic proportional controller, 112–16 force-balance type, 115–16 force-distance type, 112–15   \nPneumatic proportional-plus-derivative controller, 119–20   \nPneumatic proportional-plus-integral control action, 120–22   \nPneumatic proportional-plus-integralplus-derivative control action, 122–23   \nPneumatic relay, 111 bleed type, 111 nonbleed type, 111 reverse acting, 112   \nPneumatic systems, 106–23, 153 compared with hydraulic system, 106   \nPneumatic two-position controller, 115   \nPolar grids, 297   \nPolar plot, 403, 427–28, 430, 432   \nPole: 861 of order n, 861 simple, 861   \nPole assignment technique, 723   \nPole-placement: necessary and sufficient conditions for arbitrary, 725   \nPole placement problem, 723–35 solving with MATLAB, 735–36   \nPositive-feedback system: Nyquist plot for, 536–37 root loci for, 303–07   \nPositional servo system, 95–97   \nPressure system, 107, 109   \nPrinciple of duality, 687   \nPrinciple of superposition, 43   \nProcess, 3   \nProportional control, 219   \nProportional control action, 24   \nProportional controller, 22   \nProportional gain, 25, 61   \nProportional-plus-derivative control: of second-order system, 224 of system with inertia load, 223   \nProportional-plus-derivative control action, 25   \nProportional-plus-derivative controller, 22, 542   \nProportional-plus-integral control action, 24   \nProportional-plus-integral controller, 22, 121, 542   \nProportional-plus-integral-plusderivative control action, 25   \nProportional-plus-integral-plusderivative controller, 22   \nPulse function, 866   \nQ  \nQuadratic factor, 410 log-magnitude curves of, 411 phase-angle curves of, 411   \nQuadratic optimal control problem: MATLAB solution of, 804   \nQuadratic optimal regulator system, 793–95 MATLAB design of, 797   \nR  \nRamp response, 197   \nRank of matrix, 875   \nReduced-matrix Riccati equation, 795–97   \nReduced-order observer, 752   \nReduced-order state observer, 752   \nReference input, 21   \nRegulator system with observer controller, 778–86, 789   \nRelative stability, 160, 217, 462   \nResidue, 867   \nResidue theorem, 527   \nResistance: gas-flow, 107 laminar-flow, 101–02 of pressure system, 107, 109 of thermal system, 137 turbulent-flow, 102   \nResonant frequency, 430, 470   \nResonant peak, 413, 430, 470 versus $\\zeta$ curve, 413   \nResonant peak magnitude, 413, 470   \nResponse: to arbitrary input, 201 to initial condition, 203–11 to torque disturbance, 221   \nReverse-acting relay, 112   \nRiccati equation, 795   \nRise time, 169–171 obtaining with MATLAB, 193–94   \nRobust control: system, 16, 806–17 theory, 2, 7   \nRobust performance, 7, 807, 812   \nRobust pole placement, 735   \nRobust stability, 7, 807, 809   \nRoot loci: general rules for constructing, 283–87 for positive-feedback system, 303–07   \nRoot locus, 271 method, 269–70   \nRouth’s stability criterion, 212–18   \nS  \nSchwarz matrix, 268   \nSecond-order system, 164 impulse response of, 178–79 standard form of, 166 step response of, 165–75 transient-response specification of, 171 unit-step response curves of, 169   \nSensor, 21   \nSeries compensation, 308–09, 342   \nServo system, 95, 164–65 design of, 739–51 with tachometer feedback, 268 with velocity feedback, 175–77   \nServomechanism, 2   \nSet point, 21   \nSet-point kick, 590   \nSettling time, 170, 172–73 obtaining with MATLAB, 194 versus $\\zeta$ curve, 174   \nSign inverter, 79   \nSimple pole, 861   \nSingular points, 861   \nSinusoidal signal generator, 486   \nSinusoidal transfer function, 401   \nSmall gain theorem, 809   \nSpace vehicle control system, 367, 538–39   \nSpeed control system, 4, 148   \nSpool valve: linealized mathematical model of, 127   \nSpring-loaded pendulum system, 98   \nSpring-mass-dashpot system, 66   \nSquare-law nonlinearity, 43   \nS-shaped curve, 569   \nStability analysis, 454–62 in the complex plane, 182   \nStabilizability, 688   \nStack controller, 115   \nStandard second-order system, 189   \nState, 29   \nState controllability: complete, 676, 678, 680   \nState equation, 31 solution of homogeneous, 660 solution of nonhomogeneous, 666–67 Laplace transform solution of, 663   \nState-feedback gain matrix, 724 MATLAB approach to determine, 735–36   \nState matrix, 31   \nState observation: necessary and sufficient conditions for, 754–55   \nState observer, 751–77 design with MATLAB, 773 type 1 servo system with, 746   \nState observer gain matrix: 755 Ackermann’s formula to obtain, 756–57 direct substitution approach to obtain, 756 transformation approach to obtain, 755   \nState space, 30   \nState-space equation, 30 correlation between transfer function and, 649, 656 solution of, 660   \nState-space representation: in canonical forms, 649 of nth order system, 36–39   \nState-transition matrix, 664 properties of, 665   \nState variable, 29   \nState vector, 30   \nStatic acceleration error constant, 228, 421 determination of, 421–22   \nStatic position error constant, 226, 419   \nStatic velocity error constant, 227, 420   \nSteady-state error, 160, 226 for unit parabolic input, 229 for unit ramp input, 228 in terms of gain K, 230   \nSteady-state response, 160   \nStep response, 699–700 of second-order system, 165–69   \nSumming point, 18   \nSuspension system: automobile, 86–87 motorcycle, 87   \nSylvester’s interpolation formula, 673, 709–713   \nSystem, 3   \nSytem types, 419 type 0, 225, 230, 419, 433, 487–88 type 1, 225, 230, 420, 433, 487–88 type 2, 225, 230, 421, 433, 487–88   \nSystem response to initial condition: MATLAB approach to obtain, 203–11   \nT  \nTachometer, 176 feedback, 343   \nTaylor series expansion, 43–45   \nTemperature control systems, 4–5   \nTest signals, 159   \nText: writing on the graphic screen, 188   \nThermal capacitance, 137   \nThermal resistance, 137   \nThermal systems, 100,136–39   \nThermometer system, 151–52   \nThree-degrees-of-freedom system, 645   \nThree-dimensional plot, 192 of unit-step response curves with MATLAB, 191–93   \nTraffic control system, 8   \nTransfer function, 15 of cascaded elements, 73–74 of cascaded systems, 20 closed-loop, 20 of closed-loop system, 20 experimental determination of, 489–90 expression in terms of A ,B,C, and $D$ , 34 of feedback system, 19 feedforward, 19 of minimum-order observer-based controller, 777 of nonloading cascaded elements, 77 observer-controller, 762, 780–82 open-loop, 19 of parallel systems, 20 sinusoidal, 401   \nTransfer matrix, 35   \nTransformation: from state space to transfer function, 41–42, 657 from transfer function to state space, 40–41, 656   \nTransient response, 160 analysis with MATLAB, 183–211 of higher-order system, 180 specifications, 169, 171   \nTransport lag, 417 phase angle characteristics of, 417   \nTurbulent-flow resistance, 102   \nTwo-degrees-of-freedom control system, 593–95, 599–614, 636–41, 646–47  \nTwo-position control action, 22–23   \nTwo-position controller, 22   \nType 0 system, 225, 230, 488 log-magnitude curve for, 419, 488 polar plot of, 433   \nType 1 servo system: design of, 743–51 pole-placement design of, 739–46   \nType 1 system, 420 log-magnitude curve for, 420, 488 polar plot of, 433   \nType 2 system, 421 log-magnitude curve for, 421, 488 polar plot of, 433   \nU  \nUncontrollable system, 681   \nUndamped natural frequency, 165   \nUnderdamped system, 166–67   \nUnderlapped spool valve, 146   \nUnit acceleration input, 247   \nUnit-impulse response: of first-order system, 163 of second-order system, 178   \nUnit-impulse response curves: a family of, 178 obtained by use of MATLAB, 196–97   \nUnit-ramp response: of first-order system, 162–63 of second-order system, 197–200 of system defined in state space, 199–200   \nUnit-step response: of first-order system, 161 of second-order system, 163, 167, 169   \nUniversal gas constant, 108   \nUnstructured uncertainty: additive, 852–53 multiplicative, 809 system with, 809   \nV  \nValve: overlapped, 130 underlapped, 130 zero-lapped, 130   \nValve coefficient, 127   \nVectors: linear dependence of, 674 linear independence of, 674   \nVelocity error, 227   \nVelocity feedback, 176, 343, 519   \nW  \nWatt’s speed governor, 4   \nWeighting function, 17   \nZ  \nZero, 861 of order m, 862   \nZero-lapped valve, 130   \nZero placement, 595, 597, 612 approach to improve response characteristics, 595–97   \nZiegler–Nichols tuning rules, 11, 568–77 first method, 569–70 second method, 570–71"
  }
}